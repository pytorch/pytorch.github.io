---
layout: ecosystem_detail
title: vllm
summary: vllm is a high-throughput and memory-efficient inference and serving engine for LLMs.
link: https://github.com/vllm-project/vllm
summary-home: vllm is a high-throughput and memory-efficient inference and serving engine for LLMs.
featured-home: false
github-id: vllm-project/vllm
date-added: 12/3/24
---

<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-03-11T02:13:59-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Scaling Recommendation Systems Training to Thousands of GPUs with 2D Sparse Parallelism</title>
      <link href="https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism/" rel="alternate" type="text/html" title="Scaling Recommendation Systems Training to Thousands of GPUs with 2D Sparse Parallelism" />
      <published>2025-03-11T00:00:00-07:00</published>
      <updated>2025-03-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism/">&lt;p&gt;At Meta, recommendation systems are the cornerstone of delivering relevant and personalized ads to billions of users globally. Through technologies like PyTorch’s TorchRec, we’ve successfully developed solutions that enable model training across hundreds of GPUs. While these systems have served us well, recent research on scaling laws has revealed a compelling opportunity: we can achieve significantly better model performance by training dramatically larger neural networks.&lt;/p&gt;

&lt;p&gt;However, this insight presents us with a new challenge. Our current training infrastructure, though highly optimized for hundreds of GPUs, cannot efficiently scale to the thousands of GPUs needed to train these larger models. The leap from hundreds to thousands of GPUs introduces complex technical challenges, particularly around handling sparse operations in recommendation models. These challenges require fundamentally new approaches to distributed training, which we address with a novel parallelization strategy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To address these issues, we introduced 2D embedding parallel, a novel parallelism strategy that overcomes the sparse scaling challenges inherent in training large recommendation models across thousands of GPUs. This is available today in TorchRec through the DMPCollection API.&lt;/strong&gt; This approach combines two complementary parallelization techniques: data parallelism for the sparse components of the model, and model parallelism for the embedding tables, leveraging TorchRec’s robust sharding capabilities. By strategically integrating these techniques, we’ve created a solution that scales to thousands of GPUs and now powers Meta’s largest recommendation model training runs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the sparse scaling challenges?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We identified three key challenges that prevented us from naively scaling our model to thousands of GPUs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Imbalancing and straggler issue:&lt;/strong&gt; with more GPUs it’s harder to achieve balanced sharding, some ranks can have much heavier workload for embedding computations, which can slow down the entire training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication across nodes:&lt;/strong&gt; As training jobs utilize an increased number of GPUs, the all-to-all communication bandwidth can drop under certain network topologies which can increase communication latency significantly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory overhead:&lt;/strong&gt; The memory used by input features is often negligible, however, as we use thousands of GPUs, we can introduce larger input features and the memory requirements can become significant.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With 2D embedding parallel, we can describe our new parallelism scheme like this, in this example we have 2 model replicas (Replica 1: GPU1/GPU3, Replica 2: GPU2/GPU4)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/scaling-recommendation-2d-sparse-parallelism/fg1.png&quot; alt=&quot;Flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1: Layout illustration of 2D Sparse Parallelism&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With 2D sparse parallelism we address these challenges, instead of sharding tables across all ranks, we first evenly divide all ranks into several parallel groups:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Within each group, we use model parallel for the embedding tables, such as column-wise/row-wise sharding. At scale, for our largest tables, we have also developed a grid sharding, which shards embedding tables on the row and column dimension.&lt;/li&gt;
  &lt;li&gt;Across groups, we do data parallel, such that each rank in a group has its corresponding replica rank in the other groups (replica rank means storing the same embedding table shards).
    &lt;ol&gt;
      &lt;li&gt;After each group has completed its own backward pass, we all reduce the embedding table weights across the replicas to keep them synchronized.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;our-production-solution&quot;&gt;Our production solution&lt;/h2&gt;

&lt;p&gt;TorchRec is our library to build the sparse part of the recommendation models in native PyTorch. With the traditional API being DistributedModelParallel which applies model parallel to the embedding tables. We introduce a new API alongside it, known as DMPCollection, which serves as the main entry point for enabling 2D parallel on TorchRec models. We designed it to be as easy of a change as applying FSDP/DDP is.&lt;/p&gt;

&lt;p&gt;To understand what DMPCollection does, we have to understand what DistributedModelParallel (DMP) does first:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create embedding tables, known as EmbeddingBagCollection and EmbeddingCollections.&lt;/li&gt;
  &lt;li&gt;Generate a sharding plan with respect to GPU topology, embedding tables, memory available, input data, and more.&lt;/li&gt;
  &lt;li&gt;Wrap model with DMP and the associated sharding plan passed in.&lt;/li&gt;
  &lt;li&gt;DMP initializes and shards the embedding tables in accordance with the sharding plan.&lt;/li&gt;
  &lt;li&gt;On a train step, DMP takes an input batch, communicates it to the appropriate GPUs containing the embedding table shard of interest, looks up the value, and returns it back to the GPU that requested it. This is all done on the global process group, with some exceptions for special sharding (such as table row wise sharding)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DistributedModelParallel was built for model parallel with many parts working under the assumption of sharding and working around the global world size. We need to change these parts in a way where we can introduce additional dimensions of parallelism without losing the optimizations and feature set of TorchRec.&lt;/p&gt;

&lt;p&gt;DMPCollection changes a few key parts to enable 2D parallel in an extensible way,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate sharding plans for the smaller sharding group once, once passed in we communicate to the appropriate ranks across the global group and remap the ranks to fit the new sharding group ranks.&lt;/li&gt;
  &lt;li&gt;Create two new NCCL process groups, known as sharding and replica process groups. The sharding process group is passed into sharding and train step components of TorchRec. The replica process group is used for the weight and optimizer state synchronization, the all reduce call happens over this process group.
    &lt;ul&gt;
      &lt;li&gt;The sub NCCL process groups allow us to efficiently communicate only between the ranks that are relevant for a particular comm. Each rank will have two associated process groups.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To the user, the change is very simple, while taking away all the complexity around applying the parallelism strategies to the model.&lt;/p&gt;

&lt;h2 id=&quot;how-do-we-create-these-sharding-and-replication-groups&quot;&gt;How do we create these sharding and replication groups?&lt;/h2&gt;

&lt;p&gt;These process groups are one of the keys to DMPCollection’s performant implementation. From our earlier diagram, we showed a simple 2x2 GPU setup, however, at scale, how do we assign which ranks are part of a given sharding group and what are their replica ranks across the sharding groups?&lt;/p&gt;

&lt;p&gt;Consider the following setup with 2 nodes, each with 4 GPUs. The sharding and replication groups under 2D parallel will be,&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Sharding Group
   &lt;/td&gt;
   &lt;td&gt;Sharding Ranks
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;0
   &lt;/td&gt;
   &lt;td&gt;0, 2, 4, 6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1, 3, 5, 7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


   &lt;/td&gt;
   &lt;td&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Replication Group
   &lt;/td&gt;
   &lt;td&gt;Replication Ranks
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;0
   &lt;/td&gt;
   &lt;td&gt;0, 1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2, 3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;4, 5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;6, 7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We use the following formulation,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Divide all trainers into G sharding groups, each with L trainers
    &lt;ol&gt;
      &lt;li&gt;Groups, G, is determined by G = T / L, where T is total number of trainers&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;For each group, G, we assigned non-contiguous trainer ranks based on the group it’s in, following,
    &lt;ol&gt;
      &lt;li&gt;[i, G+i, 2G+i, …, (L - 1) G+i], where* i = 0 to G-1*&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;From the groups, G, we can create the replication group, which is every G continuous ranks
    &lt;ol&gt;
      &lt;li&gt;(0 to G-1, G to 2* G - 1) each continuous set stores the duplicate embedding table shards.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This means our sharding groups, G, are of size L, which can be known as the number of ranks to apply model parallel across. This, in turn, gives us replica groups, each of size G, which are the ranks we data parallel across.&lt;/p&gt;

&lt;p&gt;In DMPCollection, we’re able to create these process groups efficiently with the use of DeviceMesh, we create the entire GPU topology in a 2x2 matrix, with each row representing the group of sharding ranks and each column representing the corresponding replica ranks,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create peer matrix
num_groups = global_world_size // sharding_group_size
for each group_rank in num_groups:
	peers = [num_groups * rank + group_rank for rank in range(sharding_group_size)]
	add peer to peer matrix

initalize DeviceMesh with two dimensions (shard, replicate)
slice DeviceMesh on shard for sharding process group
slide DeviceMesh on replicate for replica process group
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our DeviceMesh approach, should we want to change the topology or provide further flexibility in the future, we can easily extend our creation logic to any form of topologies and even extend for further dimensions of parallelism if needed.&lt;/p&gt;

&lt;h2 id=&quot;performance-of-2d-parallel&quot;&gt;Performance of 2D parallel&lt;/h2&gt;

&lt;p&gt;Our rank partitioning strategy optimizes communication patterns by strategically placing model replica ranks for each shard within the same compute node. This architecture provides significant performance benefits for the weight synchronization operation. After the backward pass, we perform all-reduce operations to synchronize model weights—which is an expensive process given the large parameter counts we have to communicate and sync—with our setup of placing replicas on the same node we leverage intra node’s high-bandwidth over-relying on slower inter-node bandwidth.&lt;/p&gt;

&lt;p&gt;The effect of this design choice on the other communication collectives generally improves the latencies. The improvement stems from two factors.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;By sharding the embedding tables over a reduced number of ranks and conducting communications for the model within the smaller group, we achieve a lower all-to-all latency.&lt;/li&gt;
  &lt;li&gt;With the replication in 2D parallel, our embedding lookup latency on a rank reduces, we can reduce the local batch size to 1/Nth of the equivalent global batch size, where N is the number of model replicas.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A production model trace exemplifies these two factors, here we run the 2D parallel job on 1024 GPUs, with a sharding group size of 256 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/scaling-recommendation-2d-sparse-parallelism/fg2.png&quot; alt=&quot;State diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 2: Comparing latencies between non 2D parallel and 2D parallel workloads&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two key levers users have to tune to maximize performance for their workloads:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The size of the model sharding group relative to the global world size. The global world size divided by the sharding group size represents the number of model replicas we will have.
    &lt;ol&gt;
      &lt;li&gt;To maximize performance, users can look to scale up their model up to 8x, this scaling factor maintains the intra-host all reduce.
        &lt;ol&gt;
          &lt;li&gt;For further scaling, the all reduce would have to happen over inter host. From our experiments, we did not see an obvious performance regression and in fact note advantages of an inter host all reduce. We can change our sharding and replica topology to inter host all reduce, which can help us introduce fault tolerance strategies should a particular host go down.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Frequency of all reduce synchronization, DMPCollection comes with a sync() call, which can be tuned to be called every N training steps, performing a sort of local SGD training. With scale, reducing the frequency of synchronization can bring significant gains to performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Readers should note that 2D sparse parallel training differs from non-parallelized training because we synchronize the embedding table weights rather than the gradients. This approach is made possible by TorchRec’s use of FBGEMM, which provides optimized kernels under the hood. One of FBGEMM’s key optimizations is the fusion of the optimizer in the backward pass. Instead of fully materializing the embedding table gradients—which would consume significant memory—they are passed directly to the optimizer update. Attempting to materialize and synchronize these gradients would create substantial overhead, making that approach impractical.&lt;/p&gt;

&lt;p&gt;Our exploration revealed that to achieve training results comparable to the baseline, we synchronize optimizer states on a delayed schedule, with the timing dependent on the number of sharding/replica groups (ie: for Adagrad we update the momentum behind by one sync step). This approach also enables users to implement local SGD or semi-synchronized training strategies, which can achieve convergence and potentially produce better loss curves than the baseline.&lt;/p&gt;

&lt;p&gt;We thank you for reading our post! This is an exciting direction we have come across that we hope to develop further to maximize performance of recommendation systems and push the state of the art.&lt;/p&gt;

&lt;style&gt;
@media screen and (min-width: 768px) {
    article.pytorch-article ul, article.pytorch-article ol {
        padding-left: 3.5rem;
    }
}
ol {
  list-style-type: decimal; /* 1, 2, 3 */
}

ol ol {
  list-style-type: lower-alpha; /* a, b, c */
}

ol ol ol {
  list-style-type: lower-roman; /* i, ii, iii */
}


&lt;/style&gt;</content>

      
      
      
      
      

      <author>
          <name>PyTorch Team at Meta: Chunzhi Yang, Rich Zhu, Zain Huda, Liangbei Xu, Xin Zhang, Jiyan Yang, Dennis van der Staay</name>
        
        
      </author>

      

      

      
        <summary type="html">At Meta, recommendation systems are the cornerstone of delivering relevant and personalized ads to billions of users globally. Through technologies like PyTorch’s TorchRec, we’ve successfully developed solutions that enable model training across hundreds of GPUs. While these systems have served us well, recent research on scaling laws has revealed a compelling opportunity: we can achieve significantly better model performance by training dramatically larger neural networks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Powering AI with PyTorch, Fedora, and Open Source Communities</title>
      <link href="https://pytorch.org/blog/pt-fedora-os-communities/" rel="alternate" type="text/html" title="Powering AI with PyTorch, Fedora, and Open Source Communities" />
      <published>2025-03-07T00:00:00-08:00</published>
      <updated>2025-03-07T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pt-fedora-os-communities</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-fedora-os-communities/">&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg1.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://www.devconf.info/in/&quot;&gt;DevConf.IN 2025&lt;/a&gt; in Pune, I had the opportunity to host a &lt;strong&gt;&lt;a href=&quot;https://pretalx.devconf.info/devconf-in-2025/talk/W3YURM/&quot;&gt;PyTorch Meetup&lt;/a&gt;&lt;/strong&gt; on February 28th. The session, titled “&lt;strong&gt;Powering AI with PyTorch, Fedora, and Open Source Communities&lt;/strong&gt;” was aimed at introducing PyTorch to students and professionals, explaining why &lt;strong&gt;PyTorch+Fedora&lt;/strong&gt; form an ideal AI development platform. The other key aspect I covered was collaboration between open source communities.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-pytorch&quot;&gt;Introduction to PyTorch&lt;/h2&gt;

&lt;h2 id=&quot;the-power-of-deep-learning-made-simple&quot;&gt;The Power of Deep Learning made simple&lt;/h2&gt;

&lt;p&gt;With the explosion of GPTs, there is a renowned interest in the field of AI and ML. The myth of developing AI/ML technologies and its applications is rocket science and far-fetched, needs correction. Only open source has the power to demystify this myth and further evolve the technology to make it versatile and developer friendly. Since its inception, PyTorch has evolved and has been a driving force to make AI/ML development extremely simple. I covered the aspects of PyTorch key components, its features and why PyTorch is the best choice as a deep learning framework.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg2.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The codewalk through was designed to showcase how easy and simple it is to utilise the power of GPUs, creating a simple neural network and training the model. The code walkthrough was very well received and it was great to hear back from the attendees that they never knew how powerful PyTorch is for deep learning. The real world examples sighted how this powerful framework can be used beyond the common GPTs and has the power to influence across a broad spectrum of applications.&lt;/p&gt;

&lt;h2 id=&quot;fedorapytorch-the-ideal-aiml-development-platform&quot;&gt;Fedora+PyTorch the Ideal AI/ML Development Platform&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg3.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg4.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the highlights of the event was the discussion on Fedora’s role as an AI platform. Fedora’s reliability, flexibility, and strong community support make it an ideal partner for PyTorch, allowing developers to focus on model-building without worrying about infrastructure. The students were intrigued by the idea of contributing to Fedora’s AI/ML ecosystem while building their own projects. Sumantro Mukherjee spoke about the AI policy in Fedora and how one can start contributing to the AI/ML using Fedora as a platform. He highlighted how Fedora is evolving to meet the needs of AI practitioners. The idea that an open-source operating system could provide the perfect foundation for AI research sparked an engaging conversation.&lt;/p&gt;

&lt;h2 id=&quot;innovation-in-open-source-when-communities-come-together&quot;&gt;Innovation in Open Source When Communities Come Together&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg5.jpg&quot; alt=&quot;charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important that we learn from history and repeat the good things! When open source communities come together they can create seismic shifts in the industry. To drive this home, I took the audience on a journey through history, revisiting a pivotal moment when Apache and Linux came together, solving common problems and fundamentally reshaping enterprise computing. That moment was not just about technology; it was about collaboration. It was about two powerful communities recognizing that they were stronger together. Today, we stand at the cusp of another such moment - PyTorch and Linux, particularly Fedora, are coming together to shape the future of AI/ML. This is not just an opportunity but a responsibility for contributors, developers, and AI/ML enthusiasts to be part of this movement.&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg6.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the best parts of the event was the enthusiasm it generated. Diverse audience, including students, AI enthusiasts, and industry professionals. Notably, Vincent Caldeira (CTO, APAC, Red Hat) and Chris Butler (Senior Principal Chief Architect, Red Hat) were present, reinforcing the growing interest in open-source AI/ML. Many students were eager to explore PyTorch and Fedora, contribute to open-source AI projects, and start their own AI experiments. Industry experts saw the potential for scalable, community-driven AI innovation. The session sparked curiosity and conversations that continued long after the event ended.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sudhir Dharanendraiah</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Peak Performance, Minimized Memory: Optimizing torchtune’s performance with torch.compile &amp;amp; Liger Kernel</title>
      <link href="https://pytorch.org/blog/peak-performance-minimized-memory/" rel="alternate" type="text/html" title="Peak Performance, Minimized Memory: Optimizing torchtune’s performance with torch.compile &amp; Liger Kernel" />
      <published>2025-03-06T00:00:00-08:00</published>
      <updated>2025-03-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/peak-performance-minimized-memory</id>
      <content type="html" xml:base="https://pytorch.org/blog/peak-performance-minimized-memory/">&lt;p&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;: Shivam Sahni, Byron Hsu, Yanning Chen&lt;br /&gt;
&lt;strong&gt;Meta&lt;/strong&gt;: Ankith Gunapal, Evan Smothers&lt;/p&gt;

&lt;p&gt;This blog explores the integration of a custom triton kernel, Liger Kernel with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to enhance the performance of fine-tuning large language models (LLMs) using torchtune. torchtune, a PyTorch-native library, offers modular building blocks and customizable finetuning recipes which include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; support for various LLMs, while Liger Kernel provides optimized Triton kernels to improve training efficiency and reduce memory usage. The integration involves modifying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransformerDecoder&lt;/code&gt; module in torchtune to bypass the linear layer computation, allowing the Liger Fused Linear Cross Entropy Loss to handle the forward projection weights. Experiments conducted on an NVIDIA A100 instance demonstrate that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; outperforms PyTorch Eager in throughput and memory efficiency, with Liger Kernel further reducing peak memory allocation and enabling larger batch sizes. The results show a 47% reduction in peak memory at batch size 256 and a marginal increase in throughput with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;meta-llama/Llama-3.2-1B&lt;/code&gt; , confirming the effectiveness of the integration without affecting the loss curves.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-torchtune&quot;&gt;Introduction to torchtune&lt;/h2&gt;

&lt;p&gt;torchtune is a PyTorch-native library which has been designed for finetuning LLMs. torchtune provides composable and modular building blocks along with finetuning recipes that can be easily customized for your use case, as will be shown in this blog.  &lt;br /&gt;
torchtune provides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch implementations of popular LLM model architectures from Llama, Gemma, Mistral, Phi, and Qwen model families&lt;/li&gt;
  &lt;li&gt;Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more&lt;/li&gt;
  &lt;li&gt;Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;YAML configs for easily configuring training, evaluation, quantization or inference recipes&lt;/li&gt;
  &lt;li&gt;Built-in support for many popular dataset formats and prompt templates&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-liger-kernel&quot;&gt;Introduction to Liger Kernel&lt;/h2&gt;

&lt;p&gt;Liger Kernel is an open source library of optimized Triton kernels designed to enhance the efficiency and scalability of training Large Language Models (LLMs). It focuses on kernel-level optimizations such as operation fusing and input chunking, achieving significant improvements in training throughput and GPU memory usage compared to existing implementations like those from HuggingFace. By using a single line of code, Liger Kernel can improve &lt;a href=&quot;https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training&quot;&gt;training throughput by 20% and reduce memory usage by 60%&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/peak-performance-minimized-memory/fg1.png&quot; alt=&quot;Fused Linear Cross Entropy&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;text-center mb-3&quot;&gt;
&lt;p&gt;Figure 1: &lt;a href=&quot;https://arxiv.org/pdf/2410.10989&quot; target=&quot;_blank&quot;&gt;Fused Linear Cross Entropy&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The bulk of LIger Kernel’s performance improvement comes from the Fused Linear Cross Entropy (FLCE) Loss, whose core idea is as follows:&lt;/p&gt;

&lt;p&gt;In LLMs, the vocabulary size has increased significantly, leading to a large logit tensor during cross-entropy (CE) loss computation. This logit tensor consumes excessive memory, causing a bottleneck in training. For example, when training with a batch size of 8 and sequence length of 4096, the 256k vocabulary size results in a 16.8 GB logit tensor. The FLCE kernel breaks down the computation into smaller chunks, reducing memory consumption.&lt;/p&gt;

&lt;p&gt;Here’s how it works:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Flattens the 3D hidden states into a 2D matrix by collapsing the batch size and sequence length dimensions.&lt;/li&gt;
  &lt;li&gt;Applies the linear projection head sequentially on the chunked hidden states.&lt;/li&gt;
  &lt;li&gt;Computes the partial loss and returns the chunked logits gradient using the Liger CE kernel.&lt;/li&gt;
  &lt;li&gt;Derives the chunked hidden states gradients and accumulates the projection head gradients.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Torchtune’s recipes provide &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; support out of the box. It has been shown that utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; with FLCE makes &lt;a href=&quot;https://github.com/linkedin/Liger-Kernel/issues/227&quot;&gt;FLCE 2x faster&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;integrating-liger-kernel-with-torchcompile--torchtune&quot;&gt;Integrating Liger Kernel with torch.compile &amp;amp; torchtune&lt;/h2&gt;

&lt;p&gt;We demonstrate integration of Liger Kernel with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; &amp;amp;  torchtune by running a full fine-tuning recipe with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;meta-llama/Llama-3.2-1B&lt;/code&gt;.  To make this integration happen, we have defined a custom full finetuning recipe, the details of the changes are mentioned below.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 tune run --nproc_per_node 4 recipes/full_finetune_distributed.py --config llama3_2/1B_full optimizer=torch.optim.AdamW optimizer.fused=True optimizer_in_bwd=False gradient_accumulation_steps=1  dataset.packed=True compile=True enable_activation_checkpointing=True tokenizer.max_seq_len=512  batch_size=128
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One of the inputs to the LCE Kernel is the forward projection weights. torchtune is designed as a modular library with composable blocks. There is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransformerDecoder&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/torchtune/modules/transformer.py#L322&quot;&gt;block&lt;/a&gt; where at the end of the block, we pass the final hidden state through a linear layer to get the final output. Since the linear layer is combined with the CE loss in LCE Kernel, we write a custom &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; function for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransformerDecoder&lt;/code&gt; where we skip the computation through the linear layer.&lt;/p&gt;

&lt;p&gt;In the full finetuning recipe, we override the model’s forward method with this custom method&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import types
from liger_kernel.torchtune.modules.transformers import decoder_forward
self._model.forward = types.MethodType(decoder_forward, self._model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then pass the model’s forward projection weights to calculate the loss with LCE Kernel&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from liger_kernel.transformers.fused_linear_cross_entropy import (
    LigerFusedLinearCrossEntropyLoss,
)

# Use LCE loss instead of CE loss
self._loss_fn = LigerFusedLinearCrossEntropyLoss()

# call torch.compile on the loss function
if self._compile:
    training.compile_loss(self._loss_fn, verbose=self._is_rank_zero)

# pass the model's forward projection weights for loss computation
current_loss = (
     self._loss_fn(
         self._model.output.tied_module.weight,
         logits,
         labels,
     )
     * current_num_tokens
 )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The complete code and instructions can be found in the &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/liger_kernel/third_party&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;experiments--benchmarking-results&quot;&gt;Experiments &amp;amp; Benchmarking Results&lt;/h2&gt;

&lt;p&gt;We conduct 3 types of experiments to demonstrate how Liger Kernel integration with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; enhances the performance of torchtune. We set up our experiments on an instance running NVIDIA A100. We fine-tune a small LLM  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;meta-llama/Llama-3.2-1B &lt;/code&gt;with differing batch sizes. We record the throughput in terms of tokens/second and measure the peak memory allocated during finetuning. Since it’s a small model, we only use 4 A100 GPUs for the benchmarking. The following are the experiments we conducted:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increase batch_size in powers of 2 with PyTorch eager&lt;/li&gt;
  &lt;li&gt;Increase batch_size in powers of 2 with torch.compile&lt;/li&gt;
  &lt;li&gt;Increase batch_size in powers of 2 with torch.compile &amp;amp; Liger integration&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We notice that with PyTorch Eager, throughput increases with increasing batch_size till we hit OOM at batch_size 256. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, the throughput is higher than PyTorch Eager for each batch_size. We see that the peak memory allocation reduces drastically with increasing batch_size and more than 50% reduction in peak memory at batch_size 128. This results in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; being able to support batch_size 256 and hence, the overall throughput with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; being 36% greater than PyTorch Eager. Integrating Liger Kernel with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; doesn’t drop the throughput at lower batch_size but with increasing batch_size, we notice that torchtune is consuming less memory compared to torch.compile. At batch_size 256, we see a 47% reduction in peak memory allocation with the Liger kernel. This allows us to use batch_size 512 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; &amp;amp; Liger. We notice that there is a marginal 1-2% increase in throughput compared to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; without custom triton kernels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/peak-performance-minimized-memory/fg2.png&quot; alt=&quot;Plot of tokens/sec per rank vs batch_size&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;text-center mb-3&quot;&gt;
&lt;p&gt;Figure 2: Plot of tokens/sec per rank vs batch_size&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/peak-performance-minimized-memory/fg3.png&quot; alt=&quot;Peak memory allocated vs batch_size&quot; style=&quot;width:100%;margin-top: 60px;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;text-center mb-3&quot;&gt;
&lt;p&gt;Figure 3: Peak memory allocated vs batch_size&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;To rule out any potential functional issues with our integration of Liger Kernel with torchtune, we plot the loss curve against training steps with &amp;amp; without Liger. We see that there is no visible difference in the loss curves.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/peak-performance-minimized-memory/fg4.png&quot; alt=&quot;Plot of loss vs training steps for batch_size=128&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;text-center mb-3&quot;&gt;
&lt;p&gt;Figure 4: Plot of loss vs training steps for batch_size=128&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Enable Liger kernels for &lt;a href=&quot;https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/chunked_loss/dpo_loss.py#L7&quot;&gt;DPO loss&lt;/a&gt; and &lt;a href=&quot;https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/chunked_loss/fused_linear_distillation.py#L9&quot;&gt;distillation loss&lt;/a&gt; in torchtune’s recipes for &lt;a href=&quot;https://pytorch.org/torchtune/main/recipes/dpo.html&quot;&gt;DPO&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/blog/llama-into-torchtune/&quot;&gt;knowledge distillation&lt;/a&gt;, respectively.&lt;/li&gt;
  &lt;li&gt;Support Liger integration in torchtune with &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/2330&quot;&gt;tensor parallel training&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We thank Hamid Shojanazeri (Meta), Less Wright (Meta), Horace He (Meta) &amp;amp; Gregory Chanan (Meta) for their feedback and support in making this blog post happen.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>LinkedIn and Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">LinkedIn: Shivam Sahni, Byron Hsu, Yanning Chen Meta: Ankith Gunapal, Evan Smothers</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Current and New Activation Checkpointing Techniques in PyTorch</title>
      <link href="https://pytorch.org/blog/activation-checkpointing-techniques/" rel="alternate" type="text/html" title="Current and New Activation Checkpointing Techniques in PyTorch" />
      <published>2025-03-05T00:00:00-08:00</published>
      <updated>2025-03-05T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/activation-checkpointing-techniques</id>
      <content type="html" xml:base="https://pytorch.org/blog/activation-checkpointing-techniques/">&lt;p&gt;As models scale in depth, batch size, and sequence length, etc, activation memory becomes an increasingly significant contributor to the overall memory usage. To help address this, PyTorch provides utilities for &lt;a href=&quot;https://pytorch.org/docs/stable/checkpoint.html&quot;&gt;activation checkpointing&lt;/a&gt;, which reduce the number of saved tensors by recomputing them when needed, trading off memory usage for additional compute.&lt;/p&gt;

&lt;p&gt;In this post, we’ll walk through the basics of what activation memory is, the high-level ideas behind existing activation checkpointing techniques, and also introduce some newer techniques that aim to improve flexibility and provide more optimization/automation out of the box.&lt;/p&gt;

&lt;p&gt;As we look at these techniques, we’ll compare how these methods fit into a speed vs. memory trade-off diagram and hopefully provide some insight on how to choose the right strategy for your use case.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you prefer to jump straight to the new APIs, please skip ahead to the “Selective Activation Checkpoint” and “Memory Budget API” sections below.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;activation-memory-basics&quot;&gt;Activation Memory Basics&lt;/h2&gt;

&lt;p&gt;By default, in eager mode (rather than using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;), PyTorch’s autograd preserves intermediate activations for backward computation. For example, if you call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sin&lt;/code&gt; on a tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; during the forward pass, autograd must remember &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; to compute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cos(x)&lt;/code&gt; during backward.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg2.png&quot; alt=&quot;flow diagram&quot; style=&quot;max-width:400px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If this tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is saved at the beginning of the forward pass, it remains in memory throughout both the forward and backward phases. It can only be cleared after it is used to compute the gradient, which happens at the end of the backward pass (due to the reverse order of execution).&lt;/p&gt;

&lt;p&gt;Thus, as you proceed through the forward pass and perform more and more operations, you accumulate more and more activations, resulting in more and more activation memory until it (typically) reaches its peak at the start of backward (at which point activations can start to get cleared).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the diagram above, the orange boxes represent operations, black arrows represent their tensor inputs and outputs. The black arrows that cross over the right represent tensors that autograd saves for backward.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A useful way to visually organize this default saving behavior in eager as well as the techniques we’re about to introduce is based on how they trade off speed versus memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg4.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ideal place to be on this diagram is the top-left, where you have “high” speed but also low memory usage.&lt;/p&gt;

&lt;p&gt;We begin by putting the default saving behavior on the &lt;strong&gt;top-right&lt;/strong&gt; (for reasons we’ll explain in more detail as we introduce more points for other techniques).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;activation-checkpointing-ac&quot;&gt;Activation Checkpointing (AC)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/checkpoint.html&quot;&gt;Activation checkpointing (AC)&lt;/a&gt;&lt;/strong&gt; is a popular technique to reduce memory usage in PyTorch.&lt;/p&gt;

&lt;p&gt;During forward, any operations performed inside the AC’d region do not save tensors for backward. (Only the inputs to the function are saved.) During backward, the intermediate activations needed for gradient computation are rematerialized by running the function a second time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg5.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the diagram (right), the black box shows where activation checkpointing is applied. Compared to the default eager approach (left), this setup results in fewer tensors being saved (1 versus 3).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Applying AC on the right parts of the model has the effect of reducing peak memory, because the intermediate activations are no longer materialized in memory when the memory usage typically peaks (at the beginning of backward).&lt;/p&gt;

&lt;p&gt;On the speed-versus-memory tradeoff diagram, AC is plotted on the &lt;strong&gt;bottom-left.&lt;/strong&gt; Relative to eager mode, it reduces the amount of memory saved for backward but comes with an added cost in compute due to recomputation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg6.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that AC’s speed–memory tradeoff /can/ be adjusted by selecting which parts of the forward pass to checkpoint and by defining how many checkpoint regions to use. However, implementing these changes may require modifying your model’s structure and can be cumbersome depending on how your code is organized. For the purposes of this diagram, we assume only one region is checkpointed; under this assumption, AC appears as a single point on the tradeoff diagram.&lt;/p&gt;

&lt;p&gt;Also note that “memory” here does not refer to peak memory usage; rather, it indicates the how much memory is saved for backward for a fixed region.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;torchcompile-and-min-cut-partitioner&quot;&gt;torch.compile and min-cut partitioner&lt;/h2&gt;

&lt;p&gt;Another notable approach to keep in mind is &lt;strong&gt;torch.compile&lt;/strong&gt; (introduced in PyTorch 2.0). Like activation checkpointing, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; can also perform some level of recomputation under the hood. Specifically, it traces the forward and backward computations into a single joint graph, which is then processed by a &lt;a href=&quot;https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467&quot;&gt;“min-cut” partitioner&lt;/a&gt;. This partitioner uses a min-cut/max-flow algorithm to split the graph such that it minimizes the number of tensors that need to be saved for backward.&lt;/p&gt;

&lt;p&gt;At first glance, this might sound a lot like what we want for activation memory reduction. However, the reality is more nuanced. By default, the partitioner’s primary goal is to reduce runtime. As a result, it only recomputes certain types of operations—primarily simpler, fusible, and non-compute-intensive ops (like pointwise ops).&lt;/p&gt;

&lt;p&gt;Placing “compile” on the speed-versus-memory tradeoff diagram…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg7.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is to the top-left of the eager non-AC point, as we expect  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to improve on both speed and memory.&lt;/p&gt;

&lt;p&gt;On the other hand, relative to activation checkpointing, torch.compile is more conservative about what it recomputes, placing it closer to the top-left on the speed-versus-memory diagram.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;selective-activation-checkpoint-new&quot;&gt;Selective Activation Checkpoint [NEW!]&lt;/h2&gt;

&lt;p&gt;While normal checkpointing recomputes every op in a chosen region, &lt;a href=&quot;https://pytorch.org/docs/main/checkpoint.html#torch.utils.checkpoint.create_selective_checkpoint_contexts&quot;&gt;selective activation checkpointing (SAC)&lt;/a&gt; is an additional setting on top of activation checkpointing that you can apply to have a more granular control over which operations to recompute.&lt;/p&gt;

&lt;p&gt;This can be useful if you have certain more expensive operations like matmuls which you prefer to avoid recomputing, but still generally want to recompute cheaper operations like pointwise.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg8.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Where plain AC (left) would save a single tensor and then recompute the entire AC’d region, with SAC (right) you can selectively save specific operations (marked red) in the region, so you can avoid recomputing them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To specify what to selectively save, you can specify a policy_fn. To illustrate the additional trade offs you can make with this, we present two simple policy functions.&lt;/p&gt;

&lt;h3 id=&quot;policy-1-not-recomputing-matmuls&quot;&gt;Policy 1: Not recomputing matmuls:&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aten = torch.ops.aten
compute_intensive_ops = [  
        aten.mm,
        aten.bmm,
        aten.addmm,
] 
def policy_fn(ctx, op, *args, **kwargs):
    if op in compute_intensive_ops:
        return CheckpointPolicy.MUST_SAVE
    else:
        return CheckpointPolicy.PREFER_RECOMPUTE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg9.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;policy-2-more-aggressively-save-anything-compute-intensive&quot;&gt;Policy 2: More aggressively save anything compute intensive&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# torch/_functorch/partitioners.py
aten = torch.ops.aten
compute_intensive_ops = [  
   aten.mm,
   aten.convolution,
   aten.convolution_backward,
   aten.bmm,
   aten.addmm,
   aten._scaled_dot_product_flash_attention,
   aten._scaled_dot_product_efficient_attention,
   aten._flash_attention_forward,
   aten._efficient_attention_forward,
   aten.upsample_bilinear2d,
   aten._scaled_mm
] 
def policy_fn(ctx, op, *args, **kwargs):
    if op in compute_intensive_ops:
        return CheckpointPolicy.MUST_SAVE
    else:
        return CheckpointPolicy.PREFER_RECOMPUTE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg10.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the speed-versus-memory diagram, SAC is plotted as a range of points from closer to AC to closer to Eager, depending on your chosen policy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg11.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try it out!&lt;/strong&gt; (Available in 2.5 as a prototype feature; see &lt;a href=&quot;https://pytorch.org/docs/main/checkpoint.html#torch.utils.checkpoint.create_selective_checkpoint_contexts&quot;&gt;docs&lt;/a&gt; for more info + copy-pastable example)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.utils.checkpoint import checkpoint, create_selective_checkpoint_contexts

# Create a policy function that returns a CheckpointPolicy
def policy_fn(ctx, op, *args, **kwargs):
    if op in ops_to_save:
        return CheckpointPolicy.MUST_SAVE
    else:
        return CheckpointPolicy.PREFER_RECOMPUTE

# Use the context_fn= arg of the existing checkpoint API
out = checkpoint(
    fn, *args,
    use_reentrant=False,
    # Fill in SAC context_fn's policy_fn with functools.partial
    context_fn=partial(create_selective_checkpoint_contexts, policy_fn),
)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;compile-only-memory-budget-api-new&quot;&gt;(compile-only) Memory Budget API [NEW!]&lt;/h2&gt;

&lt;p&gt;As mentioned previously, any given SAC policy can be represented as a point on a speed-memory tradeoff diagram. Not all policies are created equal, however. The “optimal” policies are the ones that fall on a pareto curve, e.g. for all policies that incur the same memory overhead, this policy is the one that minimizes the amount of required compute.&lt;/p&gt;

&lt;p&gt;For users who are using torch.compile, we offer a &lt;strong&gt;memory budget API&lt;/strong&gt; that automatically applies SAC over your compiled region with a pareto-optimal policy given a user-specified “memory budget” between 0 and 1, where a budget of 0 behaves like plain-AC and a budget of 1 behaves like default torch.compile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg12.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below are some real results on a transformer model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg13.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We observe a 50% memory reduction by recomputing only pointwise ops, with a steady drop-off as you recompute more and more of your matmuls. Attention is the most expensive, so you tend to want to recompute those last.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try it out!&lt;/strong&gt; (Available in 2.4 as an experimental feature; see this &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/68a363548409a3ff17965770304ee5e12fe718d9/torch/_functorch/config.py#L110-L122&quot;&gt;comment block&lt;/a&gt; for more info)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch._dynamo.config.activation_memory_budget = 0.5

out = torch.compile(fn)(inp)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/activation-checkpointing-techniques/fg14.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In summary, activation checkpointing techniques in PyTorch offer a variety of ways to balance memory and compute demands, from simple region-based checkpointing to more selective and automated methods. By choosing the option that best matches your model’s structure and resource constraints, you can achieve significant memory savings with an acceptable trade-off in compute.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Meta’s &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xformers&lt;/a&gt; team including &lt;a href=&quot;https://github.com/fmassa&quot;&gt;Francisco Massa&lt;/a&gt; for working on the original version of Selective Activation Checkpoint.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">As models scale in depth, batch size, and sequence length, etc, activation memory becomes an increasingly significant contributor to the overall memory usage. To help address this, PyTorch provides utilities for activation checkpointing, which reduce the number of saved tensors by recomputing them when needed, trading off memory usage for additional compute.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">📣 Submit to Speak at PyTorch Conference + Save on Registration</title>
      <link href="https://pytorch.org/blog/submit-to-speak/" rel="alternate" type="text/html" title="📣 Submit to Speak at PyTorch Conference + Save on Registration" />
      <published>2025-03-04T00:00:00-08:00</published>
      <updated>2025-03-04T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/submit-to-speak</id>
      <content type="html" xml:base="https://pytorch.org/blog/submit-to-speak/">&lt;p&gt;Step into the Future of AI at PyTorch Conference 2025.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/submit-to-speak/fg1.png&quot; alt=&quot;banner ad for conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Call for Proposals for &lt;strong&gt;PyTorch Conference 2025&lt;/strong&gt; is officially open!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Join us in San Francisco from October 22–23, 2025,&lt;/strong&gt; to showcase your expertise and innovations with PyTorch—the industry-leading, open-source machine learning framework powering innovations from bare-metal infrastructure to sophisticated application and agent layers. This is your opportunity to share insights, breakthroughs, and case studies with a global audience of AI and Generative AI practitioners, researchers, and developers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/submit-to-speak/fg2.jpg&quot; alt=&quot;people watching presentation at conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Submit your proposals and prepare to engage, learn, and network alongside some of the brightest minds in the AI/ML community. We’re seeking sessions, Birds of a Feather discussions, lightning talks, and poster sessions on the following topics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Core PyTorch Framework&lt;/li&gt;
  &lt;li&gt;PyTorch on Accelerator Hardware&lt;/li&gt;
  &lt;li&gt;PyTorch Ecosystem and Tools&lt;/li&gt;
  &lt;li&gt;AI Applications and Use Cases&lt;/li&gt;
  &lt;li&gt;AI in Research and Academia&lt;/li&gt;
  &lt;li&gt;AI in Industry and Enterprise Applications&lt;/li&gt;
  &lt;li&gt;AI Infrastructure and Scalability&lt;/li&gt;
  &lt;li&gt;Ethical AI, Governance, and Regulation&lt;/li&gt;
  &lt;li&gt;Training, Fine-Tuning, and Alignment&lt;/li&gt;
  &lt;li&gt;Inference, Deployment, and Serving&lt;/li&gt;
  &lt;li&gt;Performance Measurement and Benchmarking&lt;/li&gt;
  &lt;li&gt;Data Engineering and Management for AI&lt;/li&gt;
  &lt;li&gt;Generative AI and Large Language Models (LLMs)&lt;/li&gt;
  &lt;li&gt;Model Optimization and Efficiency&lt;/li&gt;
  &lt;li&gt;Open Source Collaboration, Education and Community Building&lt;/li&gt;
  &lt;li&gt;Edge AI and On-Device&lt;/li&gt;
  &lt;li&gt;DL Compilers and Kernel Authoring&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;p&gt;&lt;strong&gt;Learn more and submit your talk by Sunday, June 1, at 11:59 PDT!&lt;/strong&gt;&lt;/p&gt;
&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/program/cfp/&quot; target=&quot;_blank&quot; class=&quot;btn btn-lg with-right-arrow&quot;&gt;
   SUBMIT TO SPEAK
&lt;/a&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/submit-to-speak/fg3.jpg&quot; alt=&quot;people arriving at conference&quot; style=&quot;max-width:300px; display: block; float: right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Save up to USD$500 with Super Early Bird Pricing!&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reserve your pass by &lt;strong&gt;11:59 PM PDT on March 21&lt;/strong&gt; and score Super Early Bird pricing for just &lt;strong&gt;USD$499&lt;/strong&gt;. That’s a savings of up to USD$500!&lt;/li&gt;
  &lt;li&gt;Student or faculty? Learn more about our &lt;strong&gt;&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/#registration-rates&quot;&gt;discounted academic rate&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Need help covering travel costs? We offer discretionary travel funding for those community members who would otherwise not be able to attend. &lt;strong&gt;&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/#additional-information&quot;&gt;Learn more&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/&quot; target=&quot;_blank&quot; class=&quot;btn mb-4 btn-lg with-right-arrow&quot;&gt;
   REGISTER NOW &amp;amp; SAVE
&lt;/a&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Become a Sponsor at PyTorch Conference 2025!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Seize your opportunity to influence the future of Generative AI and Machine Learning by sponsoring PyTorch Conference 2025. PyTorch is at the forefront of innovation—empowering rapid experimentation, flexible model development, and efficient deployment into production environments with its powerful, versatile ecosystem of tools and thriving community of dedicated users.&lt;/p&gt;

&lt;p&gt;As a sponsor, you’ll gain more than visibility; you’ll strategically position your organization at the heart of a vibrant, global AI/ML ecosystem. Connect directly with &lt;strong&gt;3,000+&lt;/strong&gt; expert attendees, researchers, engineers, and decision-makers, and actively shape the conversations driving the next generation of AI advancements.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/sponsor/&quot; target=&quot;_blank&quot; class=&quot;btn mt-3 mb-3 btn-lg with-right-arrow&quot;&gt;
   BECOME A SPONSOR
&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;For more details on CFP submissions, registration, and sponsorship, visit &lt;strong&gt;the&lt;/strong&gt; &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference Website&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Step into the Future of AI at PyTorch Conference 2025.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai-2/" rel="alternate" type="text/html" title="Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts" />
      <published>2025-02-26T00:00:00-08:00</published>
      <updated>2025-02-26T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai-2/">&lt;p&gt;This post is a follow-up to our &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;first entry in the multi-series blog focused on how to accelerate generative AI models&lt;/a&gt; with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.&lt;/p&gt;

&lt;p&gt;By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, reduced precision, batched prompts and GPU preprocessing we observe up to &lt;strong&gt;13x improvement in p90 execution latency&lt;/strong&gt; and &lt;strong&gt;queue times compared to regular eager mode PyTorch&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We calculate our final results and demonstrate the improvement in a realistic deployment on auto-scaling cloud infrastructure from &lt;a href=&quot;https://modal.com&quot;&gt;Modal&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p50 execution latency
&lt;br /&gt;
(ms / improvement)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p90 execution latency
&lt;br /&gt;
(ms / improvement)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AMG
   &lt;/td&gt;
   &lt;td&gt;741
   &lt;/td&gt;
   &lt;td&gt;112 (6.6x)
   &lt;/td&gt;
   &lt;td&gt;1140
   &lt;/td&gt;
   &lt;td&gt;176 (6.5x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;SPS
   &lt;/td&gt;
   &lt;td&gt;98
   &lt;/td&gt;
   &lt;td&gt;20 (4.9x)
   &lt;/td&gt;
   &lt;td&gt;130
   &lt;/td&gt;
   &lt;td&gt;28 (4.6x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MPS
   &lt;/td&gt;
   &lt;td&gt;269
   &lt;/td&gt;
   &lt;td&gt;38 (7.1x)
   &lt;/td&gt;
   &lt;td&gt;714
   &lt;/td&gt;
   &lt;td&gt;52 (13.7x)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p50 queue time (ms / improvement)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p90 queue time (ms / improvement)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AMG
   &lt;/td&gt;
   &lt;td&gt;201
   &lt;/td&gt;
   &lt;td&gt;41 (4.9x)
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;327 (2.6x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;SPS
   &lt;/td&gt;
   &lt;td&gt;31
   &lt;/td&gt;
   &lt;td&gt;33 (0.9x)
   &lt;/td&gt;
   &lt;td&gt;441
   &lt;/td&gt;
   &lt;td&gt;49 (9.0x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MPS
   &lt;/td&gt;
   &lt;td&gt;40
   &lt;/td&gt;
   &lt;td&gt;37 (1.1x)
   &lt;/td&gt;
   &lt;td&gt;942
   &lt;/td&gt;
   &lt;td&gt;75 (12.6x)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;the-tasks&quot;&gt;The Tasks&lt;/h2&gt;

&lt;p&gt;The first post focused on processing a small number of varying prompts (points of interest) per image. These points represented the center points of the ground truth masks. For this post, we’ll now focus on a broader set of tasks. Single prompt segmentation (SPS), multi prompt segmentation (MPS), automatic mask generation (AMG) which generates the full set of masks for the input image without a given set of prompts. The first post focused on MPS only.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2.jpg&quot; alt=&quot;comparison of 3 images&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The little star in the image represents a user prompt. For AMG there are no prompts and masks are filtered down heuristically from a dense grid of initial candidate prompts (guesses). For SPS and MPS user prompts are derived from the center points of AMG masks. For SPS we choose the mask with the largest area.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note that SAM2 uses a different backbone than SAM1. In particular, we only consider the largest and most accurate sam2.1_hiera_large backbone for this blog.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We aggregate the scripts needed to reproduce the results in &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server&quot;&gt;torchao’s example folder&lt;/a&gt; and incrementally upstream the more stable parts of the &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/_models/sam2&quot;&gt;changes to the SAM2 model in torchao&lt;/a&gt; to the main &lt;a href=&quot;https://github.com/facebookresearch/sam2&quot;&gt;SAM2&lt;/a&gt; repository. So if you are interested in taking a look at the cutting-edge variant or would like to contribute experimental features, please don’t hesitate to reach out to the torchao repository and team. For the more stable and latest model version, please head on over to SAM2 directly.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;We categorize the changes presented here into two. &lt;strong&gt;Fast&lt;/strong&gt; changes constrain themselves to techniques that are not meant to affect model accuracy. &lt;strong&gt;Furious&lt;/strong&gt; changes sacrifice some numerical accuracy for additional speed by making use of approximations such as low-precision data types.&lt;/p&gt;

&lt;p&gt;Approximations may slightly lower precision metrics in favor of significantly improved performance while still passing an end-to-end check based on mean intersection over union (mIoU).&lt;/p&gt;

&lt;p&gt;To measure the performance improvements we processed 1000 images, which were selected at random from the SAM2 validation dataset. We look at the p50 and p90 latency per image. To measure accuracy we consider the mIoU. Most notably for the AMG task we also define a fail count metric. We consider a comparison failed if the &lt;strong&gt;number of masks&lt;/strong&gt; differs. This turns out to be a fairly unstable quantity and we can see that the other tasks are not as sensitive to small numeric changes as AMG.&lt;/p&gt;

&lt;h2 id=&quot;the-setup&quot;&gt;The Setup&lt;/h2&gt;

&lt;p&gt;We are running the offline experiments on a regular H100 devserver, which is a fairly beefy and performant machine.&lt;/p&gt;

&lt;p&gt;However, we try to look at these tasks with realistic constraints. In particular, we would like to emulate a server-side inference environment. That means we don’t use DataLoader to hide the latency of image preprocessing or decoding routines.&lt;/p&gt;

&lt;p&gt;For the latency calculations we include decoding, segmentation and conversion of masks to a dictionary of run-length encoded masks. Or put differently, we exclude loading the images into in-memory host bytearrays and storing the resulting dictionaries as json files on disk. This is meant to emulate a more realistic setting.&lt;/p&gt;

&lt;p&gt;More concretely, consider the code below for the routines we include in our measurements. For any task &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gen_masks&lt;/code&gt; produces a batched bool Tensor bitmask that represents the corresponding object masks. We then compress this bitmask into a run length encoded (rle) format that can be used to transfer back the results from a remote server much more efficiently.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;image_tensors = decode_img_bytes(...)
masks = gen_masks(image_tensors, ...)
rle_dicts = [rle_dict_from_masks(m) for m in masks]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;h3 id=&quot;ao-eager-code-optimizations&quot;&gt;ao: eager code optimizations&lt;/h3&gt;

&lt;p&gt;The most effective tool for this work is the PyTorch autograd profiler combined with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;record_function&lt;/code&gt;. To build this software, we’ve used the profiler repeatedly to observe the program and confirm the effectiveness of any changes. It’s also important to keep in mind that the profiler itself has overhead. The more data you collect, such as stack traces, the more overhead you introduce, which might skew the collected trace. But it is excellent to find synchronization points, space between kernels and GPU kernels that take a long time.&lt;/p&gt;

&lt;p&gt;GPU traces help you understand bottlenecks that are not necessarily easily addressed by compile. We found that AutomaticMaskGeneration in particular is dominated by the data structure used to store the masks and by the routine used to convert the masks to a run-length encoded compressed format. We also found a large part of AMG performance is dominated by the large number of masks created as a single batch. Sometimes candidate masks can be filtered down to fewer candidates earlier in the postprocessing stage by reordering operations. This in turn significantly speeds up the later operations.&lt;/p&gt;

&lt;p&gt;In order to confirm the accuracy of our implementation we first compare without any changes in settings and using float32 precision. We see that mIoU is unchanged and the masks match perfectly when using the exact same settings. This means that these eager mode changes did not affect the accuracy of these tasks.&lt;/p&gt;

&lt;p&gt;AMG&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / fail count
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline
   &lt;/td&gt;
   &lt;td&gt;864
   &lt;/td&gt;
   &lt;td&gt;1144
   &lt;/td&gt;
   &lt;td&gt;4350
   &lt;/td&gt;
   &lt;td&gt;reference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO
   &lt;/td&gt;
   &lt;td&gt;693
   &lt;/td&gt;
   &lt;td&gt;786
   &lt;/td&gt;
   &lt;td&gt;4010
   &lt;/td&gt;
   &lt;td&gt;1 / 0
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;ao-batching-prompts&quot;&gt;ao: batching prompts&lt;/h3&gt;

&lt;p&gt;Another lossless performance optimization that we were able to apply is batching the user input prompt calculations. When optimizing for latency at batch size 1 on a server-grade GPU such as an H100 we are often left with a lot of spare memory. We can easily trade off that memory for more performance by processing more points of interest (also called user prompts) at once. Remember that SAM2 is split into two parts: First the backbone (image encoder), second the prediction and decoding of masks based on a set of user prompts / points of interest. It is the second part where we may expect a larger or even varying number of inputs and it is this second part where we apply batching.&lt;/p&gt;

&lt;p&gt;This causes a large increase in memory, but also much better latency. The baseline generates one mask per prompt in a loop. For AMG the baseline processes 64 prompts at once and all that is needed is to change it to 1024, which is the number of candidate prompts generated. For SPS we process one prompt at a time, but it’s still included below for completeness.&lt;/p&gt;

&lt;p&gt;AMG&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / fail count
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline
   &lt;/td&gt;
   &lt;td&gt;864
   &lt;/td&gt;
   &lt;td&gt;1144
   &lt;/td&gt;
   &lt;td&gt;4350
   &lt;/td&gt;
   &lt;td&gt;reference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO + batching
   &lt;/td&gt;
   &lt;td&gt;613
   &lt;/td&gt;
   &lt;td&gt;706
   &lt;/td&gt;
   &lt;td&gt;33786
   &lt;/td&gt;
   &lt;td&gt;0.9999995 / 0
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;SPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;181
   &lt;/td&gt;
   &lt;td&gt;1337
   &lt;/td&gt;
   &lt;td&gt;reference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO
   &lt;/td&gt;
   &lt;td&gt;110
   &lt;/td&gt;
   &lt;td&gt;170
   &lt;/td&gt;
   &lt;td&gt;1339
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;MPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline
   &lt;/td&gt;
   &lt;td&gt;276
   &lt;/td&gt;
   &lt;td&gt;681
   &lt;/td&gt;
   &lt;td&gt;1337
   &lt;/td&gt;
   &lt;td&gt;reference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO + batching
   &lt;/td&gt;
   &lt;td&gt;126
   &lt;/td&gt;
   &lt;td&gt;225
   &lt;/td&gt;
   &lt;td&gt;8021
   &lt;/td&gt;
   &lt;td&gt;0.9999992
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As a technical side note: Most notably to enable batching for MPS, and to avoid a significant manual rewrite of the code base to support multiple prompts at the same time, we used a Tensor subclass we call MapTensor. A MapTensor allows us to pass a batch of N prompts, but have it advertise a batch size of 1. Any operation is then automatically broadcast to the wrapped Tensor and propagated throughout the prediction part of the model. This works because individual prompt predictions are independent of one another. This is very similar to torch.vmap.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;center_points_torch = to_map_tensor(center_points_torch)
center_points_label_torch = to_map_tensor(center_points_label_torch)
masks, scores, _ = mask_generator.predictor.predict(
    point_coords=center_points_torch,
    point_labels=center_points_label_torch,
    multimask_output=True,
    return_logits=False,
    return_type=&quot;torch&quot;,
)
# Unwrapping MapTensor
masks = masks.elems
scores = scores.elems
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;fast-fullgraph-compilation&quot;&gt;fast: fullgraph compilation&lt;/h3&gt;

&lt;p&gt;Just as with our first post, we first remove GPU syncs and graph breaks to make use of fullgraph compiled model code with max-autotune kernels where appropriate. After some rewriting, we are able to compile the image encoder and the prediction of masks.&lt;/p&gt;

&lt;p&gt;We run the experiments twice to get a sense of the overhead due to compilation. We run it once in an environment with an empty TORCHINDUCTOR_CACHE_DIR and then again while ingesting the artifacts from the previous run. In particular, auto-tuning can take a long time and happens on the first call in a pristine environment. We call the second run “warm”. The first iteration is typically expected to be slow due to various other related initialization processes, but compile increases it significantly, even if an existing cache is used and the same exact shapes are fed again. Having said that, an overhead of a few seconds in a warm environment is often still stomachable on the very first call.&lt;/p&gt;

&lt;p&gt;Most of these drawbacks can be mitigated and compiling causes a significant improvement in latency and reduction in memory.&lt;/p&gt;

&lt;p&gt;AMG&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / 
&lt;br /&gt;
fail count
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO + batching
   &lt;/td&gt;
   &lt;td&gt;613
   &lt;/td&gt;
   &lt;td&gt;706
   &lt;/td&gt;
   &lt;td&gt;33786
   &lt;/td&gt;
   &lt;td&gt;0.9999995 / 0
   &lt;/td&gt;
   &lt;td&gt;1125
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (cold)
   &lt;/td&gt;
   &lt;td&gt;423
   &lt;/td&gt;
   &lt;td&gt;513
   &lt;/td&gt;
   &lt;td&gt;29349
   &lt;/td&gt;
   &lt;td&gt;skipped
   &lt;/td&gt;
   &lt;td&gt;404866
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;439
   &lt;/td&gt;
   &lt;td&gt;530
   &lt;/td&gt;
   &lt;td&gt;29349
   &lt;/td&gt;
   &lt;td&gt;0.994 / 190
   &lt;/td&gt;
   &lt;td&gt;8544
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The number of masks produced per mask can vary slightly when using automatic mask segmentation. There is ambiguity in the number of masks per object the model may produce. For example, a car may be subdivided into frames, windows and doors or treated as a whole. When a modification causes the number of masks to change, we consider the comparison failed and we only calculate the mIoU on masks with an exact match. This does not apply to the other tasks. We found that the number of masks generated is very sensitive to small numerical changes. The other tasks use the same code and MPS in particular can help us further verify correctness.&lt;/p&gt;

&lt;p&gt;SPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO
   &lt;/td&gt;
   &lt;td&gt;110
   &lt;/td&gt;
   &lt;td&gt;170
   &lt;/td&gt;
   &lt;td&gt;1339
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;562
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (cold)
   &lt;/td&gt;
   &lt;td&gt;102
   &lt;/td&gt;
   &lt;td&gt;158
   &lt;/td&gt;
   &lt;td&gt;1343
   &lt;/td&gt;
   &lt;td&gt;skipped
   &lt;/td&gt;
   &lt;td&gt;319954
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;160
   &lt;/td&gt;
   &lt;td&gt;1302
   &lt;/td&gt;
   &lt;td&gt;0.9999
   &lt;/td&gt;
   &lt;td&gt;8947
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;MPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO + batching
   &lt;/td&gt;
   &lt;td&gt;126
   &lt;/td&gt;
   &lt;td&gt;225
   &lt;/td&gt;
   &lt;td&gt;8021
   &lt;/td&gt;
   &lt;td&gt;0.9999992
   &lt;/td&gt;
   &lt;td&gt;504
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (cold)
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;215
   &lt;/td&gt;
   &lt;td&gt;8021
   &lt;/td&gt;
   &lt;td&gt;skipped
   &lt;/td&gt;
   &lt;td&gt;333308
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;113
   &lt;/td&gt;
   &lt;td&gt;213
   &lt;/td&gt;
   &lt;td&gt;8021
   &lt;/td&gt;
   &lt;td&gt;0.998
   &lt;/td&gt;
   &lt;td&gt;8617
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;furious-tf32-float16-and-gpu-preprocessing&quot;&gt;furious: TF32, float16 and GPU preprocessing&lt;/h3&gt;

&lt;p&gt;We found that using float16 is the right level of precision for a few significant subcomponents of the model. In particular, the image encoder and mask decoder weights can be converted entirely to float16. We can also use TensorFloat32 precision for the remaining float32 matrix operations. It should be possible to further reduce the precision and we may address this in a future post. We also move image preprocessing such as image normalization onto the GPU with the furious mode. We can’t use GPU decoding (nvJPEG) routines, because the differences are too significant and the model suffers from significant degradation in mIoU, so image decoding still happens on the CPU.&lt;/p&gt;

&lt;p&gt;AMG&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / 
&lt;br /&gt;
fail count
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO 
&lt;br /&gt;
+ batching 
&lt;br /&gt;
+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;439
   &lt;/td&gt;
   &lt;td&gt;530
   &lt;/td&gt;
   &lt;td&gt;29349
   &lt;/td&gt;
   &lt;td&gt;0.994 / 190
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ furious
   &lt;/td&gt;
   &lt;td&gt;165
   &lt;/td&gt;
   &lt;td&gt;240
   &lt;/td&gt;
   &lt;td&gt;28335
   &lt;/td&gt;
   &lt;td&gt;0.978 / 306
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This causes a significant degradation in mIoU for the AMG task, but doesn’t affect the other tasks. After an in-depth investigation, we still chalk this up to numerical instability and reordering of operations. More work is needed to further investigate this and it may not be interesting to run the AMG task in lower precision. The other tasks, however, benefit drastically in latency with minimal changes in mIoU.&lt;/p&gt;

&lt;p&gt;SPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO 
&lt;br /&gt;
+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;160
   &lt;/td&gt;
   &lt;td&gt;1302
   &lt;/td&gt;
   &lt;td&gt;0.9999
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ furious
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;63
   &lt;/td&gt;
   &lt;td&gt;861
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;MPS&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AO 
   &lt;br /&gt;
+ batching
&lt;br /&gt;
+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;113
   &lt;/td&gt;
   &lt;td&gt;213
   &lt;/td&gt;
   &lt;td&gt;8021
   &lt;/td&gt;
   &lt;td&gt;0.998
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ furious
   &lt;/td&gt;
   &lt;td&gt;36
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;4222
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;aotinductors-aoti-ahead-of-time-compilation-via-torchexport&quot;&gt;AOTInductor’s (AOTI) ahead-of-time compilation via torch.export&lt;/h3&gt;

&lt;p&gt;When scaling elastically it often is not possible to accommodate long startup times. That means the first iteration cannot be slow, but we must quickly deliver results. This is when torch.compile’s current compilation overhead can get in the way. To address this we can use AOTInductor’s (AOTI) ahead-of-time compilation via torch.export. AOTI lets us compile the model on a representative input and store the resulting code in a binary that is quick to load and run.&lt;/p&gt;

&lt;p&gt;AOTI via torch.export is a new feature and we currently can’t export everything that is compilable. We’ve been able to export the image encoder for all tasks but have only been able to export the mask prediction for the AMG and SPS tasks due to varying prompts. torch.export also supports dynamic shapes, but we need to invest a bit more time to prepare the code for it.&lt;/p&gt;

&lt;p&gt;AMG: AO + batching + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / 
&lt;br /&gt;
fail count
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;165
   &lt;/td&gt;
   &lt;td&gt;240
   &lt;/td&gt;
   &lt;td&gt;28335
   &lt;/td&gt;
   &lt;td&gt;0.978 / 306
   &lt;/td&gt;
   &lt;td&gt;10341
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;162
   &lt;/td&gt;
   &lt;td&gt;233
   &lt;/td&gt;
   &lt;td&gt;27927
   &lt;/td&gt;
   &lt;td&gt;0.974 / 308
   &lt;/td&gt;
   &lt;td&gt;906
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;SPS: AO + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;63
   &lt;/td&gt;
   &lt;td&gt;861
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
   &lt;td&gt;7989
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;35
   &lt;/td&gt;
   &lt;td&gt;66
   &lt;/td&gt;
   &lt;td&gt;1686
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
   &lt;td&gt;763
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Note that loading the exported model significantly increases memory. It likely only increases peak memory utilization, because initialization really needs to be delayed before loading up an exported model to avoid having twice the weights in memory at once. This is something we could address, but the memory consumption is nowhere near the limit. We don’t see an increase in the other tasks, because AMG and MPS peak memory is dominated by processing batches of masks. One way to reduce that could be to operate on masks in the rle format (or some other sparse format) earlier on, but for now, there is no reason for this given the current memory consumption and focus on latency.&lt;/p&gt;

&lt;p&gt;MPS: AO + batching + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ compile (warm)
   &lt;/td&gt;
   &lt;td&gt;36
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;4222
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
   &lt;td&gt;9626
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;43
   &lt;/td&gt;
   &lt;td&gt;72
   &lt;/td&gt;
   &lt;td&gt;3813
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
   &lt;td&gt;747
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Using export by itself doesn’t seem to benefit from extensive warmup and can be run in a pristine new inductor cache directory. But again, we do not evict the CUDA cache or other caches. In the section on Modal, we are running some of these experiments in a pristine environment.&lt;/p&gt;

&lt;p&gt;When only processing 1000 images in a new process, using export can really be worth it to save out on compile and other cold start overhead.&lt;/p&gt;

&lt;h3 id=&quot;bonus-more-gpu-preprocessing&quot;&gt;bonus: More GPU preprocessing&lt;/h3&gt;

&lt;p&gt;At this point, the latency is fairly low. In particular, for the SPS and MPS tasks we are processing at around 30ms to 40ms. Let’s bring back the pseudo-code from the setup section again.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;image_tensors = decode_img_bytes(...)
masks = gen_masks(image_tensors, ...)
rle_dicts = [rle_dict_from_masks(m) for m in masks]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Further profiling showed that at this point &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decode_img_bytes&lt;/code&gt; takes about 10ms. In particular, it uses torchvision’s ToTensor transform to convert from a numpy Tensor to a scaled, float32 torch.Tensor. The bytes passed to ToTensor have already been decoded and converted to an numpy ndarray. By slightly rewriting ToTensor, using torchvision’s v2 API and moving the uint8 decoded smaller integer Tensor to GPU first before scaling, we can gain another 10ms in latency. Without including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decode_img_bytes&lt;/code&gt; in our analysis we would have missed this opportunity that has real-world impact on server-side inference.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;image_tensor = torch.from_numpy(image_tensor)
image_tensor = image_tensor.permute((2, 0, 1))
image_tensor = image_tensor.cuda()
image_tensor = v2.ToDtype(torch.float32, scale=True)( image_tensor)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note in particular that using pinned memory to perform asynchronous data transfers doesn’t apply, since the time it takes to move the Tensor into pinned memory isn’t worth the gain in asynchronicity for this data movement. For future work, we might want to explore further improvements here by using more advanced direct memory transfer techniques.&lt;/p&gt;

&lt;p&gt;AMG: AO + batching + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU / 
&lt;br /&gt;
fail count
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;162
   &lt;/td&gt;
   &lt;td&gt;233
   &lt;/td&gt;
   &lt;td&gt;27927
   &lt;/td&gt;
   &lt;td&gt;0.974 / 308
   &lt;/td&gt;
   &lt;td&gt;906
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
   &lt;/td&gt;
   &lt;td&gt;157
   &lt;/td&gt;
   &lt;td&gt;230
   &lt;/td&gt;
   &lt;td&gt;27927
   &lt;/td&gt;
   &lt;td&gt;0.974 / 308
   &lt;/td&gt;
   &lt;td&gt;799
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
&lt;br /&gt;
+ preproc
   &lt;/td&gt;
   &lt;td&gt;136
   &lt;/td&gt;
   &lt;td&gt;208
   &lt;/td&gt;
   &lt;td&gt;27950
   &lt;/td&gt;
   &lt;td&gt;0.977 / 311
   &lt;/td&gt;
   &lt;td&gt;908
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;SPS: AO + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;35
   &lt;/td&gt;
   &lt;td&gt;66
   &lt;/td&gt;
   &lt;td&gt;1686
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
   &lt;td&gt;763
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
   &lt;/td&gt;
   &lt;td&gt;31
   &lt;/td&gt;
   &lt;td&gt;63
   &lt;/td&gt;
   &lt;td&gt;1686
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
   &lt;td&gt;683
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
&lt;br /&gt;
+ preproc
   &lt;/td&gt;
   &lt;td&gt;19
   &lt;/td&gt;
   &lt;td&gt;25
   &lt;/td&gt;
   &lt;td&gt;1711
   &lt;/td&gt;
   &lt;td&gt;0.9997
   &lt;/td&gt;
   &lt;td&gt;658
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;MPS: AO + batching + furious&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;p50 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;p90 latency (ms)
   &lt;/td&gt;
   &lt;td&gt;memory (MiB)
   &lt;/td&gt;
   &lt;td&gt;mIoU
   &lt;/td&gt;
   &lt;td&gt;first iteration
&lt;br /&gt;
(ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export
&lt;br /&gt;
(cold)
   &lt;/td&gt;
   &lt;td&gt;43
   &lt;/td&gt;
   &lt;td&gt;72
   &lt;/td&gt;
   &lt;td&gt;3813
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
   &lt;td&gt;747
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
   &lt;/td&gt;
   &lt;td&gt;53
   &lt;/td&gt;
   &lt;td&gt;81
   &lt;/td&gt;
   &lt;td&gt;3813
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
   &lt;td&gt;807
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;+ load export (warm)
&lt;br /&gt;
+ preproc
   &lt;/td&gt;
   &lt;td&gt;31
   &lt;/td&gt;
   &lt;td&gt;41
   &lt;/td&gt;
   &lt;td&gt;3837
   &lt;/td&gt;
   &lt;td&gt;0.997
   &lt;/td&gt;
   &lt;td&gt;671
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This small change has a significant impact on the SPS and MPS task.&lt;/p&gt;

&lt;h2 id=&quot;deploying-on-modal&quot;&gt;Deploying on Modal&lt;/h2&gt;

&lt;p&gt;Finally, we deployed our optimized inference onto &lt;a href=&quot;https://modal.com&quot;&gt;Modal&lt;/a&gt;, a serverless infrastructure provider, to demonstrate that the benefits of these optimizations can be realized in a more realistic deployment setting.&lt;/p&gt;

&lt;p&gt;In particular, compilation and AOTI via torch.export requires extra work. In a naïve deployment that work might be added to every single inference execution, adding latency that dwarfs any improvements from a faster model. This is particularly challenging with elastic or autoscaling infrastructure, where replicas of our inference service need to be regularly and automatically created and destroyed.&lt;/p&gt;

&lt;p&gt;We share a deployment script in the torchao repository (&lt;a href=&quot;https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server&quot;&gt;cli_on_modal.py&lt;/a&gt;) to demonstrate one pattern for an elastic deployment. We build the exported models ahead of time and then upload them to &lt;a href=&quot;https://modal.com/docs/guide/volumes&quot;&gt;distributed storage&lt;/a&gt;. Relative to eager execution, this adds a bit of extra work when replicas spin up since they need to read this data over a network, but this is far less costly than compilation or export.&lt;/p&gt;

&lt;p&gt;We benchmarked this deployment with a large batch inference workload: sending 1000 images for concurrent processing. The deployment scales up to ten replicas on ten GPUs at peak and scales down to zero GPUs when inactive.&lt;/p&gt;

&lt;p&gt;First, let’s look at the execution latencies.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;p50 execution latency
&lt;br /&gt;
(ms / improvement)
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;p90 execution latency
&lt;br /&gt;
(ms / improvement)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;AOTI float16
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;AOTI float16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Modal
   &lt;/td&gt;
   &lt;td&gt;Offline
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Modal
   &lt;/td&gt;
   &lt;td&gt;Offline
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AMG
   &lt;/td&gt;
   &lt;td&gt;741
   &lt;/td&gt;
   &lt;td&gt;112 (6.6x)
   &lt;/td&gt;
   &lt;td&gt;136 (5.4x)
   &lt;/td&gt;
   &lt;td&gt;1140
   &lt;/td&gt;
   &lt;td&gt;176 (6.5x)
   &lt;/td&gt;
   &lt;td&gt;208 (5.5x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;SPS
   &lt;/td&gt;
   &lt;td&gt;98
   &lt;/td&gt;
   &lt;td&gt;20 (4.9x)
   &lt;/td&gt;
   &lt;td&gt;19 (5.2x)
   &lt;/td&gt;
   &lt;td&gt;130
   &lt;/td&gt;
   &lt;td&gt;28 (4.6x)
   &lt;/td&gt;
   &lt;td&gt;25 (5.2x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MPS
   &lt;/td&gt;
   &lt;td&gt;269
   &lt;/td&gt;
   &lt;td&gt;38 (7.1x)
   &lt;/td&gt;
   &lt;td&gt;31 (8.7x)
   &lt;/td&gt;
   &lt;td&gt;714
   &lt;/td&gt;
   &lt;td&gt;52 (13.7x)
   &lt;/td&gt;
   &lt;td&gt;41 (17.4x)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We notice that execution latencies on Modal and Offline are fairly close, especially relative to the baseline, indicating that optimizing the deployment offline was a reasonable proxy for optimizing the deployment directly.&lt;/p&gt;

&lt;p&gt;In addition to execution latency, our batch workload has queueing time, since there are fewer replicas than there are inputs, and so some inputs have to wait in line.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p50 queue time (ms)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;p90 queue time (ms)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
   &lt;td&gt;eager float32
   &lt;/td&gt;
   &lt;td&gt;AOTI float16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AMG
   &lt;/td&gt;
   &lt;td&gt;201
   &lt;/td&gt;
   &lt;td&gt;41 (4.9x)
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;327 (2.6x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;SPS
   &lt;/td&gt;
   &lt;td&gt;31
   &lt;/td&gt;
   &lt;td&gt;33 (0.9x)
   &lt;/td&gt;
   &lt;td&gt;441
   &lt;/td&gt;
   &lt;td&gt;49 (9.0x)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MPS
   &lt;/td&gt;
   &lt;td&gt;40
   &lt;/td&gt;
   &lt;td&gt;37 (1.1x)
   &lt;/td&gt;
   &lt;td&gt;942
   &lt;/td&gt;
   &lt;td&gt;75 (12.6x)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Even though the queueing system provided by the infrastructure is unchanged, the queue latencies also decrease when we use our optimized model – in the p90 case by a factor of 2 to 12. That’s because when we finish previous inputs faster (from reduced execution latency) we can pull our next inputs sooner (reducing their queueing time).&lt;/p&gt;

&lt;p&gt;If you’re interested in optimizing SAM2 inference or deployments further, don’t hesitate to reach out to us at the &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao repository&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We rewrote Meta’s original SAM2 in pure PyTorch with little loss of accuracy and a strong focus on latency. We deployed our optimized inference onto &lt;a href=&quot;https://modal.com&quot;&gt;Modal&lt;/a&gt;, a serverless infrastructure provider, to demonstrate that the benefits of these optimizations can be realized in a more realistic deployment setting.&lt;/p&gt;

&lt;p&gt;By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, reduced precision, batched prompts and GPU preprocessing we observe up to 13x improvement in p90 execution latency and queue times compared to regular eager mode PyTorch.&lt;/p&gt;

&lt;p&gt;With elastic or autoscaling infrastructure, where replicas of our inference service need to be regularly and automatically created and destroyed, a naïve deployment of torch.compile can add work to inference execution that dwarfs any improvements from a faster model. By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, we are able to upload exported models ahead of time and read this data over a network, which enables us to get the benefits of compilation without significantly increased work.&lt;/p&gt;

&lt;p&gt;For more details on how to reproduce the data in this blog post, &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server&quot;&gt;check out the experiments folder of torchao&lt;/a&gt;. Please don’t hesitate to contact us or &lt;a href=&quot;https://github.com/pytorch/ao/issues/new&quot;&gt;open an issue&lt;/a&gt; if you run into any technical issues.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is a follow-up to our first entry in the multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Optimize LLMs for Efficiency &amp;amp; Sustainability</title>
      <link href="https://pytorch.org/blog/optimize-llms/" rel="alternate" type="text/html" title="Optimize LLMs for Efficiency &amp; Sustainability" />
      <published>2025-02-19T00:00:00-08:00</published>
      <updated>2025-02-19T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/optimize-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimize-llms/">&lt;p&gt;The rapid growth of large language model (LLM) applications is linked to rapid growth in energy demand. According to the International Energy Agency (IEA), data center electricity consumption is projected to roughly double by 2026 primarily driven by AI. This is due to the energy-intensive training requirements for massive LLMs – however, the increase in AI Inferencing workloads also plays a role. For example, compared with traditional search queries, a single AI inference can consume about &lt;a href=&quot;https://www.weforum.org/stories/2024/07/generative-ai-energy-emissions/&quot;&gt;10x more energy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As developers, we directly affect how energy-intensive our AI solution is. There are technical decisions we can take to help make our AI solution more environmentally sustainable. Minimizing compute to deliver LLM solutions is not the only requirement for creating sustainable AI use. For example, systemic changes, such as policy interventions may be needed, but utilizing energy efficient solutions is an important factor and is an impactful intervention we can adopt right away.&lt;/p&gt;

&lt;p&gt;With that said, minimizing your LLM inference cloud compute requirements also leads to reducing your cloud bill and makes your app more energy efficient, creating a win-win situation. In this blog, we will take you through the steps to creating an LLM chatbot by optimizing and deploying a Llama 3.1 model on PyTorch, quantifying the computational efficiency benefits of specific architecture decisions.&lt;/p&gt;

&lt;h2 id=&quot;what-will-we-evaluate&quot;&gt;What will we evaluate?&lt;/h2&gt;

&lt;p&gt;For this blog, our goal is to create an immersive fantasy storytelling app where users enter a fantasy world by chatting with a Generative AI. The first location is the land of Wicked, allowing people to role-play walking around the Emerald City and observe the sights and scenes in real-time. We’ll implement this via a chatbot and a custom system prompt.&lt;/p&gt;

&lt;p&gt;We will be evaluating LLM performance on CPUs. You can see the advantages of&lt;a href=&quot;https://www.arm.com/resources/ebook/cpu-inference&quot;&gt; CPU vs GPU inference here&lt;/a&gt;. In general, leveraging CPUs in the cloud for LLM inference is a great choice for models around 10B parameters or less like the Llama series.&lt;/p&gt;

&lt;p&gt;We will also be using Arm-based CPUs, specifically the AWS Graviton series. Based on studies,&lt;a href=&quot;https://newsroom.arm.com/blog/aws-graviton-decarbonize-compute&quot;&gt; the Arm-based Graviton3 server can provide 67.6 percent lower workload carbon intensity built in&lt;/a&gt;. While this study was based on a simulation, it is an excellent start to showing the possibilities for minimizing our app’s energy requirements.&lt;/p&gt;

&lt;p&gt;First, you’ll see how to run a simple LLM chatbot on PyTorch, then explore three techniques to optimize your application for computational efficiency:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Model optimization: Utilizing 4-bit quantization and added KleidiAI kernels.&lt;/li&gt;
  &lt;li&gt;Shortcut optimization: Implementing a vector database to handle common queries.&lt;/li&gt;
  &lt;li&gt;Architecture optimization: Adopting a serverless architecture.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;run-llama-31-via-pytorch-on-aws-graviton4&quot;&gt;Run Llama-3.1 via PyTorch on AWS Graviton4&lt;/h2&gt;

&lt;p&gt;To maximize energy efficiency, we will only use the minimum server resources needed to support this LLM chatbot. For this &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-3.1-8B&quot;&gt;Llama-3.1 8-billion parameter model&lt;/a&gt;, 16 cores, 64GB RAM, and disk space of 50GB is required. We will use the r8g.4xlarge Graviton4 instance running Ubuntu 24.04, as it meets these specifications.&lt;/p&gt;

&lt;p&gt;Spin up this EC2 instance, connect to it, and start installing the requirements:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    sudo apt-get update
    sudo apt install gcc g++ build-essential python3-pip python3-venv google-perftools -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then install Torchchat, the library developed by the PyTorch team that enables running LLMs across devices:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    git clone https://github.com/pytorch/torchchat.git
    cd torchchat
    python3 -m venv .venv
    source .venv/bin/activate
    ./install/install_requirements.sh 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, install the Llama-3.1-8b model from Hugging Face through the CLI. You will first need to make a Hugging Face access token on your HF account. This will download the 16GB model to your instance, which may take a few minutes:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip install -U &quot;huggingface_hub[cli]&quot;
    huggingface-cli login
    	&amp;lt;enter your access token when prompted&amp;gt;
    python torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so --device cpu --max-seq-length 1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you are ready to run the LLM model, adding a system prompt to be a guiding storyteller in the land of Wicked:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libtcmalloc.so.4 TORCHINDUCTOR_CPP_WRAPPER=1 TORCHINDUCTOR_FREEZING=1 OMP_NUM_THREADS=16 python torchchat.py generate llama3.1 --device cpu --chat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Type ‘y’ to enter a system prompt and enter the following prompt:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are the guiding storyteller for a fantasy adventure application. Immerse users in the enchanting world of Wicked, guiding them through interactive, real-time experiences in the Emerald City. Describe vivid sights, dynamic scenes, and engage users in storytelling that feels alive and responsive. Allow users to make choices that shape their journey while maintaining the magical tone of the Wicked universe.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Then enter your user query:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I walk through the Emerald City gates and look up&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The output will show on the screen, taking about 7 seconds to generate the first token with less than 1 token per second.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimize-llms.png&quot; alt=&quot;terminal&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This example took 245 seconds, or 4 minutes, to generate its complete reply—not very fast. The first optimization we’ll look at will speed up the LLM generation, reducing its computational footprint.&lt;/p&gt;

&lt;h3 id=&quot;optimization-1-kleidiai-and-quantization&quot;&gt;Optimization 1: KleidiAI and Quantization&lt;/h3&gt;

&lt;p&gt;Several optimizations are possible from the basic implementation above. The simplest and quickest one t to do is to quantize the model from FP16 to INT4. This approach trades-off some accuracy while cutting the model size from 16Gb to about 4Gb, increasing the inference speed in the process.&lt;/p&gt;

&lt;p&gt;Another common optimization comes in leveraging TorchAO (Torch Architecture Optimization), the PyTorch library that works seamlessly with TorchChat to enhance model performance through various quantization and sparsity methods.&lt;/p&gt;

&lt;p&gt;Lastly, we’ll use Arm KleidiAI optimizations. These are micro-kernels written in assembly that lead to significant performance improvements for LLM inference on Arm CPUs. You can read more about &lt;a href=&quot;https://learn.arm.com/learning-paths/cross-platform/kleidiai-explainer/&quot;&gt;how KleidiAI kernels work if interested&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To implement these optimizations, spin up a fresh EC2 instance and follow the instructions &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/pytorch-llama/&quot;&gt;on how to run a Large Language Model (LLM) chatbot with PyTorch&lt;/a&gt;. When ready, run the model and enter the same system prompt and user query as above. You’ll get results that significantly speed up the inference: Less than 1 second to first token, and about 25 tokens per second.&lt;/p&gt;

&lt;p&gt;This cuts the inference time from 245 seconds to about 10 seconds. This results in less power-draw from your server, as it is spending more time idle vs running a power-hungry inference. All else being equal, this is a more carbon-friendly solution than the non-optimized app. The next two approaches go beyond model inference optimization, modifying the solution architectural to further reduce computational load.&lt;/p&gt;

&lt;h3 id=&quot;optimization-2-faiss-to-match-database-for-common-questions&quot;&gt;Optimization 2: FAISS to match database for common questions&lt;/h3&gt;

&lt;p&gt;As stated in the introduction, model inferences are typically more computationally expensive than other search techniques. What if you could automatically respond to common user queries without performing an LLM inference? Using a query/response database is an option to bypass LLM inference and respond efficiently. For this interactive storytelling app, you can imagine common questions about specific characters, the world itself, and rules about what the chatbot is/is not capable of that can have pre-generated answers.&lt;/p&gt;

&lt;p&gt;However, a traditional exact-match database isn’t sufficient as users can phrase the same query in many ways. Asking about the chatbot’s capabilities could all invite the same answer but be phrased differently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“What are you capable of?”&lt;/li&gt;
  &lt;li&gt;“Tell me what you can do.”&lt;/li&gt;
  &lt;li&gt;“How can I interact with you?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implementing semantic search solves this issue by matching a user’s query to the most relevant pre-generated answer by understanding the user’s intent. The &lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;FAISS library&lt;/a&gt; is a great option to implement semantic search.&lt;/p&gt;

&lt;p&gt;The computational savings of this approach depends on three factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Percentage of user queries that can be serviced by semantic search instead of LLM.&lt;/li&gt;
  &lt;li&gt;Computational cost of running the LLM inference.&lt;/li&gt;
  &lt;li&gt;Computational cost of running the semantic search.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With the savings equation being:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Computational_savings = (% of queries) * (LLM_cost – search_cost).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This type of architecture makes sense in a few situations. One is if your system has common queries with many repeat questions. Another is large-scale systems with hundreds of thousands of incoming queries, where small percentage savings add up to meaningful changes. Lastly, if your LLM inference is very computationally expensive compared to the search cost, particularly with larger parameter models.&lt;/p&gt;

&lt;p&gt;The final optimization approach is transitioning from server to serverless.&lt;/p&gt;

&lt;h3 id=&quot;optimization-3-serverless-approach&quot;&gt;Optimization 3: Serverless approach&lt;/h3&gt;

&lt;p&gt;Using serverless architectures are popular for many reasons, one being only paying for active compute time, and eliminating costs with idle servers. Idling servers require a non-trivial amount of power to keep on, wasting energy while waiting.&lt;/p&gt;

&lt;p&gt;This cost efficiency translates into being an inherently more environmentally friendly architecture, as it reduces wasteful energy consumption. Further, multiple applications share underlying physical infrastructure, improving resource efficiency.&lt;/p&gt;

&lt;p&gt;To set up your own serverless chatbot, you need to first containerize the quantized Llama-3.1-8b with TorchChat, TorchAO, and Arm KleidiAI optimizations with a python script containing a Lambda entry function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lambda_handler&lt;/code&gt;. One deployment option is to upload your container to AWS ECR and attach the container to your Lambda function. Then set up an API Gateway WebSocket or similar to interact with your Lambda through an API.&lt;/p&gt;

&lt;p&gt;There are two notable limitations to using a serverless architecture to host your LLM, the first being token generation speed. Recall that the server-based approach delivered about 25 tokens/second with KleidiAI optimizations. The serverless approach delivers an order of magnitude slower, which we measured at around about 2.5 tokens/second. This limitation mainly results from Lambda functions deploying onto Graviton2 servers. When deployment moves to CPUs with more SIMD channels, like Graviton3 and Graviton4, the tokens/second should increase over time. Learn more about architecture optimizations introduced in Graviton3 via the &lt;a href=&quot;https://developer.arm.com/Processors/Neoverse%20V1&quot;&gt;Arm Neoverse-V1 CPU here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This slower speed restricts the viable use cases for serverless LLM architectures, but there are certain cases where this can be seen as an advantage. In our use cases of interactive storytelling, slowly revealing information creates a sense of immersion, building anticipation and mimicking real-time narration. Other use cases include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Guided meditation apps with slow, relaxing word delivery&lt;/li&gt;
  &lt;li&gt;Virtual friend engaging in thoughtful conversation, or a therapeutic conversation.&lt;/li&gt;
  &lt;li&gt;Poetry generation or interactive art to slow delivery creating a contemplative aesthetic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users may have a better experience with slower token generation in the right applications. When prioritizing a more sustainable solution, restrictions end up becoming strengths. As an analogy, a common critique of modern movies today is that their overreliance on visual effects leads to fewer compelling storylines vs older movies. The cost restrictions of VFX meant older movies had to craft captivating dialog, leveraging skillful camera angles and character positioning to fully engage viewers. Similarly, focusing on sustainable AI architectures can lead to more engaging, immersive experiences when done thoughtfully.&lt;/p&gt;

&lt;p&gt;The second serverless limitation on LLM inferences is the cold-start time of about 50 seconds. If implemented poorly, a user waiting 50 seconds with no alternative will likely leave the app. You can turn this limitation into a feature in our Wicked-based experience with several design tricks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a “prologue experience” where you guide users through hard-coded questions and answers, priming them for where they will land in Emerald City and collecting input to shape their upcoming experience.&lt;/li&gt;
  &lt;li&gt;Make the waiting period a countdown timer, revealing hard-coded text snippets of the story or world-building. A character, like the wizard, could communicate with the user with fragmented lines to build suspense and prime the user into the right mindset.&lt;/li&gt;
  &lt;li&gt;Create an audio intro with music from the movie or musical, along with rotating visuals to draw users into the atmosphere of the Wicked world.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;thinking-outside-the-box&quot;&gt;Thinking outside the box&lt;/h3&gt;

&lt;p&gt;Implementing a sustainability-minded solution architecture includes and goes beyond optimizing your AI inferences. Understand how users will interact with your system, and right-size your implementation accordingly. Always optimizing for fast tokens per second or time to first token will hide opportunities for engaging features.&lt;/p&gt;

&lt;p&gt;With that said, you should be leveraging straightforward optimizations when possible. Using TorchAO and Arm KleidiAI micro-kernels are great ways to speed up your LLM chatbot. By combining creative solution architectures and optimizing where possible, you can build more sustainable LLM-based applications. Happy coding!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Zach Lasiuk, Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">The rapid growth of large language model (LLM) applications is linked to rapid growth in energy demand. According to the International Energy Agency (IEA), data center electricity consumption is projected to roughly double by 2026 primarily driven by AI. This is due to the energy-intensive training requirements for massive LLMs – however, the increase in AI Inferencing workloads also plays a role. For example, compared with traditional search queries, a single AI inference can consume about 10x more energy.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Solve Real-Word AI Challenges with PyTorch at Datathon 2025: DataOrbit</title>
      <link href="https://pytorch.org/blog/datathon-2025/" rel="alternate" type="text/html" title="Solve Real-Word AI Challenges with PyTorch at Datathon 2025: DataOrbit" />
      <published>2025-02-12T00:00:00-08:00</published>
      <updated>2025-02-12T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/datathon-2025</id>
      <content type="html" xml:base="https://pytorch.org/blog/datathon-2025/">&lt;p&gt;&lt;strong&gt;We’re excited to have PyTorch sponsor &lt;a href=&quot;https://dataorbit-2025.devpost.com/&quot;&gt;Datathon 2025: DataOrbit&lt;/a&gt;&lt;/strong&gt;, a place where students can collaborate with a team to solve problems using real-world datasets! This event, hosted by Data Science UCSB in collaboration with Gaucho Sports Analytics and ACM@UCSB, will take place on &lt;strong&gt;February 22–23rd, 2025 at UC Santa Barbara&lt;/strong&gt;, with the incredible opportunity to present your project to a panel of corporate and faculty judges – &lt;strong&gt;including the executive director of Pytorch!&lt;/strong&gt; – for a chance to win prizes up to $3000.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/datathon-2025.png&quot; alt=&quot;logo&quot; style=&quot;max-width:700px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch’s versatility and power have made it an essential tool for tackling complex data problems in domains ranging from computer vision and natural language processing to time series analysis. At Datathon 2025: DataOrbit, participants will have the chance to leverage PyTorch’s dynamic framework, ease of use, and robust ecosystem to build innovative solutions. Whether you’re building machine learning models, experimenting with deep learning architectures, or applying PyTorch to solve real-world challenges, workshops and mentors will be available to help you dive deeper into its capabilities and accelerate your project’s success.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Register Here:&lt;/strong&gt; &lt;a href=&quot;http://tinyurl.com/dataorbit25-reg&quot;&gt;tinyurl.com/dataorbit25-reg&lt;/a&gt; (Open until February 21st or until capacity is reached)&lt;/p&gt;

&lt;p&gt;Additional information regarding the timeline of events can be found on the registration form.&lt;/p&gt;

&lt;p&gt;About the Datathon&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open only to undergraduate students in the United States&lt;/li&gt;
  &lt;li&gt;In-person events over 36 hours&lt;/li&gt;
  &lt;li&gt;Teams sizes of 2-5 people&lt;/li&gt;
  &lt;li&gt;10 different prize tracks&lt;/li&gt;
  &lt;li&gt;Workshops and office hours teaching essential data science tools and techniques&lt;/li&gt;
  &lt;li&gt;Professional development workshops + networking opportunities with our sponsors&lt;/li&gt;
  &lt;li&gt;All meals provided&lt;/li&gt;
  &lt;li&gt;A fun time!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;If you have a group you would like to work with, we require that every member register separately. If you do not have a group, we will have an opportunity at the beginning of the event to participate in an activity to form groups. Unfortunately, at this time we do not provide travel accommodations or lodging for participants.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you are interested in mentoring students virtually during the course of our datathon, or have any other questions contact us at datascience.ucsb@gmail.com.&lt;/em&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Aakash Senthilnathan</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re excited to have PyTorch sponsor Datathon 2025: DataOrbit, a place where students can collaborate with a team to solve problems using real-world datasets! This event, hosted by Data Science UCSB in collaboration with Gaucho Sports Analytics and ACM@UCSB, will take place on February 22–23rd, 2025 at UC Santa Barbara, with the incredible opportunity to present your project to a panel of corporate and faculty judges – including the executive director of Pytorch! – for a chance to win prizes up to $3000.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Unlocking the Latest Features in PyTorch 2.6 for Intel Platforms</title>
      <link href="https://pytorch.org/blog/unlocking-pt-2-6-intel/" rel="alternate" type="text/html" title="Unlocking the Latest Features in PyTorch 2.6 for Intel Platforms" />
      <published>2025-02-11T00:00:00-08:00</published>
      <updated>2025-02-11T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/unlocking-pt-2-6-intel</id>
      <content type="html" xml:base="https://pytorch.org/blog/unlocking-pt-2-6-intel/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/pytorch2-6/&quot;&gt;PyTorch* 2.6&lt;/a&gt; has just been released with a set of exciting new features including torch.compile compatibility with Python 3.13, new security and performance enhancements, and a change in the default parameter for torch.load. PyTorch also announced the deprecation of its official Anaconda channel.&lt;/p&gt;

&lt;p&gt;Among the performance features are three that enhance developer productivity on Intel platforms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Improved Intel GPU availability&lt;/li&gt;
  &lt;li&gt;FlexAttention optimization on x86 CPU for LLM&lt;/li&gt;
  &lt;li&gt;FP16 on x86 CPU support for eager and Inductor modes&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;improved-intel-gpu-availability&quot;&gt;Improved Intel GPU Availability&lt;/h2&gt;

&lt;p&gt;To provide developers working in artificial intelligence (AI) with better support for Intel GPUs, the PyTorch user experience on these GPUs has been enhanced. This improvement includes simplified installation steps, a Windows* release binary distribution, and expanded coverage of supported GPU models, including the latest Intel® Arc™ B-Series discrete graphics.&lt;/p&gt;

&lt;p&gt;These new features help promote accelerated machine learning workflows within the PyTorch ecosystem, providing a consistent developer experience and support. Application developers and researchers seeking to fine-tune, perform inference, and develop with PyTorch models on &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Core™ Ultra AI PCs &lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Arc™ discrete graphics&lt;/a&gt; will now be able to install PyTorch directly with binary releases for Windows, Linux*, and Windows Subsystem for Linux 2.&lt;/p&gt;

&lt;p&gt;The new features include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simplified Intel GPU software stack setup to enable one-click installation of the torch-xpu PIP wheels to run deep learning workloads in a ready-to-use fashion, thus eliminating the complexity of installing and activating Intel GPU development software bundles. &lt;/li&gt;
  &lt;li&gt;Windows binary releases for torch core, torchvision and torchaudio have been made available for Intel GPUs, expanding from &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra Series 2&lt;/a&gt; with Intel® Arc™ Graphics and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html&quot;&gt;Intel® Arc™ A-Series graphics &lt;/a&gt;to the latest GPU hardware &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series graphics&lt;/a&gt; support. &lt;/li&gt;
  &lt;li&gt;Further enhanced coverage of Aten operators on Intel GPUs with SYCL* kernels for smooth eager mode execution, as well as bug fixes and performance optimizations for torch.compile on Intel GPUs. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Get a tour of new environment setup, PIP wheels installation, and examples on Intel® Client GPUs and Intel® Data Center GPU Max Series in the &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;flexattention-optimization-on-x86-cpu-for-llm&quot;&gt;FlexAttention Optimization on X86 CPU for LLM&lt;/h2&gt;

&lt;p&gt;FlexAttention was first introduced in &lt;a href=&quot;https://pytorch.org/blog/pytorch2-5/&quot;&gt;PyTorch 2.5&lt;/a&gt;, to address the need to support various Attentions or even combinations of them. This PyTorch API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations.&lt;/p&gt;

&lt;p&gt;Previously, FlexAttention was implemented for CUDA* devices based on the Triton backend. Since PyTorch 2.6, X86 CPU support of FlexAttention was added through TorchInductor CPP backend. This new feature leverages and extends current CPP template abilities to support broad attention variants (e.g., PageAttention, which is critical for LLMs inference) based on the existing FlexAttention API, and brings optimized performance on x86 CPUs. With this feature, user can easily use FlexAttention API to compose their Attention solutions on CPU platforms and achieve good performance.&lt;/p&gt;

&lt;p&gt;Typically, FlexAttention is utilized by popular LLM ecosystem projects, such as Hugging Face transformers and vLLM in their LLM related modeling (e.g., PagedAttention) to achieve better out-of-the-box performance. Before the official adoption happens, &lt;a href=&quot;https://github.com/huggingface/transformers/pull/35419&quot;&gt;this enabling PR&lt;/a&gt; in Hugging Face can help us the performance benefits that FlexAttention can bring on x86 CPU platforms.&lt;/p&gt;

&lt;p&gt;The graph below shows the performance comparison of PyTorch 2.6 (with this feature) and PyTorch 2.5 (without this feature) on typical Llama models. For real-time mode (Batch Size = 1), there is about 1.13x-1.42x performance improvement for next token across different input token lengths. As for best throughput under a typical SLA (P99 token latency &amp;lt;=50ms), PyTorch 2.6 achieves more than 7.83x performance than PyTorch 2.5 as PyTorch 2.6 can run at 8 inputs (Batch Size = 8) together and still keep SLA while PyTorch 2.5 can only run 1 input, because FlexAttention based PagedAttention in PyTorch 2.6 provides more efficiency during multiple batch size scenarios.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unlocking-pt-2-6-intel.png&quot; alt=&quot;Figure 1. Performance comparison of PyTorch 2.6 and PyTorch 2.5 on Typical Llama Models&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. Performance comparison of PyTorch 2.6 and PyTorch 2.5 on Typical Llama Models&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;fp16-on-x86-cpu-support-for-eager-and-inductor-modes&quot;&gt;FP16 on X86 CPU Support for Eager and Inductor Modes&lt;/h2&gt;

&lt;p&gt;Float16 is a commonly used reduced floating-point type that improves performance in neural network inference and training. CPUs like recently launched &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/xeon/xeon6-p-cores.html&quot;&gt;Intel® Xeon® 6 with P-Cores&lt;/a&gt; support Float16 datatype with native accelerator &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html&quot;&gt;AMX&lt;/a&gt;, which highly improves the Float16 performance. Float16 support on x86 CPU was first introduced in PyTorch 2.5 as a prototype feature. Now it has been further improved for both eager mode and Torch.compile + Inductor mode, which is pushed to Beta level for broader adoption. This helps the deployment on the CPU side without the need to modify the model weights when the model is pre-trained with mixed precision of Float16/Float32. On platforms that support AMX Float16 (i.e., the Intel® Xeon® 6 processors with P-cores), Float16 has the same pass rate as Bfloat16 across the typical PyTorch benchmark suites: TorchBench, Hugging Face, and Timms. It also shows good performance comparable to 16 bit datatype Bfloat16.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blog, we discussed three features to enhance developer productivity on Intel platforms in PyTorch 2.6.  These three features are designed to improve Intel GPU availability, optimize FlexAttention for x86 CPUs tailored for large language models (LLMs), and support FP16 on x86 CPUs in both eager and Inductor modes. Get &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch 2.6&lt;/a&gt; and try them for yourself or you can access PyTorch 2.6 on the &lt;a href=&quot;https://ai.cloud.intel.com/&quot;&gt;Intel® Tiber™ AI Cloud&lt;/a&gt; to take advantage of hosted notebooks that are optimized for Intel hardware and software.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The release of PyTorch 2.6 is an exciting milestone for Intel platforms, and it would not have been possible without the deep collaboration and contributions from the community. We extend our heartfelt thanks to &lt;a href=&quot;https://github.com/albanD&quot;&gt;Alban&lt;/a&gt;, &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey&lt;/a&gt;, &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason&lt;/a&gt;, &lt;a href=&quot;https://github.com/jerryzh168&quot;&gt;Jerry&lt;/a&gt; and &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita&lt;/a&gt; for sharing their invaluable ideas, meticulously reviewing PRs, and providing insightful feedback on RFCs. Their dedication has driven continuous improvements and pushed the ecosystem forward for Intel platforms.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention in PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;PagedAttention Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;•%09https:/www.intel.com/content/www/us/en/products/details/processors/xeon/xeon6-p-cores.html&quot;&gt;Intel® Xeon® 6 with P-Cores&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h2&gt;

&lt;p&gt;Measurement on AWS EC2 m7i.metal-48xl using: 2x Intel® Xeon® Platinum 8488C, HT On, Turbo On, NUMA 2, Integrated Accelerators Available [used]: DLB [8], DSA [8], IAA[8], QAT[on CPU, 8], Total Memory 512GB (16x32GB DDR5 4800 MT/s [4400 MT/s]), BIOS Amazon EC2 1.0, microcode 0x2b000603, 1x Elastic Network Adapter (ENA) 1x Amazon Elastic Block Store 800G, Ubuntu 24.04.1 LTS 6.8.0-1018-aws  Test by Intel on Jan 15&lt;sup&gt;th&lt;/sup&gt; 2025.&lt;/p&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;ai-disclaimer&quot;&gt;AI disclaimer:&lt;/h2&gt;

&lt;p&gt;AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the Intel PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch* 2.6 has just been released with a set of exciting new features including torch.compile compatibility with Python 3.13, new security and performance enhancements, and a change in the default parameter for torch.load. PyTorch also announced the deprecation of its official Anaconda channel.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Enabling advanced GPU features in PyTorch - Warp Specialization</title>
      <link href="https://pytorch.org/blog/warp-specialization/" rel="alternate" type="text/html" title="Enabling advanced GPU features in PyTorch - Warp Specialization" />
      <published>2025-02-05T00:00:00-08:00</published>
      <updated>2025-02-05T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/warp-specialization</id>
      <content type="html" xml:base="https://pytorch.org/blog/warp-specialization/">&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;: Hongtao Yu, Manman Ren, Bert Maher, Shane Nay  &lt;br /&gt;
&lt;strong&gt;NVIDIA&lt;/strong&gt;: Gustav Zhu, Shuhao Jiang&lt;/p&gt;

&lt;p&gt;Over the past few months, we have been working on enabling advanced GPU features for PyTorch and Triton users through the Triton compiler. One of our key goals has been to introduce warp specialization support on NVIDIA Hopper GPUs. Today, we are thrilled to announce that our efforts have resulted in the rollout of fully automated Triton warp specialization, now available to users in the upcoming release of Triton &lt;a href=&quot;https://github.com/triton-lang/triton/tree/release/3.2.x&quot;&gt;3.2&lt;/a&gt;, which will ship with PyTorch 2.6. PyTorch users can leverage this feature by &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;implementing user-defined Triton kernels&lt;/a&gt;. This work leveraged an initial implementation of warp specialization in Triton by NVIDIA and we look forward to further development with the community in the future.&lt;/p&gt;

&lt;p&gt;Warp specialization (WS) is a GPU programming technique where warps (a group of 32 threads on NVIDIA GPUs) within a threadblock are assigned distinct roles or tasks. This approach optimizes performance by enabling efficient execution of workloads that require task differentiation or cooperative processing. It enhances kernel performance by leveraging an asynchronous execution model, where different parts of the kernel are managed by separate hardware units. Data communication between these units, facilitated via shared memory on the NVIDIA H100, is highly efficient. Compared to a uniform warp approach, warp specialization allows the hardware multitasking warp scheduler to operate more effectively, maximizing resource utilization and overall performance.&lt;/p&gt;

&lt;p&gt;Using GEMM as an example, a typical uniform warp approach on the H100 GPU involves 8 warps per thread block collectively computing a tile of the output tensor. These 8 warps are divided into two warp groups (WG), with each group cooperatively computing half of the tile using efficient warp-group-level MMA (WGMMA) instructions, as illustrated in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/warp-specialization/fg1.jpg&quot; alt=&quot;Figure 1. GEMM K-loop Body with Uniform Warps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1. GEMM K-loop Body with Uniform Warps&lt;/p&gt;

&lt;p&gt;The implementation is clean, easy to understand, and generally performs well, thanks to an elegant software pipeliner. The pipeliner’s purpose is to enhance instruction-level parallelism by executing non-dependent operations on different hardware units. For instance, load operations from the next loop iteration can be executed simultaneously with WGMMA operations in the current iteration. However, this approach relies heavily on the compiler to craft an instruction sequence that ensures load and WGMMA instructions are issued at precisely the right time. While this is relatively straightforward for GEMM, which involves a limited number of operations, it becomes significantly more challenging for more complex kernels, such as flash attention.&lt;/p&gt;

&lt;p&gt;On the other hand, warp specialization addresses programming challenges by separating operations intended to run simultaneously on different hardware units into distinct warps, synchronizing them efficiently using low-cost barriers in shared memory. This allows each warp to have its own instruction sequence, enabling instructions to be issued and executed continuously without being interrupted by other operations, thanks to the multi-way warp scheduler. An illustration of a warp-specialized GEMM can be seen in Figure 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/warp-specialization/fg2.jpg&quot; alt=&quot;Figure 2. GEMM K-loop Body with Specialized Warps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2. GEMM K-loop Body with Specialized Warps&lt;/p&gt;

&lt;h2 id=&quot;how-to-enable-ws&quot;&gt;How to enable WS&lt;/h2&gt;

&lt;p&gt;To enable warp specialization, users simply need to specify two autotune flags: num_consumer_groups and num_buffers_warp_spec. For example, a warp-specialized GEMM implementation might look as shown below.  Users can enable warp specialization by setting a non-zero value for num_consumer_groups, which defines the number of consumer warp groups. There is no corresponding flag to set the number of producer warp groups, as currently only one producer is supported. The num_buffers_warp_spec flag specifies the number of buffers the producer warp group will use to communicate with the consumer warp groups. A working example of a warp-specialized kernel is provided in the persistent GEMM &lt;a href=&quot;https://github.com/triton-lang/triton/blob/6771065cb3137f7e64454cc047b9b74d577cbf7f/python/tutorials/09-persistent-matmul.py#L620&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@triton.autotune(
    configs=[
        triton.Config(
            {
                &quot;BLOCK_SIZE_M&quot;: 128,
                &quot;BLOCK_SIZE_N&quot;: 256,
                &quot;BLOCK_SIZE_K&quot;: 64,
                &quot;GROUP_SIZE_M&quot;: 8,
            },
            num_stages=2,
            num_warps=4,
            num_consumer_groups=2,
            num_buffers_warp_spec=3,
        ),
    ],
    key=[&quot;M&quot;, &quot;N&quot;, &quot;K&quot;],
)
@triton.jit
def matmul_persistent_ws_kernel(
   a_ptr, b_ptr, c_ptr, M, N, K,
   stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,
   BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
   pid = tl.program_id(axis=0)
   num_pid_m = tl.cdiv(M, BLOCK_M)
   num_pid_n = tl.cdiv(N, BLOCK_N)
   pid_m = pid // num_pid_m
   pid_n = pid % num_pid_n
   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
   offs_k = tl.arange(0, BLOCK_K)
   a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)
   b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)
   acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
   for k in range(0, tl.cdiv(K, BLOCK_K)):
       a = tl.load(a_ptrs)
       b = tl.load(b_ptrs)
       acc += tl.dot(a, b)
       a_ptrs += BLOCK_K * stride_ak
       b_ptrs += BLOCK_K * stride_bk
   c = acc.to(tl.float16)
   c_ptrs = c_ptr + stride_cm * offs_m[:, None] + stride_cn * offs_n[None, :]
   tl.store(c_ptrs, c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;Warp specialization uses a set of hierarchical compiler transformations and IR changes to translate a user’s non-warp-specialized kernel into warp-specialized machine code. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Task Partitioning&lt;/strong&gt;: The entire kernel is automatically divided into asynchronous tasks based on predefined heuristics. The compiler determines how to utilize one producer warp group and a user-specified number of consumer warp groups to execute the kernel. It assigns task IDs to specific anchor operations, which then influence the task assignments for remaining operations through asynchronous task ID propagation and dependency analysis. Since shared memory is the most efficient method for data transfer between warp groups across all supported platforms, the compiler optimizes task partitions to minimize register spills to shared memory, ensuring efficient execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Partitioning for Multiple Consumer Groups&lt;/strong&gt;: Efficiently partitioning data among multiple consumer groups is key to optimizing workload distribution. On the H100 GPU, the compiler, by default, attempts to partition the input tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt; dimension, allowing each consumer group to compute half of the output tensor independently. This strategy, known as &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization&quot;&gt;cooperative partitioning&lt;/a&gt;, maximizes efficiency under most conditions. However, if this split leads to inefficiencies—such as producing a workload smaller than the native WGMMA instruction size—the compiler dynamically adjusts and partitions along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; dimension instead.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dataflow Pipelining&lt;/strong&gt;: The compiler creates cyclic shared memory buffers to pipeline dataflows across multiple-dimensional loops. Warp-specialized pipelining supports complex control flow. For example, our warp-specialized persistent GEMM kernel uses a doubly-nested loop, allowing the producer to begin fetching data for the next output tile while the consumer is finishing the compute for the prior tile.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication Operations&lt;/strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;: &lt;/code&gt;We introduced four high-level Triton GPU IR (TTGIR) communication operations&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;—ProducerAcquireOp, ProducerCommitOp, ConsumerWaitOp, &lt;/code&gt;and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; ConsumerReleaseOp—&lt;/code&gt;to manage pipelined dataflows. These support both TMA and non-TMA memory operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code Partitioning&lt;/strong&gt;: Each async task is outlined into its own standalone code region, guarded by warp group ID checks. Control dependencies are duplicated as needed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TTGIR to LLVM/PTX Materialization&lt;/strong&gt;: TTGIR communication operations are materialized into corresponding LLVM/PTX barrier operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/triton-lang/triton/pull/5622&quot;&gt;warp specialization release&lt;/a&gt; introduces a range of Triton compiler transformations that collectively convert user code into warp-specialized kernels. This feature has been applied to several key kernels, including Flash Attention and FP8 row-wise GEMM, resulting in significant performance gains of 10% to 15%. Below, we highlight the latest performance metrics for these high-impact kernels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/warp-specialization/fg3.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/warp-specialization/fg4.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Looking ahead, we plan to further enhance Triton’s warp specialization support by introducing new features such as Ping-Pong scheduling, expanded buffer sharing support, improved transparent handling for TMA, refined partitioning heuristics for upcoming NVIDIA hardware.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta and NVIDIA</name>
        
        
      </author>

      

      

      
        <summary type="html">Meta: Hongtao Yu, Manman Ren, Bert Maher, Shane Nay NVIDIA: Gustav Zhu, Shuhao Jiang</summary>
      

      
      
    </entry>
  
</feed>



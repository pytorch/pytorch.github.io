<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-10-02T10:10:26-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">PyTorch Conference 2024 Recap: On Fire ðŸ”¥</title>
      <link href="https://pytorch.org/blog/pytorch-conference-2024-recap/" rel="alternate" type="text/html" title="PyTorch Conference 2024 Recap: On Fire ðŸ”¥" />
      <published>2024-10-02T00:00:00-07:00</published>
      <updated>2024-10-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-conference-2024-recap</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-conference-2024-recap/">&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018197476_9fce5b234d_k.jpg&quot; alt=&quot;women dancing with fire&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 2024 PyTorch Conference in San Francisco gathered nearly 1,500 AI researchers, developers, and enthusiasts. Over two days, the event featured engaging discussions, insightful keynotes, and hands-on sessions focused on artificial intelligence (AI) and advancements in PyTorch, the leading open-source machine learning framework. Attendees delved into the future of generative AI, Large Language Models (LLMs), and the crucial role open-source technology plays in driving AI innovation. Hereâ€™s a recap of the key themes, highlights, and major takeaways from this yearâ€™s conference.&lt;/p&gt;

&lt;h2 id=&quot;key-themes-of-the-pytorch-conference-2024&quot;&gt;Key Themes of the PyTorch Conference 2024&lt;/h2&gt;

&lt;p&gt;Three core themes emerged throughout the conference:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Generative AI and LLMs&lt;/strong&gt;: Many sessions focused on how PyTorch continues to evolve as a primary framework for Large Language Models and Generative AI applications. From scaling these models to optimizing their performance on various hardware platforms, the conference showcased the ongoing advancements and challenges in LLM architecture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Democratizing AI Through Open Source&lt;/strong&gt;: One of the recurring themes was the importance of open source tools and communities in shaping the future of AI. PyTorch is committed to inclusivity, ease of use, and accessibility to developers of all levels, with a focus on bringing AI to an even larger global audience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distributed and Edge Computing&lt;/strong&gt;: Distributed computing and edge deployment appeared in many discussions, highlighting how PyTorch is being used to drive AI to the edge. The focus on edge accelerators, scalable training, and inference showcased how PyTorch enables the deployment of powerful models across diverse environments, from the cloud to on-device applications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54017358432_8d9b53a2c8_k.jpg&quot; alt=&quot;panel of people on a conference stage&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;watch-the-sessions-from-pytorch-conference&quot;&gt;Watch the Sessions from PyTorch Conference&lt;/h2&gt;

&lt;p&gt;The PyTorch Conference featured keynote sessions from top AI leaders and interesting lightning talks. You can &lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l&amp;amp;feature=shared&quot;&gt;view all of the conference sessions&lt;/a&gt; on our YouTube channel.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/videoseries?si=qoVqnWpWR_LQOSt6&amp;amp;list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;style&gt;
.video-container {
    position: relative;
    width: 100%;
    padding-bottom: 56.25%; /* This maintains a 16:9 aspect ratio */
    height: 0;
    overflow: hidden;
}

.video-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
&lt;/style&gt;

&lt;h2 id=&quot;pytorch-conference-startup-showcase&quot;&gt;PyTorch Conference Startup Showcase&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018500933_4df67cbbd4_k.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;New this year, the Startup Showcase was an exciting addition to the PyTorch Conference. Featuring early-stage founders pitching their AI startups to a panel of top venture capitalists, this event showcased the next generation of AI-driven innovation. The finalists for the inaugural PyTorch Conference Startup Showcase included Remix Inc., Cartesia, OpenBabylon, Remyx AI, A2 Labs, Inc., QuicSnap, Iso AI, CTGT, and Creao.ai, representing some of the most innovative AI/ML startups in the industry. Attendees got a front-row seat to see cutting-edge AI startups in action, while top VCs from the AI industry evaluated the pitches.&lt;/p&gt;

&lt;p&gt;Congratulations to the PyTorch Conference Startup Showcase winner, CTGT!  Deep learning can be opaque and biased, which limits its potential in crucial areas like healthcare and finance. CTGT is changing the game by enhancing data lineage in LLMs and cutting hallucinations. Theyâ€™re empowering companies to create customized models using 500x less compute.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/xAePG2YVz7c?feature=shared&quot;&gt;View the Startup Showcase&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;mini-summits&quot;&gt;Mini-Summits&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;DL Compiler Mini-Summit&lt;/strong&gt; offered attendees a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2DyFOVyBzDS5scLfUotrG52&amp;amp;feature=shared&quot;&gt;View the DL Compiler Mini-Summit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54036162068_0afdec2ca6_k.jpg&quot; alt=&quot;People watching an event&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Fine-Tuning Mini-Summit&lt;/strong&gt; brought together a thriving community of researchers, developers, practitioners and hobbyists which focuses on topics ranging from memory efficiency, parameter-efficient fine-tuning and quantization to performance at scale and reproducible evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2D6l1brEg0DuDShep5p33nu&amp;amp;feature=shared&quot;&gt;View the Fine-Tuning Mini-Summit&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;major-takeaways-from-the-pytorch-conference-2024&quot;&gt;Major Takeaways from the PyTorch Conference 2024&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018555324_daae473637_k.jpg&quot; alt=&quot;Matt giving his keynote&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;LLMs are Here to Stay&lt;/strong&gt;: were a focal point of the event, reaffirming their pivotal role in the future of AI. As these models continue to scale, PyTorch remains the preferred framework for developing, training, and deploying them across various platforms and industries.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Open Source Drives Innovation&lt;/strong&gt;: A key takeaway from the conference was that open-source tools like PyTorch are vital for democratizing AI. This community-driven approach accelerates innovation, enabling researchers and developers globally to collaborate and contribute to faster advancements and more accessible AI technologies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ethics and Sustainability Matter&lt;/strong&gt;: The focus on ethical AI development was a significant takeaway. Talks on the inclusivity of computer vision models, the environmental impacts of AI infrastructure, and the need for transparent, unbiased AI models highlighted the growing importance of ethical considerations in the future of AI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Expands Beyond the Cloud&lt;/strong&gt;: With several sessions dedicated to edge AI and distributed computing, the conference showcased how PyTorch is expanding beyond cloud-based applications into edge devices and diverse computing environments. This shift is crucial as AI advances into areas like autonomous vehicles, mobile applications, and IoT devices.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;thank-you-to-our-sponsors&quot;&gt;Thank You to Our Sponsors&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54006027240_be489d89a3_k.jpg&quot; alt=&quot;A crowd of people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/sponsors.png&quot; alt=&quot;Sponsor logos&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We would like to thank each of the sponsors that made the PyTorch Conference 2024 possible. These include:&lt;/p&gt;

&lt;h3 id=&quot;diamond-sponsors&quot;&gt;Diamond Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AMD&lt;/li&gt;
  &lt;li&gt;Cloud Native Computing Foundation&lt;/li&gt;
  &lt;li&gt;IBM&lt;/li&gt;
  &lt;li&gt;Intel â€“ PyTorch&lt;/li&gt;
  &lt;li&gt;Lightning.ai&lt;/li&gt;
  &lt;li&gt;Meta â€“ PyTorch&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;platinum-sponsors&quot;&gt;Platinum Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Arm&lt;/li&gt;
  &lt;li&gt;Google&lt;/li&gt;
  &lt;li&gt;Lambda Labs&lt;/li&gt;
  &lt;li&gt;Nvidia&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;silver-sponsors&quot;&gt;Silver Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Anyscale â€“ PyTorch&lt;/li&gt;
  &lt;li&gt;Baseten&lt;/li&gt;
  &lt;li&gt;Chainguard&lt;/li&gt;
  &lt;li&gt;Databricks&lt;/li&gt;
  &lt;li&gt;Fal&lt;/li&gt;
  &lt;li&gt;FuriosaAi&lt;/li&gt;
  &lt;li&gt;HPE&lt;/li&gt;
  &lt;li&gt;Jane Street&lt;/li&gt;
  &lt;li&gt;Microsoft â€“ PyTorch&lt;/li&gt;
  &lt;li&gt;MinIO&lt;/li&gt;
  &lt;li&gt;Outerbounds&lt;/li&gt;
  &lt;li&gt;Together.AI&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bronze-sponsors&quot;&gt;Bronze Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;d-Matrix&lt;/li&gt;
  &lt;li&gt;MemVerge&lt;/li&gt;
  &lt;li&gt;Perforated AI&lt;/li&gt;
  &lt;li&gt;Quansight&lt;/li&gt;
  &lt;li&gt;Rotational Labs&lt;/li&gt;
  &lt;li&gt;ScaleGenAI&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;special-event-sponsors&quot;&gt;Special Event Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Flare Party: Hugging Face&lt;/li&gt;
  &lt;li&gt;Startup Showcase: Mayfield&lt;/li&gt;
  &lt;li&gt;Diversity Scholarship: AWS&lt;/li&gt;
  &lt;li&gt;Women and Non-Binary in PyTorch Lunch: Google&lt;/li&gt;
  &lt;li&gt;Happy Hour Reception: Lightning.AI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for your continued support in advancing the PyTorch ecosystem and helping to shape the future of AI!&lt;/p&gt;

&lt;h2 id=&quot;save-the-date&quot;&gt;Save the Date&lt;/h2&gt;

&lt;p&gt;See you next year for the &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference-2025/&quot;&gt;PyTorch Conference in San Francisco at the Palace of Fine Arts&lt;/a&gt; from October 22-23, 2025.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">The 2024 PyTorch Conference in San Francisco gathered nearly 1,500 AI researchers, developers, and enthusiasts. Over two days, the event featured engaging discussions, insightful keynotes, and hands-on sessions focused on artificial intelligence (AI) and advancements in PyTorch, the leading open-source machine learning framework. Attendees delved into the future of generative AI, Large Language Models (LLMs), and the crucial role open-source technology plays in driving AI innovation. Hereâ€™s a recap of the key themes, highlights, and major takeaways from this yearâ€™s conference. Key Themes of the PyTorch Conference 2024 Three core themes emerged throughout the conference: Generative AI and LLMs: Many sessions focused on how PyTorch continues to evolve as a primary framework for Large Language Models and Generative AI applications. From scaling these models to optimizing their performance on various hardware platforms, the conference showcased the ongoing advancements and challenges in LLM architecture. Democratizing AI Through Open Source: One of the recurring themes was the importance of open source tools and communities in shaping the future of AI. PyTorch is committed to inclusivity, ease of use, and accessibility to developers of all levels, with a focus on bringing AI to an even larger global audience. Distributed and Edge Computing: Distributed computing and edge deployment appeared in many discussions, highlighting how PyTorch is being used to drive AI to the edge. The focus on edge accelerators, scalable training, and inference showcased how PyTorch enables the deployment of powerful models across diverse environments, from the cloud to on-device applications. Watch the Sessions from PyTorch Conference The PyTorch Conference featured keynote sessions from top AI leaders and interesting lightning talks. You can view all of the conference sessions on our YouTube channel. PyTorch Conference Startup Showcase New this year, the Startup Showcase was an exciting addition to the PyTorch Conference. Featuring early-stage founders pitching their AI startups to a panel of top venture capitalists, this event showcased the next generation of AI-driven innovation. The finalists for the inaugural PyTorch Conference Startup Showcase included Remix Inc., Cartesia, OpenBabylon, Remyx AI, A2 Labs, Inc., QuicSnap, Iso AI, CTGT, and Creao.ai, representing some of the most innovative AI/ML startups in the industry. Attendees got a front-row seat to see cutting-edge AI startups in action, while top VCs from the AI industry evaluated the pitches. Congratulations to the PyTorch Conference Startup Showcase winner, CTGT! Deep learning can be opaque and biased, which limits its potential in crucial areas like healthcare and finance. CTGT is changing the game by enhancing data lineage in LLMs and cutting hallucinations. Theyâ€™re empowering companies to create customized models using 500x less compute. View the Startup Showcase Mini-Summits The DL Compiler Mini-Summit offered attendees a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads. View the DL Compiler Mini-Summit The Fine-Tuning Mini-Summit brought together a thriving community of researchers, developers, practitioners and hobbyists which focuses on topics ranging from memory efficiency, parameter-efficient fine-tuning and quantization to performance at scale and reproducible evaluations. View the Fine-Tuning Mini-Summit Major Takeaways from the PyTorch Conference 2024 LLMs are Here to Stay: were a focal point of the event, reaffirming their pivotal role in the future of AI. As these models continue to scale, PyTorch remains the preferred framework for developing, training, and deploying them across various platforms and industries. Open Source Drives Innovation: A key takeaway from the conference was that open-source tools like PyTorch are vital for democratizing AI. This community-driven approach accelerates innovation, enabling researchers and developers globally to collaborate and contribute to faster advancements and more accessible AI technologies. Ethics and Sustainability Matter: The focus on ethical AI development was a significant takeaway. Talks on the inclusivity of computer vision models, the environmental impacts of AI infrastructure, and the need for transparent, unbiased AI models highlighted the growing importance of ethical considerations in the future of AI. PyTorch Expands Beyond the Cloud: With several sessions dedicated to edge AI and distributed computing, the conference showcased how PyTorch is expanding beyond cloud-based applications into edge devices and diverse computing environments. This shift is crucial as AI advances into areas like autonomous vehicles, mobile applications, and IoT devices. Thank You to Our Sponsors We would like to thank each of the sponsors that made the PyTorch Conference 2024 possible. These include: Diamond Sponsors: AMD Cloud Native Computing Foundation IBM Intel â€“ PyTorch Lightning.ai Meta â€“ PyTorch Platinum Sponsors: Arm Google Lambda Labs Nvidia Silver Sponsors: Anyscale â€“ PyTorch Baseten Chainguard Databricks Fal FuriosaAi HPE Jane Street Microsoft â€“ PyTorch MinIO Outerbounds Together.AI Bronze Sponsors: d-Matrix MemVerge Perforated AI Quansight Rotational Labs ScaleGenAI Special Event Sponsors: PyTorch Flare Party: Hugging Face Startup Showcase: Mayfield Diversity Scholarship: AWS Women and Non-Binary in PyTorch Lunch: Google Happy Hour Reception: Lightning.AI Thank you for your continued support in advancing the PyTorch ecosystem and helping to shape the future of AI! Save the Date See you next year for the PyTorch Conference in San Francisco at the Palace of Fine Arts from October 22-23, 2025.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Native Architecture Optimization: torchao</title>
      <link href="https://pytorch.org/blog/pytorch-native-architecture-optimization/" rel="alternate" type="text/html" title="PyTorch Native Architecture Optimization: torchao" />
      <published>2024-09-26T00:00:00-07:00</published>
      <updated>2024-09-26T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-native-architecture-optimization</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-native-architecture-optimization/">&lt;p&gt;Weâ€™re happy to officially launch torchao, a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao&lt;/a&gt; is an accessible toolkit of techniques written (mostly) in easy to read PyTorch code spanning both inference and training. This blog will help you pick which techniques matter for your workloads.&lt;/p&gt;

&lt;p&gt;We benchmarked our techniques on popular GenAI models like LLama 3 and Diffusion models and saw minimal drops in accuracy. Unless otherwise noted the baselines are bf16 run on A100 80GB GPU.&lt;/p&gt;

&lt;p&gt;Our topline metrics for llama 3 are&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;97% speedup for Llama 3 8B inference using autoquant with int4 weight only quantization and hqq&lt;/li&gt;
  &lt;li&gt;73% peak VRAM reduction for Llama 3.1 8B inference at 128K context length with a quantized KV cache&lt;/li&gt;
  &lt;li&gt;50% speedup for Llama 3 70B pretraining using float8 training on H100&lt;/li&gt;
  &lt;li&gt;30% peak VRAM reduction for Llama 3 8B using 4 bit quantized optimizers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our topline metrics for diffusion model inference&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;53% speedup using float8 dynamic quantization inference with float8 row-wise scaling on flux1.dev onH100&lt;/li&gt;
  &lt;li&gt;50% reduction in model VRAM for CogVideoX using int8 dynamic quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below weâ€™ll walk through some of the techniques available in torchao you can apply to your models for inference and training.&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/quantization&quot;&gt;Our inference quantization algorithms&lt;/a&gt; work over arbitrary PyTorch models that contain nn.Linear layers. Weight only and dynamic activation quantization for various dtypes and sparse layouts can be chosen using our top level &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; api&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;quantize_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;quantize_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sometimes quantizing a layer can make it slower because of overhead so if youâ€™d rather we just pick how to quantize each layer in a model for you then you can instead run&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchao&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoquant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max-autotune'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; API has a few different options depending on whether your model is compute bound or memory bound.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;  
    &lt;span class=&quot;c1&quot;&gt;# Memory bound models  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int8_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Compute bound models  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;int8_dynamic_activation_int8_semi_sparse_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int8_dynamic_activation_int8_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
      
    &lt;span class=&quot;c1&quot;&gt;# Device capability 8.9+  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;float8_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;float8_dynamic_activation_float8_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also have extensive benchmarks on diffusion models in collaboration with the HuggingFace diffusers team in &lt;a href=&quot;https://github.com/sayakpaul/diffusers-torchao&quot;&gt;diffusers-torchao&lt;/a&gt; where we demonstrated 53.88% speedup on Flux.1-Dev and 27.33% speedup on CogVideoX-5b&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our APIs are composable so weâ€™ve for example composed sparsity and quantization to bring 5% &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity&quot;&gt;speedup for ViT-H inference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But also can do things like quantize weights to int4 and the kv cache to int8 to support &lt;a href=&quot;https://github.com/pytorch/ao/pull/738&quot;&gt;Llama 3.1 8B at the full 128K context length running in under 18.9GB of VRAM&lt;/a&gt;. &lt;br /&gt;
&lt;img src=&quot;/assets/images/Figure_2.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qat&quot;&gt;QAT&lt;/h2&gt;

&lt;p&gt;Post training quantization, especially at less than 4 bit can suffer from serious accuracy degradations. Using &lt;a href=&quot;https://pytorch.org/blog/quantization-aware-training/&quot;&gt;Quantization Aware Training&lt;/a&gt; (QAT) weâ€™ve managed to recover up to 96% of the accuracy degradation on hellaswag. Weâ€™ve integrated this as an end to end recipe in torchtune with a minimal &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/quantization/prototype/qat&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_3.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;training&quot;&gt;Training&lt;/h1&gt;

&lt;h2 id=&quot;low-precision-compute-and-communications&quot;&gt;Low precision compute and communications&lt;/h2&gt;

&lt;p&gt;torchao provides easy to use e2e workflows for reducing the precision of training compute and distributed communications, starting with float8 for `torch.nn.Linear` layers.Here is a one-liner to convert the compute gemms of your training run to float8:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.float8&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_to_float8_training&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;convert_to_float8_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For an e2e example of how to speed up LLaMa 3 70B pretraining by up to &lt;strong&gt;1.5x&lt;/strong&gt; with float8, see our &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/float8&quot;&gt;README&lt;/a&gt;, and torchtitanâ€™s &lt;a href=&quot;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&quot;&gt;blog&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/torchtitan/blob/main/docs/float8.md&quot;&gt;float8 recipe&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;performance-and-accuracy-of-float8-pretraining-of-llama-3-70b-vs-bfloat16&quot;&gt;Performance and accuracy of float8 pretraining of LLaMa 3 70B, vs bfloat16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_4.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;
(source: &lt;a href=&quot;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&quot;&gt;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;We are expanding our training workflows to more dtypes and layouts&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/torchtune/main/tutorials/qlora_finetune.html&quot;&gt;NF4 QLoRA in torchtune&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/pull/748&quot;&gt;Prototype int8 training support&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/accelerating-neural-network-training/&quot;&gt;Accelerated sparse 2:4 training&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;low-bit-optimizers&quot;&gt;Low bit Optimizers&lt;/h2&gt;

&lt;p&gt;Inspired by Bits and Bytes weâ€™ve also added prototype support for 8 and 4 bit optimizers as a drop in replacement for AdamW.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.prototype.low_bit_optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW8bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW4bit&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW8bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_5.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;integrations&quot;&gt;Integrations&lt;/h1&gt;

&lt;p&gt;Weâ€™ve been actively working on making sure torchao works well in some of the most important projects in open source.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Huggingface transformers as an &lt;a href=&quot;https://huggingface.co/docs/transformers/main/quantization/torchao&quot;&gt;inference backend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sayakpaul/diffusers-torchao&quot;&gt;In diffusers-torchao&lt;/a&gt; as a reference implementation for accelerating diffusion models&lt;/li&gt;
  &lt;li&gt;In HQQ for &lt;a href=&quot;https://github.com/mobiusml/hqq#faster-inference&quot;&gt;fast 4 bit inference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;torchtune&lt;/a&gt; for PyTorch native QLoRA and QAT recipes&lt;/li&gt;
  &lt;li&gt;In &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt; for post training quantization&lt;/li&gt;
  &lt;li&gt;In SGLang for for &lt;a href=&quot;https://github.com/sgl-project/sglang/pull/1341&quot;&gt;int4 and int8 post training quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;If youâ€™re interested in making your models faster and smaller for training or inference, we hope youâ€™ll find torchao useful and easy to integrate.&lt;/p&gt;

&lt;p&gt;pip install torchao&lt;/p&gt;

&lt;p&gt;There are a lot of things weâ€™re excited about next ranging from going lower than 4 bit, performant kernels for high-throughput inference, expanding to more layers, scaling types or granularities, MX hardware support and supporting more hardware backends. If any of the above sounds exciting you can follow our progress at: &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;https://github.com/pytorch/ao&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If youâ€™re interested in working on torchao, weâ€™ve created a &lt;a href=&quot;https://github.com/pytorch/ao/issues/391&quot;&gt;contributors guide&lt;/a&gt;, and if you have any questions we hang out on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#torchao&lt;/code&gt; channel on &lt;a href=&quot;http://discord.gg/gpumode&quot;&gt;discord.gg/gpumode&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We are fortunate to stand on the shoulders of giants and collaborate with some of the best people in open source. Thank you!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Bits and Bytes for pioneering work in low bit optimizers and QLoRA&lt;/li&gt;
  &lt;li&gt;Answer.ai for their engineering work to get FSDP and QLoRA composing&lt;/li&gt;
  &lt;li&gt;Mobius Labs for the lovely back and forths on quantization algorithms and low bit kernels&lt;/li&gt;
  &lt;li&gt;HuggingFace transformers for their help in battle testing and integrating our work&lt;/li&gt;
  &lt;li&gt;HuggingFace diffusers for our collaboration on extensive benchmarks and best practices&lt;/li&gt;
  &lt;li&gt;torch.compile so we could write our algorithms in pure PyTorch&lt;/li&gt;
  &lt;li&gt;GPU MODE for most of our early contributors&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Weâ€™re happy to officially launch torchao, a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. torchao is an accessible toolkit of techniques written (mostly) in easy to read PyTorch code spanning both inference and training. This blog will help you pick which techniques matter for your workloads. We benchmarked our techniques on popular GenAI models like LLama 3 and Diffusion models and saw minimal drops in accuracy. Unless otherwise noted the baselines are bf16 run on A100 80GB GPU. Our topline metrics for llama 3 are 97% speedup for Llama 3 8B inference using autoquant with int4 weight only quantization and hqq 73% peak VRAM reduction for Llama 3.1 8B inference at 128K context length with a quantized KV cache 50% speedup for Llama 3 70B pretraining using float8 training on H100 30% peak VRAM reduction for Llama 3 8B using 4 bit quantized optimizers. Our topline metrics for diffusion model inference 53% speedup using float8 dynamic quantization inference with float8 row-wise scaling on flux1.dev onH100 50% reduction in model VRAM for CogVideoX using int8 dynamic quantization Below weâ€™ll walk through some of the techniques available in torchao you can apply to your models for inference and training. Inference Our inference quantization algorithms work over arbitrary PyTorch models that contain nn.Linear layers. Weight only and dynamic activation quantization for various dtypes and sparse layouts can be chosen using our top level quantize_ api from torchao.quantization import ( quantize_, int4_weight_only, ) quantize_(model, int4_weight_only()) Sometimes quantizing a layer can make it slower because of overhead so if youâ€™d rather we just pick how to quantize each layer in a model for you then you can instead run model = torchao.autoquant(torch.compile(model, mode='max-autotune')) quantize_ API has a few different options depending on whether your model is compute bound or memory bound. from torchao.quantization import ( # Memory bound models int4_weight_only, int8_weight_only, # Compute bound models int8_dynamic_activation_int8_semi_sparse_weight, int8_dynamic_activation_int8_weight, # Device capability 8.9+ float8_weight_only, float8_dynamic_activation_float8_weight, ) We also have extensive benchmarks on diffusion models in collaboration with the HuggingFace diffusers team in diffusers-torchao where we demonstrated 53.88% speedup on Flux.1-Dev and 27.33% speedup on CogVideoX-5b Our APIs are composable so weâ€™ve for example composed sparsity and quantization to bring 5% speedup for ViT-H inference But also can do things like quantize weights to int4 and the kv cache to int8 to support Llama 3.1 8B at the full 128K context length running in under 18.9GB of VRAM. QAT Post training quantization, especially at less than 4 bit can suffer from serious accuracy degradations. Using Quantization Aware Training (QAT) weâ€™ve managed to recover up to 96% of the accuracy degradation on hellaswag. Weâ€™ve integrated this as an end to end recipe in torchtune with a minimal tutorial Training Low precision compute and communications torchao provides easy to use e2e workflows for reducing the precision of training compute and distributed communications, starting with float8 for `torch.nn.Linear` layers.Here is a one-liner to convert the compute gemms of your training run to float8: from torchao.float8 import convert_to_float8_training convert_to_float8_training(model) For an e2e example of how to speed up LLaMa 3 70B pretraining by up to 1.5x with float8, see our README, and torchtitanâ€™s blog and float8 recipe. Performance and accuracy of float8 pretraining of LLaMa 3 70B, vs bfloat16 (source: https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359) We are expanding our training workflows to more dtypes and layouts NF4 QLoRA in torchtune Prototype int8 training support Accelerated sparse 2:4 training Low bit Optimizers Inspired by Bits and Bytes weâ€™ve also added prototype support for 8 and 4 bit optimizers as a drop in replacement for AdamW. from torchao.prototype.low_bit_optim import AdamW8bit, AdamW4bit optim = AdamW8bit(model.parameters()) Integrations Weâ€™ve been actively working on making sure torchao works well in some of the most important projects in open source. Huggingface transformers as an inference backend In diffusers-torchao as a reference implementation for accelerating diffusion models In HQQ for fast 4 bit inference In torchtune for PyTorch native QLoRA and QAT recipes In torchchat for post training quantization In SGLang for for int4 and int8 post training quantization Conclusion If youâ€™re interested in making your models faster and smaller for training or inference, we hope youâ€™ll find torchao useful and easy to integrate. pip install torchao There are a lot of things weâ€™re excited about next ranging from going lower than 4 bit, performant kernels for high-throughput inference, expanding to more layers, scaling types or granularities, MX hardware support and supporting more hardware backends. If any of the above sounds exciting you can follow our progress at: https://github.com/pytorch/ao If youâ€™re interested in working on torchao, weâ€™ve created a contributors guide, and if you have any questions we hang out on the #torchao channel on discord.gg/gpumode Acknowledgements We are fortunate to stand on the shoulders of giants and collaborate with some of the best people in open source. Thank you! Bits and Bytes for pioneering work in low bit optimizers and QLoRA Answer.ai for their engineering work to get FSDP and QLoRA composing Mobius Labs for the lovely back and forths on quantization algorithms and low bit kernels HuggingFace transformers for their help in battle testing and integrating our work HuggingFace diffusers for our collaboration on extensive benchmarks and best practices torch.compile so we could write our algorithms in pure PyTorch GPU MODE for most of our early contributors</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Challenges and Efforts in PyTorch Multi-Device Integration: Compatibility, Portability, and Integration Efficiencies</title>
      <link href="https://pytorch.org/blog/pt-multidevice-integration/" rel="alternate" type="text/html" title="Challenges and Efforts in PyTorch Multi-Device Integration: Compatibility, Portability, and Integration Efficiencies" />
      <published>2024-09-18T00:00:00-07:00</published>
      <updated>2024-09-18T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-multidevice-integration</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-multidevice-integration/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As the demand for diverse hardware accelerators grows, the need for a robust and adaptable deep learning framework becomes increasingly critical. While working through this integration, several challenges have surfaced in the PyTorch ecosystem, potentially affecting various hardware vendors. This blog aims to highlight these issues and propose solutions to enhance PyTorchâ€™s adaptability, portability, and resilience across different hardware platforms.&lt;/p&gt;

&lt;h2 id=&quot;improve-users-code-portability-via-accelerator-autoloading&quot;&gt;Improve Usersâ€™ Code Portability via Accelerator Autoloading&lt;/h2&gt;

&lt;p&gt;Currently, users face additional work when running their code on different accelerators. One such task is manually importing modules for out-of-tree devices. This requires users to not only understand the different usage patterns between accelerators but also make their code aware of these differences. If you have projects originally running on GPU/CPU and want to migrate to other accelerators, this can lead to significant work and potential frustration.&lt;/p&gt;

&lt;p&gt;Examples of extra import:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Case 1: Use HPU
import torch
import torchvision.models as models
import habana_frameworks.torch # &amp;lt;-- extra import
model = models.resnet50().eval().to(&quot;hpu&quot;)
input = torch.rand(128, 3, 224, 224).to(&quot;hpu&quot;)
output = model(input)

# Case 2: Use torch_npu
import torch
import torch_npu # &amp;lt;-- extra import
print(torch.ones(1, 2, device='npu'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As a high-level machine learning framework, PyTorchâ€™s ability to shield users from device differences is a competitive feature. &lt;strong&gt;Accelerator Autoloading&lt;/strong&gt; allows users to continue using the familiar PyTorch device programming model without explicitly loading or importing device-specific extensions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it works?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Utilize Pythonâ€™s plugin architecture to enable automatic loading of device extensions via entry points in the PyTorch package.&lt;/p&gt;

&lt;p&gt;Python entry points provide a standardized way for Python packages to expose and discover components or plugins within an application. Via definition in acceleratorâ€™s package &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; , PyTorch can automatically initialize accelerator modules when calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch&lt;/code&gt; , which gives users consistent experience between different backend devices.&lt;/p&gt;

&lt;p&gt;From device perspective, only need to claim following setup in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; (as example of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_npu&lt;/code&gt; )&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// setup.py 
entry_points={
 'torch.backends': ['torch_npu = torch_npu:_autoload', ],
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch&lt;/code&gt; is invoked, the accelerator module will be loaded automatically. This provides users with a consistent programming experience across out-of-tree devices, eliminating the need to be aware of differences between CUDA, HPU, and NPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Case 1: Use HPU 
import torch 
import torchvision.models as models 
model = models.resnet50().eval().to(&quot;hpu&quot;) 
input = torch.rand(128, 3, 224, 224).to(&quot;hpu&quot;) 
output = model(input) 

# Case 2: Use torch_npu 
import torch 
print(torch.ones(1, 2, device='npu'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;device-integration-optimization&quot;&gt;Device Integration Optimization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What is PrivateUse1?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In PyTorch, the dispatcher is a crucial component of the frameworkâ€™s backend that manages how operations are routed to the appropriate device-specific implementation. Dispatch keys are an integral part of this system, serving as identifiers that represent various execution contextsâ€”such as the device (CPU, CUDA, XPU), layout (dense, sparse), and autograd functionality. These keys ensure that operations are directed to the correct implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PrivateUse1&lt;/strong&gt; is a customizable device dispatch key, similar to CUDA/CPU/XPU, etc.), reserved for out-of-tree devices. It provides developers with a way to extend PyTorchâ€™s functionality without modifying the core framework, allowing for the integration of new devices, hardware accelerators, or other specialized computing environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why do we need PrivateUse1?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Internally, dispatch keys are represented as bit masks, each bit represents whether a certain key is active. This bit mask representation is efficient for quick lookup and combination of keys, but it inherently limits the number of distinct keys (typically to 64 or fewer).&lt;/p&gt;

&lt;p&gt;The current implementation of BackendComponent dispatch keys in PyTorch has encountered a critical bottleneck, which restricts the addition of new backends and, as a result, limits the expansion of the PyTorch ecosystem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multidevice-integration/fg1.png&quot; alt=&quot;bit diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In response to this challenge, a series of optimizations have been applied to the PrivateUse1 mechanism to enhance its capacity.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PrivateUse1 integration mechanism&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Initially reserved as fallback options, &lt;strong&gt;PrivateUse1&lt;/strong&gt;, along with &lt;strong&gt;PrivateUse2&lt;/strong&gt; and &lt;strong&gt;PrivateUse3&lt;/strong&gt;, were designed to be activated only when existing key resources became scarce.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;PrivateUse1&lt;/strong&gt; is now being developed to match the robustness and versatility of established keys like CUDA and CPU. Achieving this required a deep integration across critical PyTorch modules. This integration wasnâ€™t just a simple switchâ€”it involved significant updates to core components such as &lt;strong&gt;AMP (Automatic Mixed Precision)&lt;/strong&gt;, &lt;strong&gt;Autograd&lt;/strong&gt;, &lt;strong&gt;Distributed Training&lt;/strong&gt;, &lt;strong&gt;Checkpointing&lt;/strong&gt;, &lt;strong&gt;DataLoader&lt;/strong&gt;, &lt;strong&gt;Optimization&lt;/strong&gt;, and &lt;strong&gt;Quantization,&lt;/strong&gt; etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multidevice-integration/fg2.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The activation of &lt;strong&gt;PrivateUse1&lt;/strong&gt; was a massive collaborative effort, culminating in over 100 pull requests aimed at making it from a placeholder to a fully operational dispatch key.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PrivateUse1 UT/CI Quality Assurance&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;While unit tests are essential for ensuring quality during the development of the &lt;strong&gt;PrivateUse1&lt;/strong&gt; mechanism, they are not sufficient on their own to prevent new pull requests from inadvertently affecting existing functionality or compatibility of out-of-tree devices.&lt;/p&gt;

    &lt;p&gt;To mitigate this risk, the community has added the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch_openreg&lt;/code&gt; module to the test suite. This module leverages a CPU backend to simulate interactions with accelerators, creating a controlled environment for rigorous testing. After implemented, this will enable automatic execution of device-generic test cases whenever relevant code is updated, allowing us to quickly detect and address any potential issues affecting the PrivateUse1 integration mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Comprehensive Documentation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;By providing comprehensive and easy-to-understand documentation, we aim to lower the barrier to entry for developers and encourage wider adoption of the PrivateUse1 mechanism in the PyTorch ecosystem. This documentation includes:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Step-by-step guides for integrating new backends using PrivateUse1&lt;/li&gt;
      &lt;li&gt;Clear explanations of PrivateUse1â€™s functionality and benefits&lt;/li&gt;
      &lt;li&gt;Code examples and best practices for efficient implementation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These enhancements aim to improve the robustness and reliability of the PrivateUse1 mechanism, facilitating better integration of new backends and expanding the capabilities of PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;compatibility-between-upstream-and-downstream&quot;&gt;Compatibility Between Upstream and Downstream&lt;/h2&gt;

&lt;h3 id=&quot;device-generic-unit-tests&quot;&gt;Device-Generic Unit Tests&lt;/h3&gt;

&lt;p&gt;Most unit tests in PyTorch focus on CPU and CUDA devices, which limits participation from users with other hardware. To address this, a plan to modify PyTorchâ€™s unit testing framework, enabling better support for non-CUDA devices. This plan includes removing existing device restrictions, implementing dynamic data type loading, and generalizing decorators to accommodate a broader range of devices. Additionally, we aim to enforce the use of universal device code and expand distributed testing to support non-NCCL backends.&lt;/p&gt;

&lt;p&gt;Through these improvements, we hope to significantly increase test coverage and pass rates for non-CUDA devices, integrating them into PyTorchâ€™s continuous integration process. Initial changes have already been implemented, paving the way for new hardware support and creating a reference template for other devices.&lt;/p&gt;

&lt;h3 id=&quot;ensuring-robust-device-integration-through-automated-testing&quot;&gt;Ensuring Robust Device Integration through Automated Testing&lt;/h3&gt;

&lt;p&gt;To uphold the high standards of quality assurance in PyTorch, an independent build repository and daily continuous integration (CI) workflows have been established, focusing on smoke and integration testing.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch-integration-tests&lt;/code&gt; repository automates the testing of PyTorchâ€™s device-specific functionalities, ensuring that they operate correctly and efficiently across a variety of hardware platforms(NPUs and other specialized devices). In repository we are trying to make a fully automated system that continuously validates PyTorchâ€™s compatibility with different hardware backends.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Automated Integration Tests&lt;/strong&gt;: Run automated tests across different devices using GitHub Actions. This automation ensures that every change in the codebase is thoroughly tested against multiple hardware platforms, catching potential issues early in the development process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reusable Workflows&lt;/strong&gt;: Workflows in this repository are modular and reusable, which streamlines the testing process. Developers can easily adapt these workflows to new devices or testing scenarios, making the system both flexible and scalable as PyTorch evolves.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Awareness of Out-of-Tree Devices&lt;/strong&gt;: The repository displays the existence and behavior of all out-of-tree devices, keeping the community informed. This approach minimizes the risk of accidentally breaking downstream functionalities and provides fast feedback on changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Efforts to enhance multi-device integration are pivotal for its adaptability in the evolving deep learning landscape. These initiatives not only benefit current users but also lower entry barriers for new hardware vendors and developers, fostering innovation in AI and machine learning. As PyTorch continues to evolve, its commitment to flexibility, robustness, and inclusivity positions it as a leading framework capable of meeting the diverse needs of the deep learning community.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Zesheng Zong (Huawei), Jiawei Li (Huawei) | Co-authors: Jiong Gong (Intel), Bartosz Sochacki (Intel), Eikan Wang (Intel)</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Arm Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/arm-joins-pytorch/" rel="alternate" type="text/html" title="Arm Joins the PyTorch Foundation as a Premier Member" />
      <published>2024-09-12T00:00:00-07:00</published>
      <updated>2024-09-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/arm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/arm-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that &lt;a href=&quot;https://www.arm.com/&quot;&gt;Arm&lt;/a&gt; has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Arm designs a high-performance, power-efficient compute platform with unmatched scalability, supporting a vast ecosystem of developers deploying AI at the edge and in the cloud, ranging from the Arm instances offered by all major cloud service providers to smartphones, laptops, software-defined vehicles and more.&lt;/p&gt;

&lt;p&gt;â€œOur continued investments in software are accelerating development and AI performance for over 20 million software developers, ensuring they can develop for Arm, on Arm,â€ said Alex Spinelli, VP Developer Technology at Arm. â€œPyTorch is a pivotal framework in advancing AI research and development. This membership demonstrates our strong commitment to open source - ensuring PyTorch just works on Arm and can leverage seamless acceleration for the most demanding AI models, now and in the future.â€&lt;/p&gt;

&lt;p&gt;Last year at the PyTorch Conference, Arm partnered with Apple, Meta and Qualcomm to release &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;ExecuTorch&lt;/a&gt;, an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers.&lt;/p&gt;

&lt;p&gt;â€œWeâ€™re thrilled to welcome Arm to the PyTorch Foundation. As we look to the future of AI and machine learning, the role of specialized silicon and edge devices becomes increasingly crucial. Armâ€™s expertise in these areas will be invaluable as we work to make PyTorch more efficient and accessible across a wider range of hardware,â€ said PyTorch Foundation Executive Director Matt White. â€œThis collaboration underscores our commitment to fostering innovation and expanding PyTorchâ€™s capabilities to meet the evolving needs of developers and researchers worldwide.â€&lt;/p&gt;

&lt;p&gt;As a premier member, Arm is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;Weâ€™re happy to welcome Alex Spinelli, VP Developer Technology at Arm, to our board. Prior to Arm, Alex was VP of Product for Core Machine Learning at Google, where he led Googleâ€™s technology and infrastructure for building, training, and serving machine learning, including the TensorFlow stack.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the worldâ€™s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the worldâ€™s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>The PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Arm has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Shanghai Meetup Notes</title>
      <link href="https://pytorch.org/blog/pytorch-shanghai-notes/" rel="alternate" type="text/html" title="PyTorch Shanghai Meetup Notes" />
      <published>2024-09-08T00:00:00-07:00</published>
      <updated>2024-09-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-shanghai-notes</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-shanghai-notes/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg1.jpg&quot; alt=&quot;group photo&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are honored to successfully host the PyTorch Shanghai Meetup on August 15, 2024. This Meetup has received great attention from the industry. We invited senior PyTorch developers from Intel and Huawei as guest speakers, who shared their valuable experience and the latest technical trends. In addition, this event also attracted PyTorch enthusiasts from many technology companies and well-known universities. A total of more than 40 participants gathered together to discuss and exchange the latest applications and technological advances of PyTorch.&lt;/p&gt;

&lt;p&gt;This Meetup not only strengthened the connection between PyTorch community members, but also provided a platform for local AI technology enthusiasts to learn, communicate and grow. We look forward to the next gathering to continue to promote the development of PyTorch technology in the local area.&lt;/p&gt;

&lt;h2 id=&quot;1-pytorch-foundation-updates&quot;&gt;1. PyTorch Foundation Updates&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg2.jpg&quot; alt=&quot;man instructing students&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Board member Fred Li shared the latest updates in the PyTorch community, He reviewed the development history of the PyTorch community, explained in detail the growth path of community developers, encouraged everyone to delve deeper into technology, and introduced the upcoming PyTorch Conference 2024 related matters.&lt;/p&gt;

&lt;h2 id=&quot;2-intels-journey-with-pytorch-democratizing-ai-with-ubiquitous-hardware-and-open-software&quot;&gt;2. Intelâ€™s Journey with PyTorch Democratizing AI with ubiquitous hardware and open software&lt;/h2&gt;

&lt;p&gt;PyTorch CPU module maintainer Jiong Gong shared 6-year technical contributions from Intel to PyTorch and its ecosystem, explored the remarkable advancements that Intel has made in both software and hardware democratizing AI, ensuring accessibility, and optimizing performance across a diverse range of Intel hardware platforms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg3.jpg&quot; alt=&quot;man instructing students&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-exploring-multi-backend-support-in-pytorch-ecosystem-a-case-study-of-ascend&quot;&gt;3. Exploring Multi-Backend Support in PyTorch Ecosystem: A Case Study of Ascend&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg4.jpg&quot; alt=&quot;man instructing students&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fengchun Hua, a PyTorch contributor from Huawei, took Huawei Ascend NPU as an example to demonstrate the latest achievements in multi-backend support for PyTorch applications. He introduced the hardware features of Huawei Ascend NPU and the infrastructure of CANN (Compute Architecture for Neural Networks), and explained the key achievements and innovations in native support work. He also shared the current challenges and the next work plan.&lt;/p&gt;

&lt;p&gt;Yuanhao Ji, another PyTorch contributor from Huawei, then introduced the Autoload Device Extension proposal, explained its implementation details and value in improving the scalability of PyTorch, and introduced the latest work progress of the PyTorch Chinese community.&lt;/p&gt;

&lt;h2 id=&quot;4-intel-xpu-backend-for-inductor&quot;&gt;4. Intel XPU Backend for Inductor&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg5.jpg&quot; alt=&quot;man instructing students&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eikan is a PyTorch contributor from Intel. He focuses on torch.compile stack for both Intel CPU and GPU. In this session, Eikan presented Intelâ€™s efforts on torch.compile for Intel GPUs. He provided updates on the current status of Intel GPUs within PyTorch, covering both functionality and performance aspects. Additionally, Eikan used Intel GPU as a case study to demonstrate how to integrate a new backend into the Inductor using Triton.&lt;/p&gt;

&lt;h2 id=&quot;5-pytorch-privateuse1-evolution-approaches-and-insights&quot;&gt;5. PyTorch PrivateUse1 Evolution Approaches and Insights&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-shanghai-notes/fg6.jpg&quot; alt=&quot;man instructing students&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Jiawei Li, a PyTorch collaborator from Huawei, introduced PyTorchâ€™s Dispatch mechanism and emphasized the limitations of DIspatchKey. He took Huawei Ascend NPU as an example to share the best practices of the PyTorch PrivateUse1 mechanism. He mentioned that while using the PrivateUse1 mechanism, Huawei also submitted many improvements and bug fixes for the mechanism to the PyTorch community. He also mentioned that due to the lack of upstream CI support for out-of-tree devices, changes in upstream code may affect their stability and quality, and this insight was recognized by everyone.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">CUDA-Free Inference for LLMs</title>
      <link href="https://pytorch.org/blog/cuda-free-inference-for-llms/" rel="alternate" type="text/html" title="CUDA-Free Inference for LLMs" />
      <published>2024-09-04T00:00:00-07:00</published>
      <updated>2024-09-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/cuda-free-inference-for-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/cuda-free-inference-for-llms/">&lt;p&gt;In this blog, we discuss the methods we used to achieve FP16 inference with popular LLM models such as &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-8B&quot;&gt;Metaâ€™s Llama3-8B&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/ibm-granite/granite-8b-code-base&quot;&gt;IBMâ€™s Granite-8B Code&lt;/a&gt;, where &lt;strong&gt;100%&lt;/strong&gt; of the computation is performed using &lt;a href=&quot;https://github.com/triton-lang/triton&quot;&gt;OpenAIâ€™s Triton Language&lt;/a&gt;. &lt;br /&gt;
For single token generation times using our Triton kernel based models, we were able to approach &lt;strong&gt;0.76-0.78x&lt;/strong&gt; performance relative to the CUDA kernel dominant workflows for both Llama and Granite on Nvidia H100 GPUs, and &lt;strong&gt;0.62-0.82x&lt;/strong&gt; on Nvidia A100 GPUs.&lt;/p&gt;

&lt;p&gt;Why explore using 100% Triton?  Triton provides a path for enabling LLMs to run on different types of GPUs - NVIDIA, AMD, and in the future Intel and other GPU based accelerators. It also provides a higher layer of abstraction in Python for programming GPUs and has allowed us to write performant kernels faster than authoring them using vendor specific APIs. In the rest of this blog, we will share how we achieve CUDA-free compute, micro-benchmark individual kernels for comparison, and discuss how we can further improve future Triton kernels to close the gaps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/granite_llama_throughput.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. Inference throughput benchmarks with Triton and CUDA variants of Llama3-8B and Granite-8B, on NVIDIA H100 and A100&lt;/strong&gt; &lt;br /&gt;
&lt;em&gt;Settings: batch size = 2, input sequence length = 512, output sequence length = 256&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.0 Composition of a Transformer Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We start with a breakdown of the computations that happen in Transformer-based models. The figure below shows the â€œkernelsâ€ of a typical Transformer block.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/transformer_block.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;
 &lt;strong&gt;Figure 2.&lt;/strong&gt; Transformer Block by core kernels&lt;/p&gt;

&lt;p&gt;The core operations for a Llama3 architecture are summarized in this list:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;RMSNorm&lt;/li&gt;
  &lt;li&gt;Matrix multiplication: Fused QKV&lt;/li&gt;
  &lt;li&gt;RoPE&lt;/li&gt;
  &lt;li&gt;Attention&lt;/li&gt;
  &lt;li&gt;Matrix multiplication: Output Projection&lt;/li&gt;
  &lt;li&gt;RMSNorm&lt;/li&gt;
  &lt;li&gt;Matrix multiplication: Fused Gate + Up Projection&lt;/li&gt;
  &lt;li&gt;Activation function: SiLU&lt;/li&gt;
  &lt;li&gt;Element Wise Multiplication&lt;/li&gt;
  &lt;li&gt;Matrix multiplication: Down Projection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of these operations is computed on the GPU through the execution of one (or multiple) kernels. While the specifics of each of these kernels can vary across different transformer models, the core operations remain the same. For example, IBMâ€™s Granite 8B Code model uses bias in the MLP layer, different from Llama3. Such changes do require modifications to the kernels. A typical model is a stack of these transformer blocks wired together with embedding layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.0 Model Inference&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Typical model architecture code is shared with a python model.py file that is launched by PyTorch. In the default PyTorch &lt;a href=&quot;https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/&quot;&gt;eager execution&lt;/a&gt; mode, these kernels are all executed with CUDA. To achieve 100% Triton for end-to-end Llama3-8B and Granite-8B inference we need to write and integrate handwritten Triton kernels as well as leverage torch.compile (to generate Triton ops). First, we replace smaller ops with compiler generated Triton kernels, and second, we replace more expensive and complex computations (e.g. matrix multiplication and flash attention) with handwritten Triton kernels.&lt;/p&gt;

&lt;p&gt;Torch.compile generates Triton kernels automatically for RMSNorm, RoPE, SiLU and Element Wise Multiplication. Using tools like &lt;a href=&quot;https://developer.nvidia.com/nsight-systems&quot;&gt;Nsight Systems&lt;/a&gt; we can observe these generated kernels; they appear as tiny dark green kernels in-between the matrix multiplications and attention.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nsys_trace_cuda.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;
&lt;strong&gt;Figure 3.&lt;/strong&gt; Trace of Llama3-8B with torch.compile, showing CUDA kernels being used for matrix multiplications and flash attention&lt;/p&gt;

&lt;p&gt;For the above trace, we note that the two major ops that make up &lt;strong&gt;80%&lt;/strong&gt; of the E2E latency in a Llama3-8B style model are matrix multiplication and attention kernels and both remain CUDA kernels. Thus to close the remaining gap, we replace both matmul and attention kernels with handwritten Triton kernels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4.0 Triton SplitK GEMM Kernel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the matrix multiplications in the linear layers, we wrote a custom FP16 Triton GEMM (General Matrix-Matrix Multiply) kernel that leverages a &lt;a href=&quot;https://pytorch.org/blog/accelerating-moe-model/\#30-work-decomposition---splitk&quot;&gt;SplitK work decomposition&lt;/a&gt;. We have previously discussed this parallelization in other blogs as a way to accelerate the decoding portion of LLM inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5.0 GEMM Kernel Tuning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To achieve optimal performance we used the exhaustive search approach to tune our SplitK GEMM kernel. Granite-8B and Llama3-8B have linear layers with the following shapes:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Linear Layer&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Shape (in_features, out_features)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Fused QKV Projection&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(4096, 6144)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Output Projection&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(4096, 4096)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Fused Gate + Up Projection&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(4096, 28672)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Down Projection&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(14336, 4096)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Granite-8B and Llama3-8B Linear Layer Weight Matrix Shapes&lt;/p&gt;

&lt;p&gt;Each of these linear layers have different weight matrix shapes. Thus, for optimal performance the Triton kernel must be tuned for each of these shape profiles. After tuning for each linear layer we were able to achieve &lt;strong&gt;1.20x&lt;/strong&gt; E2E speedup on Llama3-8B and Granite-8B over the untuned Triton kernel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6.0 Flash Attention Kernel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We evaluated a suite of existing Triton flash attention kernels with different configurations, namely:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ROCm/triton/blob/triton-mlir/python/perf-kernels/flash-attention.py&quot;&gt;AMD Flash&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/triton-lang/triton/blob/main/python/tutorials/06-fused-attention.py&quot;&gt;OpenAI Flash&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/blob/3669b25206d5938e3cc74a5f7860e31c38af8204/flash_attn/flash_attn_triton.py#L812&quot;&gt;Dao AI Lab Flash&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/xformers/blob/fae0ceb195a41f2ab762d89449c6012fbcf2ffda/xformers/ops/fmha/triton_splitk.py#L96&quot;&gt;XFormers Flash&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/e7b870c88bc3b854a95399a96a274d2f1f908172/torch/nn/attention/flex_attention.py#L800&quot;&gt;PyTorch FlexAttention&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We evaluated the text generation quality of each of these kernels, first, in eager mode and then (if we were able to torch.compile the kernel with standard methods) compile mode. For kernels 2-5, we noted the following:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Kernel&lt;/th&gt;
      &lt;th&gt;Text Generation Quality&lt;/th&gt;
      &lt;th&gt;Torch.compile&lt;/th&gt;
      &lt;th&gt;Support for Arbitrary Sequence Length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;AMD Flash&lt;/td&gt;
      &lt;td&gt;Coherent&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OpenAI Flash&lt;/td&gt;
      &lt;td&gt;Incoherent&lt;/td&gt;
      &lt;td&gt;Did not evaluate. WIP to debug precision in eager mode first&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dao AI Lab Flash&lt;/td&gt;
      &lt;td&gt;Incoherent&lt;/td&gt;
      &lt;td&gt;Did not evaluate. WIP to debug precision in eager mode first&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Xformers FlashDecoding&lt;/td&gt;
      &lt;td&gt;Hit a compilation error before we were able to evaluate text quality&lt;/td&gt;
      &lt;td&gt;WIP&lt;/td&gt;
      &lt;td&gt;No (This kernel is optimized for decoding)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PyTorch FlexAttention&lt;/td&gt;
      &lt;td&gt;Coherent&lt;/td&gt;
      &lt;td&gt;WIP&lt;/td&gt;
      &lt;td&gt;WIP&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Table of combinations we tried with different Flash Attention Kernels&lt;/p&gt;

&lt;p&gt;The above table summarizes what we observed out-of-the box.  With some effort we expect that kernels 2-5 can be modified to meet the above criteria.  However, this also shows that having a kernel that works for benchmarking is often only the start of having it usable as an end to end production kernel.  &lt;br /&gt;
We chose to use the AMD flash attention kernel in our subsequent tests as it can be compiled via torch.compile and produces legible output in both eager and compiled mode.&lt;/p&gt;

&lt;p&gt;To satisfy torch.compile compatibility with the AMD flash attention kernel, we had to define it as a torch custom operator. This process is explained in detail &lt;a href=&quot;https://pytorch.org/tutorials/advanced/python\_custom\_ops.html&quot;&gt;here&lt;/a&gt;. The tutorial link discusses how to wrap a simple image crop operation.  However, we note that wrapping a more complex flash attention kernel follows a similar process. The two step approach is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Wrap the function into a PyTorch Custom Operator&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torch_op_warpping_2.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add a FakeTensor Kernel to the operator, which given the shapes of the input tensors of flash (q, k and v) provides a way to compute the output shape of the flash kernel&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torch_op_wrapping_1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After defining the Triton flash kernel as a custom op, we were able to successfully compile it for our E2E runs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nsys_trace_triton.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Trace of Llama3-8B with torch.compile, after swapping in Triton matmul and Triton flash attention kernels&lt;/p&gt;

&lt;p&gt;From Figure 5, we note that now, after integrating both the SplitK matrix multiplication kernel, the torch op wrapped flash attention kernel, and then running torch.compile, we are able to achieve a forward pass that uses 100% Triton computation kernels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7.0 End-to-End Benchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We performed end-to-end measurements on NVIDIA H100s and A100s (single GPU) with Granite-8B and Llama3-8B models. We performed our benchmarks with two different configurations.&lt;/p&gt;

&lt;p&gt;The Triton kernel configuration uses:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Triton SplitK GEMM&lt;/li&gt;
  &lt;li&gt;AMD Triton Flash Attention&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The CUDA Kernel configuration uses:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cuBLAS GEMM&lt;/li&gt;
  &lt;li&gt;cuDNN Flash Attention - Scaled Dot-Product Attention (SDPA)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We found the following throughput and inter-token latencies for both eager and torch compiled modes, with typical inference settings:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;GPU&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Kernel Config&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Median Latency (Eager) [ms/tok]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Median Latency (Compiled) [ms/tok]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;H100&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Granite-8B&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Triton&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;27.42&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11.59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CUDA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18.84&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Llama3-8B&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Triton&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10.61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CUDA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16.59&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;A100&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Granite-8B&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Triton&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;53.44&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16.88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CUDA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;37.13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Llama3-8B&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Triton&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;44.44&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Â &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CUDA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32.45&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12.96&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Granite-8B and Llama3-8B Single Token Generation Latency on H100 and A100,&lt;br /&gt;
(batch size = 2, input sequence length = 512, output sequence length = 256)&lt;/p&gt;

&lt;p&gt;To summarize, the Triton models can get up to &lt;strong&gt;78%&lt;/strong&gt; of the performance of the CUDA models on the H100 and up to &lt;strong&gt;82%&lt;/strong&gt; on the A100.&lt;/p&gt;

&lt;p&gt;The performance gap can be explained by the kernel latencies we observe for matmul and flash attention, which are discussed in the next section.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8.0 Microbenchmarks&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Kernel&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Triton [us]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CUDA [us]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;QKV Projection Matmul&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Flash Attention&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Projection Matmul&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Gate + Up Projection Matmul&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;84&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Down Projection Matmul&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;58&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;42&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Triton and CUDA Kernel Latency Comparison (Llama3-8B on NVIDIA H100)&lt;br /&gt;
Input was an arbitrary prompt (bs=1, prompt = 44 seq length), decoding latency time&lt;/p&gt;

&lt;p&gt;From the above, we note the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Triton matmul kernels are &lt;strong&gt;1.2-1.4x&lt;/strong&gt; slower than CUDA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AMDs Triton Flash Attention kernel is &lt;strong&gt;1.6x&lt;/strong&gt; slower than CUDA SDPA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These results highlight the need to further improve the performance of kernels that are core primitives like GEMM and Flash Attention. We leave this as future research, as recent works (e.g. &lt;a href=&quot;https://pytorch.org/blog/flashattention-3/&quot;&gt;FlashAttention-3&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention&lt;/a&gt;) provide ways to leverage the underlying hardware better as well as Triton pathways that we hope to be able to build on to produce greater speedups. To illustrate this, we compared FlexAttention with SDPA and AMDâ€™s Triton Flash kernel.&lt;/p&gt;

&lt;p&gt;We are working to verify E2E performance with FlexAttention. For now, initial microbenchmarks with Flex show promise for longer context lengths and decoding problem shapes, where the query vector is small:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flash_attention_tflops.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; FlexAttention Kernel Benchmarks on NVIDIA H100 SXM5 80GB&lt;br /&gt;
(batch=1, num_heads=32, seq_len=seq_len, head_dim=128)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9.0 Future Work&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For future work we plan to explore ways to further optimize our matmuls that leverage the hardware better, such as this blog we published on &lt;a href=&quot;https://pytorch.org/blog/hopper-tma-unit/&quot;&gt;utilizing TMA for H100&lt;/a&gt;, as well as different work decompositions (persistent kernel techniques like StreamK etc.) to get greater speedups for our Triton-based approach. For flash attention, we plan to explore FlexAttention and FlashAttention-3 as the techniques used in these kernels can be leveraged to help further close the gap between Triton and CUDA. &lt;br /&gt;
We also note that our prior work has shown promising results for FP8 Triton GEMM kernel performance versus cuBLAS FP8 GEMM, thus in a future post we will explore E2E FP8 LLM inference.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Raghu Ganti and Mudhakar Srivatsa</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we discuss the methods we used to achieve FP16 inference with popular LLM models such as Metaâ€™s Llama3-8B and IBMâ€™s Granite-8B Code, where 100% of the computation is performed using OpenAIâ€™s Triton Language. For single token generation times using our Triton kernel based models, we were able to approach 0.76-0.78x performance relative to the CUDA kernel dominant workflows for both Llama and Granite on Nvidia H100 GPUs, and 0.62-0.82x on Nvidia A100 GPUs. Why explore using 100% Triton? Triton provides a path for enabling LLMs to run on different types of GPUs - NVIDIA, AMD, and in the future Intel and other GPU based accelerators. It also provides a higher layer of abstraction in Python for programming GPUs and has allowed us to write performant kernels faster than authoring them using vendor specific APIs. In the rest of this blog, we will share how we achieve CUDA-free compute, micro-benchmark individual kernels for comparison, and discuss how we can further improve future Triton kernels to close the gaps. Figure 1. Inference throughput benchmarks with Triton and CUDA variants of Llama3-8B and Granite-8B, on NVIDIA H100 and A100 Settings: batch size = 2, input sequence length = 512, output sequence length = 256 2.0 Composition of a Transformer Block We start with a breakdown of the computations that happen in Transformer-based models. The figure below shows the â€œkernelsâ€ of a typical Transformer block. Figure 2. Transformer Block by core kernels The core operations for a Llama3 architecture are summarized in this list: RMSNorm Matrix multiplication: Fused QKV RoPE Attention Matrix multiplication: Output Projection RMSNorm Matrix multiplication: Fused Gate + Up Projection Activation function: SiLU Element Wise Multiplication Matrix multiplication: Down Projection Each of these operations is computed on the GPU through the execution of one (or multiple) kernels. While the specifics of each of these kernels can vary across different transformer models, the core operations remain the same. For example, IBMâ€™s Granite 8B Code model uses bias in the MLP layer, different from Llama3. Such changes do require modifications to the kernels. A typical model is a stack of these transformer blocks wired together with embedding layers. 3.0 Model Inference Typical model architecture code is shared with a python model.py file that is launched by PyTorch. In the default PyTorch eager execution mode, these kernels are all executed with CUDA. To achieve 100% Triton for end-to-end Llama3-8B and Granite-8B inference we need to write and integrate handwritten Triton kernels as well as leverage torch.compile (to generate Triton ops). First, we replace smaller ops with compiler generated Triton kernels, and second, we replace more expensive and complex computations (e.g. matrix multiplication and flash attention) with handwritten Triton kernels. Torch.compile generates Triton kernels automatically for RMSNorm, RoPE, SiLU and Element Wise Multiplication. Using tools like Nsight Systems we can observe these generated kernels; they appear as tiny dark green kernels in-between the matrix multiplications and attention. Figure 3. Trace of Llama3-8B with torch.compile, showing CUDA kernels being used for matrix multiplications and flash attention For the above trace, we note that the two major ops that make up 80% of the E2E latency in a Llama3-8B style model are matrix multiplication and attention kernels and both remain CUDA kernels. Thus to close the remaining gap, we replace both matmul and attention kernels with handwritten Triton kernels. 4.0 Triton SplitK GEMM Kernel For the matrix multiplications in the linear layers, we wrote a custom FP16 Triton GEMM (General Matrix-Matrix Multiply) kernel that leverages a SplitK work decomposition. We have previously discussed this parallelization in other blogs as a way to accelerate the decoding portion of LLM inference. 5.0 GEMM Kernel Tuning To achieve optimal performance we used the exhaustive search approach to tune our SplitK GEMM kernel. Granite-8B and Llama3-8B have linear layers with the following shapes: Linear Layer Shape (in_features, out_features) Fused QKV Projection (4096, 6144) Output Projection (4096, 4096) Fused Gate + Up Projection (4096, 28672) Down Projection (14336, 4096) Figure 4. Granite-8B and Llama3-8B Linear Layer Weight Matrix Shapes Each of these linear layers have different weight matrix shapes. Thus, for optimal performance the Triton kernel must be tuned for each of these shape profiles. After tuning for each linear layer we were able to achieve 1.20x E2E speedup on Llama3-8B and Granite-8B over the untuned Triton kernel. 6.0 Flash Attention Kernel We evaluated a suite of existing Triton flash attention kernels with different configurations, namely: AMD Flash OpenAI Flash Dao AI Lab Flash XFormers Flash PyTorch FlexAttention We evaluated the text generation quality of each of these kernels, first, in eager mode and then (if we were able to torch.compile the kernel with standard methods) compile mode. For kernels 2-5, we noted the following: Kernel Text Generation Quality Torch.compile Support for Arbitrary Sequence Length AMD Flash Coherent Yes Yes OpenAI Flash Incoherent Did not evaluate. WIP to debug precision in eager mode first No Dao AI Lab Flash Incoherent Did not evaluate. WIP to debug precision in eager mode first Yes Xformers FlashDecoding Hit a compilation error before we were able to evaluate text quality WIP No (This kernel is optimized for decoding) PyTorch FlexAttention Coherent WIP WIP Figure 5. Table of combinations we tried with different Flash Attention Kernels The above table summarizes what we observed out-of-the box. With some effort we expect that kernels 2-5 can be modified to meet the above criteria. However, this also shows that having a kernel that works for benchmarking is often only the start of having it usable as an end to end production kernel. We chose to use the AMD flash attention kernel in our subsequent tests as it can be compiled via torch.compile and produces legible output in both eager and compiled mode. To satisfy torch.compile compatibility with the AMD flash attention kernel, we had to define it as a torch custom operator. This process is explained in detail here. The tutorial link discusses how to wrap a simple image crop operation. However, we note that wrapping a more complex flash attention kernel follows a similar process. The two step approach is as follows: Wrap the function into a PyTorch Custom Operator Add a FakeTensor Kernel to the operator, which given the shapes of the input tensors of flash (q, k and v) provides a way to compute the output shape of the flash kernel After defining the Triton flash kernel as a custom op, we were able to successfully compile it for our E2E runs. Figure 6. Trace of Llama3-8B with torch.compile, after swapping in Triton matmul and Triton flash attention kernels From Figure 5, we note that now, after integrating both the SplitK matrix multiplication kernel, the torch op wrapped flash attention kernel, and then running torch.compile, we are able to achieve a forward pass that uses 100% Triton computation kernels. 7.0 End-to-End Benchmarks We performed end-to-end measurements on NVIDIA H100s and A100s (single GPU) with Granite-8B and Llama3-8B models. We performed our benchmarks with two different configurations. The Triton kernel configuration uses: Triton SplitK GEMM AMD Triton Flash Attention The CUDA Kernel configuration uses: cuBLAS GEMM cuDNN Flash Attention - Scaled Dot-Product Attention (SDPA) We found the following throughput and inter-token latencies for both eager and torch compiled modes, with typical inference settings: GPU Model Kernel Config Median Latency (Eager) [ms/tok] Median Latency (Compiled) [ms/tok] H100 Granite-8B Triton 27.42 11.59 Â  Â  CUDA 18.84 9.50 Â  Llama3-8B Triton 20.36 10.61 Â  Â  CUDA 16.59 8.59 A100 Granite-8B Triton 53.44 16.88 Â  Â  CUDA 37.13 14.25 Â  Llama3-8B Triton 44.44 17.94 Â  Â  CUDA 32.45 12.96 Figure 7. Granite-8B and Llama3-8B Single Token Generation Latency on H100 and A100, (batch size = 2, input sequence length = 512, output sequence length = 256) To summarize, the Triton models can get up to 78% of the performance of the CUDA models on the H100 and up to 82% on the A100. The performance gap can be explained by the kernel latencies we observe for matmul and flash attention, which are discussed in the next section. 8.0 Microbenchmarks Kernel Triton [us] CUDA [us] QKV Projection Matmul 25 21 Flash Attention 13 8 Output Projection Matmul 21 17 Gate + Up Projection Matmul 84 83 Down Projection Matmul 58 42 Figure 8. Triton and CUDA Kernel Latency Comparison (Llama3-8B on NVIDIA H100) Input was an arbitrary prompt (bs=1, prompt = 44 seq length), decoding latency time From the above, we note the following: Triton matmul kernels are 1.2-1.4x slower than CUDA AMDs Triton Flash Attention kernel is 1.6x slower than CUDA SDPA These results highlight the need to further improve the performance of kernels that are core primitives like GEMM and Flash Attention. We leave this as future research, as recent works (e.g. FlashAttention-3, FlexAttention) provide ways to leverage the underlying hardware better as well as Triton pathways that we hope to be able to build on to produce greater speedups. To illustrate this, we compared FlexAttention with SDPA and AMDâ€™s Triton Flash kernel. We are working to verify E2E performance with FlexAttention. For now, initial microbenchmarks with Flex show promise for longer context lengths and decoding problem shapes, where the query vector is small: Figure 9. FlexAttention Kernel Benchmarks on NVIDIA H100 SXM5 80GB (batch=1, num_heads=32, seq_len=seq_len, head_dim=128) 9.0 Future Work For future work we plan to explore ways to further optimize our matmuls that leverage the hardware better, such as this blog we published on utilizing TMA for H100, as well as different work decompositions (persistent kernel techniques like StreamK etc.) to get greater speedups for our Triton-based approach. For flash attention, we plan to explore FlexAttention and FlashAttention-3 as the techniques used in these kernels can be leveraged to help further close the gap between Triton and CUDA. We also note that our prior work has shown promising results for FP8 Triton GEMM kernel performance versus cuBLAS FP8 GEMM, thus in a future post we will explore E2E FP8 LLM inference.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate Your AI: PyTorch 2.4 Now Supports Intel GPUs for Faster Workloads</title>
      <link href="https://pytorch.org/blog/intel-gpus-pytorch-2-4/" rel="alternate" type="text/html" title="Accelerate Your AI: PyTorch 2.4 Now Supports Intel GPUs for Faster Workloads" />
      <published>2024-08-29T00:00:00-07:00</published>
      <updated>2024-08-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-gpus-pytorch-2-4</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-gpus-pytorch-2-4/">&lt;p&gt;We have exciting news! PyTorch 2.4 now supports IntelÂ® Data Center GPU Max Series and the SYCL software stack, making it easier to speed up your AI workflows for both training and inference. This update allows for you to have a consistent programming experience with minimal coding effort and extends PyTorchâ€™s device and runtime capabilities, including device, stream, event, generator, allocator, and guard, to seamlessly support streaming devices. This enhancement simplifies deploying PyTorch on ubiquitous hardware, making it easier for you to integrate different hardware back ends.&lt;/p&gt;

&lt;p&gt;Intel GPU support upstreamed into PyTorch provides support for both eager and graph modes, fully running Dynamo Hugging Face benchmarks. Eager mode now includes common Aten operators implemented with SYCL. The most performance-critical graphs and operators are highly optimized by using oneAPI Deep Neural Network Library (oneDNN) and oneAPI Math Kernel Library (oneMKL). Graph mode (torch.compile) now has an enabled Intel GPU back end to implement the optimization for Intel GPUs and to integrate Triton. Furthermore, data types such as FP32, BF16, FP16, and automatic mixed precision (AMP) are supported. The PyTorch Profiler, based on Kineto and oneMKL, is being developed for the upcoming PyTorch 2.5 release.&lt;/p&gt;

&lt;p&gt;Take a look at the current and planned front-end and back-end improvements for Intel GPU upstreamed into PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/intel-gpus-pytorch-2-4.jpg&quot; alt=&quot;the current and planned front-end and back-end improvements for Intel GPU upstreamed into PyTorch&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.4 on Linux supports Intel Data Center GPU Max Series for training and inference while maintaining the same user experience as other hardware. If youâ€™re migrating code from CUDA, you can run your existing application on an Intel GPU with minimal changesâ€”just update the device name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cuda&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xpu&lt;/code&gt;. For example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# CUDA Code 
tensor = torch.tensor([1.0, 2.0]).to(&quot;cuda&quot;) 
 
# Code for Intel GPU 
tensor = torch.tensor([1.0, 2.0]).to(&quot;xpu&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;get-started&quot;&gt;Get Started&lt;/h2&gt;

&lt;p&gt;Try PyTorch 2.4 on the Intel Data Center GPU Max Series through the &lt;a href=&quot;https://cloud.intel.com/&quot;&gt;IntelÂ® Tiberâ„¢ Developer Cloud&lt;/a&gt;. Get a tour of the &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html#examples&quot;&gt;environment setup, source build,â€¯andâ€¯examples&lt;/a&gt;. To learn how to create a free Standard account, see &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;Get Started&lt;/a&gt;, then do the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Sign in to the &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;cloud console&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;From the &lt;a href=&quot;https://console.cloud.intel.com/training&quot;&gt;Training&lt;/a&gt; section, open the &lt;strong&gt;PyTorch 2.4 on Intel GPUs&lt;/strong&gt; notebook.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that the &lt;strong&gt;PyTorch 2.4&lt;/strong&gt; kernel is selected for the notebook.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PyTorch 2.4 introduces initial support for Intel Data Center GPU Max Series to accelerate your AI workloads. With Intel GPU, youâ€™ll get continuous software support, unified distribution, and synchronized release schedules for a smoother development experience. Weâ€™re enhancing this functionality to reach Beta quality in PyTorch 2.5. Planned features in 2.5 include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;More Aten operators and full Dynamo Torchbench and TIMM support in Eager Mode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Full Dynamo Torchbench and TIMM benchmark support in torch.compile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Intel GPU support in torch.profile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PyPI wheels distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Windows and Intel Client GPU Series support.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We welcome the community to evaluate these new contributions toâ€¯&lt;a href=&quot;https://github.com/pytorch/pytorch?tab=readme-ov-file#intel-gpu-support&quot;&gt;Intel GPU support on PyTorch&lt;/a&gt;.â€¯&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;PyTorch 2.4: Get Started on an Intel GPU&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;PyTorch Release Notes&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We want thank PyTorch open source community for their technical discussions and insights: &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;p&gt;1 To enable GPU support and improve performance, we suggest installing the &lt;a href=&quot;https://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;IntelÂ® Extension for PyTorch&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the PyTorch Team at Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">We have exciting news! PyTorch 2.4 now supports IntelÂ® Data Center GPU Max Series and the SYCL software stack, making it easier to speed up your AI workflows for both training and inference. This update allows for you to have a consistent programming experience with minimal coding effort and extends PyTorchâ€™s device and runtime capabilities, including device, stream, event, generator, allocator, and guard, to seamlessly support streaming devices. This enhancement simplifies deploying PyTorch on ubiquitous hardware, making it easier for you to integrate different hardware back ends.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Enabling Fast Gradient Clipping and Ghost Clipping in Opacus</title>
      <link href="https://pytorch.org/blog/clipping-in-opacus/" rel="alternate" type="text/html" title="Enabling Fast Gradient Clipping and Ghost Clipping in Opacus" />
      <published>2024-08-20T00:00:00-07:00</published>
      <updated>2024-08-20T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/clipping-in-opacus</id>
      <content type="html" xml:base="https://pytorch.org/blog/clipping-in-opacus/">&lt;h2 id=&quot;introduction-and-context&quot;&gt;Introduction and Context&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;&gt;Differentially Private Stochastic Gradient Descent (DP-SGD)&lt;/a&gt; is the canonical method for training machine learning models with differential privacy. It involves the following two modifications to its non-private counterpart, Stochastic Gradient Descent.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Per-sample gradient clipping&lt;/strong&gt;: Clip gradients with respect to every sample in the mini-batch, ensuring that its norm is at most a pre-specified value, â€œClipping Normâ€, C, in every iteration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Noise addition&lt;/strong&gt;: Add Gaussian noise of pre-specified variance, depending on the clipping norm and privacy parameters, to the average clipped gradient, in every iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first change, &lt;strong&gt;per-sample gradient clipping&lt;/strong&gt;, introduces additional complexities since, in general, it requires instantiating &lt;strong&gt;per-sample&lt;/strong&gt; &lt;strong&gt;gradients&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://opacus.ai&quot;&gt;Opacus&lt;/a&gt; is a PyTorch implementation of DP-SGD. Opacus addresses the above task by employing &lt;a href=&quot;https://medium.com/pytorch/differential-privacy-series-part-2-efficient-per-sample-gradient-computation-in-opacus-5bf4031d9e22&quot;&gt;hook functions&lt;/a&gt;, which allows intervening on specific events, such as forward and backward passes. For more details about Opacus, we encourage readers to review the previous blog posts: &lt;a href=&quot;https://bit.ly/dp-sgd-algorithm-explained&quot;&gt;DP-SGD Algorithm Explained&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch/differential-privacy-series-part-2-efficient-per-sample-gradient-computation-in-opacus-5bf4031d9e22&quot;&gt;Efficient Per-Sample Gradient Computation in Opacus&lt;/a&gt; and &lt;a href=&quot;https://pytorch.medium.com/differential-privacy-series-part-3-efficient-per-sample-gradient-computation-for-more-layers-in-39bd25df237&quot;&gt;Efficient Per-Sample Gradient Computation for More Layers in Opacus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While Opacus provides substantial efficiency gains compared to the naive approaches, the memory cost of instantiating per-sample gradients is significant. In particular, memory usage is proportional to the batch size times the number of trainable parameters. Consequently, memory limits Opacus to small batch sizes and/or small models, significantly restricting its range of applications.&lt;/p&gt;

&lt;p&gt;We introduce &lt;a href=&quot;https://arxiv.org/abs/2009.03106&quot;&gt;Fast Gradient Clipping&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;Ghost Clipping&lt;/a&gt; to Opacus, which enable developers and researchers to perform gradient clipping without instantiating the per-sample gradients. As an example, this allows for fine-tuning 7M parameters of BERT, on a single 16GB GPU, with a batch size of 1024, with memory comparable to using PyTorch (without applying DP-SGD). In contrast, the previous version of Opacus, supported a maximum batch size of roughly 256 for the same setting. We provide a &lt;a href=&quot;https://github.com/pytorch/opacus/blob/main/tutorials/building\_text\_classifier.ipynb&quot;&gt;tutorial&lt;/a&gt; on how to use Fast Gradient Clipping in Opacus with the aforementioned task as an example.&lt;/p&gt;

&lt;h2 id=&quot;fast-gradient-clipping-and-ghost-clipping&quot;&gt;Fast Gradient Clipping and Ghost Clipping&lt;/h2&gt;

&lt;p&gt;The key idea behind these techniques is based on the following observation: suppose per-sample gradient norms are known, then gradient clipping can be achieved by backpropagation on a re-weighted loss function $ \bar{L} $. This loss function is defined as  $ \bar{L} = \sum_{i} R_{i} L_{i} $, where $ R_i = \min\left(\frac{C}{C_i}, 1\right) $ are the clipping coefficients computed from the per-sample gradient norms $ {C_i} $ and $ {L_i} $ are per-sample losses.&lt;/p&gt;

&lt;p&gt;The above idea may seem circular at first glance, as it appears to require instantiating per-sample gradients in order to calculate per-sample gradient norms. However, for certain widely-used components of neural network architectures, such as fully connected/linear layers, it is indeed possible to obtain per-sample gradient norms in a single backpropagation pass without the need for per-sample gradients. This suggests a workflow that involves two backpropagation passes: the first to compute per-sample gradient norms, and the second to compute the aggregated (not per-sample) clipped gradient. The second backpropagation is simply the standard batched backpropagation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clipping-in-opacus/fg1.jpg&quot; alt=&quot;backpropagation diagram&quot; style=&quot;max-width:800px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clipping-in-opacus/fg2.png&quot; alt=&quot;backpropagation diagram&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Comparison between vanilla &lt;strong&gt;Opacus&lt;/strong&gt; (top left), &lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt; (top right), and &lt;strong&gt;Ghost clipping&lt;/strong&gt; (bottom). We marked in red gradient instantiations that become memory bottlenecks. For vanilla Opacus, it has to instantiate the &lt;strong&gt;per-sample gradients&lt;/strong&gt;. &lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt; instantiates per-sample gradients for each layer to compute its norm, which is immediately released once the backward pass moves on to the next layer. Ghost Clipping works directly from &lt;strong&gt;per-sample activation gradients&lt;/strong&gt; and &lt;strong&gt;per-sample activations&lt;/strong&gt;, and avoids the need for gradient instantiation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.03106&quot;&gt;&lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
In Fast Gradient Clipping, the per-sample gradient norm is calculated in three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For each layer, the per-sample gradient is instantiated and its norm is calculated.&lt;/li&gt;
  &lt;li&gt;The per-sample gradient is then immediately discarded.&lt;/li&gt;
  &lt;li&gt;The (squared) per-sample gradient norms of each layer are summed up to obtain the overall (squared) per-sample gradient norm.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;&lt;strong&gt;Ghost Clipping&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;
Extending the approach of Fast Gradient Clipping, Ghost Clipping uses the &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;&gt;fact&lt;/a&gt; that for &lt;strong&gt;linear layers&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;/strong&gt; per-sample gradient norms can be calculated just from &lt;strong&gt;activation gradients&lt;/strong&gt; and  &lt;strong&gt;activations&lt;/strong&gt;. In particular, let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backprops&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;activations&lt;/code&gt; be per-sample activation gradients and activations, of dimensions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size âœ• output_width&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size âœ• input_width&lt;/code&gt;, respectively. The per-sample gradient is the outer product of the two, which takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch_size âœ• input_width âœ• output_width)&lt;/code&gt; time and space.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;&gt;ghost clipping trick&lt;/a&gt; instead calculates the (squared) norm of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backprops&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;activations&lt;/code&gt;, sample-wise, and takes their product, which gives the (squared) norm of the gradient. This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch-size âœ• (input_width + output_width))&lt;/code&gt; time and takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch-size)&lt;/code&gt; space to store. Since &lt;strong&gt;per-sample activation&lt;/strong&gt; and &lt;strong&gt;per-sample activation gradients&lt;/strong&gt; are already stored, additional memory is needed only for storing the norms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship between Fast Gradient Clipping and Ghost Clipping&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fast Gradient Clipping and Ghost Clipping are complementary techniques. Fast Gradient Clipping can be applied to any type of layer, while Ghost Clipping is a strictly better technique for supported layers.&lt;/li&gt;
  &lt;li&gt;Our implementation automatically switches to Fast Gradient Clipping when the layer is not supported by Ghost Clipping.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;how-to-use-fast-gradient-clipping-in-opacus&quot;&gt;How to use Fast Gradient Clipping in Opacus&lt;/h3&gt;

&lt;p&gt;The training loop is identical to that of the standard PyTorch loop. As in Opacus before, we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PrivacyEngine()&lt;/code&gt;, which â€œsanitizesâ€ the model and optimizer. To enable Ghost Clipping, the argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_sample_mode=&quot;ghost&quot;&lt;/code&gt; is used. Additionally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_private()&lt;/code&gt; takes the loss criterion as an extra input and sanitizes it. This allows us to hide the two backward passes and the loss rescaling in between in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;opacus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PrivacyEngine&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# example loss function
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;privacy_engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PrivacyEngine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;privacy_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_private&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;noise_multiplier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noise_multiplier&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_grad_norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_grad_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad_sample_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ghost&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The training loop below is identical to that of PyTorch
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_gc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Add noise and update the model
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Internally, before the first pass, we enable the &lt;em&gt;hooks&lt;/em&gt;, which allows us to capture layer-wise values corresponding to forward and backward calls. They are used to compute the per-sample gradient norms. We then compute the clipping coefficients, rescale the loss function and disable hooks, which lets us use the standard PyTorch backward pass.&lt;/p&gt;

&lt;h3 id=&quot;memory-complexity-analysis&quot;&gt;Memory Complexity Analysis&lt;/h3&gt;

&lt;p&gt;Consider a multi-layer neural network with the following properties:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;L&lt;/strong&gt;: Number of layers&lt;br /&gt;
&lt;strong&gt;d&lt;/strong&gt;: Maximum layer width&lt;br /&gt;
&lt;strong&gt;B&lt;/strong&gt;: Batch size&lt;br /&gt;
&lt;strong&gt;K&lt;/strong&gt;: Number of non-supported/non-linear layers&lt;/p&gt;

&lt;p&gt;The memory overhead of DP-SGD with Ghost Clipping compared to plain (PyTorch) SGD is an additive O(BL), required to store the per-sample gradient norms for all layers. Further, if there is a non-supported layer (if Kâ‰¥1), then there is an additional O(Bd&lt;sup&gt;2&lt;/sup&gt;) memory to instantiate the gradient of that layer.&lt;/p&gt;

&lt;h3 id=&quot;memory-benchmarking&quot;&gt;Memory Benchmarking&lt;/h3&gt;

&lt;p&gt;We provide results on the memory usage for a variety of settings.&lt;/p&gt;

&lt;h4 id=&quot;fine-tuning-bert&quot;&gt;Fine-Tuning BERT&lt;/h4&gt;

&lt;p&gt;We consider the problem of &lt;a href=&quot;https://github.com/pytorch/opacus/blob/main/tutorials/building\_text\_classifier.ipynb&quot;&gt;privately fine-tuning&lt;/a&gt; the last three layers of BERT for a text classification task. The base model has over 100M parameters, of which we fine-tune the last three layers, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertEncoder,&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertPooler,&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classifier&lt;/code&gt;, comprising roughly 7.6M parameters. The experiments are run on a P100 GPU with 16 GB of memory.&lt;/p&gt;

&lt;p&gt;The following table reports the maximum memory and time taken per iteration for the various methods:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt; 
   &lt;/td&gt;
   &lt;td colspan=&quot;9&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 32&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 128&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 512&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 1024&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;B = 2048&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;236 MB
   &lt;/td&gt;
   &lt;td&gt;0.15 s
   &lt;/td&gt;
   &lt;td&gt;1.04 GB
   &lt;/td&gt;
   &lt;td&gt;0.55 s
   &lt;/td&gt;
   &lt;td&gt;5.27 GB
   &lt;/td&gt;
   &lt;td&gt;2.1 s
   &lt;/td&gt;
   &lt;td&gt;12.7 GB
   &lt;/td&gt;
   &lt;td&gt;4.2 s
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;1,142 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;4.55 GB
   &lt;/td&gt;
   &lt;td&gt;0.68 s
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;FGC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;908 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;3.6 GB
   &lt;/td&gt;
   &lt;td&gt;0.75 s
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;362 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;1.32 GB
   &lt;/td&gt;
   &lt;td&gt;0.67 s
   &lt;/td&gt;
   &lt;td&gt;5.27 GB
   &lt;/td&gt;
   &lt;td&gt;2.5 s
   &lt;/td&gt;
   &lt;td&gt;12.7 GB
   &lt;/td&gt;
   &lt;td&gt;5 s
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In terms of peak memory footprint, DP-SGD &amp;gt; FGC DP-SGD â‰« GC DP-SGD â‰ˆ PyTorch SGD. Further, the runtimes are similar because most of the parameters are frozen and the forward pass takes up most of the time.&lt;/p&gt;

&lt;h4 id=&quot;synthetic-setup-memory-profiling&quot;&gt;Synthetic Setup: Memory Profiling&lt;/h4&gt;

&lt;p&gt;We consider the following setup to profile the memory used by PyTorch SGD, Vanilla DP-SGD and Ghost Clipping, GC DP-SGD.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2-layer fully connected neural network
    &lt;ul&gt;
      &lt;li&gt;Input: 5120&lt;/li&gt;
      &lt;li&gt;Hidden: 2560&lt;/li&gt;
      &lt;li&gt;Output: 1280&lt;/li&gt;
      &lt;li&gt;Total number of model parameters = 15.6M&lt;/li&gt;
      &lt;li&gt;Model size = 62.5 MB&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Batch size, different values, as seen in the table below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The table below summarizes the max memory increase (in MB) broken down by stages of the training loop for each of the methods.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Model to GPU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Forward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;First Backward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Second Backward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Optimizer Step&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;32
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.5
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Vanilla DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;3,663
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;162.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;63.13
   &lt;/td&gt;
   &lt;td&gt;50
   &lt;/td&gt;
   &lt;td&gt;125
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;2&lt;sup&gt;17&lt;/sup&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;1920
   &lt;/td&gt;
   &lt;td&gt;1932.5
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Vanilla DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;5&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;1920
   &lt;/td&gt;
   &lt;td&gt;2625
   &lt;/td&gt;
   &lt;td&gt;1932.5
   &lt;/td&gt;
   &lt;td&gt;125
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;industry-use-case&quot;&gt;Industry use case&lt;/h4&gt;

&lt;p&gt;We tested Ghost Clipping DP-SGD on an internal Meta use case, consisting of a model of size roughly 100B with 40M trainable parameters. Our initial results show that Ghost Clipping SGD reduces 95% memory of vanilla DP-SGD, and achieves comparable memory usage to PyTorch SGD.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we describe implementations of Fast Gradient Clipping and Ghost Clipping in Opacus that enable memory-efficient training of machine learning models with differential privacy. Currently, the Ghost Clipping implementation only applies to linear layers, but, as outlined in &lt;a href=&quot;https://pytorch.medium.com/differential-privacy-series-part-3-efficient-per-sample-gradient-computation-for-more-layers-in-39bd25df237&quot;&gt;part 3 of the series&lt;/a&gt;, it can be extended to â€œgeneralizedâ€ linear layers such as convolutions and multi-head attention. The current techniques require two explicit backpropagation steps, which increases runtime. We will explore developments on top of Ghost Clipping such as the &lt;a href=&quot;https://arxiv.org/abs/2210.00038&quot;&gt;Book-Keeping algorithm&lt;/a&gt; for mitigation.&lt;/p&gt;

&lt;p&gt;To learn more about Opacus, visit &lt;a href=&quot;https://opacus.ai/&quot;&gt;opacus.ai&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/opacus&quot;&gt;github.com/pytorch/opacus&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank Iden Kalemaj, Darren Liu, Karthik Prasad, Hao Shi, Igor Shilov, Davide Testuggine, Eli Uriegas, Haicheng Wang, and Richard Zou for valuable feedback and suggestions.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;There are &lt;a href=&quot;https://proceedings.neurips.cc/paper\_files/paper/2023/file/a45d344b28179c8da7646bc38ff50ad8-Paper-Conference.pdf&quot;&gt;ways&lt;/a&gt; to extend Ghost Clipping to non-linear layers.Â &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Enayat Ullah, Huanyu Zhang, Will Bullock, Ilya Mironov</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction and Context</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention</title>
      <link href="https://pytorch.org/blog/flexattention/" rel="alternate" type="text/html" title="FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention" />
      <published>2024-08-07T00:00:00-07:00</published>
      <updated>2024-08-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flexattention</id>
      <content type="html" xml:base="https://pytorch.org/blog/flexattention/">&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg1.jpg&quot; alt=&quot;a cartoon chart flexing his muscles&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In theory, Attention is All You Need. In practice, however, we also need optimized attention implementations like FlashAttention.&lt;/p&gt;

&lt;p&gt;Although these fused attention implementations have substantially improved performance and enabled long contexts, this efficiency has come with a loss of flexibility. You can no longer try out a new attention variant by writing a few PyTorch operators - you often need to write a new custom kernel! This operates as a sort of â€œsoftware lotteryâ€ for ML researchers - if your attention variant doesnâ€™t fit into one of the existing optimized kernels, youâ€™re doomed to slow runtime and CUDA OOMs.&lt;/p&gt;

&lt;p&gt;For some examples of attention variants, we have Causal, &lt;a href=&quot;https://paperswithcode.com/method/relative-position-encodings&quot;&gt;Relative Positional Embeddings&lt;/a&gt;, &lt;a href=&quot;https://paperswithcode.com/method/alibi&quot;&gt;Alibi&lt;/a&gt;, &lt;a href=&quot;https://mistral.ai/news/announcing-mistral-7b/&quot;&gt;Sliding Window Attention&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/andersonbcdefg/status/1800907703688339569&quot;&gt;PrefixLM&lt;/a&gt;,  &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/875&quot;&gt;Document Masking/Sample Packing/Jagged Tensors&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/LysandreJik/status/1807779471891538199&quot;&gt;Tanh Soft-Capping&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;PagedAttention&lt;/a&gt;, etc. Even worse, folks often want combinations of these! Sliding Window Attention + Document Masking + Causal + Context Parallelism? Or what about PagedAttention + Sliding Window + Tanh Soft-Capping?&lt;/p&gt;

&lt;p&gt;The left picture below represents the state of the world today - some combinations of masking + biases + setting have existing kernels implemented. But the various options lead to an exponential number of settings, and so overall we end up with fairly spotty support. Even worse, new attention variants researchers come up with will have &lt;em&gt;zero&lt;/em&gt; support.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg2.jpg&quot; alt=&quot;Attention variant support diagram&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To solve this hypercube problem once and for all, we introduce &lt;strong&gt;FlexAttention&lt;/strong&gt;, a new PyTorch API.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We provide a flexible API that allows implementing many attention variants (including all the ones mentioned in the blog post so far) in a few lines of idiomatic PyTorch code.&lt;/li&gt;
  &lt;li&gt;We lower this into a fused FlashAttention kernel through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, generating a FlashAttention kernel that doesnâ€™t materialize any extra memory and has performance competitive with handwritten ones.&lt;/li&gt;
  &lt;li&gt;We also automatically generate the backwards pass, leveraging PyTorchâ€™s autograd machinery.&lt;/li&gt;
  &lt;li&gt;Finally, we can also take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With FlexAttention, we hope that trying new attention variants will only be limited by your imagination.&lt;/p&gt;

&lt;p&gt;You can find many FlexAttention examples at the Attention Gym: &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;https://github.com/pytorch-labs/attention-gym&lt;/a&gt;. If you have any cool applications, feel free to submit an example!&lt;/p&gt;

&lt;p&gt;PS: We also find this API very exciting since it leverages a lot of existing PyTorch infra in a fun way - more on that in the end.&lt;/p&gt;

&lt;h2 id=&quot;flexattention&quot;&gt;FlexAttention&lt;/h2&gt;

&lt;p&gt;Here is the classic attention equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg3.png&quot; alt=&quot;math equation&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In code form:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FlexAttention allows for an user-defined function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg4.png&quot; alt=&quot;math equation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In code form:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function allows you to &lt;em&gt;modify&lt;/em&gt; the attention scores prior to softmax. Surprisingly, this ends up being sufficient for the vast majority of attention variants (examples below)!&lt;/p&gt;

&lt;p&gt;Concretely, the expected signature for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is somewhat unique.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# noop - standard attention
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; is a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments tell you &lt;em&gt;which&lt;/em&gt; dot product youâ€™re currently computing - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; (current element in batch), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt; (current head), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q_idx&lt;/code&gt; (position in query), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kv_idx&lt;/code&gt; (position in key/value tensors).&lt;/p&gt;

&lt;p&gt;To apply this function, we could implement it as&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, this is not how FlexAttention is implemented under the hood. Leveraging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, we automatically lower your function into a single &lt;em&gt;fused&lt;/em&gt; FlexAttention kernel - guaranteed or your money back!&lt;/p&gt;

&lt;p&gt;This API ends up being surprisingly expressive. Letâ€™s look at some examples.&lt;/p&gt;

&lt;h2 id=&quot;score-mod-examples&quot;&gt;Score Mod Examples&lt;/h2&gt;

&lt;h3 id=&quot;full-attention&quot;&gt;Full Attention&lt;/h3&gt;

&lt;p&gt;Letâ€™s first do â€œfull attentionâ€, or standard bidirectional attention. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is a no-op - it takes as input the scores and then returns them as is..&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;noop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And to use it end to end (including both forwards &lt;em&gt;and&lt;/em&gt; backwards):&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention.flex_attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;relative-position-encodings&quot;&gt;Relative Position Encodings&lt;/h3&gt;

&lt;p&gt;One common attention variant is the &lt;a href=&quot;https://paperswithcode.com/method/relative-position-encodings&quot;&gt;â€œrelative position encoding&lt;/a&gt;â€. Instead of encoding the absolute distance in the queries and keys, relative position encoding adjusts scores based on the â€œdistanceâ€ between the queries and keys.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;relative_positional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that unlike typical implementations, this does &lt;em&gt;not&lt;/em&gt; need to materialize a SxS tensor. Instead, FlexAttention computes the bias values â€œon the flyâ€ within the kernel, leading to significant memory and performance improvements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg5.png&quot; alt=&quot;relative position encoding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alibi-bias&quot;&gt;ALiBi Bias&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg6.png&quot; alt=&quot;alibi bias&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2108.12409&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ALiBi was introduced in &lt;a href=&quot;https://arxiv.org/abs/2108.12409&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;, and claims to have beneficial properties for length extrapolation at inference. Notably, MosaicML has pointed to &lt;a href=&quot;https://twitter.com/jefrankle/status/1804567458092605736&quot;&gt;â€œlack of kernel supportâ€&lt;/a&gt; as the main reason why they eventually switched from ALiBi to rotary embeddings.&lt;/p&gt;

&lt;p&gt;Alibi is similar to relative positional encodings with one exception - it has a per-head factor that is typically precomputed.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alibi_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_alibi_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [num_heads]
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;alibi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alibi_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This demonstrates one interesting piece of flexibility &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; provides - we can load from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; even though it &lt;em&gt;wasnâ€™t explicitly passed in as an input&lt;/em&gt;! The generated Triton kernel will calculate the correct loads from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; tensor and fuse it. Note that you could regenerate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; and we still wouldnâ€™t need to recompile.&lt;/p&gt;

&lt;h3 id=&quot;soft-capping&quot;&gt;Soft-capping&lt;/h3&gt;

&lt;p&gt;Soft-capping is a technique used in &lt;a href=&quot;https://huggingface.co/blog/gemma2#soft-capping-and-attention-implementations&quot;&gt;Gemma2&lt;/a&gt; and Grok-1 that prevents logits from growing excessively large. In FlexAttention, it looks like:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;soft_cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we also automatically generate the backwards pass from the forwards pass here. Also, although this implementation is semantically correct, we likely want to use a tanh approximation in this case for performance reasons. See &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym/blob/main/attn_gym/mods/softcapping.py&quot;&gt;attention-gym&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;causal-mask&quot;&gt;Causal Mask&lt;/h3&gt;

&lt;p&gt;Although bidirectional attention is the simplest, the original &lt;em&gt;Attention is All You Need&lt;/em&gt; paper and the vast majority of LLMs use attention in a decoder-only setting where each token can only attend to the tokens prior to it. Folks often think of this as a lower-triangular mask, but with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; API it can be expressed as:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically, if the query token is â€œafterâ€ the key token, we keep the score. Otherwise, we mask it out by setting it to -inf, thus ensuring it wonâ€™t participate in the softmax calculation.&lt;/p&gt;

&lt;p&gt;However, masking is special compared to other modifications - if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is sufficient to implement causal masking &lt;em&gt;correctly&lt;/em&gt;, getting the performance benefits of sparsity requires another concept - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mask-mods&quot;&gt;Mask Mods&lt;/h2&gt;

&lt;p&gt;To take advantage of sparsity from masking, we need to do some more work. Specifically, by passing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/e49c0acc396e89baf8c6450e1fa0571d4ce2d4ed/torch/nn/attention/flex_attention.py#L594&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;&lt;/a&gt;, we can create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;. FlexAttention can then use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt; to take advantage of the sparsity!&lt;/p&gt;

&lt;p&gt;The signature of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; is very similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; - just without the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt;. In particular&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# returns True if this position should participate in the computation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is strictly &lt;em&gt;more&lt;/em&gt; expressive than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;. However, for masking, itâ€™s recommended to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;, as itâ€™s more performant. See the FAQ on why &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; are separate.&lt;/p&gt;

&lt;p&gt;Now, letâ€™s take a look at how we might implement causal mask with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;causal-mask-1&quot;&gt;Causal Mask&lt;/h3&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention.flex_attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_LEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KV_LEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# In this case, we don't need a score_mod, so we won't pass any in.
# However, score_mod can still be combined with block_mask if you need the additional flexibility.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; is a &lt;strong&gt;relatively expensive operation!&lt;/strong&gt; Although FlexAttention will not need to recompile when it changes, if you arenâ€™t careful about caching it, it can lead to significant slowdowns (check out the FAQ for suggestions on best practices).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg7.png&quot; alt=&quot;flexattention performance charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the TFlops are roughly the same, the execution time is 2x faster for the mask_mod version! This demonstrates that we can leverage the sparsity that BlockMask provides us &lt;em&gt;without&lt;/em&gt; losing hardware efficiency.&lt;/p&gt;

&lt;h3 id=&quot;sliding-window--causal&quot;&gt;Sliding Window + Causal&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg8.png&quot; alt=&quot;Sliding Window Causal diagrams&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2310.06825&quot;&gt;Mistral 7B&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Popularized by &lt;a href=&quot;https://arxiv.org/abs/2310.06825&quot;&gt;Mistral&lt;/a&gt;, sliding window attention (also known as local attention) takes advantage of the intuition that the most recent tokens are the most useful. In particular, it allows the query token to only attend to, say, the 1024 most recent tokens. This is often used together with causal attention.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sliding_window_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;window_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window_mask&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If you want to be cute...
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sliding_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sliding_window_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sliding_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We benchmark it against &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F.scaled_dot_product_attention&lt;/code&gt; with a sliding window mask as well as FA2 with a causal mask (as a reference point for performance). Not only are we significantly faster than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F.scaled_dot_product_attention&lt;/code&gt;, weâ€™re &lt;em&gt;also&lt;/em&gt; significantly faster than FA2 with a causal mask as this mask has significantly more sparsity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg9.png&quot; alt=&quot;execution time charts&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;prefixlm&quot;&gt;PrefixLM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg10.png&quot; alt=&quot;PrefixLM diagram&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;PaliGemma: A versatile 3B VLM for transfer&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The T5 architecture, proposed in &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;, describes an attention variant that performs full bidirectional attention on a â€œprefixâ€, and causal attention on the rest. We again compose two mask functions to accomplish this, one for causal masking and one that is based off of the prefix length.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# In this case, our mask is different per sequence so we set B equal to our batch size
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just like with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; allows us to refer to additional tensors that arenâ€™t explicitly an input to the function! However, with prefixLM, the sparsity pattern changes &lt;em&gt;per&lt;/em&gt; &lt;em&gt;input&lt;/em&gt;. This means that for each new input batch, weâ€™ll need to recompute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;. One common pattern is to call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; at the beginning of your model and reuse that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;block_mask&lt;/code&gt; for all attention calls in your model. See &lt;em&gt;Recomputing Block Masks vs. Recompilation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, in exchange for that, weâ€™re not only able to have an efficient attention kernel for prefixLM, weâ€™re &lt;em&gt;also&lt;/em&gt; able to take advantage of however much sparsity exists in the input! FlexAttention will dynamically adjust its performance based off of the BlockMask data, &lt;em&gt;without&lt;/em&gt; needing to recompile the kernel.&lt;/p&gt;

&lt;h3 id=&quot;document-maskingjagged-sequences&quot;&gt;Document Masking/Jagged Sequences&lt;/h3&gt;

&lt;p&gt;Another common attention variant is document masking/jagged sequences. Imagine that you have a number of sequences of varying length. You want to train on all of them together, but unfortunately, most operators only accept rectangular tensors.&lt;/p&gt;

&lt;p&gt;Through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;, we can support this efficiently in FlexAttention as well!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we flatten all sequences into a single sequence with sum(sequence lengths) tokens.&lt;/li&gt;
  &lt;li&gt;Then, we compute the document_id that each token belongs to.&lt;/li&gt;
  &lt;li&gt;Finally, in our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;, we simply whether the query and kv token belong to the same document!&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# The document that each token belongs to.
# e.g. [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2] corresponds to sequence lengths 3, 2, and 6.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;document_masking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And thatâ€™s it! In this case, we see that we end up with a blockdiagonal mask.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg11.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One interesting aspect about document masking is that itâ€™s easy to see how it might combine with an arbitrary combination of other masks . For example, we already defined &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefixlm_mask&lt;/code&gt; in the previous section. Do we now need to define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefixlm_document_mask&lt;/code&gt; function as well?&lt;/p&gt;

&lt;p&gt;In these cases, one pattern weâ€™ve found quite useful is what we call a â€œhigher level modificationâ€. In this case, we can take an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; and automatically transform it into one that works with jagged sequences!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_doc_mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Get unique document IDs and their counts
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_consecutive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create cumulative counts (offsets)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;doc_mask_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;same_doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_logical&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kv_logical&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inner_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_logical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_logical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;same_doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner_mask&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_mask_wrapper&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, given the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix_lm_causal&lt;/code&gt; mask from above, we can transform it into one that works on on packed documents like so:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc_prefix_lm_causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_doc_mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg12.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, this mask is â€œblock-prefixLM-diagonalâ€ shaped. :)&lt;/p&gt;

&lt;p&gt;Thatâ€™s all of our examples! There are far more attention variants than we have space to list, so check out &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;Attention Gym&lt;/a&gt; for more examples. We hope that the community will contribute some of their favorite applications of FlexAttention as well.&lt;/p&gt;

&lt;h3 id=&quot;faq&quot;&gt;FAQ&lt;/h3&gt;

&lt;h5 id=&quot;q-when-does-flexattention-need-to-recompile&quot;&gt;&lt;strong&gt;Q: When does FlexAttention need to recompile?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;As FlexAttention leverages &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; for graph capture, it can actually avoid recompilation in a broad spectrum of cases. Notably, it does &lt;em&gt;not&lt;/em&gt; need to recompile even if captured tensors change values!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_mod&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_mod1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_mod1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Compiles the kernel here 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_mod2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_mod2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Doesn't need to recompile! 
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even changing the block-sparsity doesnâ€™t require a recompile. However, if the block-sparsity changes, we do need to &lt;em&gt;recompute&lt;/em&gt; the BlockMask.&lt;/p&gt;

&lt;h5 id=&quot;q-when-should-we-recompute-the-blockmask&quot;&gt;&lt;strong&gt;Q: When should we recompute the BlockMask?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;We need to recompute the BlockMask whenever the block-sparsity changes. Although computing the BlockMask is much cheaper than recompilation (on the order of hundreds of microseconds as opposed to seconds), you should still take care to not excessively recompute the BlockMask.&lt;/p&gt;

&lt;p&gt;Here are some common patterns and some recommendations on how you might approach them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mask never changes (e.g. causal mask)&lt;/strong&gt;&lt;br /&gt;
In this case, you can simply precompute the block mask and cache it globally, reusing it for all attention calls.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;causal_attention&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mask changes every batch (e.g. document masking)&lt;/strong&gt;&lt;br /&gt;
In this case, we would suggest computing the BlockMask at the beginning of the model and threading it through the model - reusing the BlockMask for all layers.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Compute block mask at beginning of forwards
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# amortize block mask construction cost across all layers
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mask changes every layer (e.g. data-dependent sparsity)&lt;/strong&gt;&lt;br /&gt;
This is the hardest setting, since weâ€™re unable to amortize the block mask computation across multiple FlexAttention invocations. Although FlexAttention can certainly still benefit this case, the actual benefits from BlockMask depend on how sparse your attention mask is and how fast we can construct the BlockMask. That leads us toâ€¦&lt;/p&gt;

&lt;h5 id=&quot;q-how-can-we-compute-blockmask-quicker&quot;&gt;&lt;strong&gt;Q: How can we compute BlockMask quicker?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; is unfortunately fairly expensive, both from a memory and compute perspective, as determining whether a block is completely sparse requires evaluating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; at every single point in the block. There are a couple ways to address this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If your mask is the same across batch size or heads, make sure that youâ€™re broadcasting over those (i.e. set them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Compile &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;. Unfortunately, today, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; does not work directly on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; due to some unfortunate limitations. However, you can set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_compile=True&lt;/code&gt;, which will significantly reduce the peak memory and runtime (often an order of magnitude in our testing).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Write a custom constructor for BlockMask. The metadata for BlockMask is quite simple (see the &lt;a href=&quot;https://pytorch.org/docs/main/nn.attention.flex_attention.html#blockmask&quot;&gt;documentation&lt;/a&gt;). Itâ€™s essentially two tensors.
a. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_blocks&lt;/code&gt;: The number of KV blocks computed for each query block.&lt;br /&gt;
b. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;indices&lt;/code&gt;: The positions of the KV blocks computed for each query block.&lt;/p&gt;

    &lt;p&gt;For example, hereâ€™s a custom BlockMask constructor for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causal_mask&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# The first query block computes one block, the second query block computes 2 blocks, etc.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Since we're always computing from the left to the right,
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# we can use the indices [0, 1, 2, ...] for every query block.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BlockMask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;q-why-are-score_mod-and-mask_mod-different-isnt-mask_mod-just-a-special-case-of-score_mod&quot;&gt;&lt;strong&gt;Q: Why are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; different? Isnâ€™t &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; just a special case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;Very astute question, hypothetical audience member! In fact, any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; can be easily converted to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; (we do not recommend using this function in practice!)&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mask_mod_as_score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; can implement everything &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; can, whatâ€™s the point of having &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;One immediate challenge: a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; requires the actual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; value as an input, but when weâ€™re precomputing the BlockMask, we donâ€™t have the actual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; value. We can perhaps fake the values by passing in all zeros, and if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt;, then we consider it to be masked (in fact, we originally did this!).&lt;/p&gt;

&lt;p&gt;However, there are two issues. The first is that this is hacky - what if the userâ€™s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; returned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt; when the input is 0? Or what if the userâ€™s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; masked out with a large negative value instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt;? It seems weâ€™re trying to cram a round peg into a square hole. However, thereâ€™s a more important reason to separate out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; - itâ€™s fundamentally more efficient!.&lt;/p&gt;

&lt;p&gt;As it turns out, applying masking to every single computed element is actually quite expensive - our benchmarks see about a 15-20% degradation in performance! So, although we can get significant speedups by skipping half the computation, we lose a meaningful part of that speedup from needing to mask out every element!&lt;/p&gt;

&lt;p&gt;Luckily, if we visualize the causal mask, we notice that the vast majority of blocks do not require a â€œcausal maskâ€ at all - theyâ€™re fully computed! It is only the blocks on the diagonal, partially computed and partially masked, that require masking to be applied.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg13.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The BlockMask previously told us which blocks we need to compute and which blocks we can skip. Now, we further augment this data structure to also tell us which blocks are â€œfully computedâ€ (i.e. masking can be skipped) vs. â€œpartially computedâ€ (i.e. a mask needs to be applied). Note, however, that although masks can be skipped on â€œfully computedâ€ blocks, other &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;s like relative positional embeddings still need to be applied.&lt;/p&gt;

&lt;p&gt;Given just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;, thereâ€™s no sound way for us to tell which parts of it are â€œmaskingâ€. Hence, the user must separate these out themselves into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;q-how-much-additional-memory-does-the-blockmask-need&quot;&gt;&lt;strong&gt;Q: How much additional memory does the BlockMask need?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;The BlockMask metadata is of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[BATCH_SIZE, NUM_HEADS, QUERY_LEN//BLOCK_SIZE, KV_LEN//BLOCK_SIZE].&lt;/code&gt; If the mask is the same across the batch or heads dimension it can be broadcasted over that dimension to save memory.&lt;/p&gt;

&lt;p&gt;At the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BLOCK_SIZE&lt;/code&gt; of 128, we expect that the memory usage will be fairly negligible for most use cases. For example, for a sequence length of 1 million, the BlockMask would only use 60MB of additional memory. If this is a problem, you can increase the block size:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask(..., BLOCK_SIZE=1024).&lt;/code&gt; For example, increasing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BLOCK_SIZE&lt;/code&gt; to 1024 would result in this metadata dropping to under a megabyte.&lt;/p&gt;

&lt;h5 id=&quot;q-how-do-the-numerics-compare&quot;&gt;&lt;strong&gt;Q: How do the numerics compare?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;Although the results are not bitwise identical, we are confident that FlexAttention is as numerically accurate as FlashAttention. We generate the following distribution of differences comparing FlashAttention versus FlexAttention over a large range of inputs on both causal and non causal attention variants. The errors are nearly identical.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg14.png&quot; alt=&quot;distribution chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Generally speaking, FlexAttention is nearly as performant as a handwritten Triton kernel, which is unsurprising, as we heavily leverage a handwritten Triton kernel. However, due to its generality, we do incur a small performance penalty. For example, we must incur some additional latency to determine which block to compute next. In some cases, we provide some kernel options that can affect the performance of the kernel while changing its behavior. They can be found here: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/ee09d066d35d7e17cf7e9479c0b8bfc70cffc264/torch/_inductor/kernel/flex_attention.py#L146-L155&quot;&gt;performance knobs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a case study, letâ€™s explore how the knobs affect the performance of causal attention. We will compare performance of the triton kernel versus FlashAttentionv2 on A100. The script can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/benchmarks/transformer/score_mod.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FlexAttention achieves 90% of FlashAttention2â€™s performance in the forward pass and 85% in the backward pass. FlexAttention is currently utilizing a deterministic algorithm that recomputes more intermediates than FAv2, but we have plans to improve FlexAttentionâ€™s backward algorithm and hope to close this gap!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg15.png&quot; alt=&quot;flexattention speed chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg16.png&quot; alt=&quot;flexattention speed chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We hope you have as much fun using FlexAttention as we did developing it! While working on this, we ended up finding way more applications of this API than we could have expected. Weâ€™ve already seen it accelerate torchtuneâ€™s &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/1193&quot;&gt;sample packing throughput by 71%&lt;/a&gt;, replace the need for a researcher to spend over a week writing their own custom Triton kernel, and deliver competitive performance with custom handwritten attention variants.&lt;/p&gt;

&lt;p&gt;One final thing that made implementing FlexAttention quite fun is that we were able to leverage a lot of existing PyTorch infra in an interesting way. For example, one of the unique aspects about TorchDynamo (torch.compileâ€™s frontend) is that it does &lt;em&gt;not&lt;/em&gt; require tensors used in the compiled function to be explicitly passed in as inputs. This allows us to compile mods like document masking, which require accessing &lt;em&gt;global&lt;/em&gt; variables where the global variables need to change!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The bias tensor can change!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Furthermore, the fact that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a generic graph-capture mechanism also allows it to support more â€œadvancedâ€ transformations, such as the higher order transform that transforms any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; into one that works with jagged tensors.&lt;/p&gt;

&lt;p&gt;We also leverage TorchInductor (torch.compileâ€™s backend) infrastructure for Triton templates. Not only did this make it easy to support codegening FlexAttention - it also automatically gave us support for dynamic shapes as well as epilogue fusion (i.e. fusing an operator onto the end of attention)! In the future, we plan on extending this support to allow for quantized versions of attention or things like &lt;a href=&quot;https://lmsys.org/blog/2024-01-17-sglang/&quot;&gt;RadixAttention&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;In addition, we also leveraged higher order ops, PyTorchâ€™s autograd to automatically generate the backwards pass, as well as vmap to automatically apply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; for creating the BlockMask.&lt;/p&gt;

&lt;p&gt;And, of course, this project wouldnâ€™t have been possible without Triton and TorchInductorâ€™s ability to generate Triton code.&lt;/p&gt;

&lt;p&gt;We look forward to leveraging the approach we used here to more applications in the future!&lt;/p&gt;

&lt;h3 id=&quot;limitations-and-future-work&quot;&gt;Limitations and Future Work&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FlexAttention is currently available in PyTorch nightly releases, we plan to release it as a prototype feature in 2.5.0&lt;/li&gt;
  &lt;li&gt;We did not cover how to use FlexAttention for inference here (or how to implement PagedAttention) - we will cover those in a later post.&lt;/li&gt;
  &lt;li&gt;We are working to improve the performance of FlexAttention to match FlashAttention3 on H100 GPUs.&lt;/li&gt;
  &lt;li&gt;FlexAttention requires that all sequence lengths be a multiple of 128 - this will be addressed soon.&lt;/li&gt;
  &lt;li&gt;We plan on adding GQA support soon - for now, you can just replicate the kv heads.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;We want to highlight some prior work (and people) that have inspired FlexAttention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tri Daoâ€™s work on FlashAttention&lt;/li&gt;
  &lt;li&gt;Francisco Massa and the Xformers team for BlockSparseAttention in Triton&lt;/li&gt;
  &lt;li&gt;The Jax teamâ€™s work on SplashAttention&lt;/li&gt;
  &lt;li&gt;Philippe Tillet and Keren Zhou for helping us with Triton&lt;/li&gt;
  &lt;li&gt;Ali Hassani for discussions on neighborhood attention&lt;/li&gt;
  &lt;li&gt;Everybody whoâ€™s complained about attention kernels not supporting their favorite attention variant :)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch: Horace He, Driss Guessous, Yanbo Liang, Joy Dong</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Quantization-Aware Training for Large Language Models with PyTorch</title>
      <link href="https://pytorch.org/blog/quantization-aware-training/" rel="alternate" type="text/html" title="Quantization-Aware Training for Large Language Models with PyTorch" />
      <published>2024-07-30T00:00:00-07:00</published>
      <updated>2024-07-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/quantization-aware-training</id>
      <content type="html" xml:base="https://pytorch.org/blog/quantization-aware-training/">&lt;p&gt;In this blog, we present an end-to-end Quantization-Aware Training (QAT) flow for large language models in PyTorch. We demonstrate how QAT in PyTorch can &lt;strong&gt;recover up to 96% of the accuracy degradation&lt;/strong&gt; &lt;strong&gt;on hellaswag and&lt;/strong&gt; &lt;strong&gt;68% of the perplexity degradation on wikitext&lt;/strong&gt; &lt;strong&gt;for Llama3 compared to post-training quantization (PTQ).&lt;/strong&gt; We present the QAT APIs in &lt;a href=&quot;https://github.com/pytorch/ao/&quot;&gt;torchao&lt;/a&gt; and showcase how users can leverage them for fine-tuning in &lt;a href=&quot;https://github.com/pytorch/torchtune/&quot;&gt;torchtune&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg1.jpg&quot; alt=&quot;Llama3-8B fine-tuned on the C4 dataset (en subset) with and without QAT using int8 per token dynamic activations + int4 grouped per channel weights, evaluated on hellaswag and wikitext on a A100 GPU. Note the log scale for wikitext (lower is better).&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Llama3-8B fine-tuned on the C4 dataset (en subset) with and without QAT using int8 per token dynamic activations + int4 grouped per channel weights, evaluated on hellaswag and wikitext on a A100 GPU. Note the log scale for wikitext (lower is better).&lt;/p&gt;

&lt;p&gt;To demonstrate the effectiveness of QAT in an end-to-end flow, we further lowered the quantized model to &lt;a href=&quot;https://github.com/google/XNNPACK&quot;&gt;XNNPACK&lt;/a&gt;, a highly optimized neural network library for backends including iOS and Android, through &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2&quot;&gt;executorch&lt;/a&gt;. &lt;strong&gt;After lowering to XNNPACK, the QAT model saw 16.8% lower perplexity than the PTQ model, while maintaining the same model size and on-device inference and generation speeds.&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Lowered model metric&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PTQ&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;QAT&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext word perplexity (â†“)
   &lt;/td&gt;
   &lt;td&gt;23.316
   &lt;/td&gt;
   &lt;td&gt;19.403
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext byte perplexity (â†“)
   &lt;/td&gt;
   &lt;td&gt;1.850
   &lt;/td&gt;
   &lt;td&gt;1.785
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext bits per byte (â†“)
   &lt;/td&gt;
   &lt;td&gt;0.887
   &lt;/td&gt;
   &lt;td&gt;0.836
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Model size
   &lt;/td&gt;
   &lt;td&gt;3.881 GB
   &lt;/td&gt;
   &lt;td&gt;3.881 GB
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;On-device inference speed
   &lt;/td&gt;
   &lt;td&gt;5.065 tok/s
   &lt;/td&gt;
   &lt;td&gt;5.265 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;On-device generation speed
   &lt;/td&gt;
   &lt;td&gt;8.369 tok/s
   &lt;/td&gt;
   &lt;td&gt;8.701 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; QAT achieved 16.8% lower perplexity and unchanged model sizes and on-device inference and generation speeds on the Llama3-8B model lowered to XNNPACK. Linear layers are quantized using int8 per token dynamic activations + int4 grouped per channel weights, and embeddings are additionally quantized to int4 using a group size of 32 (QAT is only applied to linear layers). Wikitext evaluation is performed using 5 samples and a max sequence length of 127 on server CPU, since evaluation is not available on device (lower is better for all wikitext results). On-device inference and generation is benchmarked on the Samsung Galaxy S22 smartphone.&lt;/p&gt;

&lt;h3 id=&quot;qat-apis&quot;&gt;QAT APIs&lt;/h3&gt;

&lt;p&gt;We are excited for users to try our &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/prototype/qat.py&quot;&gt;QAT API&lt;/a&gt; in torchao, which can be leveraged for both training and fine-tuning. This API involves two steps, prepare and convert: prepare applies a transformation on the linear layers in the model to simulate the numerics of quantization during training, and convert actually quantizes these layers into lower bit-widths after training. The converted model can then be used in the exact same way as the PTQ model:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtune.models.llama3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization.prototype.qat&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Int8DynActInt4WeightQATQuantizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Smaller version of llama3 to fit in a single GPU
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_kv_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embed_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Quantizer for int8 dynamic per token activations +
# int4 grouped per channel weights, only for linear layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Int8DynActInt4WeightQATQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Insert &quot;fake quantize&quot; operations into linear layers.
# These operations simulate quantization numerics during
# training without performing any dtype casting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Standard training loop
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert fake quantize to actual quantize operations
# The quantized model has the exact same structure as the
# quantized model produced in the corresponding PTQ flow
# through `Int8DynActInt4WeightQuantizer`
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# inference or generate
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;fine-tuning-with-torchtune&quot;&gt;Fine-tuning with torchtune&lt;/h4&gt;

&lt;p&gt;We also integrated this QAT flow into &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;torchtune&lt;/a&gt; and provided &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama3/8B_qat_full.yaml&quot;&gt;recipes&lt;/a&gt; to run this in a distributed setting, similar to the existing full fine-tune distributed recipe. Users can additionally apply QAT during LLM fine-tuning by running the following command. See &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantization.md&quot;&gt;this README&lt;/a&gt; for more details.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nproc_per_node&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_distributed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B_qat_full&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-quantization-aware-training&quot;&gt;What is Quantization-Aware Training?&lt;/h2&gt;

&lt;p&gt;Quantization-Aware Training (QAT) is a common quantization technique for mitigating model accuracy/perplexity degradation that arises from quantization. This is achieved by simulating quantization numerics during training while keeping the weights and/or activations in the original data type, typically float, effectively â€œfake quantizingâ€ the values instead of actually casting them to lower bit-widths:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# PTQ: x_q is quantized and cast to int8
# scale and zero point (zp) refer to parameters used to quantize x_float
# qmin and qmax refer to the range of quantized values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_float&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# QAT: x_fq is still in float
# Fake quantize simulates the numerics of quantize + dequantize
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_float&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since quantization involves non-differentiable operations like rounding, the QAT backward pass typically uses &lt;a href=&quot;https://arxiv.org/pdf/1308.3432&quot;&gt;straight-through estimators (STE)&lt;/a&gt;, a mechanism to estimate the gradients flowing through non-smooth functions, to ensure the gradients passed to the original weights are still meaningful. In this manner, the gradients are computed with the knowledge that the weights will ultimately be quantized after training, effectively allowing the model to adjust for quantization noise during the training process. Note that an alternative to QAT is quantized training, which actually casts the values to lower bit dtypes during training, but &lt;a href=&quot;https://cloud.google.com/blog/products/compute/accurate-quantized-training-aqt-for-tpu-v5e&quot;&gt;prior efforts&lt;/a&gt; have only seen success up to 8-bits, whereas QAT is effective even at lower bit-widths.&lt;/p&gt;

&lt;h3 id=&quot;qat-in-pytorch&quot;&gt;QAT in PyTorch&lt;/h3&gt;

&lt;p&gt;We added an initial QAT flow in torchao under prototype &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.2.0/torchao/quantization/prototype/qat.py&quot;&gt;here&lt;/a&gt;. Currently we support int8 dynamic per-token activations + int4 grouped per-channel weights (abbreviated 8da4w) for linear layers. These settings are motivated by a combination of &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/models/llama2/README.md#quantization&quot;&gt;kernel availability on edge backends&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2305.17888&quot;&gt;prior research on LLM quantization&lt;/a&gt;, which found that per-token activation and per-group weight quantization achieves the best model quality for LLMs compared to other quantization schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg2.png&quot; alt=&quot;torchao QAT flow. This flow involves two steps: (1) prepare, which inserts the fake quantization ops into the modelâ€™s linear layers, and (2) convert, which converts these fake quantization ops with actual quantize and dequantize ops after training.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; torchao QAT flow. This flow involves two steps: (1) prepare, which inserts the fake quantization ops into the modelâ€™s linear layers, and (2) convert, which converts these fake quantization ops with actual quantize and dequantize ops after training.&lt;/p&gt;

&lt;p&gt;This flow produces the exact same quantized model as the PTQ flow using the same quantization settings (through &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/GPTQ.py#L941&quot;&gt;Int8DynActInt4WeightQuantizer&lt;/a&gt;), but with quantized weights that achieve superior accuracies and perplexities. Thus, we can use the model converted from the QAT flow as a drop-in replacement for the PTQ model and reuse all the backend delegation logic and underlying kernels.&lt;/p&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;p&gt;All experiments in this blog post are performed using the torchtune QAT integration described above. We use 6-8 A100 GPUs with 80 GBs each to fine-tune &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-2-7b&quot;&gt;Llama2-7B&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&quot;&gt;Llama3-8B&lt;/a&gt; on the &lt;a href=&quot;https://huggingface.co/datasets/allenai/c4&quot;&gt;C4 dataset&lt;/a&gt; (en subset) for 5000 steps. For all experiments, we use batch size = 2, learning rate = 2e-5, max sequence length = 4096 for Llama2 and 8192 for Llama3, &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html&quot;&gt;Fully Sharded Data Parallel&lt;/a&gt; (FSDP) as our distribution strategy, and activation checkpointing to reduce memory footprint. For 8da4w experiments, we use a group size of 256 for weights.&lt;/p&gt;

&lt;p&gt;Since the pre-training dataset is not easily accessible, we perform QAT during the fine-tuning process. Empirically, we found that disabling fake quantization for the first N steps led to better results, presumably because doing so allows the weights to stabilize before we start introducing quantization noise to the fine-tuning process. We disable fake quantization for the first 1000 steps for all our experiments.&lt;/p&gt;

&lt;p&gt;We evaluate our quantized models using the &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;lm-evaluation-harness&lt;/a&gt; integration in torchtune. We report evaluation results from a variety of tasks commonly used to evaluate LLMs, including hellaswag, a commonsense sentence completion task, wikitext, a next token/byte prediction task, and a few question-answering tasks such as arc, openbookqa, and piqa. For wikitext, perplexity refers to the inverse of how well the model can predict the next word or byte (lower is better), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bits_per_byte&lt;/code&gt; refers to how many bits are needed to predict the next byte (lower is also better here). For all other tasks, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acc_norm&lt;/code&gt; refers to the accuracy normalized by the byte-length of the target string.&lt;/p&gt;

&lt;h4 id=&quot;int8-dynamic-activations--int4-weight-quantization-8da4w&quot;&gt;Int8 Dynamic Activations + Int4 Weight Quantization (8da4w)&lt;/h4&gt;

&lt;p&gt;Starting with Llama2 8da4w quantization, we saw that QAT was able to recover 62% of the normalized accuracy degradation on hellaswag compared to PTQ, and 58% and 57% of the word and byte perplexity degradation (respectively) on wikitext. We see similar improvements for most of the other tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg3a.png&quot; alt=&quot;Llama2-7B 8da4w quantization with and without QAT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3a:&lt;/strong&gt; Llama2-7B 8da4w quantization with and without QAT&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg3b.png&quot; alt=&quot;Llama2-7B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3b:&lt;/strong&gt; Llama2-7B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&lt;/p&gt;

&lt;p&gt;Llama3 8da4w quantization saw even more pronounced improvements with QAT. On the hellaswag evaluation task, we were able to recover 96% of the normalized accuracy degradation on hellaswag compared to PTQ, with minimal overall degradation (&amp;lt;1%) compared to the non-quantized accuracy. On the wikitext evaluation task, QAT recovered 68% and 65% of the word and byte perplexity degradation (respectively). Even on arc_challenge, which was difficult for Llama2 QAT, we were able to recover 51% of the normalized accuracy degradation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg4a.png&quot; alt=&quot;Llama3-8B 8da4w quantization with and without QAT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4a:&lt;/strong&gt; Llama3-8B 8da4w quantization with and without QAT&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg4b.png&quot; alt=&quot;Llama3-8B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4b:&lt;/strong&gt; Llama3-8B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&lt;/p&gt;

&lt;h4 id=&quot;lower-bit-weight-only-quantization&quot;&gt;Lower Bit Weight Only Quantization&lt;/h4&gt;

&lt;p&gt;We further extended the torchao QAT flow to 2-bit and 3-bit weight only quantization and repeated the same experiments for Llama3-8B. Quantization degradation is more severe at lower bit-widths, so we use a group size of 32 for all experiments for finer-grained quantization.&lt;/p&gt;

&lt;p&gt;However, this is still not enough for 2-bits PTQ, which saw wikitext perplexity explode. To mitigate this problem, we leverage knowledge from prior sensitivity analysis that the first 3 and last 2 layers of the Llama3 model are the most sensitive, and skip quantizing these layers in exchange for a moderate increase in quantized model size (1.78 GB for 2-bits and 1.65 GB for 3-bits). This brought the wikitext word perplexity down from 603336 to 6766, which is significant but still far from acceptable. To further improve the quantized model, we turn to QAT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg5a.png&quot; alt=&quot;Llama3-8B 2-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5a:&lt;/strong&gt; Llama3-8B 2-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&lt;/p&gt;

&lt;p&gt;We observe that applying QAT while skipping quantization for the first 3 and last 2 layers further brought the word perplexity down to a much more reasonable value of 30 (from 6766). More generally, QAT was able to recover 53% of the normalized accuracy degradation on hellaswag compared to PTQ, and 99% and 89% of the word and byte perplexity degradation (respectively) on wikitext. Without skipping the sensitive layers, however, QAT was far less effective at mitigating degradation in quantized model quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg5b.png&quot; alt=&quot;Llama3-8B 2-bit weight only quantization with and without QAT. Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5b:&lt;/strong&gt; Llama3-8B 2-bit weight only quantization with and without QAT. Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&lt;/p&gt;

&lt;p&gt;For 3-bit weight only quantization, QAT was effective even without skipping the first 3 and last 2 layers, though skipping these layers still led to better results for both PTQ and QAT. In the skip case, QAT was able to recover 63% of the normalized accuracy degradation on hellaswag compared to PTQ, and 72% and 65% of the word and byte perplexity degradation (respectively) on wikitext.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg6a.png&quot; alt=&quot;Llama3-8B 3-bit weight only quantization with and without QAT. Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6a:&lt;/strong&gt; Llama3-8B 3-bit weight only quantization with and without QAT. Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg6b.png&quot; alt=&quot;Llama3-8B 3-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6b:&lt;/strong&gt; Llama3-8B 3-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with â€œskipâ€ refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&lt;/p&gt;

&lt;h4 id=&quot;qat-overhead&quot;&gt;QAT Overhead&lt;/h4&gt;

&lt;p&gt;QAT inserts many fake quantize operations throughout the model, adding considerable overhead to both the fine-tuning speed and the memory usage. For a model like Llama3-8B for example, we have (32 * 7) + 1 = 225 linear layers, each of which has at least 1 fake quantize for the weights and potentially 1 fake quantize for the input activations. Memory footprint increase is also significant, since we cannot mutate the weights in-place and so we need to clone them before applying fake quantization, though this overhead can be mostly mitigated by enabling activation checkpointing.&lt;/p&gt;

&lt;p&gt;In our microbenchmarks, we found that 8da4w QAT fine-tuning is ~34% slower than regular full fine-tuning. With activation checkpointing, the memory increase per GPU is around 2.35 GB. Most of these overheads are fundamental to how QAT works, though we may be able to speed up computation with &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;torch.compile&lt;/a&gt; in the future.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Per GPU statistics
   &lt;/td&gt;
   &lt;td&gt;Full fine-tuning
   &lt;/td&gt;
   &lt;td&gt;QAT fine-tuning
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Median tokens per second
   &lt;/td&gt;
   &lt;td&gt;546.314 tok/s
   &lt;/td&gt;
   &lt;td&gt;359.637 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Median peak memory
   &lt;/td&gt;
   &lt;td&gt;67.501 GB
   &lt;/td&gt;
   &lt;td&gt;69.850 GB
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; Llama3 QAT fine-tuning overhead for int8 per token dynamic activations + int4 grouped per channel weights on 6 A100 GPUs (each with 80GB memory).&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;In this blog, we presented a QAT flow for LLMs through &lt;a href=&quot;https://github.com/pytorch/ao/&quot;&gt;torchao&lt;/a&gt;, integrated this flow with the fine-tuning APIs in &lt;a href=&quot;https://github.com/pytorch/torchtune/&quot;&gt;torchtune&lt;/a&gt;, and demonstrated its potential to recover most of the quantization degradation compared to PTQ and match non-quantized performance on certain tasks. There are many directions for future explorations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hyperparameter tuning.&lt;/strong&gt; It is likely that extensive hyperparameter tuning can further improve the results of finetuning and QAT. In addition to the general hyperparameters like the learning rate, batch size, dataset size, and number of fine-tuning steps, we should also tune QAT-specific ones, such as when to start/stop fake quantization, how many steps to fake quantize, and regularization parameters for fake quantized values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Outlier reduction techniques.&lt;/strong&gt; In our experiments, we found that both PTQ and QAT were susceptible to outliers. In addition to simple clamping and regularization during fine-tuning, we can explore techniques that allow the network to learn how to control these outliers (e.g. &lt;a href=&quot;https://arxiv.org/pdf/1902.08153&quot;&gt;learned quantization ranges&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2306.12929&quot;&gt;clipped softmax&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2306.12929&quot;&gt;gated attention&lt;/a&gt;), or possibly even borrow outlier suppression techniques from post-training settings (e.g. &lt;a href=&quot;https://arxiv.org/pdf/2405.16406&quot;&gt;SpinQuant&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2211.10438&quot;&gt;SmoothQuant&lt;/a&gt;) and apply them sparingly throughout the fine-tuning process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mixed-precision and more complex dtypes.&lt;/strong&gt; Especially in the lower bit regime, we saw that skipping quantization for certain sensitive layers was effective for both PTQ and QAT. Did we need to skip quantizing these layers altogether, or can we still quantize them, just to lower bit-widths? It will be interesting to explore mixed-precision quantization in the context of QAT. Training with newer dtypes such as MX4 is another promising direction, especially given that the upcoming Blackwell GPUs will &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;no longer support int4 tensor cores&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Composability with LoRA and QLoRA.&lt;/strong&gt; Our QAT integration in torchtune currently only supports the full fine-tuning workflow. However, many users wish to fine-tune their models using low-ranked adaptors to substantially reduce their memory footprint. Composing QAT with techniques like LoRA / QLoRA will enable users to reap the memory and performance benefits of these approaches while producing a model that will ultimately be quantized with minimal model quality degradation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Composability with &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;torch.compile&lt;/a&gt;.&lt;/strong&gt; This is another potential way to significantly speed up fake quantization computations in QAT while reducing memory footprint. torch.compile is currently not compatible with the distribution strategy used in full distributed fine-tuning recipes in torchtune (with or without QAT), but support will be added in the near future.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Quantizing other layers.&lt;/strong&gt; In this work, we only explored quantizing the linear layers. However, in the context of long sequence lengths, the KV cache often becomes the throughput bottleneck and can reach tens of GBs, hence &lt;a href=&quot;https://arxiv.org/pdf/2305.17888&quot;&gt;LLM-QAT&lt;/a&gt; explored quantizing the KV cache alongside activations and weights. &lt;a href=&quot;https://arxiv.org/pdf/2109.12948&quot;&gt;Prior work&lt;/a&gt; has also had success with quantizing the embedding layer down to 2-bits in other transformer-based models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;End-to-end evaluation on performant cuda kernels.&lt;/strong&gt; A natural extension of this work is to provide an end-to-end QAT flow evaluated on performant cuda kernels, similar to the existing 8da4w QAT flow lowered to XNNPACK kernels through executorch. For int4 weight only quantization, we can leverage the efficient &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/v2.3.1/aten/src/ATen/native/cuda/int4mm.cu#L865&quot;&gt;int4 weight mm kernel with bitpacking&lt;/a&gt; for quantization, and there is ongoing work to add QAT support for this kernel: &lt;a href=&quot;https://github.com/pytorch/ao/pull/383&quot;&gt;https://github.com/pytorch/ao/pull/383&lt;/a&gt;. For 8da4w quantization, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1413&quot;&gt;mixed 4-bit/8-bit GEMM&lt;/a&gt; is also being added in cutlass. This will be needed to build an efficient 8da4w cuda kernel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The QAT code can be found &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/prototype/qat.py&quot;&gt;here&lt;/a&gt;. Please refer to &lt;a href=&quot;https://pytorch.org/torchtune/main/tutorials/qat_finetune.html&quot;&gt;this torchtune tutorial&lt;/a&gt; to get started. If you have any further questions, please feel free to open an issue on the torchao &lt;a href=&quot;https://github.com/pytorch/ao/issues&quot;&gt;github&lt;/a&gt; or reach out to &lt;a href=&quot;mailto:andrewor@meta.com&quot;&gt;andrewor@meta.com&lt;/a&gt;. We welcome your feedback and contributions!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Andrew Or, Jerry Zhang, Evan Smothers, Kartikay Khandelwal, Supriya Rao</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we present an end-to-end Quantization-Aware Training (QAT) flow for large language models in PyTorch. We demonstrate how QAT in PyTorch can recover up to 96% of the accuracy degradation on hellaswag and 68% of the perplexity degradation on wikitext for Llama3 compared to post-training quantization (PTQ). We present the QAT APIs in torchao and showcase how users can leverage them for fine-tuning in torchtune.</summary>
      

      
      
    </entry>
  
</feed>



<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-01-07T03:59:46-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">High-Performance Low-Bit Operators for PyTorch</title>
      <link href="https://pytorch.org/blog/hi-po-low-bit-operators/" rel="alternate" type="text/html" title="High-Performance Low-Bit Operators for PyTorch" />
      <published>2025-01-06T00:00:00-08:00</published>
      <updated>2025-01-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/hi-po-low-bit-operators</id>
      <content type="html" xml:base="https://pytorch.org/blog/hi-po-low-bit-operators/">&lt;p&gt;We are excited to announce the addition of embedding operators with low-bit weights (1-8 bit) and linear operators with 8-bit dynamically quantized activations and low-bit weights (1-8 bit) for Arm CPUs in TorchAO, PyTorch’s native low-precision library. These operators work seamlessly across all PyTorch surfaces, including eager, torch.compile, AOTI, and ExecuTorch, and are &lt;a href=&quot;https://github.com/pytorch/torchchat/blob/main/docs/quantization.md#experimental-torchao-lowbit-kernels&quot;&gt;available to use in torchchat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In developing these linear operators, our focus was on &lt;strong&gt;code sharing between PyTorch and ExecuTorch&lt;/strong&gt;, and establishing a clear boundary between the higher-level operator and the lower-level kernel. This design &lt;strong&gt;allows third-party vendors to easily swap in their own kernels&lt;/strong&gt;. We also set out to &lt;strong&gt;create a place and infrastructure to experiment&lt;/strong&gt; with new CPU quantization ideas and test those across the PyTorch ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;universal-low-bit-kernels&quot;&gt;Universal low-bit kernels&lt;/h2&gt;

&lt;p&gt;There is no hardware support for low-bit arithmetic. In what we call universal kernels, we explicitly separated the logic that unpacks low-bit values to int8 values, and the int8 GEMV kernel logic in a modular fashion. We started with an 8-bit kernel, for example, this &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/channelwise_8bit_activation_groupwise_lowbit_weight_1x8x16_f32_neondot-impl.h#L64&quot;&gt;1x8 8-bit GEMV kernel&lt;/a&gt; that uses the Arm neondot instruction. Within the 8-bit kernel, we invoke an &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/channelwise_8bit_activation_groupwise_lowbit_weight_1x8x16_f32_neondot-impl.h#L169&quot;&gt;inlined unpacking routine&lt;/a&gt; to convert low-bit values into int8 values. This unpacking routine is force-inlined and templated on some low-bit value. Our experiments showed no performance difference between using a separate force-inlined unpacking routine and directly embedding the unpacking code inline.&lt;/p&gt;

&lt;p&gt;The advantage of this modular design is improved development speed and code maintainability. After writing an 8-bit kernel, we quickly achieved full low-bit coverage by writing &lt;a href=&quot;https://github.com/pytorch/ao/tree/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/bitpacking&quot;&gt;simple bitpacking routines&lt;/a&gt;. In fact, developers who worked on the bit packing routines did not need to be experts on GEMV/GEMM kernel writing. We also reused the same bitpacking routines from the linear kernels &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/embedding/embedding.h#L161&quot;&gt;within the embedding kernels&lt;/a&gt;. In future we could reuse the same bitpacking routines for universal GEMM kernels or kernels based on fma or i8mm instructions.&lt;/p&gt;

&lt;h2 id=&quot;shared-code-between-pytorch-and-executorch&quot;&gt;Shared code between PyTorch and ExecuTorch&lt;/h2&gt;

&lt;p&gt;To achieve shared code between PyTorch and ExecuTorch, we wrote kernels &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/linear.h&quot;&gt;using raw pointers instead of PyTorch tensors&lt;/a&gt;. Moreover, we implemented the &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight-impl.h#L259&quot;&gt;linear operator in a header &lt;/a&gt;that is included in separate &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight_aten.cpp&quot;&gt;PyTorch&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight_executorch/w4s.cpp&quot;&gt;ExecuTorch&lt;/a&gt; operator registration code. By using only features common to both ATen and ExecuTorch tensors, we ensured compatibility between the two frameworks. For multi-threaded compute, we introduced &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel.h#L13&quot;&gt;torchao::parallel_1d&lt;/a&gt;, which compiles to either &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel-aten-impl.h&quot;&gt;at::parallel_for&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel-executorch-impl.h&quot;&gt;ExecuTorch’s threadpool&lt;/a&gt; based on compile-time flags.&lt;/p&gt;

&lt;h2 id=&quot;swappable-kernels&quot;&gt;Swappable kernels&lt;/h2&gt;

&lt;p&gt;Our design for the higher-level multi-threaded linear operator is agnostic to the lower-level single-threaded kernels, allowing third-party vendors to swap in their own implementations. The interface between the operator and kernel is defined by a &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/linear_8bit_act_xbit_weight.h#L14&quot;&gt;ukernel config&lt;/a&gt;, which specifies kernel function pointers for preparing activation data, preparing weight data, and running the kernel. The operator, responsible for tiling and scheduling, interacts with kernels solely through this config.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;In the table below, we show Llama3.1 8B token generation performance using 6 CPU threads on an M1 Macbook Pro with 32GB of RAM.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Bitwidth x&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;torch.compile (Decode tokens/sec)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;ExecuTorch (Decode tokens/sec)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;ExecuTorch PTE size (GiB)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;24.18
   &lt;/td&gt;
   &lt;td&gt;17.86
   &lt;/td&gt;
   &lt;td&gt;1.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;27.02
   &lt;/td&gt;
   &lt;td&gt;19.65
   &lt;/td&gt;
   &lt;td&gt;2.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;21.01
   &lt;/td&gt;
   &lt;td&gt;22.25
   &lt;/td&gt;
   &lt;td&gt;3.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;19.51
   &lt;/td&gt;
   &lt;td&gt;19.47
   &lt;/td&gt;
   &lt;td&gt;4.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;5
   &lt;/td&gt;
   &lt;td&gt;14.78
   &lt;/td&gt;
   &lt;td&gt;16.34
   &lt;/td&gt;
   &lt;td&gt;5.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;6
   &lt;/td&gt;
   &lt;td&gt;12.80
   &lt;/td&gt;
   &lt;td&gt;13.61
   &lt;/td&gt;
   &lt;td&gt;6.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;7
   &lt;/td&gt;
   &lt;td&gt;8.16
   &lt;/td&gt;
   &lt;td&gt;11.73
   &lt;/td&gt;
   &lt;td&gt;7.48
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Results were run on an M1 Macbook Pro (with 8 perf cores, and 2 efficiency cores) with 32GB of RAM and 6 threads &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;using torchchat&lt;/a&gt;. In each test, the max-seq-length of 128 tokens were generated. For each bit width x, the embedding layer was groupwise quantized to x-bits with group size 32. In the linear layers, activations were dynamically quantized per token to 8 bits and weights were groupwise quantized to x-bits with group size 256.  Our focus here is performance and we do not report accuracy or perplexity numbers. Depending on the model, lower bit widths may require quantization-aware training, quantizing a model with a mixture of bit widths, or adjusting the group sizes for acceptable accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hi-po-low-bit.png&quot; alt=&quot;Llama 3.1 chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-them-out-and-contribute&quot;&gt;Try them out and contribute!&lt;/h2&gt;

&lt;p&gt;If you want to see the new low-bit kernels in action, give them a try by &lt;a href=&quot;https://github.com/pytorch/torchchat/tree/main&quot;&gt;setting up torchchat&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/torchchat/blob/main/docs/quantization.md#experimental-torchao-lowbit-kernels&quot;&gt;quantizing and running an LLM locally using the kernels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to help contribute, consider adding support for one of the following areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/issues/1394&quot;&gt;Add universal low-bit GEMM kernels&lt;/a&gt; for Arm CPU, reusing the same bitpacking routines from the universal GEMV kernels.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/issues/1376&quot;&gt;Improve runtime selection&lt;/a&gt; of ukernel configs based on ISA, packing format, and activation shape.&lt;/li&gt;
  &lt;li&gt;Add low-bit kernels for other CPU ISAs like x86.&lt;/li&gt;
  &lt;li&gt;Integrate third-party libraries like &lt;a href=&quot;https://gitlab.arm.com/kleidi/kleidiai&quot;&gt;KleidiAI&lt;/a&gt; with the operator framework.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Scott Roy, Digant Desai, Kimish Patel</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the addition of embedding operators with low-bit weights (1-8 bit) and linear operators with 8-bit dynamically quantized activations and low-bit weights (1-8 bit) for Arm CPUs in TorchAO, PyTorch’s native low-precision library. These operators work seamlessly across all PyTorch surfaces, including eager, torch.compile, AOTI, and ExecuTorch, and are available to use in torchchat.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Grows as the Dominant Open Source Framework for AI and ML: 2024 Year in Review</title>
      <link href="https://pytorch.org/blog/2024-year-in-review/" rel="alternate" type="text/html" title="PyTorch Grows as the Dominant Open Source Framework for AI and ML: 2024 Year in Review" />
      <published>2024-12-23T00:00:00-08:00</published>
      <updated>2024-12-23T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/2024-year-in-review</id>
      <content type="html" xml:base="https://pytorch.org/blog/2024-year-in-review/">&lt;p&gt;This past year was a monumental year for PyTorch from major releases to the flagship PyTorch Conference. We’ve seen incredible growth in contributions from more than 3,500 individuals and 3,000 organizations. It’s safe to say PyTorch has now become the dominant deep learning framework for AI/ML.  PyTorch leads the model training space with a 63% adoption rate according to the recent &lt;a href=&quot;https://www.linuxfoundation.org/research/gen-ai-2024&quot;&gt;Shaping the Future of Generative AI Report&lt;/a&gt; from the Linux Foundation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg1.jpg&quot; alt=&quot;group at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation was formed in 2022 with the goal to drive the adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects centered around PyTorch and today remains a vibrant, collaborative hub created for and by the deep learning community. As we wrap up the year, let’s take a look back at a few highlights and how this year has been one of growth, collaboration, innovation, and community.&lt;/p&gt;

&lt;h2 id=&quot;2024-highlights-a-year-of-growth-and-impact&quot;&gt;2024 Highlights: A Year of Growth and Impact&lt;/h2&gt;

&lt;p&gt;PyTorch accelerated its growth this year. Contributions are up 133%, from double the amount of  organizations worldwide compared to last year.&lt;/p&gt;

&lt;p&gt;The project has seen 20% year-over-year growth in new repositories using PyTorch, and a 30% increase in forks and users this past year.&lt;/p&gt;

&lt;p&gt;Over 70% of AI research implementations are now using PyTorch.&lt;/p&gt;

&lt;p&gt;Statistics based on the &lt;a href=&quot;https://www.linuxfoundation.org/resources/publications/linux-foundation-annual-report-2024&quot;&gt;2024 Linux Foundation Annual Report&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg2.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Tools ecosystem grew by over 25%, enhancing both software and hardware capabilities. Working with all major cloud service providers, dozens of major software vendors, and industry partners, PyTorch is setting a new bar for the pace and breadth of AI innovation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg3.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This year featured 4 milestone releases for PyTorch in the 2.2, 2.3, 2.4 and 2.5 releases. We observed the release of various hallmark features like &lt;a href=&quot;https://pytorch.org/blog/pytorch2-2/#beta-aotinductor-ahead-of-time-compilation-and-deployment-for-torchexport-ed-programs&quot;&gt;AOTInductor&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch2-2/#beta-aotinductor-ahead-of-time-compilation-and-deployment-for-torchexport-ed-programs&quot;&gt;FlashAttention-2 support&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch2-3/#beta-tensor-parallelism-introduces-more-efficient-ways-to-train-llms&quot;&gt;Tensor Parallelism&lt;/a&gt;, a new &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#beta-new-higher-level-python-custom-operator-api&quot;&gt;Python Custom Operator API&lt;/a&gt;, and the introduction of &lt;a href=&quot;https://pytorch.org/blog/pytorch2-5/#prototype-flexattention&quot;&gt;FlexAttention&lt;/a&gt;. Engineers from across PyTorch Foundation member companies have also come together to introduce support and optimizations for platforms like &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;Intel GPUs&lt;/a&gt; (XPU), AWS &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;Graviton&lt;/a&gt; processors, Inductor performance, etc.&lt;/p&gt;

&lt;p&gt;Throughout the year the PyTorch Team has been working hard to introduce a number of new PyTorch-native libraries! The &lt;a href=&quot;https://pytorch.org/blog/executorch-alpha/&quot;&gt;ExecuTorch&lt;/a&gt; team released their alpha in collaboration with partners from Arm, Apple, and Qualcomm Technologies, Inc. then quickly followed with a &lt;a href=&quot;https://pytorch.org/blog/executorch-beta/&quot;&gt;beta&lt;/a&gt; focused on stability and adding MediaTek. &lt;a href=&quot;https://pytorch.org/blog/torchtune-fine-tune-llms/&quot;&gt;TorchTune&lt;/a&gt; established a PyTorch-native library for easily fine-tuning large language models. &lt;a href=&quot;https://pytorch.org/blog/pytorch-native-architecture-optimization/&quot;&gt;TorchAO&lt;/a&gt; introduced a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. &lt;a href=&quot;https://pytorch.org/blog/torchcodec/&quot;&gt;TorchCodec&lt;/a&gt; was launched to give developers a simple, performant, and PyTorch native way to decode videos into tensors. &lt;a href=&quot;https://pytorch.org/blog/torchrec-fbgemm-1/&quot;&gt;TorchRec&lt;/a&gt; 1.0 was released, the first stable release of the PyTorch native recommendation systems library.&lt;/p&gt;

&lt;p&gt;We’ve also had a number of strong technical showcases throughout the year to highlight how PyTorch can be used! &lt;a href=&quot;https://arxiv.org/html/2410.06511v1&quot;&gt;TorchTitan&lt;/a&gt; exhibited what an open source, PyTorch-native distributed training system could look like for training large language models (LLMs). &lt;a href=&quot;https://pytorch.org/blog/torchchat-local-llm-inference/&quot;&gt;TorchChat&lt;/a&gt; showcased how to seamlessly and performantly run LLMs across laptop, desktop, and mobile devices.&lt;/p&gt;

&lt;p&gt;As well we were very excited to include &lt;a href=&quot;https://pytorch.org/blog/enhancing-deep-learning/&quot;&gt;multiple new projects&lt;/a&gt; into the PyTorch ecosystem throughout 2024, including the introduction of  &lt;a href=&quot;https://pytorch.org/blog/vllm-joins-pytorch/&quot;&gt;vLLM&lt;/a&gt; into the PyTorch Ecosystem, a state-of-the-art inference engine, which gives machine learning engineers an easy, fast, and cheap way of serving LLMs. If you are interested in joining the PyTorch Ecosystem, please &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;join&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg4.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In June in Paris, France we premiered the&lt;a href=&quot;https://pytorch.org/blog/pytorch-documentary/&quot;&gt; official PyTorch documentary&lt;/a&gt; on powering the AI Revolution that spotlights PyTorch’s vibrant ecosystem and its role in advancing AI innovation. The film unveiled the authentic narrative of PyTorch’s inception, attributing its existence to a dedicated group of unsung heroes driving technological innovation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg5.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch.org/blog/pytorch-conference-2024-recap/&quot;&gt;PyTorch Conference 2024&lt;/a&gt;, brought in triple the registrations compared to 2023, reflecting the rapid growth of AI and machine learning communities around open source technologies. The two day event included insightful talks, hands-on sessions, and lively discussions about the future of AI, covering everything from generative AI to large language models.&lt;/p&gt;

&lt;p&gt;A brand new Startup Showcase featured early-stage founders pitching their AI startups to a panel of top venture capitalists, a DL Compiler Mini-Summit took a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads, and a Fine-Tuning Mini-Summit brought together a thriving community of researchers, developers, practitioners and hobbyists to discuss topics like memory efficiency, parameter-efficient fine-tuning, and performance at scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg6.jpg&quot; alt=&quot;speaking on stage at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Outstanding contributors were honored with &lt;a href=&quot;https://pytorch.org/ecosystem/contributor-awards-2024&quot;&gt;PyTorch Contributor Awards&lt;/a&gt;. Congratulations to this year’s nominees and recipients for the outstanding individuals and teams who have played a pivotal role in PyTorch’s journey this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg7.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Foundation membership is growing with the addition of Arm and Rebellions this year. At the year-end mark, Premier Members include: AMD, Arm, AWS, Google Cloud, Huawei, Hugging Face, IBM, Intel, Lightning AI, Meta, Microsoft Azure, and NVIDIA. General Members include: Graphcore, Rebellions, and Snowflake. If your organization is interested in joining, find out how you can &lt;a href=&quot;/join&quot;&gt;become a member&lt;/a&gt; of the PyTorch Foundation.&lt;/p&gt;

&lt;p&gt;PyTorch hosted numerous in-person and virtual events, including&lt;a href=&quot;https://pytorch.org/blog/pytorch-docathon-h2-2024-wrap-up/&quot;&gt; The PyTorch Docathon&lt;/a&gt; where contributors worked to improve PyTorch documentation and foster collaboration, Local meetups around the world brought together interested parties in locations from Shanghai to Seoul, and more than a dozen &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;webinars&lt;/a&gt; brought in attendees from everywhere during our Summer Webinar Series, live Q&amp;amp;As, and Expert Exchanges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg8.jpg&quot; alt=&quot;Matt speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Foundation welcomed new leadership this year.&lt;a href=&quot;https://pytorch.org/blog/new-executive-director/&quot;&gt; Executive Director Matt White&lt;/a&gt; took the reins in April and immediately began raising the profile of PyTorch across the AI landscape. The&lt;a href=&quot;https://pytorch.org/tac&quot;&gt; Technical Advisory Council (TAC)&lt;/a&gt; also elected&lt;a href=&quot;https://pytorch.org/blog/tac-elects-new-leadership/&quot;&gt; new leadership&lt;/a&gt; with  Luca Antiga, Lightning AI as the Chair and Jiong Gong, Intel as Vice Chair.&lt;/p&gt;

&lt;p&gt;The&lt;a href=&quot;https://pytorch.org/governing-board&quot;&gt; PyTorch Governing Board&lt;/a&gt; continued to set the direction and lead the Foundation in accomplishing its mission. The PyTorch Marketing and Outreach Committee developed programs to maximize the visibility of PyTorch and advance the interests of the community. The PyTorch CI Working Group assembled to successfully migrate the PyTorch CI pipeline to the Linux Foundation.&lt;/p&gt;

&lt;p&gt;Our community joined us on social media with 775 thousand followers strong across X, LinkedIn, Facebook, and YouTube with more than 12 million impressions of PyTorch content throughout the year.  The PyTorch Ecosystem also grew, adding many new projects to leverage PyTorch deep learning across many vertical domains.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg9.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch was mentioned in the media in top technology publications such as The New Stack’s article on &lt;a href=&quot;https://thenewstack.io/why-pytorch-gets-all-the-love/&quot;&gt;Why PyTorch Gets All the Love&lt;/a&gt; and InfoWorld’s article on how the TorchAO&lt;a href=&quot;https://www.infoworld.com/article/3543651/pytorch-library-makes-models-faster-and-smaller.html&quot;&gt; PyTorch library makes models faster and smaller&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We published 74 technical and community blogs, and nearly ten million people visited the PyTorch website throughout the year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg10.jpg&quot; alt=&quot;fire dancers at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks to each of you who helped make this year an outstanding success! The evolution and growth we’ve seen PyTorch undergo over the past year is driven by the passion, dedication, and ingenuity of this amazing community. Looking ahead to next year, we’re excited to build on this momentum as we continue to push the boundaries of AI.&lt;/p&gt;

&lt;p&gt;Save the date for the &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference-2025/&quot;&gt;PyTorch Conference&lt;/a&gt; which will be held October 22-23, 2025 in San Francisco. 2025 promises even greater innovation and stronger community collaboration.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Eli Uriegas, Meta and Jennifer Bly, PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">This past year was a monumental year for PyTorch from major releases to the flagship PyTorch Conference. We’ve seen incredible growth in contributions from more than 3,500 individuals and 3,000 organizations. It’s safe to say PyTorch has now become the dominant deep learning framework for AI/ML. PyTorch leads the model training space with a 63% adoption rate according to the recent Shaping the Future of Generative AI Report from the Linux Foundation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Improve RAG performance with torch.compile on AWS Graviton Processors</title>
      <link href="https://pytorch.org/blog/improve-rag-performance/" rel="alternate" type="text/html" title="Improve RAG performance with torch.compile on AWS Graviton Processors" />
      <published>2024-12-20T00:00:00-08:00</published>
      <updated>2024-12-20T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/improve-rag-performance</id>
      <content type="html" xml:base="https://pytorch.org/blog/improve-rag-performance/">&lt;p&gt;Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to support tasks like answering questions, translating languages, and completing sentences. There are a few challenges when working with LLMs such as domain knowledge gaps, factuality issues, and hallucination, which affect their reliability especially for the fields that require high levels of accuracy, such as healthcare, law, or engineering. Retrieval Augmented Generation (RAG) provides a solution to mitigate some of these issues by augmenting LLMs with a specific domain or an organization’s internal knowledge base, without the need to retrain the model.&lt;/p&gt;

&lt;p&gt;The RAG knowledge source is generally business specific databases which are typically deployed on general-purpose CPU infrastructure. So, deploying RAG on general-purpose CPU infrastructure alongside related business services is both efficient and cost-effective. With this motivation, we evaluated RAG deployment on &lt;a href=&quot;https://aws.amazon.com/ec2/graviton/&quot;&gt;AWS Graviton&lt;/a&gt; based Amazon EC2 instances which have been delivering up to &lt;a href=&quot;https://aws.amazon.com/ec2/graviton/getting-started/&quot;&gt;40% price-performance advantage&lt;/a&gt; compared to comparable instances for the majority of the workloads including databases, in-memory caches, big data analytics, media codecs, gaming servers, and machine learning inference.&lt;/p&gt;

&lt;p&gt;In the past we published a few blog posts on how PyTorch was optimized for AWS Graviton processors to accelerate ML Inference performance for both eager mode (&lt;a href=&quot;https://pytorch.org/blog/optimized-pytorch-w-graviton/&quot;&gt;blog&lt;/a&gt;) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; mode (&lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-inference/&quot;&gt;blog&lt;/a&gt;). In this blog we cover how to deploy a typical RAG workload using PyTorch and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, how we improved its performance up to &lt;strong&gt;1.7x&lt;/strong&gt; for embedding model and &lt;strong&gt;1.3x&lt;/strong&gt; for RAG query on AWS Graviton3-based m7g.xlarge instance compared to the default PyTorch “eager mode”, and finally a few recommendations that you can apply for your RAG use cases.&lt;/p&gt;

&lt;h2 id=&quot;how-to-optimize-rag&quot;&gt;How to Optimize RAG?&lt;/h2&gt;

&lt;p&gt;Without RAG, the LLM takes the user input and creates a response based on information it was trained on (what it already knows). With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following diagram shows the conceptual flow of using RAG with LLMs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/improve-rag-performance.png&quot; alt=&quot;Image 1: Conceptual flow of using RAG with LLMs&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Conceptual flow of using RAG with LLMs&lt;/p&gt;

&lt;p&gt;Source:&lt;a href=&quot;https://aws.amazon.com/what-is/retrieval-augmented-generation/&quot;&gt; https://aws.amazon.com/what-is/retrieval-augmented-generation/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;embedding-model&quot;&gt;Embedding model&lt;/h3&gt;

&lt;p&gt;At the core of RAG is an embedding model that takes the text data and converts into a vector representation. These vectors are then stored in a vector db. When a user makes a query, the query is first converted to a vector and the RAG does a similarity search on the vector db. Hence, the first step in optimizing RAG performance is optimizing an embedding model’s inference performance. We used the AWS Graviton3-based m7g.xlarge instance and the HuggingFace sentence-transformer embedding model for the optimization work. Here is a sample script for profiling the HuggingFace sentence-transformer embedding model inference with PyTorch Eager mode.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.profiler import profile, ProfilerActivity, record_function
from transformers import AutoModel, AutoTokenizer

model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;
input_text = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]

model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoded_input = tokenizer(
    input_text, padding=True, truncation=True, return_tensors=&quot;pt&quot;
)

warmup, actual = 100, 100
model.eval()

with torch.no_grad():
    # warmup
    for i in range(warmup):
        embeddings = model(**encoded_input)

    with profile(activities=[ProfilerActivity.CPU]) as prof:
        with record_function(&quot;model_inference&quot;):
            for i in range(actual):
                embeddings = model(**encoded_input)
        print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;eager-mode&quot;&gt;Eager mode&lt;/h4&gt;

&lt;p&gt;Since PyTorch eager mode was already optimized on AWS Graviton processors with the following runtime environment settings, we included them in the baseline and measured the following performance. Please refer to &lt;a href=&quot;https://pytorch.org/blog/optimized-pytorch-w-graviton/&quot;&gt;Optimized PyTorch 2.0 Inference with AWS Graviton processors&lt;/a&gt; for more details on how we optimized the PyTorch eager mode on AWS Graviton processors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Enable the fast math GEMM kernels, to accelerate fp32 inference with bfloat16 gemm
export DNNL_DEFAULT_FPMATH_MODE=BF16

# Enable Linux Transparent Huge Page (THP) allocations,
# to reduce the tensor memory allocation latency
export THP_MEM_ALLOC_ENABLE=1

# Set LRU Cache capacity to cache the primitives and avoid redundant
# memory allocations
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                aten::addmm        61.01%        2.638s        62.49%        2.702s     370.197us          7300  
            model_inference        12.01%     519.161ms       100.00%        4.324s        4.324s             1  
                  aten::bmm         6.25%     270.084ms        11.96%     517.089ms     215.454us          2400  
               aten::select         3.98%     172.165ms         5.34%     230.863ms       1.331us        173500  
                aten::copy_         2.11%      91.133ms         2.11%      91.133ms       6.200us         14700   
---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 4.324s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Profiler output for HuggingFace sentence-transformer embedding model inference on AWS Graviton3-based m7g.xlarge instance with PyTorch Eager mode&lt;/p&gt;

&lt;p&gt;Next, we added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-inference/#technical-deep-dive-what-are-the-challenges-and-optimization-details&quot;&gt;weights pre-packing&lt;/a&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.inference_mode&lt;/code&gt; and observed around 1.7x performance improvement. The following section talks about each of these optimizations and the resulting speedup.&lt;/p&gt;

&lt;h4 id=&quot;torchcompile&quot;&gt;torch.compile&lt;/h4&gt;

&lt;p&gt;In contrast to eager mode, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; pre-compiles the entire model into a single graph in a manner that’s optimized for running on given hardware. Please refer to &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-inference/&quot;&gt;Accelerated PyTorch Inference with torch.compile on AWS Graviton processors&lt;/a&gt; for more details on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; features and how we optimized them on AWS Graviton processors. Invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; as shown in the following snippet to trigger PyTorch dynamo compilation for the model. This resulted in around 1.04x performance improvement from the baseline.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)

----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                 aten::addmm        64.46%        2.675s        66.66%        2.766s     378.905us          7300  
       Torch-Compiled Region        19.76%     820.085ms        99.04%        4.109s      41.094ms           100  
                   aten::bmm         6.66%     276.216ms        12.52%     519.527ms     216.470us          2400  
                aten::select         3.98%     164.991ms         5.41%     224.488ms       1.299us        172800  
            aten::as_strided         1.66%      69.039ms         1.66%      69.039ms       0.383us        180100  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 4.149s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; Profiler output for HuggingFace sentence-transformer embedding model inference on AWS Graviton3-based m7g.xlarge instance with torch.compile mode&lt;/p&gt;

&lt;h4 id=&quot;weights-pre-packing&quot;&gt;Weights pre-packing&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; opens up opportunities like pre-packing the model weights into a format that is more suitable for the given hardware during the model compilation, thus improving the performance. Set the following config to trigger weights pre-packing. This resulted in around 1.69x improvement from the baseline.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch._inductor.config as config
config.cpp.weight_prepack=True
config.freezing=True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
    mkldnn::_linear_pointwise        39.10%     994.821ms        41.50%        1.056s     144.628us          7300  
        Torch-Compiled Region        35.12%     893.675ms        98.42%        2.504s      25.043ms           100  
                    aten::bmm        10.96%     278.859ms        21.66%     551.073ms     229.614us          2400  
                 aten::select         7.34%     186.838ms         9.98%     253.840ms       1.469us        172800  
             aten::as_strided         2.63%      67.002ms         2.63%      67.002ms       0.388us        172800   
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.544s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Table 3:&lt;/strong&gt; Profiler output for HuggingFace sentence-transformer embedding model inference on AWS Graviton3-based m7g.xlarge instance with torch.compile and weights pre-packing&lt;/p&gt;

&lt;h4 id=&quot;torchinference_mode&quot;&gt;torch.inference_mode&lt;/h4&gt;

&lt;p&gt;Additionally, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.inference_mode()&lt;/code&gt; to get savings from turning off version control for tensors and view tracking of tensors. Please refer to the PyTorch&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.autograd.grad_mode.inference_mode.html&quot;&gt; documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.inference_mode():
# instead of
with torch.no_grad():
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
    mkldnn::_linear_pointwise        38.92%     987.276ms        41.17%        1.044s     143.056us          7300  
        Torch-Compiled Region        34.92%     885.895ms        98.45%        2.498s      24.975ms           100  
                    aten::bmm        11.25%     285.292ms        22.22%     563.594ms     234.831us          2400  
                 aten::select         7.74%     196.223ms        10.22%     259.251ms       1.500us        172800  
             aten::as_strided         2.48%      63.027ms         2.48%      63.027ms       0.365us        172800  
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.537s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Table 4:&lt;/strong&gt; Profiler output for HuggingFace sentence-transformer embedding model inference on AWS Graviton3-based m7g.xlarge instance with torch.compile, weights pre-packing, and inference_mode&lt;/p&gt;

&lt;p&gt;The following table shows the incremental performance improvements achieved for the standalone embedding model inference.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimization level&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Latency measured (in sec)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Improvement over the baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;PyTorch eager mode (Baseline)
   &lt;/td&gt;
   &lt;td&gt;0.04324
   &lt;/td&gt;
   &lt;td&gt;NA
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.compile
   &lt;/td&gt;
   &lt;td&gt;0.04149
   &lt;/td&gt;
   &lt;td&gt;1.04x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;weights pre-packing
   &lt;/td&gt;
   &lt;td&gt;0.02544
   &lt;/td&gt;
   &lt;td&gt;1.69x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.inference_mode
   &lt;/td&gt;
   &lt;td&gt;0.02537
   &lt;/td&gt;
   &lt;td&gt;1.70x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The following script is an updated example for the embedding model inference with the previously discussed optimizations included. The optimizations are highlighted in &lt;span style=&quot;color: green;&quot;&gt;GREEN&lt;/span&gt;.&lt;/p&gt;

&lt;div class=&quot;code-block&quot;&gt;
&lt;pre&gt;
import torch
from torch.profiler import profile, record_function, ProfilerActivity
from transformers import AutoTokenizer, AutoModel
&lt;span style=&quot;color: green;&quot;&gt;import torch._inductor.config as config&lt;/span&gt;
&lt;span style=&quot;color: green;&quot;&gt;config.cpp.weight_prepack=True&lt;/span&gt;
&lt;span style=&quot;color: green;&quot;&gt;config.freezing=True&lt;/span&gt;

model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;
input_text = ['This is an example sentence', 'Each sentence is converted']

model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoded_input = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')

warmup , actual = 100, 100
model.eval()
&lt;span style=&quot;color: green;&quot;&gt;model = torch.compile(model)&lt;/span&gt;

&lt;span style=&quot;color: green;&quot;&gt;with torch.inference_mode():&lt;/span&gt;
#instead of with torch.no_grad()
# warmup
  for i in range(warmup):
  	embeddings = model(**encoded_input)

  with profile(activities=[ProfilerActivity.CPU]) as prof:
	with record_function(&quot;model_inference&quot;):
  	for i in range(actual):
     	embeddings = model(**encoded_input)
  print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))
&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;end-to-end-rag-scenario-on-cpu&quot;&gt;End-to-End RAG scenario on CPU&lt;/h3&gt;

&lt;p&gt;After optimizing the embedding model inference, we started with a PyTorch eager mode based RAG setup, mainly to validate the functionality on the CPU backend. We built the RAG solution with&lt;a href=&quot;https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html&quot;&gt; HuggingFaceEmbeddings&lt;/a&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;langchain_community.embeddings&lt;/code&gt;, as shown in the following code snippet.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader
from langchain.prompts import PromptTemplate
from langchain_core.prompts import format_document
from bs4 import BeautifulSoup as Soup
import torch

url =  &quot;https://pytorch.org/blog/pytorch2-5/&quot;
chunk_size = 1000
chunk_overlap = 0
embedding_model = &quot;sentence-transformers/all-mpnet-base-v2&quot;
N = 5

question = &quot;What's new in PyTorch 2.5?&quot;

from transformers import AutoTokenizer, AutoModel
from typing import Any, List

loader = RecursiveUrlLoader(
            url=url, max_depth=3, extractor=lambda x: Soup(x, &quot;html.parser&quot;).text
        )       
docs = loader.load()

# Split the document into chunks with a specified chunk size
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
all_splits = text_splitter.split_documents(docs)

# Store the document into a vector store with a specific embedding model
model = HuggingFaceEmbeddings(model_name=embedding_model)

warmup , actual = 100, 100

with torch.inference_mode():
    vectorstore = FAISS.from_documents(all_splits, model)

    for i in range(warmup):
        searchDocs = vectorstore.similarity_search(question, k=N)

    import time

    start = time.time()
    for i in range(actual):
        searchDocs = vectorstore.similarity_search(question, k=N)
    end = time.time()
    print(f&quot;Time for 1 inference is {(end-start)/actual} seconds&quot;)

    doc_prompt = PromptTemplate.from_template(&quot;{page_content}&quot;)
    context = &quot;&quot;
    for i, doc in enumerate(searchDocs):
        context += f&quot;\n{format_document(doc, doc_prompt)}\n&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, our goal was to optimize the end-to-end RAG use case with torch.compile and weights pre-packing that gave 1.7x improvement for the standalone embedding model inference. However, the optimizations didn’t work out of the box for the RAG scenario.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-challenges-and-solutions-to-achieve-similar-gains-in-an-end-to-end-rag-scenario&quot;&gt;What are the challenges and solutions to achieve similar gains in an end-to-end RAG scenario?&lt;/h3&gt;

&lt;h4 id=&quot;challenge-1-model-handle&quot;&gt;Challenge 1: model handle&lt;/h4&gt;

&lt;p&gt;There was no way to get the model handle that was instantiated with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HuggingFaceEmbeddings&lt;/code&gt;, and the wrapper class doesn’t provide compile APIs. So, there was no way for our application to invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to trigger the PyTorch dynamo compilation process.&lt;/p&gt;

&lt;h4 id=&quot;solution&quot;&gt;Solution&lt;/h4&gt;

&lt;p&gt;We implemented our custom embedding class so that we can get a handle for the model. This instantiated the embedding model from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sentence-transformers&lt;/code&gt; , and maintained the handle for immediate compilation or compilation at a later stage. With this, we were able to trigger &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and hence the dynamo compilation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CustomEmbedding(HuggingFaceEmbeddings):
    
    def __init__(self, **kwargs: Any):
        &quot;&quot;&quot;Initialize the sentence_transformer.&quot;&quot;&quot;
        super().__init__(**kwargs)

        # Load model from HuggingFace Hub
        self.client = AutoModel.from_pretrained(self.model_name)
    class Config:
        arbitrary_types_allowed = True


    
    def embed_documents(self, texts: List[str]) -&amp;gt; List[List[float]]:
        &quot;&quot;&quot;Compute doc embeddings using a HuggingFace transformer model.
        Args:
            texts: The list of texts to embed.
        Returns:
            List of embeddings, one for each text.
        &quot;&quot;&quot;

        texts = list(map(lambda x: x.replace(&quot;\n&quot;, &quot; &quot;), texts))

        # Tokenize sentences
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        
        embeddings = self.client(
           **encoded_input, output_hidden_states=True
        )
        embeddings = embeddings.pooler_output.detach().numpy()

        return embeddings.tolist()

# instead of model = HuggingFaceEmbeddings(model_name=embedding_model)
model = CustomEmbedding(model_name=embedding_model)

# torch.compile the model
model.client = torch.compile(model.client)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;challenge-2-triggering-the-optimization&quot;&gt;Challenge 2: triggering the optimization&lt;/h4&gt;

&lt;p&gt;For a typical inference scenario where the graph is frozen and gradient calculations are disabled, Torch inductor (the compiler backend we used for CPUs) invokes hardware specific optimizations like graph rewrite into more performant operators, operator fusion, and weights pre-packing. Though Torch dynamo was able to see the model and trigger generic compilation, it failed to trigger these additional Fx passes in the Torch inductor.&lt;/p&gt;

&lt;p&gt;There were two main reasons for Torch inductor not triggering the optimization passes: (1) The application didn’t set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no_grad()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_mode()&lt;/code&gt; for torch inductor to detect that the graph was frozen; and (2) We hit a limitation with the torch.compile framework, where, if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no_grad&lt;/code&gt; is set just at the beginning of the compiled region, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; wouldn’t be able to detect it while invoking the inductor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fx&lt;/code&gt; passes because it would not have hit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no_grad&lt;/code&gt; region by then. Please refer to&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/125474&quot;&gt; this GitHub issue&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h4 id=&quot;solution-1&quot;&gt;Solution&lt;/h4&gt;

&lt;p&gt;We work around this limitation by moving the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no_grad()&lt;/code&gt; context into the application code from within the model class. With this, the model compilation happened as expected and gave around 1.3x performance improvement when we profiled the stable inference pass for eager and compiled versions.&lt;/p&gt;

&lt;h4 id=&quot;challenge-3-extra-compilation&quot;&gt;Challenge 3: extra compilation&lt;/h4&gt;

&lt;p&gt;With the previous fixes, the query lookup inference performance was improved, but not the total execution time of the benchmarking script. We root-caused it to redundant compilation for the model during the RAG inference. Further deep diving revealed that it was because of the batch size mismatch between the word embedding and the RAG query stages. For example, in our benchmarking script, when the database was vectorized and stored in vector db, we used the batch size of 16, hence the model was compiled with shapes of &lt;strong&gt;16&lt;/strong&gt;xNxK. Whereas, the RAG query lookup is usually a single request of shape &lt;strong&gt;1&lt;/strong&gt;xNxK. So, there was a batch size mismatch (dimension “0” of these tensors) that triggered the recompilation for the query lookup stage. We confirmed it with the following Torch logging: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_LOGS=&quot;recompiles&quot;&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TORCH_LOGS=&quot;recompiles&quot; python rag_compile.py 
V1103 02:48:08.805986 34281 site-packages/torch/_dynamo/guards.py:2813] [0/1] [__recompiles] Recompiling function forward in site-packages/transformers/models/mpnet/modeling_mpnet.py:502
V1103 02:48:08.805986 34281 site-packages/torch/_dynamo/guards.py:2813] [0/1] [__recompiles]     triggered by the following guard failure(s):
V1103 02:48:08.805986 34281 site-packages/torch/_dynamo/guards.py:2813] [0/1] [__recompiles]     - 0/0: tensor 'L['input_ids']' size mismatch at index 0. expected 16, actual 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;solution-2&quot;&gt;Solution&lt;/h4&gt;

&lt;p&gt;Torch dynamo provides a decorator to mark the dimension of a given tensor as dynamic and specify an expected value for the same, so that re-compilation is not triggered. For example, specifying dimension “0” of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input_ids&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attention_mask&lt;/code&gt; as dynamic, and specifying that value of “1” is allowed in that dimension (as shown in the following code snippet), should have avoided the redundant compilations.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch._dynamo.decorators.mark_unbacked(encoded_input['input_ids'], 0)
torch._dynamo.mark_dynamic(encoded_input['input_ids'], 1)
        torch._dynamo.decorators.mark_unbacked(encoded_input['attention_mask'], 0)
torch._dynamo.mark_dynamic(encoded_input['attention_mask'], 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, the Torch dynamo decorator and marking didn’t work in this particular case. Moreover, using the decorator created graph breaks. So, we added some warmup iterations to hide the compilation latency, and profiled the query lookup performance in the steady state. However, the good news is that, in practice, this re-compilation is triggered only for the first query, so it might not affect the production scenario if the database size is fixed. Moreover, PyTorch AOT Inductor (a new feature in PyTorch) addresses re-compilation and warm up challenges with torch.compile. In a follow-up blog we will address how in a production environment we can use AOT Inductor to address these challenges.&lt;/p&gt;

&lt;p&gt;With these solutions we were able to apply torch.compile, weights pre-packing and the AWS Graviton specific optimizations for an end-end RAG scenario and improve the performance by 1.3x from the baseline eager mode.&lt;/p&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;A detailed guide on how to deploy torch compiled RAG on AWS Graviton-based Amazon EC2 instances and how to deploy it in conjunction with Llama using&lt;a href=&quot;https://github.com/pytorch/serve&quot;&gt; TorchServe&lt;/a&gt; can be found on the&lt;a href=&quot;https://pytorch.org/serve/enhancing_llm_serving_compile_rag.html&quot;&gt; PyTorch website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog, we covered how we optimized embedding model inference performance on AWS Graviton3-based EC2 instances. We also shared the challenges faced, the solutions we implemented to bring those optimizations for a RAG use case, and the resulting speedups. We hope that you will give it a try! If you need any support with ML software on Graviton, please open an issue on the AWS Graviton Technical Guide&lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt; GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We would like to express our gratitude to Eli Uriegas for the support in making this blog post happen.&lt;/p&gt;

&lt;h2 id=&quot;authors&quot;&gt;Authors&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sunita Nadampalli&lt;/strong&gt; is a Principal Engineer and AI/ML expert at AWS. She leads AWS Graviton software performance optimizations for AI/ML and HPC workloads. She is passionate about open source software development and delivering high-performance and sustainable software solutions for SoCs based on the Arm ISA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ankith Gunapal&lt;/strong&gt; is an AI Partner Engineer at Meta (PyTorch). He leads customer support, evangelizing &amp;amp; release engineering of TorchServe. He is passionate about solving production problems in model inference and model serving. He also enjoys distilling technically complex material in a user friendly format.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hamid Shojanazeri&lt;/strong&gt; leads the AI Frameworks Partner Engineering team at Meta. He is passionate about building scalable AI solutions and specializes in working with PyTorch to tackle the challenges of large-scale distributed training, inference, model serving, and optimization.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sunita Nadampalli(AWS), Ankith Gunapal(Meta), Hamid Shojanazeri(Meta)</name>
        
        
      </author>

      

      

      
        <summary type="html">Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to support tasks like answering questions, translating languages, and completing sentences. There are a few challenges when working with LLMs such as domain knowledge gaps, factuality issues, and hallucination, which affect their reliability especially for the fields that require high levels of accuracy, such as healthcare, law, or engineering. Retrieval Augmented Generation (RAG) provides a solution to mitigate some of these issues by augmenting LLMs with a specific domain or an organization’s internal knowledge base, without the need to retrain the model.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">docTR joins PyTorch Ecosystem: From Pixels to Data, Building a Recognition Pipeline with PyTorch and docTR</title>
      <link href="https://pytorch.org/blog/doctr-joins-pytorch-ecosystem/" rel="alternate" type="text/html" title="docTR joins PyTorch Ecosystem: From Pixels to Data, Building a Recognition Pipeline with PyTorch and docTR" />
      <published>2024-12-18T00:00:00-08:00</published>
      <updated>2024-12-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/doctr-joins-pytorch-ecosystem</id>
      <content type="html" xml:base="https://pytorch.org/blog/doctr-joins-pytorch-ecosystem/">&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg1.png&quot; alt=&quot;docTR logo&quot; style=&quot;width:100%;display: block;max-width:400px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the docTR project has been integrated into the PyTorch ecosystem! This integration ensures that docTR aligns with PyTorch’s standards and practices, giving developers a reliable, community-backed solution for powerful OCR workflows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For more information on what it means to be a PyTorch ecosystem project, see the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem Tools page&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;about-doctr&quot;&gt;About docTR&lt;/h2&gt;

&lt;p&gt;docTR is an Apache 2.0 project developed and distributed by &lt;a href=&quot;https://www.mindee.com/&quot;&gt;Mindee&lt;/a&gt; to help developers integrate OCR capabilities into applications with no prior knowledge required.&lt;/p&gt;

&lt;p&gt;To quickly and efficiently extract text information, docTR uses a two-stage approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, it performs text &lt;strong&gt;detection&lt;/strong&gt; to localize words.&lt;/li&gt;
  &lt;li&gt;Then, it conducts text &lt;strong&gt;recognition&lt;/strong&gt; to identify all characters in a word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Detection&lt;/strong&gt; and &lt;strong&gt;recognition&lt;/strong&gt; are performed by state-of-the-art models written in PyTorch. To learn more about this approach, you can refer &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;to the docTR documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;docTR enhances the user experience in PyTorch projects by providing high-performance OCR capabilities right out of the box. Its specially designed models require minimal to no fine-tuning for common use cases, allowing developers to quickly integrate advanced document analysis features.&lt;/p&gt;

&lt;h2 id=&quot;local-installation&quot;&gt;Local installation&lt;/h2&gt;

&lt;p&gt;docTR requires Python &amp;gt;= 3.10 and supports Windows, Mac and Linux. Please refer to our &lt;a href=&quot;https://github.com/mindee/doctr?tab=readme-ov-file#installation&quot;&gt;README&lt;/a&gt; for necessary dependencies for MacBook with the M1 chip.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install -U pip
pip3 install &quot;python-doctr[torch,viz]&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will install docTR along with the latest version of PyTorch.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Note: docTR also provides docker images for an easy deployment, such as a part of Kubernetes cluster.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;text-recognition&quot;&gt;Text recognition&lt;/h2&gt;

&lt;p&gt;Now, let’s try docTR’s OCR recognition on this sample:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg2.jpg&quot; alt=&quot;OCR sample&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The OCR recognition model expects an image with only one word on it and will output the predicted word with a confidence score. You can use the following snippet to test OCR capabilities from docTR:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python
from doctr.io import DocumentFile
from doctr.models import recognition_predictor

doc = DocumentFile.from_images(&quot;/path/to/image&quot;)

# Load the OCR model
# This will download pre-trained models hosted by Mindee
model = recognition_predictor(pretrained=True)

result = model(doc)
print(result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the most important line of code is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = recognition_predictor(pretrained=True)&lt;/code&gt;. This will load a default text recognition model, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;crnn_vgg16_bn&lt;/code&gt;, but you can select other models through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch&lt;/code&gt; parameter. You can check out the &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;available architectures&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When run on the sample, the recognition predictor retrieves the following data: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[('MAGAZINE', 0.9872216582298279)]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Note: using the DocumentFile object docTR provides an easy way to manipulate PDF or Images.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;text-detection&quot;&gt;Text detection&lt;/h2&gt;

&lt;p&gt;The last example was a crop on a single word. Now, what about an image with several words on it, like this one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg3.jpg&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A text detection model is used before the text recognition to output a segmentation map representing the location of the text. Following that, the text recognition is applied on every detected patch.&lt;/p&gt;

&lt;p&gt;Below is a snippet to run only the detection part:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from doctr.io import DocumentFile
from doctr.models import detection_predictor
from matplotlib import pyplot as plt
from doctr.utils.geometry import detach_scores
from doctr.utils.visualization import draw_boxes

doc = DocumentFile.from_images(&quot;path/to/my/file&quot;)
model = detection_predictor(pretrained=True)

result = model(doc)

draw_boxes(detach_scores([result[0][&quot;words&quot;]])[0][0], doc[0])
plt.axis('off')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running it on the full sample yields the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg4.png&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly to the text recognition, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detection_predictor&lt;/code&gt; will load a default model (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_base&lt;/code&gt; here). You can also load another one by providing it through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch&lt;/code&gt; parameter.&lt;/p&gt;

&lt;h2 id=&quot;the-full-implementation&quot;&gt;The full implementation&lt;/h2&gt;

&lt;p&gt;Now, let’s plug both components into the same pipeline.&lt;/p&gt;

&lt;p&gt;Conveniently, docTR provides a wrapper that does exactly that for us:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from doctr.io import DocumentFile
from doctr.models import ocr_predictor

doc = DocumentFile.from_images(&quot;/path/to/image&quot;)

model = ocr_predictor(pretrained=True, assume_straight_pages=False)

result = model(doc)
result.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg5.png&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The last line should display a matplotlib window which shows the detected patches. Hovering the mouse over them will display their contents.&lt;/p&gt;

&lt;p&gt;You can also do more with this output, such as reconstituting a synthetic document like so:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import matplotlib.pyplot as plt

synthetic_pages = result.synthesize()
plt.imshow(synthetic_pages[0])
plt.axis('off')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg6.png&quot; alt=&quot;black text on white&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pipeline is highly customizable, where you can modify the detection or recognition model behaviors by passing arguments to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ocr_predictor&lt;/code&gt;. Please refer to the &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;documentation&lt;/a&gt; to learn more about it.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re excited to welcome docTR into the PyTorch Ecosystem, where it seamlessly integrates with PyTorch pipelines to deliver state-of-the-art OCR capabilities right out of the box.&lt;/p&gt;

&lt;p&gt;By empowering developers to quickly extract text from images or PDFs using familiar tooling, docTR simplifies complex document analysis tasks and enhances the overall PyTorch experience.&lt;/p&gt;

&lt;p&gt;We invite you to explore the &lt;a href=&quot;https://github.com/mindee/doctr&quot;&gt;docTR GitHub repository&lt;/a&gt;, join the &lt;a href=&quot;https://slack.mindee.com/&quot;&gt;docTR community on Slack&lt;/a&gt;, and reach out at contact@mindee.com for inquiries or collaboration opportunities.&lt;/p&gt;

&lt;p&gt;Together, we can continue to push the boundaries of document understanding and develop even more powerful, accessible tools for everyone in the PyTorch community.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Olivier Dulcy &amp; Sebastian Olivera, Mindee</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">torchcodec: Easy and Efficient Video Decoding for PyTorch</title>
      <link href="https://pytorch.org/blog/torchcodec/" rel="alternate" type="text/html" title="torchcodec: Easy and Efficient Video Decoding for PyTorch" />
      <published>2024-12-11T00:00:00-08:00</published>
      <updated>2024-12-11T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/torchcodec</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchcodec/">&lt;p&gt;We are pleased to officially announce &lt;a href=&quot;https://github.com/pytorch/torchcodec&quot;&gt;torchcodec&lt;/a&gt;, a library for decoding videos into PyTorch tensors. It is fast, accurate, and easy to use. When running PyTorch models on videos, torchcodec is our recommended way to turn those videos into data your model can use.&lt;/p&gt;

&lt;p&gt;Highlights of torchcodec include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An intuitive decoding API that treats a video file as a Python sequence of frames. We support both index-based and presentation-time-based frame retrieval.&lt;/li&gt;
  &lt;li&gt;An emphasis on accuracy: we ensure you get the frames you requested, even if your video has variable frame rates.&lt;/li&gt;
  &lt;li&gt;A rich sampling API that makes it easy and efficient to retrieve batches of frames.&lt;/li&gt;
  &lt;li&gt;Best-in-class CPU decoding performance.&lt;/li&gt;
  &lt;li&gt;CUDA accelerated decoding that enables high throughput when decoding many videos at once.&lt;/li&gt;
  &lt;li&gt;Support for all codecs available in your installed version of FFmpeg.&lt;/li&gt;
  &lt;li&gt;Simple binary installs for Linux and Mac.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;easy-to-use&quot;&gt;Easy to Use&lt;/h2&gt;

&lt;p&gt;A simple, intuitive API was one of our main design principles. We start with simple decoding and extracting specific frames of a video:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchcodec.decoders import VideoDecoder
from torch import Tensor

decoder = VideoDecoder(&quot;my_video.mp4&quot;)

# Index based frame retrieval.
first_ten_frames: Tensor = decoder[10:]
last_ten_frames: Tensor = decoder[-10:]

# Multi-frame retrieval, index and time based.
frames = decoder.get_frames_at(indices=[10, 0, 15])
frames = decoder.get_frames_played_at(seconds=[0.2, 3, 4.5])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All decoded frames are already PyTorch tensors, ready to be fed into models for training.&lt;/p&gt;

&lt;p&gt;Of course, more common in ML training pipelines is sampling multiple clips from videos. A clip is just a sequence of frames in presentation order—but the frames are often &lt;em&gt;not&lt;/em&gt; consecutive. Our sampling API makes this easy:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchcodec.samplers import clips_at_regular_timestamps

clips = clips_at_regular_timestamps(
  decoder,
  seconds_between_clip_starts=10,
  num_frames_per_clip=5,
  seconds_between_frames=0.2,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above call yields a batch of clips where each clip starts 10 seconds apart, each clip has 5 frames, and those frames are 0.2 seconds apart. See our tutorials on &lt;a href=&quot;https://pytorch.org/torchcodec/0.1.0/generated_examples/basic_example.html&quot;&gt;decoding&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/torchcodec/0.1.0/generated_examples/sampling.html&quot;&gt;sampling&lt;/a&gt; for more!&lt;/p&gt;

&lt;h2 id=&quot;fast-performance&quot;&gt;Fast Performance&lt;/h2&gt;

&lt;p&gt;Performance was our other main design principle. Decoding videos for ML training has different performance requirements than decoding videos for playback. A typical ML video training pipeline will process many different videos (sometimes in the millions!), but only sample a small number of frames (dozens to hundreds) from each video.&lt;/p&gt;

&lt;p&gt;For this reason, we’ve paid particular attention to our decoder’s performance when seeking multiple times in a video, decoding a small number of frames after each seek. We present experiments with the following four scenarios:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Decoding and transforming frames from multiple videos at once, inspired by what we have seen in data loading for large-scale training pipelines:&lt;/p&gt;

    &lt;p&gt;a. Ten threads decode batches of 50 videos in parallel.&lt;br /&gt;
b. For each video, decode 10 frames at evenly spaced times.&lt;br /&gt;
c. For each frame, resize it to a 256x256 resolution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Decoding 10 frames at random locations in a single video.&lt;/li&gt;
  &lt;li&gt;Decoding 10 frames at evenly spaced times of a single video.&lt;/li&gt;
  &lt;li&gt;Decoding the first 100 frames of a single video.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We compare the following video decoders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/stable/index.html&quot;&gt;Torchaudio&lt;/a&gt;, CPU decoding only.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/vision/stable/index.html&quot;&gt;Torchvision&lt;/a&gt;, using the &lt;a href=&quot;https://pytorch.org/vision/stable/index.html#torchvision.set_video_backend&quot;&gt;video_reader&lt;/a&gt; backend which is CPU decoding only.&lt;/li&gt;
  &lt;li&gt;Torchcodec, GPU decoding with CUDA.&lt;/li&gt;
  &lt;li&gt;Torchcodec, CPU decoding only.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the following three videos:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A synthetically generated video using FFmpeg’s &lt;a href=&quot;https://ffmpeg.org/ffmpeg-filters.html#mandelbrot&quot;&gt;mandelbrot&lt;/a&gt; generation pattern. The video is 10 seconds long, 60 frames per second and 1920x1080.&lt;/li&gt;
  &lt;li&gt;Same as above, except the video is 120 seconds long.&lt;/li&gt;
  &lt;li&gt;A &lt;a href=&quot;https://download.pytorch.org/torchaudio/tutorial-assets/stream-api/NASAs_Most_Scientifically_Complex_Space_Observatory_Requires_Precision-MP4_small.mp4&quot;&gt;promotional video from NASA&lt;/a&gt; that is 206 seconds long, 29.7 frames per second and 960x540.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/torchcodec/blob/b0de66677bac322e628f04ec90ddeeb0304c6abb/benchmarks/decoders/generate_readme_data.py&quot;&gt;experimental script&lt;/a&gt; is in our repo. Our experiments run on a Linux system with an Intel processor that has 22 available cores and an NVIDIA GPU. For CPU decoding, all libraries were instructed to automatically determine the best number of threads to use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/benchmark_readme_chart.png&quot; alt=&quot;Benchmark chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From our experiments, we draw several conclusions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torchcodec is consistently the best-performing library for the primary use case we designed it for: decoding many videos at once as a part of a training data loading pipeline. In particular, high-resolution videos see great gains with CUDA where decoding and transforms both happen on the GPU.&lt;/li&gt;
  &lt;li&gt;Torchcodec is competitive on the CPU with seek-heavy use cases such as random and uniform sampling. Currently, torchcodec’s performance is better with shorter videos that have a smaller file size. This performance is due to torchcodec’s emphasis on seek-accuracy, which involves an initial linear scan.&lt;/li&gt;
  &lt;li&gt;Torchcodec is not as competitive when there is no seeking; that is, opening a video file and decoding from the beginning. This is again due to our emphasis on seek-accuracy and the initial linear scan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implementing an &lt;a href=&quot;https://github.com/pytorch/torchcodec/issues/427&quot;&gt;approximate seeking mode&lt;/a&gt; in torchcodec should resolve these performance gaps, and it’s our highest priority feature for video decoding.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;As the name implies, the long-term future for torchcodec is more than just video decoding. Our next big feature is audio support—both decoding audio streams from video, and from audio-only media. In the long term, we want torchcodec to be the media decoding library for PyTorch. That means as we implement functionality in torchcodec, we will deprecate and eventually remove complementary features from torchaudio and torchvision.&lt;/p&gt;

&lt;p&gt;We also have video decoding improvements lined up, such as the previously mentioned approximate seeking mode for those who are willing to sacrifice accuracy for performance.&lt;/p&gt;

&lt;p&gt;Most importantly, we’re looking for feedback from the community! We’re most interested in working on features that the community finds valuable. Come &lt;a href=&quot;https://github.com/pytorch/torchcodec/issues&quot;&gt;share your needs&lt;/a&gt; and influence our future direction!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are pleased to officially announce torchcodec, a library for decoding videos into PyTorch tensors. It is fast, accurate, and easy to use. When running PyTorch models on videos, torchcodec is our recommended way to turn those videos into data your model can use.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">vLLM Joins PyTorch Ecosystem: Easy, Fast, and Cheap LLM Serving for Everyone</title>
      <link href="https://pytorch.org/blog/vllm-joins-pytorch/" rel="alternate" type="text/html" title="vLLM Joins PyTorch Ecosystem: Easy, Fast, and Cheap LLM Serving for Everyone" />
      <published>2024-12-09T00:00:00-08:00</published>
      <updated>2024-12-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/vllm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/vllm-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/vllm.png&quot; alt=&quot;vllm logo&quot; style=&quot;width:100%;display: block;max-width:400px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM project&lt;/a&gt; has become a PyTorch ecosystem project, and joined the PyTorch ecosystem family!&lt;/p&gt;

&lt;p&gt;For more information on what it means to be a PyTorch ecosystem project, see the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem Tools page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Running large language models (LLMs) is both resource-intensive and complex, especially as these models scale to hundreds of billions of parameters. That’s where vLLM comes in — a high-throughput, memory-efficient inference and serving engine designed for LLMs.&lt;/p&gt;

&lt;p&gt;Originally built around the innovative &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;PagedAttention algorithm&lt;/a&gt;, vLLM has grown into a comprehensive, state-of-the-art inference engine. A thriving community is also continuously adding new features and optimizations to vLLM, including pipeline parallelism, chunked prefill, speculative decoding, and disaggregated serving.&lt;/p&gt;

&lt;p&gt;Since its release, vLLM has garnered significant attention, achieving over 31,000 GitHub stars—a testament to its popularity and thriving community. This milestone marks an exciting chapter for vLLM as we continue to empower developers and researchers with cutting-edge tools for efficient and scalable AI deployment. Welcome to the next era of LLM inference!&lt;/p&gt;

&lt;p&gt;vLLM has always had a strong connection with the PyTorch project. It is deeply integrated into PyTorch, leveraging it as a unified interface to support a wide array of hardware backends. These include NVIDIA GPUs, AMD GPUs, Google Cloud TPUs, Intel GPUs, Intel CPUs, Intel Gaudi HPUs, and AWS Neuron, among others. This tight coupling with PyTorch ensures seamless compatibility and performance optimization across diverse hardware platforms.&lt;/p&gt;

&lt;p&gt;Do you know you can experience the power of vLLM right from your phone? During this year’s Amazon Prime Day, vLLM played a crucial role in &lt;a href=&quot;https://aws.amazon.com/cn/blogs/machine-learning/scaling-rufus-the-amazon-generative-ai-powered-conversational-shopping-assistant-with-over-80000-aws-inferentia-and-aws-trainium-chips-for-prime-day/&quot;&gt;delivering lightning-fast responses to millions of users&lt;/a&gt;. Across three regions, over 80,000 Trainium and Inferentia chips powered an average of 3 million tokens per minute, all while maintaining a P99 latency of less than 1 second for the first response. That means when customers opened the Amazon app and chatted with Rufus, they were seamlessly interacting with vLLM in action!&lt;/p&gt;

&lt;p&gt;vLLM also collaborates tightly with leading model vendors to ensure support for popular models. This includes tight integration with Meta LLAMA, Mistral, QWen, and DeepSeek models, plus many others. One particularly memorable milestone was the &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;release of LLAMA 3.1 (405B)&lt;/a&gt;. As the launching partner, vLLM was the first to enable running this very large model, showcasing vLLM’s capability to handle the most complex and resource-intensive language models.&lt;/p&gt;

&lt;p&gt;To install vLLM, simply run:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;vLLM is designed for both researchers and production-grade serving.&lt;/p&gt;

&lt;p&gt;To run vLLM as an OpenAI API compatible server, just use the Huggingface model ID:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vllm serve meta-llama/Llama-3.1-8B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run vLLM as a simple function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
   &quot;Hello, my name is&quot;,
   &quot;The president of the United States is&quot;,
   &quot;The capital of France is&quot;,
   &quot;The future of AI is&quot;,
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model=&quot;meta-llama/Llama-3.1-8B&quot;)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
   prompt = output.prompt
   generated_text = output.outputs[0].text
   print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Open-source innovation is part of the vLLM’s DNA. Born out of a Berkeley academic project, it follows the legacy of other pioneering open-source initiatives such as BSD, which revolutionized operating systems in the 1980s. Other innovations from the same organization include &lt;a href=&quot;https://github.com/apache/spark&quot;&gt;Apache Spark&lt;/a&gt; and &lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;Ray&lt;/a&gt;, now the standard for big data and AI systems. In the Gen AI era, vLLM serves as a platform dedicated to democratizing AI inference.&lt;/p&gt;

&lt;p&gt;The vLLM team remains steadfast in its mission to keep the project “of the community, by the community, and for the community.” Collaboration and inclusivity lie at the heart of everything we do.&lt;/p&gt;

&lt;p&gt;If you have collaboration requests or inquiries, feel free to reach out at &lt;a href=&quot;mailto:vllm-questions@lists.berkeley.edu&quot;&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;. To join the active and growing vLLM community, explore our &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;GitHub repository&lt;/a&gt; or connect with us on the &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;vLLM Slack&lt;/a&gt;. Together, we can push the boundaries of AI innovation and make it accessible to all.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>vLLM Team</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton</title>
      <link href="https://pytorch.org/blog/accelerating-gemms-triton/" rel="alternate" type="text/html" title="Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton" />
      <published>2024-12-06T00:00:00-08:00</published>
      <updated>2024-12-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-gemms-triton</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-gemms-triton/">&lt;p&gt;2D block quantization for Float8 (FP8) holds the promise of improving the accuracy of Float8 quantization while also accelerating GEMM’s for both inference and training.  In this blog, we showcase advances using Triton for the two main phases involved in doing block quantized Float8 GEMMs.&lt;/p&gt;

&lt;p&gt;For the incoming quantization of A and B tensors from high precision (BFloat16) to Float8, we showcase GridQuant which leverages a mini-grid stride loop style of processing with nearly &lt;strong&gt;2x&lt;/strong&gt; speedups (99.31%) over a current 2D block quantization kernel.&lt;/p&gt;

&lt;p&gt;For the Float8 GEMM, we showcase 3 new developments for Triton - Warp Specialization, TMA and a persistent kernel to effectively create a cooperative style kernel (an alternative to the &lt;a href=&quot;https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/&quot;&gt;Ping-Pong schedule&lt;/a&gt;).  As a result, we achieve ~&lt;strong&gt;1.2x&lt;/strong&gt; speedup over our best-performing SplitK kernel from last year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg1.png&quot; alt=&quot;Figure 1: A comparison of the 2D quantization speedup over a current baseline, across a range of sizes.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; A comparison of the 2D quantization speedup over a current baseline, across a range of sizes. &lt;strong&gt;&lt;em&gt;(lower-is-better)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-2d-blockwise-quantization-for-fp8&quot;&gt;Why 2D Blockwise Quantization for FP8?&lt;/h2&gt;

&lt;p&gt;Generally speaking, the accuracy of fp8 quantization improves as we move from tensor-wise scaling, to row-wise scaling, to 2D block-wise, and then finally to column-wise scaling.  This is because features for a given token are stored in each column, and thus each column in that tensor is more similarly scaled.&lt;/p&gt;

&lt;p&gt;To minimize the number of outliers of a given numerical set, we want to find commonality so that numbers are being scaled in a similar fashion.  For transformers, this means column based quantization could be optimal…however, columnar memory access is massively inefficient due to the data being laid out in memory in a rowwise contiguous manner.  Thus columnwise loading would require memory access involving large strides in memory to pull isolated values, contrary to the core tenets of efficient memory access.&lt;/p&gt;

&lt;p&gt;However, 2D is the next best option as it includes some aspects of columnar while being more memory efficient to pull since we can vectorize these loads with 2D vectorization.  Therefore, we want to find ways to improve the speed for 2D block quantization which is why we developed the GridQuant kernel.&lt;/p&gt;

&lt;p&gt;For the quantization process, we need to 2D block quantize both the higher precision BF16 incoming tensors (A = input activations, B = weights) and then proceed to do the Float8 matmul using the quantized tensors and their 2D block scaling values, and return an output C tensor in BF16.&lt;/p&gt;

&lt;h2 id=&quot;how-does-gridquant-improve-2d-block-quantization-efficiency&quot;&gt;How does GridQuant improve 2D block quantization efficiency?&lt;/h2&gt;

&lt;p&gt;The GridQuant kernel has several improvements over the initial baseline quantization implementation which was a standard tile based implementation.  The GridQuant kernel has two full passes through the entire input tensor and works as follows:&lt;/p&gt;

&lt;h2 id=&quot;phase-1---determine-the-max-abs-value-for-each-256x256-sub-block-from-the-incoming-high-precision-tensor&quot;&gt;Phase 1 - Determine the max abs value for each 256x256 sub block from the incoming high precision tensor.&lt;/h2&gt;

&lt;p&gt;1 - We divide the BF16 tensor into 256 x 256 sub blocks.  This quantization size is configurable, but 256x256 is the default as it provides a blend of quantization precision and processing efficiency.&lt;/p&gt;

&lt;p&gt;2 - Each 256x256 sub-block is subdivided into 64 sub-blocks arranged in an 8x8 pattern, with each sub-block processing a 32x32 element block. A single warp (32 threads) handles the computation for all elements within its assigned 32x32 block.&lt;/p&gt;

&lt;p&gt;3 - We declare a 32x32 max_vals array in shared memory.  This will store the current max val for each position i,j as the 2d vector block moves across the entire 256x256 sub_block.&lt;/p&gt;

&lt;p&gt;This is an important improvement because it means we can do vectorized, rather than scalar, updates to the max vals scoring system and allows for much more efficient updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg2.png&quot; alt=&quot;Figure 2: The Fractionalized layout of an incoming tensor - a grid of 256x256 is created across the tensor, and within each 256x256 block, it is further refined into 32x32 sub blocks. A 32x32 max_vals is created for each 256x256 block.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; The Fractionalized layout of an incoming tensor - a grid of 256x256 is created across the tensor, and within each 256x256 block, it is further refined into 32x32 sub blocks. A 32x32 max_vals is created for each 256x256 block.&lt;/p&gt;

&lt;p&gt;4 - Each warp processes a 32x32 chunk and because we are using 4 warps, we ensure the Triton compiler can pipeline the memory loads for the next 32x32 chunk with the actual processing of absmax calculations for the current chunk.  This ensures that the warp scheduler is able to toggle warps loading data with those processing and keep the SM continuously busy.&lt;/p&gt;

&lt;p&gt;5 - The 32x32 2D vector block processing is moved across and through the entire 256x256 subblock in a grid stride looping fashion, with each warp updating the shared memory 32x32 max_vals against its current 32x32 sub-block. Thus max_vals[i,j] holds the latest max value as each sub block is processed.&lt;/p&gt;

&lt;p&gt;After completing the 256x256 block grid stride loop, the maxvals matrix is then itself reduced to find the absolute single max value for that entire 256 block.&lt;/p&gt;

&lt;p&gt;This gives us our final scaling factor value for this 2D 256 x 256 block.&lt;/p&gt;

&lt;h2 id=&quot;phase-2---quantize-the-256x256-block-values-to-float8--by-using-the-single-max-value-scaling-factor-found-during-phase-1&quot;&gt;Phase 2 - Quantize the 256x256 block values to Float8,  by using the single max value scaling factor found during Phase 1.&lt;/h2&gt;

&lt;p&gt;Next, we make a second pass through the entire 256x256 block to rescale all the numbers using this max value found in phase 1 to convert them to the float 8 format.&lt;/p&gt;

&lt;p&gt;Because we know we need to do 2 complete passes, for the loads during the phase 1 portion we instruct the triton compiler to keep these values in cache at higher priority (evict policy = last).&lt;/p&gt;

&lt;p&gt;This means that during the second pass, we can get a high hit rate from the L2 cache which provides much faster memory access than going all the way to HBM.&lt;/p&gt;

&lt;p&gt;With the 2D block quantization processing complete when all 256 x256 blocks are processed, we can return the new Float8 quantized tensor along with it’s scaling factor matrix, which we’ll use in the next phase of the GEMM processing.   This input quantization is repeated for the second input tensor as well, meaning we end up with A_Float 8, A_scaling_matrix, and B_Float8 and B_scaling matrix.&lt;/p&gt;

&lt;h2 id=&quot;gridquant---gemm-kernel&quot;&gt;GridQuant - GEMM Kernel&lt;/h2&gt;

&lt;p&gt;The GridQuant-GEMM kernel takes in the four outputs from the quantization above for processing. Our high-performance GEMM kernel features several new Triton developments to achieve SOTA performance for matrix shape profiles relevant in LLM inference during the decoding phase.&lt;/p&gt;

&lt;p&gt;These new features are commonly found in Hopper optimized kernels like &lt;a href=&quot;https://arxiv.org/abs/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt; and &lt;a href=&quot;https://neuralmagic.com/blog/introducing-machete-a-mixed-input-gemm-kernel-optimized-for-nvidia-hopper-gpus/&quot;&gt;Machete&lt;/a&gt;, built using &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;CUTLASS 3.x&lt;/a&gt;. Here, we discuss these methods and showcase the performance benefits that can be achieved leveraging them in Triton.&lt;/p&gt;

&lt;h2 id=&quot;tensor-memory-accelerator-tma&quot;&gt;Tensor Memory Accelerator (TMA)&lt;/h2&gt;

&lt;p&gt;The TMA unit on NVIDIA Hopper GPUs, is a dedicated hardware unit for load/store operations that act on multidimensional tensors commonly found in AI workloads. This has several important benefits.&lt;/p&gt;

&lt;p&gt;Transferring data from global and shared memory can occur without involving other resources on GPU SMs, freeing up registers and CUDA Cores. Further, when used in warp-specialized kernels, light-weight TMA operations can be assigned to a producer warp allowing for a high degree of overlap of memory transfers and computation.&lt;/p&gt;

&lt;p&gt;For more details on how TMA is used in Triton see our &lt;a href=&quot;https://pytorch.org/blog/hopper-tma-unit/&quot;&gt;previous blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;warp-specialization-cooperative-persistent-kernel-design&quot;&gt;Warp-Specialization (Cooperative Persistent Kernel Design)&lt;/h2&gt;

&lt;p&gt;Warp Specialization is a technique to leverage pipeline parallelism on GPUs. This experimental feature enables the expression of specialized threads through a &lt;a href=&quot;https://github.com/facebookexperimental/triton/tree/ws&quot;&gt;tl.async_task API&lt;/a&gt;, allowing the user to specify how operations in a Triton program should be “split” amongst warps. The cooperative Triton kernel performs different types of computation and loads that each take place on their own dedicated hardware. Having dedicated hardware for each of these specialized tasks makes it possible to realize parallelism efficiently for operations that have no data dependency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg3.png&quot; alt=&quot;Figure 3. Logical view of dedicated HW units in NVIDIA H100 SM&quot; style=&quot;width:100%; max-width:400px; display: block; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Logical view of dedicated HW units in NVIDIA H100 SM&lt;/p&gt;

&lt;p&gt;The operations in our kernel that create the pipeline are:&lt;/p&gt;

&lt;p&gt;A - Load per-block scale from GMEM into SMEM (cp.async engine)&lt;/p&gt;

&lt;p&gt;B - Load activation (A) and Weight (B) tiles from GMEM into SMEM (TMA)&lt;/p&gt;

&lt;p&gt;C - Matrix-Multiplication of A tile and B tile = C tile  (Tensor Core)&lt;/p&gt;

&lt;p&gt;D - Scale C tile with per-block scale from A and per-block scale from B (CUDA core)&lt;/p&gt;

&lt;p&gt;These steps can be assigned to “tasks” which are carried out by specialized warp groups in a threadblock. The cooperative strategy has three warp groups. A producer warp group that is responsible for feeding the compute units and 2 consumer warp groups that perform the computation. The two consumer warp groups each work on half of the same output tile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg4.png&quot; alt=&quot;Figure 4. Warp-Specialized Persistent Cooperative kernel&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Warp-Specialized Persistent Cooperative kernel (source: &lt;a href=&quot;https://drive.google.com/file/d/18sthk6IUOKbdtFphpm_jZNXoJenbWR8m/view&quot;&gt;NVIDIA&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This is different from the ping-pong schedule we discussed in our &lt;a href=&quot;https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/&quot;&gt;previous blog&lt;/a&gt;, where each consumer warp group works on &lt;em&gt;different&lt;/em&gt; output tiles. We note that the Tensor Core ops are not overlapped with the epilogue computation. Decreased utilization of the Tensor Core pipeline during the epilogue phase of the computation will reduce register pressure for the consumer warp group compared to ping-pong which always keeps the Tensor Core busy, thus allowing for larger tile sizes.&lt;/p&gt;

&lt;p&gt;Lastly, our kernel is designed to be persistent when the grid size exceeds the number of available compute units on H100 GPUs (132). Persistent kernels remain active on the GPU for an extended period and compute multiple output tiles during its lifetime. Our kernel leverages TMA async shared to global memory stores, while continuing to do work on the next output tile as opposed to incurring the cost of scheduling multiple threadblocks.&lt;/p&gt;

&lt;h2 id=&quot;microbenchmarks&quot;&gt;Microbenchmarks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg5.png&quot; alt=&quot;Figure 5: Latency comparison (us) of Gridquant-GEMM vs our best performing SplitK kernel for small batch regime and Llama3 8192 N,K sizing.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Latency comparison (us) of Gridquant-GEMM vs our best performing SplitK kernel for small batch regime and Llama3 8192 N,K sizing. &lt;strong&gt;&lt;em&gt;(lower-is-better)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Warp-Specialized Triton kernel achieves SOTA performance at the above small-M and square matrix shapes, achieving a nearly &lt;strong&gt;1.2x&lt;/strong&gt; speedup over the SplitK Triton kernel, which was the previous best performing strategy for Triton GEMMs in this low arithmetic intensity regime. For future work, we plan to tune our kernel performance for the medium-to-large M regime and non-square matrices.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;Future work includes benchmarking gridquant on end to end workflows. In addition, we plan to run more extensive benchmarks on non-square (rectangular) matrices as well as medium-to-large M sizes. Finally, we plan to explore ping-pong style warp-specialization in Triton versus the current cooperative implementation.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta: Less Wright, IBM: Adnan Hoque</name>
        
        
      </author>

      

      

      
        <summary type="html">2D block quantization for Float8 (FP8) holds the promise of improving the accuracy of Float8 quantization while also accelerating GEMM’s for both inference and training. In this blog, we showcase advances using Triton for the two main phases involved in doing block quantized Float8 GEMMs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">HadaCore: Tensor Core Accelerated Hadamard Transform Kernel</title>
      <link href="https://pytorch.org/blog/hadacore/" rel="alternate" type="text/html" title="HadaCore: Tensor Core Accelerated Hadamard Transform Kernel" />
      <published>2024-12-02T00:00:00-08:00</published>
      <updated>2024-12-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/hadacore</id>
      <content type="html" xml:base="https://pytorch.org/blog/hadacore/">&lt;p&gt;&lt;strong&gt;IBM&lt;/strong&gt;: Krish Agarwal, Rishi Astra, Adnan Hoque, Mudhakar Srivatsa, Raghu Ganti&lt;br /&gt;
&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Sijia Chen&lt;/p&gt;

&lt;p&gt;Quantization is a method for improving model inference speeds by compressing model weights and performing (faster) computation in lower precision data types. However, quantization can result in accuracy loss due to the presence of outliers. Recent works like &lt;a href=&quot;https://arxiv.org/abs/2404.00456&quot;&gt;QuaRot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2405.16406&quot;&gt;SpinQuant&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt; introduce methods to increase the numerical accuracy of INT4, INT8 and FP8 quantization in LLMs. These methods rely on &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadamard_transform&quot;&gt;Hadamard Transforms&lt;/a&gt;. In this blog, we present HadaCore, a Hadamard Transform CUDA kernel that achieves state-of-the-art performance on NVIDIA A100 and H100 GPUs. Our kernel achieves speedups of &lt;strong&gt;1.1–1.4x&lt;/strong&gt; and &lt;strong&gt;1.0–1.3x&lt;/strong&gt;, with a peak gain of &lt;strong&gt;3.5x&lt;/strong&gt; and &lt;strong&gt;3.6x&lt;/strong&gt; respectively, over Dao AI Lab’s &lt;a href=&quot;https://github.com/Dao-AILab/fast-hadamard-transform&quot;&gt;Fast Hadamard Transform Kernel&lt;/a&gt;. We leverage a hardware-aware work decomposition that benefits from Tensor Core acceleration while maintaining quantization error reduction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg1.png&quot; alt=&quot;Figure 1: Speedup of HadaCore vs Dao AI Hadamard CUDA kernel. A peak gain of 3.46x on the A100 is achieved using 128 rotation by 8.4M elements.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Speedup of HadaCore vs Dao AI Hadamard CUDA kernel. A peak gain of 3.46x on the A100 is achieved using 128 rotation by 8.4M elements.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/cuda/inference/hadamard_transform&quot;&gt;HadaCore Kernel is publicly available&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.00456&quot;&gt;QuaRot&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2405.16406&quot;&gt;SpinQuant&lt;/a&gt; both propose methods to increase the numerical accuracy of INT4 and INT8 quantization in LLMs. Both methods rotate model activations since rotations are statistically likely to reduce the magnitude of outliers, as it “distributes” extreme values among other (less extreme) dimensions, and rotation is also an easily invertible operation using the inverse of the rotation matrix. These methods can also improve FP8 inference accuracy, such as in &lt;a href=&quot;https://arxiv.org/pdf/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg2.png&quot; alt=&quot;Figure 2. Transformer block showing online (red) and offline rotations (blue) in QuaRot&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. Transformer block showing online (red) and offline rotations (blue) in QuaRot&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Applying these rotation matrices introduces model runtime overhead due to the online operations shown in Figure 2. These rotations can be applied through matrix multiplication, but the added overhead would diminish the benefits from quantization. Therefore, QuaRot and SpinQuant opt to use Walsh-Hadamard matrices, a special type of rotation matrix that can be applied faster than matrix multiplication using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fast_Walsh%E2%80%93Hadamard_transform&quot;&gt;Fast Walsh-Hadamard Transform&lt;/a&gt; algorithm. HadaCore is an optimized implementation of this algorithm for NVIDIA GPUs that support Tensor Cores.&lt;/p&gt;

&lt;h2 id=&quot;tensor-core-accelerated-hadamard-transform&quot;&gt;Tensor Core Accelerated Hadamard Transform&lt;/h2&gt;

&lt;p&gt;HadaCore leverages &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;NVIDIA Tensor Cores&lt;/a&gt;, which are specialized compute units on NVIDIA GPUs optimized for matrix multiplication. To achieve this, our kernel performs a hardware-aware work decomposition of the Fast Walsh-Hadamard algorithm. This work decomposition ensures that we can utilize the &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma#multiply-and-accumulate-instruction-mma&quot;&gt;MMA PTX instructions&lt;/a&gt; that execute on the Tensor Core chip. HadaCore applies a 16×16 Hadamard transform to chunks of the input data. The computation can then be offloaded to the FP16 Tensor Core with usage of the &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma#matrix-fragments-for-mma-m16n8k16-with-floating-point-type&quot;&gt;mma.m16n8k16&lt;/a&gt; instruction. The warp-level parallelism for HadaCore is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg3.png&quot; alt=&quot;Figure 3: HadaCore Parallelization, 1x256 vectors (rows) being rotated by a size 256 Hadamard.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: HadaCore Parallelization, 1x256 vectors (rows) being rotated by a size 256 Hadamard.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We process fragments of 256 elements in parallel using warp-level Tensor Core operations to achieve up to a 256-size Hadamard transform. For further sizes, we shuffle data between warps and repeat.&lt;/p&gt;

&lt;h2 id=&quot;microbenchmarks&quot;&gt;Microbenchmarks&lt;/h2&gt;

&lt;p&gt;We benchmark HadaCore against the&lt;a href=&quot;https://github.com/Dao-AILab&quot;&gt; Dao AI Lab Hadamard Kernel&lt;/a&gt; on both NVIDIA H100 and A100 GPUs across varying Hadamard and input tensor sizes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg4.png&quot; alt=&quot;Figure 4:  HadaCore Kernel Speedup on NVIDIA A100 over Dao AI Lab Fast Hadamard Kernel&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4:  HadaCore Kernel Speedup on NVIDIA A100 over Dao AI Lab Fast Hadamard Kernel&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg5.png&quot; alt=&quot;Color coded Speedup Table for NVIDIA A100, Green = Speedup over Baseline&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Color coded Speedup Table for NVIDIA A100, Green = Speedup over Baseline&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg6.png&quot; alt=&quot;Figure 5:  HadaCore Kernel Speedup on NVIDIA H100 over Dao AI Lab Fast Hadamard Kernel&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5:  HadaCore Kernel Speedup on NVIDIA H100 over Dao AI Lab Fast Hadamard Kernel&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg7.png&quot; alt=&quot;Color coded Speedup Table for NVIDIA H100, Green = Speedup over Baseline&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Color coded Speedup Table for NVIDIA H100, Green = Speedup over Baseline&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We showcase our speedup as the input tensor size (labeled element count) in our charts increase. Element count is the number of elements in the target matrix we are rotating. For example, in multi-head attention:&lt;/p&gt;

&lt;p&gt;The queries (Q), keys (K) and values (V) tensors are 4D tensors of size:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(batch_size, seq_len, n_heads, head_dim)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A Hadamard matrix of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head_dim&lt;/code&gt; is applied to these activation tensors, so we refer to this as using a Hadamard size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head_dim&lt;/code&gt; with an element count of:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size*seq_len*n_heads*head_dim.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Common element counts for query rotations in an attention block:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model \ Tokens&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prefill&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Decoding&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Llama-2 70b&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;33,554,432 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;

(1 batch * 64 heads * 4096 tokens * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
   &lt;td&gt;8192 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 64 heads * 1 token * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Llama-3 8b&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;33,554,432 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 32 heads * 8192 tokens * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
   &lt;td&gt;4,096 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 32 heads * 1 token * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;HadaCore achieves &lt;strong&gt;1.1–1.4x&lt;/strong&gt; speedup on A100 and &lt;strong&gt;1.0–1.3x&lt;/strong&gt; speedup on H100 over Dao AI Lab’s Fast Hadamard kernel, with a peak gain of &lt;strong&gt;3.5x and 3.6x&lt;/strong&gt;, respectively. For smaller sizes on H100, HadaCore’s gain decreases. For future work, we plan to incorporate usage of Hopper specific features like TMA and WGMMA for improved H100 performance.&lt;/p&gt;

&lt;h2 id=&quot;mmlu-benchmarks&quot;&gt;MMLU Benchmarks&lt;/h2&gt;

&lt;p&gt;We evaluated MMLU scores on a &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-3.1-8B&quot;&gt;Llama 3.1-8B&lt;/a&gt; inference workload where the FlashAttention computation was performed in FP8. Newer generation &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/&quot;&gt;NVIDIA Hopper GPUs &lt;/a&gt;come equipped with FP8 Tensor Cores that deliver substantial compute gain over FP16.&lt;/p&gt;

&lt;p&gt;Our results show the benefit of using HadaCore for accuracy preservation when combined with optimizations such as FP8 FlashAttention.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Format&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama3.1-8B&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;Avg. 5-Shot MMLU Accuracy&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP16&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP16&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;65.38
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP16&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;No Hadamard
   &lt;/td&gt;
   &lt;td&gt;64.40
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP8&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;HadaCore
   &lt;/td&gt;
   &lt;td&gt;65.09
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP8&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;Dao AI Fast Hadamard Kernel
   &lt;/td&gt;
   &lt;td&gt;65.45
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: MMLU scores for Llama3.1 8B with FP16 baseline and FP8 attention using Hadamard transforms, comparing an implementation with explicit Hadamard matrix multiplications vs. HadaCore (&lt;strong&gt;higher is better&lt;/strong&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From the above MMLU scores, we note that for Llama3.1-8B inference with FP8 attention, HadaCore improves the quantization error introduced from computing attention in a lower precision.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We showcased our speedups achieved by moving the Fast-Walsh Hadamard algorithm into a CUDA kernel that leverages Tensor Core acceleration and achieves a peak speedup of &lt;strong&gt;3.5x&lt;/strong&gt; and &lt;strong&gt;3.6x&lt;/strong&gt; over the Dao AI Fast-Hadamard kernel on NVIDIA A100 and H100, respectively.&lt;/p&gt;

&lt;p&gt;Further, we showed on the MMLU benchmark that rotating with HadaCore maintains similar quantization error reduction to the Fast-Hadamard kernel, while providing computational acceleration.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;We plan to implement a Triton version of our kernel and experiment with more advanced techniques such as kernel fusion to support fused Hadamard transform and quantization. Further, we plan to extend our kernel to support BF16 Tensor Core compute.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>IBM and Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">Quantization is a method for improving model inference speeds by compressing model weights and performing (faster) computation in lower precision data types. However, quantization can result in accuracy loss due to the presence of outliers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Supercharging Training using float8 and FSDP2</title>
      <link href="https://pytorch.org/blog/training-using-float8-fsdp2/" rel="alternate" type="text/html" title="Supercharging Training using float8 and FSDP2" />
      <published>2024-11-25T00:00:00-08:00</published>
      <updated>2024-11-25T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/training-using-float8-fsdp2</id>
      <content type="html" xml:base="https://pytorch.org/blog/training-using-float8-fsdp2/">&lt;p&gt;&lt;strong&gt;IBM&lt;/strong&gt;: Tuan Hoang Trong, Alexei Karve, Yan Koyfman, Linsong Chu, Divya Kumari, Shweta Salaria, Robert Walkup, Praneet Adusumilli, Nirmit Desai, Raghu Ganti, Seetharami Seelam&lt;br /&gt;
&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Wei Feng, Vasiliy Kuznetsov, Driss Guesseous&lt;/p&gt;

&lt;p&gt;In this blog, we will demonstrate how we achieve up to &lt;strong&gt;50% throughput speedup&lt;/strong&gt; while achieving loss and evaluation benchmark parity in training over &lt;a href=&quot;https://pytorch.org/blog/maximizing-training-throughput/&quot;&gt;FSDP1 bf16 training&lt;/a&gt;. We achieve this speedup by leveraging FSDP2, DTensor, and torch.compile with torchao’s float8 via linear layer updates (compute), and float8 all_gathers for weight communication. We showcase these improvements across a spectrum of Meta LLaMa model architecture sizes, ranging from small 1.8B model size all the way to 405B model size, making training faster than ever.&lt;/p&gt;

&lt;p&gt;We demonstrate these improvements using the Meta Llama3 architecture, and then perform model quality studies at two scales: 100B tokens at 8B model size, and 50B tokens at 70B model size, which provide an exact comparison of float8 and bf16 training loss curves. We demonstrate that the loss curves result in identical loss convergence across these model training runs compared to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; counterpart. Further, we train a 3B model to 1T tokens using the FineWeb-edu dataset and run standard evaluation benchmarks to ensure that the model quality is intact and comparable to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; run.&lt;/p&gt;

&lt;p&gt;At IBM Research, we plan to adopt these capabilities for our data ablations to improve the number of experiments we can perform in a given GPU budget. Longer term, we will follow up with a larger scale model run to demonstrate the end-to-end feasibility of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; training.&lt;/p&gt;

&lt;h2 id=&quot;what-is-float8&quot;&gt;What is Float8?&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; format for training models was introduced by NVIDIA, ARM, and Intel in a &lt;a href=&quot;https://arxiv.org/abs/2209.05433&quot;&gt;2022 paper&lt;/a&gt; which demonstrated the feasibility of training using lower precision float8, without sacrificing model quality. With the introduction of newer GPUs like the NVIDIA Hopper series, FP8 training became feasible with the potential of more than 2x improvement in training throughput due to native float8 tensor core support. There are a few challenges to realize this promise:  &lt;br /&gt;
(i) Enable the core model operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attention&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;,  &lt;br /&gt;
(ii) Enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; training in a distributed framework, and  &lt;br /&gt;
(iii) Enable weight communication between GPUs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;.  &lt;br /&gt;
While the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; was enabled by NVIDIA libraries, the latter two were provided in recent updates to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSDP2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchao&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this blog, we are using &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;torchtitan&lt;/a&gt; as the entry point for training, IBM’s deterministic data loader, the &lt;code&gt;float8&lt;/code&gt; linear layer implementation from &lt;a href=&quot;https://www.google.com/url?q=https://github.com/pytorch/ao/tree/main/torchao/float8&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1730743084184771&amp;amp;usg=AOvVaw21FdkNG452P-nDIO-hIwcW&quot;&gt;torchao&lt;/a&gt;, and the &lt;code&gt;float8 all gather&lt;/code&gt; from the latest PyTorch nightlies in conjunction with FSDP2. For this training, we are using the float8  per tensor (tensorwise) scaling granularity rather than rowwise. We leverage &lt;code&gt;torch.compile&lt;/code&gt; to ensure that we get maximum performance gains. We are computing &lt;code&gt;attention&lt;/code&gt; in &lt;code&gt;bf16&lt;/code&gt; using SDPA and are currently working on moving this to float8 as well.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;We perform various experiments to demonstrate the benefits of float8 training. The first is to ensure that model quality is not sacrificed. To verify this, we train an 8B model and 70B model for a few thousand steps and compare the loss curves between both the float8 and bf16 training run. Our experiments are performed on three different H100 clusters with 128, 256, and 512 H100 GPU configurations in very different environments to demonstrate reproducibility. The first cluster is customized on &lt;a href=&quot;https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/&quot;&gt;Grand Teton&lt;/a&gt; in Meta with 400Gbps custom interconnect, the second is an IBM research cluster with 3.2Tbps Infiniband interconnect, and the third is an IBM Cloud cluster with 3.2Tbps RoCE interconnect for GPU-to-GPU communication.&lt;/p&gt;

&lt;p&gt;First, we plot the loss curve comparisons for both these models in the below figures to demonstrate loss parity for a few thousand steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-using-float8-fsdp2/fg1.png&quot; alt=&quot;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-using-float8-fsdp2/fg2.png&quot; alt=&quot;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe that across these different models and in different environments, we obtain loss parity for the small scale of tokens. Next, we characterize the throughput gains for four different model sizes ranging from 1.8B to 405B. We explored the best batch size and activation checkpointing schemes for both the float8 and bf16 training runs to determine the tokens/sec/GPU (wps) metric and report the performance gain. For the 405B model, we leveraged &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTensor&lt;/code&gt; for tensor parallel training with FSDP2. We use a sequence length of 8K for all our measurements.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (bf16) &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percent gain&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.8B
   &lt;/td&gt;
   &lt;td&gt;29K
   &lt;/td&gt;
   &lt;td&gt;35K
   &lt;/td&gt;
   &lt;td&gt;18%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;8B
   &lt;/td&gt;
   &lt;td&gt;8K
   &lt;/td&gt;
   &lt;td&gt;10K
   &lt;/td&gt;
   &lt;td&gt;28%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;956
   &lt;/td&gt;
   &lt;td&gt;1430
   &lt;/td&gt;
   &lt;td&gt;50%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;405B (TP4)
   &lt;/td&gt;
   &lt;td&gt;149
   &lt;/td&gt;
   &lt;td&gt;227
   &lt;/td&gt;
   &lt;td&gt;52%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: Performance gains over bf16 (both bf16 and float8 use torch.compile)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe from Table 1 that the gains for larger models (70B and 405B) reach up to 50%, the smaller models see gains between roughly 20 and 30%. In further experiments, we observed that the addition of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; enables a boost of ~5% beyond the compute itself in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;, which is inline with the observations in this &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/efficient-pre-training-of-llama-3-like-model-architectures-using-torchtitan-on-amazon-sagemaker/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Second, to demonstrate the effectiveness of an FP8 model, we trained a 3B model following the Llama3 architecture for 1T tokens using the FineWeb-edu dataset from Hugging Face. We performed evaluations using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lm-eval-harness&lt;/code&gt; framework and present a small portion of these results in the below table. We observe that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; performance is marginally better than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; scores (about one percent). While some scores are significantly better with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; (e.g., MMLU is 3 pts higher), we expect these gaps to vanish when the right hyper parameters are chosen and across larger scale training runs (e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; run had half the batch size and it is well known that smaller batch size runs can improve evaluation scores).&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Score (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Score (bf16)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (5-shot)
   &lt;/td&gt;
   &lt;td&gt;0.26
   &lt;/td&gt;
   &lt;td&gt;0.29
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ARC-e
   &lt;/td&gt;
   &lt;td&gt;0.73
   &lt;/td&gt;
   &lt;td&gt;0.73
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ARC-c
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
   &lt;td&gt;0.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hellaswag
   &lt;/td&gt;
   &lt;td&gt;0.65
   &lt;/td&gt;
   &lt;td&gt;0.67
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;sciq
   &lt;/td&gt;
   &lt;td&gt;0.89
   &lt;/td&gt;
   &lt;td&gt;0.88
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;OpenBook QA
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;PIQA
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Winogrande
   &lt;/td&gt;
   &lt;td&gt;0.60
   &lt;/td&gt;
   &lt;td&gt;0.65
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.59&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 2: Benchmark scores for float8 trained model running in FP16 for eval (at 1T tokens of FineWeb pre-training).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally, we scale our experiments to 512 H100 GPUs on the IBM Cloud cluster. We were able to recreate the results and speedups that we observed even at 512 GPU scale. We summarize these results only for the large models in the below table (70B and 405B).&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (bf16) &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percent gain&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;960
   &lt;/td&gt;
   &lt;td&gt;1448
   &lt;/td&gt;
   &lt;td&gt;51%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;405B (TP4)
   &lt;/td&gt;
   &lt;td&gt;152
   &lt;/td&gt;
   &lt;td&gt;217
   &lt;/td&gt;
   &lt;td&gt;43%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 3: Performance gains over bf16 (both bf16 and float8 use torch.compile) for 512 GPU scale&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;We are also working on evaluating other forms of parallelism such as Context Parallelism. We plan to evaluate all of these features to demonstrate the composability and ability to make choices for training large scale models.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank Davis Wertheimer from IBM Research for enabling the data loader for torchtitan runs enabling us to replay data in the same order across multiple runs. We also thank IBM Cloud for enabling us with early test access to the H100 cluster.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>IBM and Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we will demonstrate how we achieve up to 50% throughput speedup while achieving loss and evaluation benchmark parity in training over FSDP1 bf16 training</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Rebellions Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/rebellions/" rel="alternate" type="text/html" title="Rebellions Joins the PyTorch Foundation as a General Member" />
      <published>2024-11-21T00:00:00-08:00</published>
      <updated>2024-11-21T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/rebellions</id>
      <content type="html" xml:base="https://pytorch.org/blog/rebellions/">&lt;p&gt;&lt;img src=&quot;/assets/images/rebellions-logo.svg&quot; alt=&quot;Rebellions logo&quot; style=&quot;max-width:350px;width:100%;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Rebellions has joined as a general member.&lt;/p&gt;

&lt;p&gt;Rebellions is a South Korea-based semiconductor company specializing in the design and development of AI chips for data centers and edge devices. Their innovative hardware and software solutions aim to accelerate generative AI and machine learning workloads, focusing on high energy efficiency and performance. The company successfully launched and deployed its AI chip ‘ATOM’ targeting data centers in 2023 and is developing its next-generation AI accelerator ‘REBEL’.&lt;/p&gt;

&lt;p&gt;“We’re thrilled to welcome Rebellions as a new general member of the PyTorch Foundation,” said Matt White, Executive Director of the PyTorch Foundation. “Rebellions brings a unique perspective to the PyTorch ecosystem with their focus on advancing the integration of NPU architectures for AI acceleration with PyTorch. Their expertise will play a vital role in ensuring PyTorch continues to evolve as a versatile framework, accommodating the diverse needs of modern AI workloads. We look forward to collaborating with Rebellions to drive innovation and strengthen the PyTorch ecosystem for developers worldwide.”&lt;/p&gt;

&lt;p&gt;Rebellions has introduced native support for PyTorch 2.0 in their RBLN SDK. This integration includes compatibility with torch.compile, a pivotal feature of PyTorch 2.0 that enhances model performance. Through this development, Rebellions has empowered developers to seamlessly harness the full potential of their AI accelerator lineup within the environment.&lt;/p&gt;

&lt;p&gt;Rebellions is also deeply committed to advancing the PyTorch ecosystem through collaborative innovation starting in Korea. The company has established a Special Interest Group (SIG) focusing on Pytorch Core within the PyTorch Korea community and is actively working with volunteers recruited through MODULABS, an open research institute, to integrate native support for the deep learning framework into their Neural Processing Unit (NPU).&lt;/p&gt;

&lt;p&gt;In addition, Rebellions is collaborating with academic institutions, such as Yonsei University, Hanyang University, University of Science &amp;amp; Technology (UST)  and national agencies, such as the Electronics and Telecommunications Research Institute (ETRI), to offer undergraduate and graduate courses on PyTorch and enable them to leverage Pytorch as their research platform.&lt;/p&gt;

&lt;p&gt;These initiatives highlight Rebellions’ dedication to optimizing the PyTorch experience for developers and researchers alike, while also fostering education and innovation in the field.&lt;/p&gt;

&lt;p&gt;“By integrating our hardware innovations with PyTorch, we’re building Native NPU support to accelerate diverse AI workloads.” said Hong-seok Kim, the Chief Software Architect at Rebellions. “We’re excited to contribute to the PyTorch community by community-driven initiatives and partnerships, advancing NPU architecture support for next-generation AI solutions. Together with the PyTorch community, we aim to pioneer new possibilities in AI acceleration and empower developers worldwide with efficient computing solutions.”&lt;/p&gt;

&lt;p&gt;To learn more about how your organization can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-rebellions&quot;&gt;About Rebellions&lt;/h2&gt;

&lt;p&gt;Rebellions is a South Korea-based semiconductor company specializing in the design and development of AI chips for data centers and edge devices. Their innovative hardware and software solutions aim to accelerate generative AI and machine learning workloads, focusing on high energy efficiency and performance. The company successfully launched and deployed its AI chip ‘ATOM’ targeting data centers in 2023 and is developing its next-generation AI accelerator ‘REBEL’ incorporating a scalable chiplet architecture and high-bandwidth memory.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Rebellions has joined as a general member.</summary>
      

      
      
    </entry>
  
</feed>



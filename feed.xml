<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-08-28T07:39:32-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Enabling Fast Gradient Clipping and Ghost Clipping in Opacus</title>
      <link href="https://pytorch.org/blog/clipping-in-opacus/" rel="alternate" type="text/html" title="Enabling Fast Gradient Clipping and Ghost Clipping in Opacus" />
      <published>2024-08-20T00:00:00-07:00</published>
      <updated>2024-08-20T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/clipping-in-opacus</id>
      <content type="html" xml:base="https://pytorch.org/blog/clipping-in-opacus/">&lt;h2 id=&quot;introduction-and-context&quot;&gt;Introduction and Context&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;&gt;Differentially Private Stochastic Gradient Descent (DP-SGD)&lt;/a&gt; is the canonical method for training machine learning models with differential privacy. It involves the following two modifications to its non-private counterpart, Stochastic Gradient Descent.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Per-sample gradient clipping&lt;/strong&gt;: Clip gradients with respect to every sample in the mini-batch, ensuring that its norm is at most a pre-specified value, “Clipping Norm”, C, in every iteration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Noise addition&lt;/strong&gt;: Add Gaussian noise of pre-specified variance, depending on the clipping norm and privacy parameters, to the average clipped gradient, in every iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first change, &lt;strong&gt;per-sample gradient clipping&lt;/strong&gt;, introduces additional complexities since, in general, it requires instantiating &lt;strong&gt;per-sample&lt;/strong&gt; &lt;strong&gt;gradients&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://opacus.ai&quot;&gt;Opacus&lt;/a&gt; is a PyTorch implementation of DP-SGD. Opacus addresses the above task by employing &lt;a href=&quot;https://medium.com/pytorch/differential-privacy-series-part-2-efficient-per-sample-gradient-computation-in-opacus-5bf4031d9e22&quot;&gt;hook functions&lt;/a&gt;, which allows intervening on specific events, such as forward and backward passes. For more details about Opacus, we encourage readers to review the previous blog posts: &lt;a href=&quot;https://bit.ly/dp-sgd-algorithm-explained&quot;&gt;DP-SGD Algorithm Explained&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch/differential-privacy-series-part-2-efficient-per-sample-gradient-computation-in-opacus-5bf4031d9e22&quot;&gt;Efficient Per-Sample Gradient Computation in Opacus&lt;/a&gt; and &lt;a href=&quot;https://pytorch.medium.com/differential-privacy-series-part-3-efficient-per-sample-gradient-computation-for-more-layers-in-39bd25df237&quot;&gt;Efficient Per-Sample Gradient Computation for More Layers in Opacus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While Opacus provides substantial efficiency gains compared to the naive approaches, the memory cost of instantiating per-sample gradients is significant. In particular, memory usage is proportional to the batch size times the number of trainable parameters. Consequently, memory limits Opacus to small batch sizes and/or small models, significantly restricting its range of applications.&lt;/p&gt;

&lt;p&gt;We introduce &lt;a href=&quot;https://arxiv.org/abs/2009.03106&quot;&gt;Fast Gradient Clipping&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;Ghost Clipping&lt;/a&gt; to Opacus, which enable developers and researchers to perform gradient clipping without instantiating the per-sample gradients. As an example, this allows for fine-tuning 7M parameters of BERT, on a single 16GB GPU, with a batch size of 1024, with memory comparable to using PyTorch (without applying DP-SGD). In contrast, the previous version of Opacus, supported a maximum batch size of roughly 256 for the same setting. We provide a &lt;a href=&quot;https://github.com/pytorch/opacus/blob/main/tutorials/building\_text\_classifier.ipynb&quot;&gt;tutorial&lt;/a&gt; on how to use Fast Gradient Clipping in Opacus with the aforementioned task as an example.&lt;/p&gt;

&lt;h2 id=&quot;fast-gradient-clipping-and-ghost-clipping&quot;&gt;Fast Gradient Clipping and Ghost Clipping&lt;/h2&gt;

&lt;p&gt;The key idea behind these techniques is based on the following observation: suppose per-sample gradient norms are known, then gradient clipping can be achieved by backpropagation on a re-weighted loss function $ \bar{L} $. This loss function is defined as  $ \bar{L} = \sum_{i} R_{i} L_{i} $, where $ R_i = \min\left(\frac{C}{C_i}, 1\right) $ are the clipping coefficients computed from the per-sample gradient norms $ {C_i} $ and $ {L_i} $ are per-sample losses.&lt;/p&gt;

&lt;p&gt;The above idea may seem circular at first glance, as it appears to require instantiating per-sample gradients in order to calculate per-sample gradient norms. However, for certain widely-used components of neural network architectures, such as fully connected/linear layers, it is indeed possible to obtain per-sample gradient norms in a single backpropagation pass without the need for per-sample gradients. This suggests a workflow that involves two backpropagation passes: the first to compute per-sample gradient norms, and the second to compute the aggregated (not per-sample) clipped gradient. The second backpropagation is simply the standard batched backpropagation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clipping-in-opacus/fg1.jpg&quot; alt=&quot;backpropagation diagram&quot; style=&quot;max-width:800px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clipping-in-opacus/fg2.png&quot; alt=&quot;backpropagation diagram&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Comparison between vanilla &lt;strong&gt;Opacus&lt;/strong&gt; (top left), &lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt; (top right), and &lt;strong&gt;Ghost clipping&lt;/strong&gt; (bottom). We marked in red gradient instantiations that become memory bottlenecks. For vanilla Opacus, it has to instantiate the &lt;strong&gt;per-sample gradients&lt;/strong&gt;. &lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt; instantiates per-sample gradients for each layer to compute its norm, which is immediately released once the backward pass moves on to the next layer. Ghost Clipping works directly from &lt;strong&gt;per-sample activation gradients&lt;/strong&gt; and &lt;strong&gt;per-sample activations&lt;/strong&gt;, and avoids the need for gradient instantiation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.03106&quot;&gt;&lt;strong&gt;Fast Gradient Clipping&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
In Fast Gradient Clipping, the per-sample gradient norm is calculated in three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For each layer, the per-sample gradient is instantiated and its norm is calculated.&lt;/li&gt;
  &lt;li&gt;The per-sample gradient is then immediately discarded.&lt;/li&gt;
  &lt;li&gt;The (squared) per-sample gradient norms of each layer are summed up to obtain the overall (squared) per-sample gradient norm.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.05679&quot;&gt;&lt;strong&gt;Ghost Clipping&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;
Extending the approach of Fast Gradient Clipping, Ghost Clipping uses the &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;&gt;fact&lt;/a&gt; that for &lt;strong&gt;linear layers&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;/strong&gt; per-sample gradient norms can be calculated just from &lt;strong&gt;activation gradients&lt;/strong&gt; and  &lt;strong&gt;activations&lt;/strong&gt;. In particular, let &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backprops&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;activations&lt;/code&gt; be per-sample activation gradients and activations, of dimensions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size ✕ output_width&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size ✕ input_width&lt;/code&gt;, respectively. The per-sample gradient is the outer product of the two, which takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch_size ✕ input_width ✕ output_width)&lt;/code&gt; time and space.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;&gt;ghost clipping trick&lt;/a&gt; instead calculates the (squared) norm of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backprops&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;activations&lt;/code&gt;, sample-wise, and takes their product, which gives the (squared) norm of the gradient. This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch-size ✕ (input_width + output_width))&lt;/code&gt; time and takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(batch-size)&lt;/code&gt; space to store. Since &lt;strong&gt;per-sample activation&lt;/strong&gt; and &lt;strong&gt;per-sample activation gradients&lt;/strong&gt; are already stored, additional memory is needed only for storing the norms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship between Fast Gradient Clipping and Ghost Clipping&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fast Gradient Clipping and Ghost Clipping are complementary techniques. Fast Gradient Clipping can be applied to any type of layer, while Ghost Clipping is a strictly better technique for supported layers.&lt;/li&gt;
  &lt;li&gt;Our implementation automatically switches to Fast Gradient Clipping when the layer is not supported by Ghost Clipping.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;how-to-use-fast-gradient-clipping-in-opacus&quot;&gt;How to use Fast Gradient Clipping in Opacus&lt;/h3&gt;

&lt;p&gt;The training loop is identical to that of the standard PyTorch loop. As in Opacus before, we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PrivacyEngine()&lt;/code&gt;, which “sanitizes” the model and optimizer. To enable Ghost Clipping, the argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_sample_mode=&quot;ghost&quot;&lt;/code&gt; is used. Additionally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_private()&lt;/code&gt; takes the loss criterion as an extra input and sanitizes it. This allows us to hide the two backward passes and the loss rescaling in between in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;opacus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PrivacyEngine&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# example loss function
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;privacy_engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PrivacyEngine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;privacy_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_private&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;noise_multiplier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noise_multiplier&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_grad_norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_grad_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad_sample_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ghost&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The training loop below is identical to that of PyTorch
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_gc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer_gc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Add noise and update the model
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Internally, before the first pass, we enable the &lt;em&gt;hooks&lt;/em&gt;, which allows us to capture layer-wise values corresponding to forward and backward calls. They are used to compute the per-sample gradient norms. We then compute the clipping coefficients, rescale the loss function and disable hooks, which lets us use the standard PyTorch backward pass.&lt;/p&gt;

&lt;h3 id=&quot;memory-complexity-analysis&quot;&gt;Memory Complexity Analysis&lt;/h3&gt;

&lt;p&gt;Consider a multi-layer neural network with the following properties:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;L&lt;/strong&gt;: Number of layers&lt;br /&gt;
&lt;strong&gt;d&lt;/strong&gt;: Maximum layer width&lt;br /&gt;
&lt;strong&gt;B&lt;/strong&gt;: Batch size&lt;br /&gt;
&lt;strong&gt;K&lt;/strong&gt;: Number of non-supported/non-linear layers&lt;/p&gt;

&lt;p&gt;The memory overhead of DP-SGD with Ghost Clipping compared to plain (PyTorch) SGD is an additive O(BL), required to store the per-sample gradient norms for all layers. Further, if there is a non-supported layer (if K≥1), then there is an additional O(Bd&lt;sup&gt;2&lt;/sup&gt;) memory to instantiate the gradient of that layer.&lt;/p&gt;

&lt;h3 id=&quot;memory-benchmarking&quot;&gt;Memory Benchmarking&lt;/h3&gt;

&lt;p&gt;We provide results on the memory usage for a variety of settings.&lt;/p&gt;

&lt;h4 id=&quot;fine-tuning-bert&quot;&gt;Fine-Tuning BERT&lt;/h4&gt;

&lt;p&gt;We consider the problem of &lt;a href=&quot;https://github.com/pytorch/opacus/blob/main/tutorials/building\_text\_classifier.ipynb&quot;&gt;privately fine-tuning&lt;/a&gt; the last three layers of BERT for a text classification task. The base model has over 100M parameters, of which we fine-tune the last three layers, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertEncoder,&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertPooler,&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classifier&lt;/code&gt;, comprising roughly 7.6M parameters. The experiments are run on a P100 GPU with 16 GB of memory.&lt;/p&gt;

&lt;p&gt;The following table reports the maximum memory and time taken per iteration for the various methods:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt; 
   &lt;/td&gt;
   &lt;td colspan=&quot;9&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 32&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 128&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 512&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;&lt;strong&gt;B = 1024&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;B = 2048&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Mem&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;236 MB
   &lt;/td&gt;
   &lt;td&gt;0.15 s
   &lt;/td&gt;
   &lt;td&gt;1.04 GB
   &lt;/td&gt;
   &lt;td&gt;0.55 s
   &lt;/td&gt;
   &lt;td&gt;5.27 GB
   &lt;/td&gt;
   &lt;td&gt;2.1 s
   &lt;/td&gt;
   &lt;td&gt;12.7 GB
   &lt;/td&gt;
   &lt;td&gt;4.2 s
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;1,142 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;4.55 GB
   &lt;/td&gt;
   &lt;td&gt;0.68 s
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;FGC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;908 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;3.6 GB
   &lt;/td&gt;
   &lt;td&gt;0.75 s
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;362 MB
   &lt;/td&gt;
   &lt;td&gt;0.21 s
   &lt;/td&gt;
   &lt;td&gt;1.32 GB
   &lt;/td&gt;
   &lt;td&gt;0.67 s
   &lt;/td&gt;
   &lt;td&gt;5.27 GB
   &lt;/td&gt;
   &lt;td&gt;2.5 s
   &lt;/td&gt;
   &lt;td&gt;12.7 GB
   &lt;/td&gt;
   &lt;td&gt;5 s
   &lt;/td&gt;
   &lt;td&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In terms of peak memory footprint, DP-SGD &amp;gt; FGC DP-SGD ≫ GC DP-SGD ≈ PyTorch SGD. Further, the runtimes are similar because most of the parameters are frozen and the forward pass takes up most of the time.&lt;/p&gt;

&lt;h4 id=&quot;synthetic-setup-memory-profiling&quot;&gt;Synthetic Setup: Memory Profiling&lt;/h4&gt;

&lt;p&gt;We consider the following setup to profile the memory used by PyTorch SGD, Vanilla DP-SGD and Ghost Clipping, GC DP-SGD.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2-layer fully connected neural network
    &lt;ul&gt;
      &lt;li&gt;Input: 5120&lt;/li&gt;
      &lt;li&gt;Hidden: 2560&lt;/li&gt;
      &lt;li&gt;Output: 1280&lt;/li&gt;
      &lt;li&gt;Total number of model parameters = 15.6M&lt;/li&gt;
      &lt;li&gt;Model size = 62.5 MB&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Batch size, different values, as seen in the table below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The table below summarizes the max memory increase (in MB) broken down by stages of the training loop for each of the methods.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Model to GPU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Forward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;First Backward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Second Backward&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Optimizer Step&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;32
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.5
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Vanilla DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;3,663
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;162.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;63.13
   &lt;/td&gt;
   &lt;td&gt;50
   &lt;/td&gt;
   &lt;td&gt;125
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;2&lt;sup&gt;17&lt;/sup&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;1920
   &lt;/td&gt;
   &lt;td&gt;1932.5
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Vanilla DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;5&quot; style=&quot;text-align:center&quot;&gt;OOM
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GC DP-SGD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;62.5
   &lt;/td&gt;
   &lt;td&gt;1920
   &lt;/td&gt;
   &lt;td&gt;2625
   &lt;/td&gt;
   &lt;td&gt;1932.5
   &lt;/td&gt;
   &lt;td&gt;125
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;industry-use-case&quot;&gt;Industry use case&lt;/h4&gt;

&lt;p&gt;We tested Ghost Clipping DP-SGD on an internal Meta use case, consisting of a model of size roughly 100B with 40M trainable parameters. Our initial results show that Ghost Clipping SGD reduces 95% memory of vanilla DP-SGD, and achieves comparable memory usage to PyTorch SGD.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we describe implementations of Fast Gradient Clipping and Ghost Clipping in Opacus that enable memory-efficient training of machine learning models with differential privacy. Currently, the Ghost Clipping implementation only applies to linear layers, but, as outlined in &lt;a href=&quot;https://pytorch.medium.com/differential-privacy-series-part-3-efficient-per-sample-gradient-computation-for-more-layers-in-39bd25df237&quot;&gt;part 3 of the series&lt;/a&gt;, it can be extended to “generalized” linear layers such as convolutions and multi-head attention. The current techniques require two explicit backpropagation steps, which increases runtime. We will explore developments on top of Ghost Clipping such as the &lt;a href=&quot;https://arxiv.org/abs/2210.00038&quot;&gt;Book-Keeping algorithm&lt;/a&gt; for mitigation.&lt;/p&gt;

&lt;p&gt;To learn more about Opacus, visit &lt;a href=&quot;https://opacus.ai/&quot;&gt;opacus.ai&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/opacus&quot;&gt;github.com/pytorch/opacus&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank Iden Kalemaj, Darren Liu, Karthik Prasad, Hao Shi, Igor Shilov, Davide Testuggine, Eli Uriegas, Haicheng Wang, and Richard Zou for valuable feedback and suggestions.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;There are &lt;a href=&quot;https://proceedings.neurips.cc/paper\_files/paper/2023/file/a45d344b28179c8da7646bc38ff50ad8-Paper-Conference.pdf&quot;&gt;ways&lt;/a&gt; to extend Ghost Clipping to non-linear layers. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Enayat Ullah, Huanyu Zhang, Will Bullock, Ilya Mironov</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction and Context</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention</title>
      <link href="https://pytorch.org/blog/flexattention/" rel="alternate" type="text/html" title="FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention" />
      <published>2024-08-07T00:00:00-07:00</published>
      <updated>2024-08-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flexattention</id>
      <content type="html" xml:base="https://pytorch.org/blog/flexattention/">&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg1.jpg&quot; alt=&quot;a cartoon chart flexing his muscles&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In theory, Attention is All You Need. In practice, however, we also need optimized attention implementations like FlashAttention.&lt;/p&gt;

&lt;p&gt;Although these fused attention implementations have substantially improved performance and enabled long contexts, this efficiency has come with a loss of flexibility. You can no longer try out a new attention variant by writing a few PyTorch operators - you often need to write a new custom kernel! This operates as a sort of “software lottery” for ML researchers - if your attention variant doesn’t fit into one of the existing optimized kernels, you’re doomed to slow runtime and CUDA OOMs.&lt;/p&gt;

&lt;p&gt;For some examples of attention variants, we have Causal, &lt;a href=&quot;https://paperswithcode.com/method/relative-position-encodings&quot;&gt;Relative Positional Embeddings&lt;/a&gt;, &lt;a href=&quot;https://paperswithcode.com/method/alibi&quot;&gt;Alibi&lt;/a&gt;, &lt;a href=&quot;https://mistral.ai/news/announcing-mistral-7b/&quot;&gt;Sliding Window Attention&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/andersonbcdefg/status/1800907703688339569&quot;&gt;PrefixLM&lt;/a&gt;,  &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/875&quot;&gt;Document Masking/Sample Packing/Jagged Tensors&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/LysandreJik/status/1807779471891538199&quot;&gt;Tanh Soft-Capping&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;PagedAttention&lt;/a&gt;, etc. Even worse, folks often want combinations of these! Sliding Window Attention + Document Masking + Causal + Context Parallelism? Or what about PagedAttention + Sliding Window + Tanh Soft-Capping?&lt;/p&gt;

&lt;p&gt;The left picture below represents the state of the world today - some combinations of masking + biases + setting have existing kernels implemented. But the various options lead to an exponential number of settings, and so overall we end up with fairly spotty support. Even worse, new attention variants researchers come up with will have &lt;em&gt;zero&lt;/em&gt; support.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg2.jpg&quot; alt=&quot;Attention variant support diagram&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To solve this hypercube problem once and for all, we introduce &lt;strong&gt;FlexAttention&lt;/strong&gt;, a new PyTorch API.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We provide a flexible API that allows implementing many attention variants (including all the ones mentioned in the blog post so far) in a few lines of idiomatic PyTorch code.&lt;/li&gt;
  &lt;li&gt;We lower this into a fused FlashAttention kernel through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, generating a FlashAttention kernel that doesn’t materialize any extra memory and has performance competitive with handwritten ones.&lt;/li&gt;
  &lt;li&gt;We also automatically generate the backwards pass, leveraging PyTorch’s autograd machinery.&lt;/li&gt;
  &lt;li&gt;Finally, we can also take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With FlexAttention, we hope that trying new attention variants will only be limited by your imagination.&lt;/p&gt;

&lt;p&gt;You can find many FlexAttention examples at the Attention Gym: &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;https://github.com/pytorch-labs/attention-gym&lt;/a&gt;. If you have any cool applications, feel free to submit an example!&lt;/p&gt;

&lt;p&gt;PS: We also find this API very exciting since it leverages a lot of existing PyTorch infra in a fun way - more on that in the end.&lt;/p&gt;

&lt;h2 id=&quot;flexattention&quot;&gt;FlexAttention&lt;/h2&gt;

&lt;p&gt;Here is the classic attention equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg3.png&quot; alt=&quot;math equation&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In code form:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;FlexAttention allows for an user-defined function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg4.png&quot; alt=&quot;math equation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In code form:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function allows you to &lt;em&gt;modify&lt;/em&gt; the attention scores prior to softmax. Surprisingly, this ends up being sufficient for the vast majority of attention variants (examples below)!&lt;/p&gt;

&lt;p&gt;Concretely, the expected signature for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is somewhat unique.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# noop - standard attention
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In other words, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; is a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments tell you &lt;em&gt;which&lt;/em&gt; dot product you’re currently computing - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; (current element in batch), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt; (current head), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q_idx&lt;/code&gt; (position in query), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kv_idx&lt;/code&gt; (position in key/value tensors).&lt;/p&gt;

&lt;p&gt;To apply this function, we could implement it as&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;modified_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, this is not how FlexAttention is implemented under the hood. Leveraging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, we automatically lower your function into a single &lt;em&gt;fused&lt;/em&gt; FlexAttention kernel - guaranteed or your money back!&lt;/p&gt;

&lt;p&gt;This API ends up being surprisingly expressive. Let’s look at some examples.&lt;/p&gt;

&lt;h2 id=&quot;score-mod-examples&quot;&gt;Score Mod Examples&lt;/h2&gt;

&lt;h3 id=&quot;full-attention&quot;&gt;Full Attention&lt;/h3&gt;

&lt;p&gt;Let’s first do “full attention”, or standard bidirectional attention. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is a no-op - it takes as input the scores and then returns them as is..&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;noop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And to use it end to end (including both forwards &lt;em&gt;and&lt;/em&gt; backwards):&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention.flex_attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;relative-position-encodings&quot;&gt;Relative Position Encodings&lt;/h3&gt;

&lt;p&gt;One common attention variant is the &lt;a href=&quot;https://paperswithcode.com/method/relative-position-encodings&quot;&gt;“relative position encoding&lt;/a&gt;”. Instead of encoding the absolute distance in the queries and keys, relative position encoding adjusts scores based on the “distance” between the queries and keys.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;relative_positional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that unlike typical implementations, this does &lt;em&gt;not&lt;/em&gt; need to materialize a SxS tensor. Instead, FlexAttention computes the bias values “on the fly” within the kernel, leading to significant memory and performance improvements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg5.png&quot; alt=&quot;relative position encoding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alibi-bias&quot;&gt;ALiBi Bias&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg6.png&quot; alt=&quot;alibi bias&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2108.12409&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ALiBi was introduced in &lt;a href=&quot;https://arxiv.org/abs/2108.12409&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;, and claims to have beneficial properties for length extrapolation at inference. Notably, MosaicML has pointed to &lt;a href=&quot;https://twitter.com/jefrankle/status/1804567458092605736&quot;&gt;“lack of kernel support”&lt;/a&gt; as the main reason why they eventually switched from ALiBi to rotary embeddings.&lt;/p&gt;

&lt;p&gt;Alibi is similar to relative positional encodings with one exception - it has a per-head factor that is typically precomputed.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alibi_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_alibi_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [num_heads]
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;alibi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alibi_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This demonstrates one interesting piece of flexibility &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; provides - we can load from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; even though it &lt;em&gt;wasn’t explicitly passed in as an input&lt;/em&gt;! The generated Triton kernel will calculate the correct loads from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; tensor and fuse it. Note that you could regenerate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alibi_bias&lt;/code&gt; and we still wouldn’t need to recompile.&lt;/p&gt;

&lt;h3 id=&quot;soft-capping&quot;&gt;Soft-capping&lt;/h3&gt;

&lt;p&gt;Soft-capping is a technique used in &lt;a href=&quot;https://huggingface.co/blog/gemma2#soft-capping-and-attention-implementations&quot;&gt;Gemma2&lt;/a&gt; and Grok-1 that prevents logits from growing excessively large. In FlexAttention, it looks like:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;soft_cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softcap&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we also automatically generate the backwards pass from the forwards pass here. Also, although this implementation is semantically correct, we likely want to use a tanh approximation in this case for performance reasons. See &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym/blob/main/attn_gym/mods/softcapping.py&quot;&gt;attention-gym&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;causal-mask&quot;&gt;Causal Mask&lt;/h3&gt;

&lt;p&gt;Although bidirectional attention is the simplest, the original &lt;em&gt;Attention is All You Need&lt;/em&gt; paper and the vast majority of LLMs use attention in a decoder-only setting where each token can only attend to the tokens prior to it. Folks often think of this as a lower-triangular mask, but with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; API it can be expressed as:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Basically, if the query token is “after” the key token, we keep the score. Otherwise, we mask it out by setting it to -inf, thus ensuring it won’t participate in the softmax calculation.&lt;/p&gt;

&lt;p&gt;However, masking is special compared to other modifications - if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is sufficient to implement causal masking &lt;em&gt;correctly&lt;/em&gt;, getting the performance benefits of sparsity requires another concept - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mask-mods&quot;&gt;Mask Mods&lt;/h2&gt;

&lt;p&gt;To take advantage of sparsity from masking, we need to do some more work. Specifically, by passing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/e49c0acc396e89baf8c6450e1fa0571d4ce2d4ed/torch/nn/attention/flex_attention.py#L594&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;&lt;/a&gt;, we can create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;. FlexAttention can then use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt; to take advantage of the sparsity!&lt;/p&gt;

&lt;p&gt;The signature of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; is very similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; - just without the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt;. In particular&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# returns True if this position should participate in the computation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; is strictly &lt;em&gt;more&lt;/em&gt; expressive than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;. However, for masking, it’s recommended to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;, as it’s more performant. See the FAQ on why &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; are separate.&lt;/p&gt;

&lt;p&gt;Now, let’s take a look at how we might implement causal mask with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;causal-mask-1&quot;&gt;Causal Mask&lt;/h3&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention.flex_attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_LEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KV_LEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# In this case, we don't need a score_mod, so we won't pass any in.
# However, score_mod can still be combined with block_mask if you need the additional flexibility.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; is a &lt;strong&gt;relatively expensive operation!&lt;/strong&gt; Although FlexAttention will not need to recompile when it changes, if you aren’t careful about caching it, it can lead to significant slowdowns (check out the FAQ for suggestions on best practices).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg7.png&quot; alt=&quot;flexattention performance charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the TFlops are roughly the same, the execution time is 2x faster for the mask_mod version! This demonstrates that we can leverage the sparsity that BlockMask provides us &lt;em&gt;without&lt;/em&gt; losing hardware efficiency.&lt;/p&gt;

&lt;h3 id=&quot;sliding-window--causal&quot;&gt;Sliding Window + Causal&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg8.png&quot; alt=&quot;Sliding Window Causal diagrams&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2310.06825&quot;&gt;Mistral 7B&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Popularized by &lt;a href=&quot;https://arxiv.org/abs/2310.06825&quot;&gt;Mistral&lt;/a&gt;, sliding window attention (also known as local attention) takes advantage of the intuition that the most recent tokens are the most useful. In particular, it allows the query token to only attend to, say, the 1024 most recent tokens. This is often used together with causal attention.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sliding_window_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;window_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window_mask&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If you want to be cute...
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.attention&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sliding_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLIDING_WINDOW&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sliding_window_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sliding_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We benchmark it against &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F.scaled_dot_product_attention&lt;/code&gt; with a sliding window mask as well as FA2 with a causal mask (as a reference point for performance). Not only are we significantly faster than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F.scaled_dot_product_attention&lt;/code&gt;, we’re &lt;em&gt;also&lt;/em&gt; significantly faster than FA2 with a causal mask as this mask has significantly more sparsity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg9.png&quot; alt=&quot;execution time charts&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;prefixlm&quot;&gt;PrefixLM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg10.png&quot; alt=&quot;PrefixLM diagram&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Source: &lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;PaliGemma: A versatile 3B VLM for transfer&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The T5 architecture, proposed in &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;, describes an attention variant that performs full bidirectional attention on a “prefix”, and causal attention on the rest. We again compose two mask functions to accomplish this, one for causal masking and one that is based off of the prefix length.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# In this case, our mask is different per sequence so we set B equal to our batch size
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just like with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; allows us to refer to additional tensors that aren’t explicitly an input to the function! However, with prefixLM, the sparsity pattern changes &lt;em&gt;per&lt;/em&gt; &lt;em&gt;input&lt;/em&gt;. This means that for each new input batch, we’ll need to recompute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;. One common pattern is to call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; at the beginning of your model and reuse that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;block_mask&lt;/code&gt; for all attention calls in your model. See &lt;em&gt;Recomputing Block Masks vs. Recompilation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, in exchange for that, we’re not only able to have an efficient attention kernel for prefixLM, we’re &lt;em&gt;also&lt;/em&gt; able to take advantage of however much sparsity exists in the input! FlexAttention will dynamically adjust its performance based off of the BlockMask data, &lt;em&gt;without&lt;/em&gt; needing to recompile the kernel.&lt;/p&gt;

&lt;h3 id=&quot;document-maskingjagged-sequences&quot;&gt;Document Masking/Jagged Sequences&lt;/h3&gt;

&lt;p&gt;Another common attention variant is document masking/jagged sequences. Imagine that you have a number of sequences of varying length. You want to train on all of them together, but unfortunately, most operators only accept rectangular tensors.&lt;/p&gt;

&lt;p&gt;Through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BlockMask&lt;/code&gt;, we can support this efficiently in FlexAttention as well!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we flatten all sequences into a single sequence with sum(sequence lengths) tokens.&lt;/li&gt;
  &lt;li&gt;Then, we compute the document_id that each token belongs to.&lt;/li&gt;
  &lt;li&gt;Finally, in our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;, we simply whether the query and kv token belong to the same document!&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# The document that each token belongs to.
# e.g. [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2] corresponds to sequence lengths 3, 2, and 6.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SEQ_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;document_masking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And that’s it! In this case, we see that we end up with a blockdiagonal mask.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg11.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One interesting aspect about document masking is that it’s easy to see how it might combine with an arbitrary combination of other masks . For example, we already defined &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefixlm_mask&lt;/code&gt; in the previous section. Do we now need to define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefixlm_document_mask&lt;/code&gt; function as well?&lt;/p&gt;

&lt;p&gt;In these cases, one pattern we’ve found quite useful is what we call a “higher level modification”. In this case, we can take an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; and automatically transform it into one that works with jagged sequences!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_doc_mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Get unique document IDs and their counts
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_consecutive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create cumulative counts (offsets)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;doc_mask_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;same_doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;q_logical&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kv_logical&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inner_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_logical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_logical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;same_doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner_mask&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_mask_wrapper&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, given the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefix_lm_causal&lt;/code&gt; mask from above, we can transform it into one that works on on packed documents like so:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefix_length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;or_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc_prefix_lm_causal_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_doc_mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefix_lm_causal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;document_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg12.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;max-width:600px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, this mask is “block-prefixLM-diagonal” shaped. :)&lt;/p&gt;

&lt;p&gt;That’s all of our examples! There are far more attention variants than we have space to list, so check out &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;Attention Gym&lt;/a&gt; for more examples. We hope that the community will contribute some of their favorite applications of FlexAttention as well.&lt;/p&gt;

&lt;h3 id=&quot;faq&quot;&gt;FAQ&lt;/h3&gt;

&lt;h5 id=&quot;q-when-does-flexattention-need-to-recompile&quot;&gt;&lt;strong&gt;Q: When does FlexAttention need to recompile?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;As FlexAttention leverages &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; for graph capture, it can actually avoid recompilation in a broad spectrum of cases. Notably, it does &lt;em&gt;not&lt;/em&gt; need to recompile even if captured tensors change values!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_mod&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_mod1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_mod1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Compiles the kernel here 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_mod2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_bias_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_mod2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Doesn't need to recompile! 
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even changing the block-sparsity doesn’t require a recompile. However, if the block-sparsity changes, we do need to &lt;em&gt;recompute&lt;/em&gt; the BlockMask.&lt;/p&gt;

&lt;h5 id=&quot;q-when-should-we-recompute-the-blockmask&quot;&gt;&lt;strong&gt;Q: When should we recompute the BlockMask?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;We need to recompute the BlockMask whenever the block-sparsity changes. Although computing the BlockMask is much cheaper than recompilation (on the order of hundreds of microseconds as opposed to seconds), you should still take care to not excessively recompute the BlockMask.&lt;/p&gt;

&lt;p&gt;Here are some common patterns and some recommendations on how you might approach them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mask never changes (e.g. causal mask)&lt;/strong&gt;&lt;br /&gt;
In this case, you can simply precompute the block mask and cache it globally, reusing it for all attention calls.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;causal_attention&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flex_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mask changes every batch (e.g. document masking)&lt;/strong&gt;&lt;br /&gt;
In this case, we would suggest computing the BlockMask at the beginning of the model and threading it through the model - reusing the BlockMask for all layers.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Compute block mask at beginning of forwards
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# amortize block mask construction cost across all layers
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mask changes every layer (e.g. data-dependent sparsity)&lt;/strong&gt;&lt;br /&gt;
This is the hardest setting, since we’re unable to amortize the block mask computation across multiple FlexAttention invocations. Although FlexAttention can certainly still benefit this case, the actual benefits from BlockMask depend on how sparse your attention mask is and how fast we can construct the BlockMask. That leads us to…&lt;/p&gt;

&lt;h5 id=&quot;q-how-can-we-compute-blockmask-quicker&quot;&gt;&lt;strong&gt;Q: How can we compute BlockMask quicker?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; is unfortunately fairly expensive, both from a memory and compute perspective, as determining whether a block is completely sparse requires evaluating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; at every single point in the block. There are a couple ways to address this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If your mask is the same across batch size or heads, make sure that you’re broadcasting over those (i.e. set them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;Compile &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt;. Unfortunately, today, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; does not work directly on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask&lt;/code&gt; due to some unfortunate limitations. However, you can set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_compile=True&lt;/code&gt;, which will significantly reduce the peak memory and runtime (often an order of magnitude in our testing).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Write a custom constructor for BlockMask. The metadata for BlockMask is quite simple (see the &lt;a href=&quot;https://pytorch.org/docs/main/nn.attention.flex_attention.html#blockmask&quot;&gt;documentation&lt;/a&gt;). It’s essentially two tensors.
a. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_blocks&lt;/code&gt;: The number of KV blocks computed for each query block.&lt;br /&gt;
b. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;indices&lt;/code&gt;: The positions of the KV blocks computed for each query block.&lt;/p&gt;

    &lt;p&gt;For example, here’s a custom BlockMask constructor for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causal_mask&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# The first query block computes one block, the second query block computes 2 blocks, etc.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Since we're always computing from the left to the right,
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# we can use the indices [0, 1, 2, ...] for every query block.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BlockMask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;causal_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;q-why-are-score_mod-and-mask_mod-different-isnt-mask_mod-just-a-special-case-of-score_mod&quot;&gt;&lt;strong&gt;Q: Why are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; different? Isn’t &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; just a special case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;Very astute question, hypothetical audience member! In fact, any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; can be easily converted to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; (we do not recommend using this function in practice!)&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mask_mod_as_score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; can implement everything &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; can, what’s the point of having &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;One immediate challenge: a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; requires the actual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; value as an input, but when we’re precomputing the BlockMask, we don’t have the actual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score&lt;/code&gt; value. We can perhaps fake the values by passing in all zeros, and if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt;, then we consider it to be masked (in fact, we originally did this!).&lt;/p&gt;

&lt;p&gt;However, there are two issues. The first is that this is hacky - what if the user’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; returned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt; when the input is 0? Or what if the user’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; masked out with a large negative value instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-inf&lt;/code&gt;? It seems we’re trying to cram a round peg into a square hole. However, there’s a more important reason to separate out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; - it’s fundamentally more efficient!.&lt;/p&gt;

&lt;p&gt;As it turns out, applying masking to every single computed element is actually quite expensive - our benchmarks see about a 15-20% degradation in performance! So, although we can get significant speedups by skipping half the computation, we lose a meaningful part of that speedup from needing to mask out every element!&lt;/p&gt;

&lt;p&gt;Luckily, if we visualize the causal mask, we notice that the vast majority of blocks do not require a “causal mask” at all - they’re fully computed! It is only the blocks on the diagonal, partially computed and partially masked, that require masking to be applied.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg13.png&quot; alt=&quot;blockdiagonal mask&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The BlockMask previously told us which blocks we need to compute and which blocks we can skip. Now, we further augment this data structure to also tell us which blocks are “fully computed” (i.e. masking can be skipped) vs. “partially computed” (i.e. a mask needs to be applied). Note, however, that although masks can be skipped on “fully computed” blocks, other &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;s like relative positional embeddings still need to be applied.&lt;/p&gt;

&lt;p&gt;Given just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;, there’s no sound way for us to tell which parts of it are “masking”. Hence, the user must separate these out themselves into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;q-how-much-additional-memory-does-the-blockmask-need&quot;&gt;&lt;strong&gt;Q: How much additional memory does the BlockMask need?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;The BlockMask metadata is of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[BATCH_SIZE, NUM_HEADS, QUERY_LEN//BLOCK_SIZE, KV_LEN//BLOCK_SIZE].&lt;/code&gt; If the mask is the same across the batch or heads dimension it can be broadcasted over that dimension to save memory.&lt;/p&gt;

&lt;p&gt;At the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BLOCK_SIZE&lt;/code&gt; of 128, we expect that the memory usage will be fairly negligible for most use cases. For example, for a sequence length of 1 million, the BlockMask would only use 60MB of additional memory. If this is a problem, you can increase the block size:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_block_mask(..., BLOCK_SIZE=1024).&lt;/code&gt; For example, increasing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BLOCK_SIZE&lt;/code&gt; to 1024 would result in this metadata dropping to under a megabyte.&lt;/p&gt;

&lt;h5 id=&quot;q-how-do-the-numerics-compare&quot;&gt;&lt;strong&gt;Q: How do the numerics compare?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;Although the results are not bitwise identical, we are confident that FlexAttention is as numerically accurate as FlashAttention. We generate the following distribution of differences comparing FlashAttention versus FlexAttention over a large range of inputs on both causal and non causal attention variants. The errors are nearly identical.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg14.png&quot; alt=&quot;distribution chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Generally speaking, FlexAttention is nearly as performant as a handwritten Triton kernel, which is unsurprising, as we heavily leverage a handwritten Triton kernel. However, due to its generality, we do incur a small performance penalty. For example, we must incur some additional latency to determine which block to compute next. In some cases, we provide some kernel options that can affect the performance of the kernel while changing its behavior. They can be found here: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/ee09d066d35d7e17cf7e9479c0b8bfc70cffc264/torch/_inductor/kernel/flex_attention.py#L146-L155&quot;&gt;performance knobs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a case study, let’s explore how the knobs affect the performance of causal attention. We will compare performance of the triton kernel versus FlashAttentionv2 on A100. The script can be found &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/benchmarks/transformer/score_mod.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FlexAttention achieves 90% of FlashAttention2’s performance in the forward pass and 85% in the backward pass. FlexAttention is currently utilizing a deterministic algorithm that recomputes more intermediates than FAv2, but we have plans to improve FlexAttention’s backward algorithm and hope to close this gap!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg15.png&quot; alt=&quot;flexattention speed chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention/fg16.png&quot; alt=&quot;flexattention speed chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We hope you have as much fun using FlexAttention as we did developing it! While working on this, we ended up finding way more applications of this API than we could have expected. We’ve already seen it accelerate torchtune’s &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/1193&quot;&gt;sample packing throughput by 71%&lt;/a&gt;, replace the need for a researcher to spend over a week writing their own custom Triton kernel, and deliver competitive performance with custom handwritten attention variants.&lt;/p&gt;

&lt;p&gt;One final thing that made implementing FlexAttention quite fun is that we were able to leverage a lot of existing PyTorch infra in an interesting way. For example, one of the unique aspects about TorchDynamo (torch.compile’s frontend) is that it does &lt;em&gt;not&lt;/em&gt; require tensors used in the compiled function to be explicitly passed in as inputs. This allows us to compile mods like document masking, which require accessing &lt;em&gt;global&lt;/em&gt; variables where the global variables need to change!&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;score_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The bias tensor can change!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Furthermore, the fact that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a generic graph-capture mechanism also allows it to support more “advanced” transformations, such as the higher order transform that transforms any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; into one that works with jagged tensors.&lt;/p&gt;

&lt;p&gt;We also leverage TorchInductor (torch.compile’s backend) infrastructure for Triton templates. Not only did this make it easy to support codegening FlexAttention - it also automatically gave us support for dynamic shapes as well as epilogue fusion (i.e. fusing an operator onto the end of attention)! In the future, we plan on extending this support to allow for quantized versions of attention or things like &lt;a href=&quot;https://lmsys.org/blog/2024-01-17-sglang/&quot;&gt;RadixAttention&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;In addition, we also leveraged higher order ops, PyTorch’s autograd to automatically generate the backwards pass, as well as vmap to automatically apply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; for creating the BlockMask.&lt;/p&gt;

&lt;p&gt;And, of course, this project wouldn’t have been possible without Triton and TorchInductor’s ability to generate Triton code.&lt;/p&gt;

&lt;p&gt;We look forward to leveraging the approach we used here to more applications in the future!&lt;/p&gt;

&lt;h3 id=&quot;limitations-and-future-work&quot;&gt;Limitations and Future Work&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FlexAttention is currently available in PyTorch nightly releases, we plan to release it as a prototype feature in 2.5.0&lt;/li&gt;
  &lt;li&gt;We did not cover how to use FlexAttention for inference here (or how to implement PagedAttention) - we will cover those in a later post.&lt;/li&gt;
  &lt;li&gt;We are working to improve the performance of FlexAttention to match FlashAttention3 on H100 GPUs.&lt;/li&gt;
  &lt;li&gt;FlexAttention requires that all sequence lengths be a multiple of 128 - this will be addressed soon.&lt;/li&gt;
  &lt;li&gt;We plan on adding GQA support soon - for now, you can just replicate the kv heads.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;We want to highlight some prior work (and people) that have inspired FlexAttention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tri Dao’s work on FlashAttention&lt;/li&gt;
  &lt;li&gt;Francisco Massa and the Xformers team for BlockSparseAttention in Triton&lt;/li&gt;
  &lt;li&gt;The Jax team’s work on SplashAttention&lt;/li&gt;
  &lt;li&gt;Philippe Tillet and Keren Zhou for helping us with Triton&lt;/li&gt;
  &lt;li&gt;Ali Hassani for discussions on neighborhood attention&lt;/li&gt;
  &lt;li&gt;Everybody who’s complained about attention kernels not supporting their favorite attention variant :)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch: Horace He, Driss Guessous, Yanbo Liang, Joy Dong</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Quantization-Aware Training for Large Language Models with PyTorch</title>
      <link href="https://pytorch.org/blog/quantization-aware-training/" rel="alternate" type="text/html" title="Quantization-Aware Training for Large Language Models with PyTorch" />
      <published>2024-07-30T00:00:00-07:00</published>
      <updated>2024-07-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/quantization-aware-training</id>
      <content type="html" xml:base="https://pytorch.org/blog/quantization-aware-training/">&lt;p&gt;In this blog, we present an end-to-end Quantization-Aware Training (QAT) flow for large language models in PyTorch. We demonstrate how QAT in PyTorch can &lt;strong&gt;recover up to 96% of the accuracy degradation&lt;/strong&gt; &lt;strong&gt;on hellaswag and&lt;/strong&gt; &lt;strong&gt;68% of the perplexity degradation on wikitext&lt;/strong&gt; &lt;strong&gt;for Llama3 compared to post-training quantization (PTQ).&lt;/strong&gt; We present the QAT APIs in &lt;a href=&quot;https://github.com/pytorch/ao/&quot;&gt;torchao&lt;/a&gt; and showcase how users can leverage them for fine-tuning in &lt;a href=&quot;https://github.com/pytorch/torchtune/&quot;&gt;torchtune&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg1.jpg&quot; alt=&quot;Llama3-8B fine-tuned on the C4 dataset (en subset) with and without QAT using int8 per token dynamic activations + int4 grouped per channel weights, evaluated on hellaswag and wikitext on a A100 GPU. Note the log scale for wikitext (lower is better).&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Llama3-8B fine-tuned on the C4 dataset (en subset) with and without QAT using int8 per token dynamic activations + int4 grouped per channel weights, evaluated on hellaswag and wikitext on a A100 GPU. Note the log scale for wikitext (lower is better).&lt;/p&gt;

&lt;p&gt;To demonstrate the effectiveness of QAT in an end-to-end flow, we further lowered the quantized model to &lt;a href=&quot;https://github.com/google/XNNPACK&quot;&gt;XNNPACK&lt;/a&gt;, a highly optimized neural network library for backends including iOS and Android, through &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2&quot;&gt;executorch&lt;/a&gt;. &lt;strong&gt;After lowering to XNNPACK, the QAT model saw 16.8% lower perplexity than the PTQ model, while maintaining the same model size and on-device inference and generation speeds.&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Lowered model metric&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;PTQ&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;QAT&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext word perplexity (↓)
   &lt;/td&gt;
   &lt;td&gt;23.316
   &lt;/td&gt;
   &lt;td&gt;19.403
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext byte perplexity (↓)
   &lt;/td&gt;
   &lt;td&gt;1.850
   &lt;/td&gt;
   &lt;td&gt;1.785
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Wikitext bits per byte (↓)
   &lt;/td&gt;
   &lt;td&gt;0.887
   &lt;/td&gt;
   &lt;td&gt;0.836
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Model size
   &lt;/td&gt;
   &lt;td&gt;3.881 GB
   &lt;/td&gt;
   &lt;td&gt;3.881 GB
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;On-device inference speed
   &lt;/td&gt;
   &lt;td&gt;5.065 tok/s
   &lt;/td&gt;
   &lt;td&gt;5.265 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;On-device generation speed
   &lt;/td&gt;
   &lt;td&gt;8.369 tok/s
   &lt;/td&gt;
   &lt;td&gt;8.701 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; QAT achieved 16.8% lower perplexity and unchanged model sizes and on-device inference and generation speeds on the Llama3-8B model lowered to XNNPACK. Linear layers are quantized using int8 per token dynamic activations + int4 grouped per channel weights, and embeddings are additionally quantized to int4 using a group size of 32 (QAT is only applied to linear layers). Wikitext evaluation is performed using 5 samples and a max sequence length of 127 on server CPU, since evaluation is not available on device (lower is better for all wikitext results). On-device inference and generation is benchmarked on the Samsung Galaxy S22 smartphone.&lt;/p&gt;

&lt;h3 id=&quot;qat-apis&quot;&gt;QAT APIs&lt;/h3&gt;

&lt;p&gt;We are excited for users to try our &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/prototype/qat.py&quot;&gt;QAT API&lt;/a&gt; in torchao, which can be leveraged for both training and fine-tuning. This API involves two steps, prepare and convert: prepare applies a transformation on the linear layers in the model to simulate the numerics of quantization during training, and convert actually quantizes these layers into lower bit-widths after training. The converted model can then be used in the exact same way as the PTQ model:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtune.models.llama3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization.prototype.qat&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Int8DynActInt4WeightQATQuantizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Smaller version of llama3 to fit in a single GPU
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_kv_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embed_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_seq_len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Quantizer for int8 dynamic per token activations +
# int4 grouped per channel weights, only for linear layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Int8DynActInt4WeightQATQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Insert &quot;fake quantize&quot; operations into linear layers.
# These operations simulate quantization numerics during
# training without performing any dtype casting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Standard training loop
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert fake quantize to actual quantize operations
# The quantized model has the exact same structure as the
# quantized model produced in the corresponding PTQ flow
# through `Int8DynActInt4WeightQuantizer`
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# inference or generate
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;fine-tuning-with-torchtune&quot;&gt;Fine-tuning with torchtune&lt;/h4&gt;

&lt;p&gt;We also integrated this QAT flow into &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;torchtune&lt;/a&gt; and provided &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama3/8B_qat_full.yaml&quot;&gt;recipes&lt;/a&gt; to run this in a distributed setting, similar to the existing full fine-tune distributed recipe. Users can additionally apply QAT during LLM fine-tuning by running the following command. See &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantization.md&quot;&gt;this README&lt;/a&gt; for more details.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nproc_per_node&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qat_distributed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llama3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B_qat_full&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-quantization-aware-training&quot;&gt;What is Quantization-Aware Training?&lt;/h2&gt;

&lt;p&gt;Quantization-Aware Training (QAT) is a common quantization technique for mitigating model accuracy/perplexity degradation that arises from quantization. This is achieved by simulating quantization numerics during training while keeping the weights and/or activations in the original data type, typically float, effectively “fake quantizing” the values instead of actually casting them to lower bit-widths:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# PTQ: x_q is quantized and cast to int8
# scale and zero point (zp) refer to parameters used to quantize x_float
# qmin and qmax refer to the range of quantized values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_float&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# QAT: x_fq is still in float
# Fake quantize simulates the numerics of quantize + dequantize
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_float&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_fq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since quantization involves non-differentiable operations like rounding, the QAT backward pass typically uses &lt;a href=&quot;https://arxiv.org/pdf/1308.3432&quot;&gt;straight-through estimators (STE)&lt;/a&gt;, a mechanism to estimate the gradients flowing through non-smooth functions, to ensure the gradients passed to the original weights are still meaningful. In this manner, the gradients are computed with the knowledge that the weights will ultimately be quantized after training, effectively allowing the model to adjust for quantization noise during the training process. Note that an alternative to QAT is quantized training, which actually casts the values to lower bit dtypes during training, but &lt;a href=&quot;https://cloud.google.com/blog/products/compute/accurate-quantized-training-aqt-for-tpu-v5e&quot;&gt;prior efforts&lt;/a&gt; have only seen success up to 8-bits, whereas QAT is effective even at lower bit-widths.&lt;/p&gt;

&lt;h3 id=&quot;qat-in-pytorch&quot;&gt;QAT in PyTorch&lt;/h3&gt;

&lt;p&gt;We added an initial QAT flow in torchao under prototype &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.2.0/torchao/quantization/prototype/qat.py&quot;&gt;here&lt;/a&gt;. Currently we support int8 dynamic per-token activations + int4 grouped per-channel weights (abbreviated 8da4w) for linear layers. These settings are motivated by a combination of &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/models/llama2/README.md#quantization&quot;&gt;kernel availability on edge backends&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2305.17888&quot;&gt;prior research on LLM quantization&lt;/a&gt;, which found that per-token activation and per-group weight quantization achieves the best model quality for LLMs compared to other quantization schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg2.png&quot; alt=&quot;torchao QAT flow. This flow involves two steps: (1) prepare, which inserts the fake quantization ops into the model’s linear layers, and (2) convert, which converts these fake quantization ops with actual quantize and dequantize ops after training.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; torchao QAT flow. This flow involves two steps: (1) prepare, which inserts the fake quantization ops into the model’s linear layers, and (2) convert, which converts these fake quantization ops with actual quantize and dequantize ops after training.&lt;/p&gt;

&lt;p&gt;This flow produces the exact same quantized model as the PTQ flow using the same quantization settings (through &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/GPTQ.py#L941&quot;&gt;Int8DynActInt4WeightQuantizer&lt;/a&gt;), but with quantized weights that achieve superior accuracies and perplexities. Thus, we can use the model converted from the QAT flow as a drop-in replacement for the PTQ model and reuse all the backend delegation logic and underlying kernels.&lt;/p&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;p&gt;All experiments in this blog post are performed using the torchtune QAT integration described above. We use 6-8 A100 GPUs with 80 GBs each to fine-tune &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-2-7b&quot;&gt;Llama2-7B&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&quot;&gt;Llama3-8B&lt;/a&gt; on the &lt;a href=&quot;https://huggingface.co/datasets/allenai/c4&quot;&gt;C4 dataset&lt;/a&gt; (en subset) for 5000 steps. For all experiments, we use batch size = 2, learning rate = 2e-5, max sequence length = 4096 for Llama2 and 8192 for Llama3, &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html&quot;&gt;Fully Sharded Data Parallel&lt;/a&gt; (FSDP) as our distribution strategy, and activation checkpointing to reduce memory footprint. For 8da4w experiments, we use a group size of 256 for weights.&lt;/p&gt;

&lt;p&gt;Since the pre-training dataset is not easily accessible, we perform QAT during the fine-tuning process. Empirically, we found that disabling fake quantization for the first N steps led to better results, presumably because doing so allows the weights to stabilize before we start introducing quantization noise to the fine-tuning process. We disable fake quantization for the first 1000 steps for all our experiments.&lt;/p&gt;

&lt;p&gt;We evaluate our quantized models using the &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;lm-evaluation-harness&lt;/a&gt; integration in torchtune. We report evaluation results from a variety of tasks commonly used to evaluate LLMs, including hellaswag, a commonsense sentence completion task, wikitext, a next token/byte prediction task, and a few question-answering tasks such as arc, openbookqa, and piqa. For wikitext, perplexity refers to the inverse of how well the model can predict the next word or byte (lower is better), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bits_per_byte&lt;/code&gt; refers to how many bits are needed to predict the next byte (lower is also better here). For all other tasks, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acc_norm&lt;/code&gt; refers to the accuracy normalized by the byte-length of the target string.&lt;/p&gt;

&lt;h4 id=&quot;int8-dynamic-activations--int4-weight-quantization-8da4w&quot;&gt;Int8 Dynamic Activations + Int4 Weight Quantization (8da4w)&lt;/h4&gt;

&lt;p&gt;Starting with Llama2 8da4w quantization, we saw that QAT was able to recover 62% of the normalized accuracy degradation on hellaswag compared to PTQ, and 58% and 57% of the word and byte perplexity degradation (respectively) on wikitext. We see similar improvements for most of the other tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg3a.png&quot; alt=&quot;Llama2-7B 8da4w quantization with and without QAT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3a:&lt;/strong&gt; Llama2-7B 8da4w quantization with and without QAT&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg3b.png&quot; alt=&quot;Llama2-7B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3b:&lt;/strong&gt; Llama2-7B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&lt;/p&gt;

&lt;p&gt;Llama3 8da4w quantization saw even more pronounced improvements with QAT. On the hellaswag evaluation task, we were able to recover 96% of the normalized accuracy degradation on hellaswag compared to PTQ, with minimal overall degradation (&amp;lt;1%) compared to the non-quantized accuracy. On the wikitext evaluation task, QAT recovered 68% and 65% of the word and byte perplexity degradation (respectively). Even on arc_challenge, which was difficult for Llama2 QAT, we were able to recover 51% of the normalized accuracy degradation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg4a.png&quot; alt=&quot;Llama3-8B 8da4w quantization with and without QAT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4a:&lt;/strong&gt; Llama3-8B 8da4w quantization with and without QAT&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg4b.png&quot; alt=&quot;Llama3-8B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4b:&lt;/strong&gt; Llama3-8B 8da4w quantization with and without QAT, evaluated on wikitext (lower is better)&lt;/p&gt;

&lt;h4 id=&quot;lower-bit-weight-only-quantization&quot;&gt;Lower Bit Weight Only Quantization&lt;/h4&gt;

&lt;p&gt;We further extended the torchao QAT flow to 2-bit and 3-bit weight only quantization and repeated the same experiments for Llama3-8B. Quantization degradation is more severe at lower bit-widths, so we use a group size of 32 for all experiments for finer-grained quantization.&lt;/p&gt;

&lt;p&gt;However, this is still not enough for 2-bits PTQ, which saw wikitext perplexity explode. To mitigate this problem, we leverage knowledge from prior sensitivity analysis that the first 3 and last 2 layers of the Llama3 model are the most sensitive, and skip quantizing these layers in exchange for a moderate increase in quantized model size (1.78 GB for 2-bits and 1.65 GB for 3-bits). This brought the wikitext word perplexity down from 603336 to 6766, which is significant but still far from acceptable. To further improve the quantized model, we turn to QAT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg5a.png&quot; alt=&quot;Llama3-8B 2-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5a:&lt;/strong&gt; Llama3-8B 2-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&lt;/p&gt;

&lt;p&gt;We observe that applying QAT while skipping quantization for the first 3 and last 2 layers further brought the word perplexity down to a much more reasonable value of 30 (from 6766). More generally, QAT was able to recover 53% of the normalized accuracy degradation on hellaswag compared to PTQ, and 99% and 89% of the word and byte perplexity degradation (respectively) on wikitext. Without skipping the sensitive layers, however, QAT was far less effective at mitigating degradation in quantized model quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg5b.png&quot; alt=&quot;Llama3-8B 2-bit weight only quantization with and without QAT. Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5b:&lt;/strong&gt; Llama3-8B 2-bit weight only quantization with and without QAT. Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&lt;/p&gt;

&lt;p&gt;For 3-bit weight only quantization, QAT was effective even without skipping the first 3 and last 2 layers, though skipping these layers still led to better results for both PTQ and QAT. In the skip case, QAT was able to recover 63% of the normalized accuracy degradation on hellaswag compared to PTQ, and 72% and 65% of the word and byte perplexity degradation (respectively) on wikitext.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg6a.png&quot; alt=&quot;Llama3-8B 3-bit weight only quantization with and without QAT. Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6a:&lt;/strong&gt; Llama3-8B 3-bit weight only quantization with and without QAT. Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/quantization-aware-training/fg6b.png&quot; alt=&quot;Llama3-8B 3-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&quot; style=&quot;max-width:400px; display:block; margin-left: auto; margin-right: auto; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6b:&lt;/strong&gt; Llama3-8B 3-bit weight only quantization with and without QAT, evaluated on wikitext (lower is better). Bars with “skip” refer to skipping quantization for the first 3 and last 2 layers of the model, which are more sensitive to quantization. Note the log scale.&lt;/p&gt;

&lt;h4 id=&quot;qat-overhead&quot;&gt;QAT Overhead&lt;/h4&gt;

&lt;p&gt;QAT inserts many fake quantize operations throughout the model, adding considerable overhead to both the fine-tuning speed and the memory usage. For a model like Llama3-8B for example, we have (32 * 7) + 1 = 225 linear layers, each of which has at least 1 fake quantize for the weights and potentially 1 fake quantize for the input activations. Memory footprint increase is also significant, since we cannot mutate the weights in-place and so we need to clone them before applying fake quantization, though this overhead can be mostly mitigated by enabling activation checkpointing.&lt;/p&gt;

&lt;p&gt;In our microbenchmarks, we found that 8da4w QAT fine-tuning is ~34% slower than regular full fine-tuning. With activation checkpointing, the memory increase per GPU is around 2.35 GB. Most of these overheads are fundamental to how QAT works, though we may be able to speed up computation with &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;torch.compile&lt;/a&gt; in the future.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Per GPU statistics
   &lt;/td&gt;
   &lt;td&gt;Full fine-tuning
   &lt;/td&gt;
   &lt;td&gt;QAT fine-tuning
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Median tokens per second
   &lt;/td&gt;
   &lt;td&gt;546.314 tok/s
   &lt;/td&gt;
   &lt;td&gt;359.637 tok/s
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Median peak memory
   &lt;/td&gt;
   &lt;td&gt;67.501 GB
   &lt;/td&gt;
   &lt;td&gt;69.850 GB
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; Llama3 QAT fine-tuning overhead for int8 per token dynamic activations + int4 grouped per channel weights on 6 A100 GPUs (each with 80GB memory).&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;In this blog, we presented a QAT flow for LLMs through &lt;a href=&quot;https://github.com/pytorch/ao/&quot;&gt;torchao&lt;/a&gt;, integrated this flow with the fine-tuning APIs in &lt;a href=&quot;https://github.com/pytorch/torchtune/&quot;&gt;torchtune&lt;/a&gt;, and demonstrated its potential to recover most of the quantization degradation compared to PTQ and match non-quantized performance on certain tasks. There are many directions for future explorations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hyperparameter tuning.&lt;/strong&gt; It is likely that extensive hyperparameter tuning can further improve the results of finetuning and QAT. In addition to the general hyperparameters like the learning rate, batch size, dataset size, and number of fine-tuning steps, we should also tune QAT-specific ones, such as when to start/stop fake quantization, how many steps to fake quantize, and regularization parameters for fake quantized values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Outlier reduction techniques.&lt;/strong&gt; In our experiments, we found that both PTQ and QAT were susceptible to outliers. In addition to simple clamping and regularization during fine-tuning, we can explore techniques that allow the network to learn how to control these outliers (e.g. &lt;a href=&quot;https://arxiv.org/pdf/1902.08153&quot;&gt;learned quantization ranges&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2306.12929&quot;&gt;clipped softmax&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2306.12929&quot;&gt;gated attention&lt;/a&gt;), or possibly even borrow outlier suppression techniques from post-training settings (e.g. &lt;a href=&quot;https://arxiv.org/pdf/2405.16406&quot;&gt;SpinQuant&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2211.10438&quot;&gt;SmoothQuant&lt;/a&gt;) and apply them sparingly throughout the fine-tuning process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mixed-precision and more complex dtypes.&lt;/strong&gt; Especially in the lower bit regime, we saw that skipping quantization for certain sensitive layers was effective for both PTQ and QAT. Did we need to skip quantizing these layers altogether, or can we still quantize them, just to lower bit-widths? It will be interesting to explore mixed-precision quantization in the context of QAT. Training with newer dtypes such as MX4 is another promising direction, especially given that the upcoming Blackwell GPUs will &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;no longer support int4 tensor cores&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Composability with LoRA and QLoRA.&lt;/strong&gt; Our QAT integration in torchtune currently only supports the full fine-tuning workflow. However, many users wish to fine-tune their models using low-ranked adaptors to substantially reduce their memory footprint. Composing QAT with techniques like LoRA / QLoRA will enable users to reap the memory and performance benefits of these approaches while producing a model that will ultimately be quantized with minimal model quality degradation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Composability with &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;torch.compile&lt;/a&gt;.&lt;/strong&gt; This is another potential way to significantly speed up fake quantization computations in QAT while reducing memory footprint. torch.compile is currently not compatible with the distribution strategy used in full distributed fine-tuning recipes in torchtune (with or without QAT), but support will be added in the near future.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Quantizing other layers.&lt;/strong&gt; In this work, we only explored quantizing the linear layers. However, in the context of long sequence lengths, the KV cache often becomes the throughput bottleneck and can reach tens of GBs, hence &lt;a href=&quot;https://arxiv.org/pdf/2305.17888&quot;&gt;LLM-QAT&lt;/a&gt; explored quantizing the KV cache alongside activations and weights. &lt;a href=&quot;https://arxiv.org/pdf/2109.12948&quot;&gt;Prior work&lt;/a&gt; has also had success with quantizing the embedding layer down to 2-bits in other transformer-based models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;End-to-end evaluation on performant cuda kernels.&lt;/strong&gt; A natural extension of this work is to provide an end-to-end QAT flow evaluated on performant cuda kernels, similar to the existing 8da4w QAT flow lowered to XNNPACK kernels through executorch. For int4 weight only quantization, we can leverage the efficient &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/v2.3.1/aten/src/ATen/native/cuda/int4mm.cu#L865&quot;&gt;int4 weight mm kernel with bitpacking&lt;/a&gt; for quantization, and there is ongoing work to add QAT support for this kernel: &lt;a href=&quot;https://github.com/pytorch/ao/pull/383&quot;&gt;https://github.com/pytorch/ao/pull/383&lt;/a&gt;. For 8da4w quantization, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1413&quot;&gt;mixed 4-bit/8-bit GEMM&lt;/a&gt; is also being added in cutlass. This will be needed to build an efficient 8da4w cuda kernel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The QAT code can be found &lt;a href=&quot;https://github.com/pytorch/ao/blob/v0.3.0/torchao/quantization/prototype/qat.py&quot;&gt;here&lt;/a&gt;. Please refer to &lt;a href=&quot;https://pytorch.org/torchtune/main/tutorials/qat_finetune.html&quot;&gt;this torchtune tutorial&lt;/a&gt; to get started. If you have any further questions, please feel free to open an issue on the torchao &lt;a href=&quot;https://github.com/pytorch/ao/issues&quot;&gt;github&lt;/a&gt; or reach out to &lt;a href=&quot;mailto:andrewor@meta.com&quot;&gt;andrewor@meta.com&lt;/a&gt;. We welcome your feedback and contributions!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Andrew Or, Jerry Zhang, Evan Smothers, Kartikay Khandelwal, Supriya Rao</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we present an end-to-end Quantization-Aware Training (QAT) flow for large language models in PyTorch. We demonstrate how QAT in PyTorch can recover up to 96% of the accuracy degradation on hellaswag and 68% of the perplexity degradation on wikitext for Llama3 compared to post-training quantization (PTQ). We present the QAT APIs in torchao and showcase how users can leverage them for fine-tuning in torchtune.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile</title>
      <link href="https://pytorch.org/blog/torchchat-local-llm-inference/" rel="alternate" type="text/html" title="Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile" />
      <published>2024-07-30T00:00:00-07:00</published>
      <updated>2024-07-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchchat-local-llm-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchchat-local-llm-inference/">&lt;p&gt;Today, we’re releasing &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt;, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.&lt;/p&gt;

&lt;p&gt;In our previous blog posts, we &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;showed&lt;/a&gt; how to use native PyTorch 2 to run LLMs with great performance using CUDA. Torchchat expands on this with more target environments, models and execution modes. Additionally it provides important functions such as export, quantization and eval in a way that’s easy to understand providing an E2E story for those who want to build a local inference solution.&lt;/p&gt;

&lt;p&gt;You will find the project organized into three areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Python: Torchchat provides a &lt;a href=&quot;https://github.com/pytorch/torchchat?tab=readme-ov-file#server&quot;&gt;REST API&lt;/a&gt; that is called via a Python CLI or can be accessed via the browser&lt;/li&gt;
  &lt;li&gt;C++: Torchchat produces a desktop-friendly binary using PyTorch’s &lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor&quot;&gt;AOTInductor&lt;/a&gt; backend&lt;/li&gt;
  &lt;li&gt;Mobile devices: Torchchat uses &lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;ExecuTorch&lt;/a&gt; to export a .pte binary file for on-device inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchchat.png&quot; alt=&quot;torchchat schema&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;The following table tracks the performance of torchchat for Llama 3 for a variety of configurations.&lt;br /&gt;
&lt;em&gt;Numbers for Llama 3.1 are coming soon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Llama 3 8B Instruct on Apple MacBook Pro M1 Max 64GB Laptop&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm Compile
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;5.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.99
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm AOTI
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;4.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.28
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;MPS Eager
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;12.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;16.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;17.15
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Llama 3 8B Instruct on Linux x86 and CUDA&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz with 180GB Ram + A100 (80GB)&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;x86 Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;2.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;3.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;5.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;CUDA Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;83.23
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;118.17
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;135.16
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Llama3 8B Instruct on Mobile&lt;/strong&gt;&lt;br /&gt;
Torchchat achieves &amp;gt; 8T/s on the Samsung Galaxy S23 and iPhone using 4-bit GPTQ via ExecuTorch.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We encourage you to &lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;clone the torchchat repo and give it a spin&lt;/a&gt;&lt;/strong&gt;, explore its capabilities, and share your feedback as we continue to empower the PyTorch community to run LLMs locally and on constrained devices. Together, let’s unlock the full potential of generative AI and LLMs on any device. Please submit &lt;a href=&quot;https://github.com/pytorch/torchat/issues&quot;&gt;issues&lt;/a&gt; as you see them, since we are still iterating quickly. We’re also inviting community contributions across a broad range of areas, from additional models, target hardware support, new quantization schemes, or performance improvements.  Happy experimenting!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we’re releasing torchchat, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.4 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-4/" rel="alternate" type="text/html" title="PyTorch 2.4 Release Blog" />
      <published>2024-07-24T00:00:00-07:00</published>
      <updated>2024-07-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-4</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-4/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.4 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.4.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.4 adds support for the latest version of Python (3.12) for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. AOTInductor freezing gives developers running AOTInductor more performance-based optimizations by allowing the serialization of MKLDNN weights. As well, a new default TCPStore server backend utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libuv&lt;/code&gt; has been introduced which should significantly reduce initialization times for users running large-scale jobs. Finally, a new Python Custom Operator API makes it easier than before to integrate custom kernels into PyTorch, especially for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This release is composed of 3661 commits and 475 contributors since PyTorch 2.3. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.4. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Python 3.12 support for torch.compile
   &lt;/td&gt;
   &lt;td&gt;FSDP2: DTensor-based per-parameter-sharding FSDP
   &lt;/td&gt;
   &lt;td&gt;torch.compile optimizations for AWS Graviton (aarch64-linux) processors
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AOTInductor Freezing for CPU
   &lt;/td&gt;
   &lt;td&gt;torch.distributed.pipelining, simplified pipeline parallelism
   &lt;/td&gt;
   &lt;td&gt;BF16 symbolic shape optimization in TorchInductor
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;New Higher-level Python Custom Operator API
   &lt;/td&gt;
   &lt;td&gt;Intel GPU is available through source build
   &lt;/td&gt;
   &lt;td&gt;Performance optimizations for GenAI projects utilizing CPU devices
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Switching TCPStore’s default server backend to libuv
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-python-312-support-for-torchcompile&quot;&gt;[Beta] Python 3.12 support for &lt;em&gt;torch.compile&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; previously only supported Python &lt;strong&gt;3.8-3.11&lt;/strong&gt;. Users can now optimize models with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; with Python &lt;strong&gt;3.12&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-aotinductor-freezing-for-cpu&quot;&gt;[Beta] AOTInductor Freezing for CPU&lt;/h3&gt;

&lt;p&gt;This feature enables users to turn on the freezing flag when using AOTInductor on CPU. With this feature, AOTInductor can cover the same set of op scenarios and reach on-par performance as Inductor CPP backend. Before this support, when models contain MKLDNN operators (when computation-intensive operators are involved, such as Convolution, Linear, ConvTranspose, and so on) and freezing is on, those models will fail to run since AOTInductor didn’t support serializing the MKLDNN weights which have an opaque format.&lt;/p&gt;

&lt;p&gt;The workflow is as explained in the AOTInductor &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt;tutorial&lt;/a&gt;, in addition to that users could now add the freezing flag to get better performance:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TORCHINDUCTOR_FREEZING=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;beta-new-higher-level-python-custom-operator-api&quot;&gt;[Beta] New Higher-level Python Custom Operator API&lt;/h3&gt;

&lt;p&gt;We’ve added a new higher-level Python Custom Operator API that makes it easier than before to extend PyTorch with custom operators that behave like PyTorch’s built-in operators. Operators registered using the &lt;a href=&quot;https://pytorch.org/docs/2.4/library.html#module-torch.library&quot;&gt;new high-level torch.library APIs&lt;/a&gt; are guaranteed to be compatible with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and other PyTorch subsystems; authoring a custom operator in Python using the previous &lt;a href=&quot;https://pytorch.org/docs/2.4/library.html#low-level-apis&quot;&gt;low-level torch.library APIs&lt;/a&gt; required deep understanding of PyTorch internals and has many footguns.&lt;/p&gt;

&lt;p&gt;Please see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/python_custom_ops.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-switching-tcpstores-default-server-backend-to-libuv&quot;&gt;[Beta] Switching TCPStore’s default server backend to &lt;em&gt;libuv&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Introduced a new default server backend for TCPStore built with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libuv&lt;/code&gt; which should introduce significantly lower initialization times and better scalability. This should ideally benefit users with a much shorter startup time when accounting for large-scale jobs.&lt;/p&gt;

&lt;p&gt;For more information on the motivation + fallback instructions please refer to this &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;prototype-fsdp2-dtensor-based-per-parameter-sharding-fsdp&quot;&gt;[PROTOTYPE] FSDP2: DTensor-based per-parameter-sharding FSDP&lt;/h3&gt;

&lt;p&gt;FSDP2 is a new fully sharded data parallelism implementation that uses dim-0 per-parameter sharding to resolve fundamental composability challenges with FSDP1’s flat-parameter sharding.&lt;/p&gt;

&lt;p&gt;For more information regarding the motivation / design for FSDP2 please refer to the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/114299&quot;&gt;RFC on Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-torchdistributedpipelining-simplified-pipeline-parallelism&quot;&gt;[PROTOTYPE] &lt;em&gt;torch.distributed.pipelining&lt;/em&gt;, simplified pipeline parallelism&lt;/h3&gt;

&lt;p&gt;Pipeline Parallelism is one of the primitive parallelism techniques for deep learning. It allows the execution of a model to be partitioned such that multiple micro-batches can execute different parts of the model code concurrently.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed.pipelining&lt;/code&gt; provides a toolkit that allows for easy implementation of pipeline parallelism on general models while also offering composability with other common PyTorch distributed features like DDP, FSDP, or tensor parallel.&lt;/p&gt;

&lt;p&gt;For more information on this please refer to our &lt;a href=&quot;https://pytorch.org/docs/main/distributed.pipelining.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-intel-gpu-is-available-through-source-build&quot;&gt;[PROTOTYPE] Intel GPU is available through source build&lt;/h3&gt;

&lt;p&gt;Intel GPU in PyTorch on Linux systems offers fundamental functionalities on Intel® Data Center GPU Max Series: eager mode and torch.compile.&lt;/p&gt;

&lt;p&gt;For eager mode, the commonly used Aten operators are implemented by using SYCL programming language. The most performance-critical graphs and operators are highly optimized by using oneAPI Deep Neural Network (oneDNN). For torch.compile mode, Intel GPU backend is integrated to Inductor on top of Triton.&lt;/p&gt;

&lt;p&gt;For more information for Intel GPU source build please refer to our &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-2-4-supports-gpus-accelerate-ai-workloads.html&quot;&gt;blog post&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;&lt;em&gt;torch.compile&lt;/em&gt; optimizations for AWS Graviton (aarch64-linux) processors&lt;/h3&gt;

&lt;p&gt;AWS optimized the PyTorch torch.compile feature for AWS Graviton3 processors. This optimization results in up to 2x better performance for Hugging Face model inference (based on geomean of performance improvement for 33 models) and up to 1.35x better performance for TorchBench model inference (geomean of performance improvement for 45 models) compared to the default eager mode inference across several natural language processing (NLP), computer vision (CV), and recommendation models on AWS Graviton3-based Amazon EC2 instances.&lt;/p&gt;

&lt;p&gt;For more information regarding specific technical details please refer to the &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-inference/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;bf16-symbolic-shape-optimization-in-torchinductor&quot;&gt;BF16 symbolic shape optimization in TorchInductor&lt;/h3&gt;

&lt;p&gt;Pytorch users can now experience improved quality and performance gains with the beta BF16 symbolic shape support. While static shape may afford additional optimization opportunities compared to symbolic shape, it is insufficient for scenarios such as inference services with varying batch size and sequence length, or detection models with data-dependent output shape.&lt;/p&gt;

&lt;p&gt;Verification using TorchBench, Huggingface, and timms_model shows a similar pass rate and comparable speedup with the BF16 static shape scenario. Combining the benefits of symbolic shape with BF16 AMX instructions hardware acceleration provided by Intel CPUs and general Inductor CPU backend optimizations applicable to both static and symbolic shape in PyTorch 2.4, the performance for BF16 symbolic shape has significantly improved compared to PyTorch 2.3.&lt;/p&gt;

&lt;p&gt;The API to use this feature:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;…&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dynamic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;performance-optimizations-for-genai-projects-utilizing-cpu-devices&quot;&gt;Performance optimizations for GenAI projects utilizing CPU devices&lt;/h3&gt;

&lt;p&gt;Highlighting the enhanced performance of PyTorch on CPU, as demonstrated through the optimizations made for the &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast&quot;&gt;“Segment Anything Fast”&lt;/a&gt; and &lt;a href=&quot;https://github.com/huggingface/diffusion-fast&quot;&gt;“Diffusion Fast”&lt;/a&gt; project. However, only CUDA devices are supported in the model. We have incorporated CPU support into the projects, enabling users to leverage the increased power of CPU for running the project’s experiments. Meanwhile, we have employed a &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/126961&quot;&gt;block-wise attention mask for SDPA&lt;/a&gt; as well, which can significantly reduce peak memory usage and improve performance. We have also optimized a series of &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/126961&quot;&gt;layout propagation rules in Inductor CPU&lt;/a&gt; to improve performance.&lt;/p&gt;

&lt;p&gt;To facilitate this, we have updated the README file. The API to use this feature is given below, simply providing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--device cpu&lt;/code&gt; in the command lines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For Segment Anything Fast:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
python run_experiments.py 16 vit_b &amp;lt;pytorch_github&amp;gt; &amp;lt;segment-anything_github&amp;gt;
&amp;lt;path_to_experiments_data&amp;gt; &lt;span class=&quot;nt&quot;&gt;--run-experiments&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--num-workers&lt;/span&gt; 32 &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt; cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For Diffusion Fast:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_benchmark.py &lt;span class=&quot;nt&quot;&gt;--compile_unet&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--compile_vae&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--enable_fused_projections&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users can follow the guidelines to run the experiments and observe the performance improvements firsthand, as well as explore the performance improvement trends across FP32 and BF16 data types.&lt;/p&gt;

&lt;p&gt;Additionally, users can achieve good performance using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and SDPA. By observing the performance trends across these different factors, users can gain a deeper understanding of how various optimizations enhance PyTorch’s performance on CPU.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.4 (release note)! PyTorch 2.4 adds support for the latest version of Python (3.12) for torch.compile. AOTInductor freezing gives developers running AOTInductor more performance-based optimizations by allowing the serialization of MKLDNN weights. As well, a new default TCPStore server backend utilizing libuv has been introduced which should significantly reduce initialization times for users running large-scale jobs. Finally, a new Python Custom Operator API makes it easier than before to integrate custom kernels into PyTorch, especially for torch.compile.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Dive on the Hopper TMA Unit for FP8 GEMMs</title>
      <link href="https://pytorch.org/blog/hopper-tma-unit/" rel="alternate" type="text/html" title="Deep Dive on the Hopper TMA Unit for FP8 GEMMs" />
      <published>2024-07-22T00:00:00-07:00</published>
      <updated>2024-07-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hopper-tma-unit</id>
      <content type="html" xml:base="https://pytorch.org/blog/hopper-tma-unit/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;The Hopper (H100) GPU architecture, billed as the “first truly asynchronous GPU”, includes a new, fully asynchronous hardware copy engine for bulk data movement between global and shared memory called Tensor Memory Accelerator (TMA).  While CUTLASS has &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/56b46e2d13875b46b8f6a03f9f5ac91e2bfdc01a/include/cute/arch/copy_sm90_tma.hpp&quot;&gt;built-in&lt;/a&gt; support for TMA via its asynchronous pipeline paradigm, Triton exposes TMA support via an &lt;a href=&quot;https://github.com/triton-lang/triton/blob/538556a66ee49630e1cb0b239f93e63b968b2478/python/triton/tools/experimental_descriptor.py#L25&quot;&gt;experimental API&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we provide a deeper dive into the details of how TMA works, for developers to understand the new async copy engine.  We also show the importance of leveraging TMA for H100 kernels by building a TMA enabled FP8 GEMM kernel in Triton, which delivers from 1.4-2.2x performance gains over cuBLAS FP16 for small-to-medium problem sizes.  Finally, we showcase key implementation differences between Triton and CUTLASS that may account for reports of performance regressions with TMA in Triton.  We open source our implementation for reproducibility and review at &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels&quot;&gt;https://github.com/pytorch-labs/applied-ai/tree/main/kernels&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg1.png&quot; alt=&quot;The throughput in TFLOPs of various Triton and cuBLAS FP8 and FP16 kernels, for M=M, N=4096, K=4096. The red line is the Triton TMA, which showcases the advantages of leveraging TMA.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; The throughput in TFLOPs of various Triton and cuBLAS FP8 and FP16 kernels, for M=M, N=4096, K=4096. The red line is the Triton TMA, which showcases the advantages of leveraging TMA.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;tma-background&quot;&gt;TMA Background&lt;/h2&gt;

&lt;p&gt;TMA is an H100 hardware addition that allows applications to asynchronously and bi-directionally transfer 1D-5D tensors between GPU global and shared memory.  In addition, TMA can also transfer the same data to not just the calling SM’s shared memory, but to other SM’s shared memory if they are part of the same Thread Block Cluster.  This is termed ‘multicast’.&lt;/p&gt;

&lt;p&gt;TMA is very lightweight as only a single thread is needed to kick off a TMA transfer.  By moving data directly from GMEM (global) to SMEM (shared), this avoids earlier GPU requirements of using registers for moving data between different memory spaces.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg2.png&quot; alt=&quot;A100-style data movement vs H100 with TMA.  TMA hardware eliminates the need for a large amount of threads and registers participating in bulk data transfers.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; A100-style data movement vs H100 with TMA.  TMA hardware eliminates the need for a large amount of threads and registers participating in bulk data transfers.  (Image credit Nvidia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A single thread can issue large data movement instructions, allowing the majority of a given thread block to continue working on other instructions while data is in-flight. Combined with asynchronous pipelining, this allows memory transfers to be easily hidden and ensure the majority of any given thread block cluster can focus on computational task.&lt;/p&gt;

&lt;p&gt;This lightweight invocation for data movement enables the creation of warp-group specialized kernels, where warp-groups take on different roles, namely producers and consumers. Producers elect a leader thread that fires off TMA requests, which are then asynchronously coordinated with the consumer (MMA) warp-groups via an arrival barrier.  Consumers then process the data using warp-group MMA, and signal back to the producers when they have finished reading from the SMEM buffer and the cycle repeats.&lt;/p&gt;

&lt;p&gt;Further, within threadblock clusters, producers can lower their max register requirements since they are only issuing TMA calls, and effectively transfer additional registers to MMA consumers, which helps to alleviate register pressure for consumers.&lt;/p&gt;

&lt;p&gt;In addition, TMA handles the address computation for the shared memory destination where the data requested should be placed. This is why calling threads (producers) can be so lightweight.&lt;/p&gt;

&lt;p&gt;To ensure maximum read access speed, TMA can lay out the arriving data based on swizzling instructions, to ensure the arriving data can be read as fast as possible by consumers, as the swizzling pattern helps avoid shared memory bank conflicts.&lt;/p&gt;

&lt;p&gt;Finally for TMA instructions that are outgoing, or moving data from SMEM to GMEM, TMA can also include reduction operations (add/min/max) and bitwise (and/or) operations.&lt;/p&gt;

&lt;h2 id=&quot;tma-usage-in-triton&quot;&gt;TMA usage in Triton&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Pre-Hopper Load:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;offs_m = pid_m*block_m + tl.arange(0, block_m)
offs_n = pid_n*block_n + tl.arange(0, block_n)
offs_k = tl.arange(0, block_k)

a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k[None, :]*stride_ak)
b_ptrs = b_ptr + (offs_k[:, None]*stride_bk + offs_bn[None, :]*stride_bn)

a = tl.load(a_ptrs)
b = tl.load(b_ptrs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Traditional style bulk load from global to shared memory in Triton&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the above Triton example showing a pre-Hopper load, we see how the data for tensors a and b are loaded by each thread block computing  global offsets (a_ptrs, b_ptrs) from their relevant program_id (pid_m, pid_n, k) and then making a request to move blocks of memory into shared memory for a and b.&lt;/p&gt;

&lt;p&gt;Now let’s examine how to perform a load using TMA in Triton.&lt;/p&gt;

&lt;p&gt;The TMA instruction requires a special data structure called a tensor map, in contrast to the above where we directly pass pointers to global memory. To build the tensor map, we first create a TMA descriptor on the CPU. The descriptor handles the creation of the tensor map by using the &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY&quot;&gt;cuTensorMapEncode API&lt;/a&gt;. The tensor map holds metadata such as the global and shared memory layout of the tensor and serves as a compressed representation of the structure of the multi-dimensional tensor stored in global memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg3.png&quot; alt=&quot;TMA address generation via a copy descriptor&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; TMA address generation via a copy descriptor (Image credit: Nvidia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The TMA descriptor holds the tensor’s key properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Base Pointer&lt;/li&gt;
  &lt;li&gt;Shape and Block Size&lt;/li&gt;
  &lt;li&gt;Datatype&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The TMA descriptor is created on the host before the kernel, and then moved to device by passing the descriptor to a torch tensor. Thus, in Triton, the GEMM kernel receives a global pointer to the tensor map.&lt;/p&gt;

&lt;h2 id=&quot;triton-host-code&quot;&gt;Triton Host Code&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   desc_a = np.empty(TMA_SIZE, dtype=np.int8)
   desc_b = np.empty(TMA_SIZE, dtype=np.int8)
   desc_c = np.empty(TMA_SIZE, dtype=np.int8)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(a.data_ptr(), m, k, block_m, block_k, a.element_size(), desc_a)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(b.data_ptr(), n, k, block_n, block_k, b.element_size(), desc_b)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(c.data_ptr(), m, n, block_m, block_n, c.element_size(), desc_c)
  
   desc_a = torch.tensor(desc_a, device='cuda')
   desc_b = torch.tensor(desc_b, device='cuda')
   desc_c = torch.tensor(desc_c, device='cuda')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the code that is used to set up the descriptors in the kernel invoke function.&lt;/p&gt;

&lt;h2 id=&quot;triton-device-code&quot;&gt;Triton Device Code&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Offsets/Pointer Arithmetic:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   offs_am = pid_m * block_m
   offs_bn = pid_n * block_n
   offs_k = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Load:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [block_m, block_k], tl.float8e4nv)
  b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [block_n, block_k], tl.float8e4nv)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Store:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; tl._experimental_descriptor_store(c_desc_ptr, accumulator, [offs_am, offs_bn])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We no longer need to calculate a pointer array for both load and store functions in the kernel. Instead, we pass a single descriptor pointer, the offsets, block size and the input datatype. This simplifies address calculation and reduces register pressure, as we no longer have to do complex pointer arithmetic in software and dedicate CUDA cores for address computation.&lt;/p&gt;

&lt;h2 id=&quot;tma-performance-analysis&quot;&gt;TMA Performance Analysis&lt;/h2&gt;

&lt;p&gt;Below, we discuss the PTX instructions for different load mechanisms on Hopper.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PTX for Loading Tile (cp.async) - H100 no TMA&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add.s32 	%r27, %r100, %r8;
add.s32 	%r29, %r100, %r9;
selp.b32 	%r30, %r102, 0, %p18;


@%p1 cp.async.cg.shared.global [ %r27 + 0 ], [ %rd20 + 0 ], 0x10, %r30;
@%p1 cp.async.cg.shared.global [ %r29 + 0 ], [ %rd21 + 0 ], 0x10, %r30;


cp.async.commit_group ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we observe the older cp.async instruction responsible for global memory copies. From the traces below we can see that both loads bypass the L1 cache. A major difference in the newer TMA load is that before tiles from A and B were ready to be consumed by the Tensor Core we would need to execute an ldmatrix instruction that operated on data contained in register files. On Hopper, the data can now be directly reused from shared memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg4.png&quot; alt=&quot;H100 Memory Chart showing GMEM Throughput = 910.22 GB/s&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; H100 Memory Chart showing GMEM Throughput = 910.22 GB/s (Triton GEMM &lt;strong&gt;without&lt;/strong&gt; TMA) for M=128, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By leveraging TMA through the Triton API changes we mentioned above, we can investigate the PTX that Triton generates for a single 2D tile load with TMA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PTX for Loading Tile (cp.async.bulk.tensor) - H100 using TMA&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bar.sync 	0;
shr.u32 	%r5, %r4, 5;
shfl.sync.idx.b32	%r66, %r5, 0, 31, -1;

elect.sync _|%p7, 0xffffffff;


add.s32 	%r24, %r65, %r67;
shl.b32 	%r25, %r66, 7;

@%p8
cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r24], [%rd26, {%r25,%r152}], [%r19];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The cp.async.bulk.tensor.2d.shared TMA instruction is passed the destination address in shared memory, a pointer to the tensor map, the tensor map coordinates and a pointer to the mbarrier object, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg5.png&quot; alt=&quot;H100 Memory Chart GMEM Throughput =1.45 TB/s&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; H100 Memory Chart GMEM Throughput =1.45 TB/s (Triton GEMM &lt;strong&gt;with&lt;/strong&gt; TMA) for M=128, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For optimal performance we tuned the TMA GEMM kernel extensively. Amongst other parameters such as tile sizes, number of warps and number of pipeline stages, the biggest increase in memory throughput  was observed when we increased the TMA_SIZE (descriptor size) from 128 to 512. From the above NCU profiles, we can see that the final tuned kernel has increased global memory transfer throughput from 910 GB/s to 1.45 TB/s, a &lt;strong&gt;59%&lt;/strong&gt; increase in GMEM throughput, over the non-TMA Triton GEMM kernel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of CUTLASS and Triton FP8 GEMM and TMA Implementation - Kernel Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg6.png&quot; alt=&quot;Triton vs CUTLASS Ping-Pong FP8 GEMM TFLOPs, M=M, N=4096, K=4096&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Triton vs CUTLASS Ping-Pong FP8 GEMM TFLOPs, M=M, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The above chart shows the performance of a CUTLASS &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp&quot;&gt;Ping-Pong GEMM kernel&lt;/a&gt; against Triton. The Ping-Pong kernel leverages TMA differently than Triton. It makes use of all of its HW and SW software capabilities, while Triton currently does not. Specifically, CUTLASS supports the below TMA features that help explain the performance gaps in pure GEMM performance:.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;TMA Multicast&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables copy of data from GMEM to multiple SMs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warp Specialization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables warp groups within a threadblock to take on different roles&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tensor Map (TMA Descriptor) Prefetch&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables prefetching the Tensor Map object from GMEM, which allows pipelining of TMA loads&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To put the performance numbers in perspective, below we show a ‘speed-up’ chart highlighting the latency differences on a percentage basis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg7.png&quot; alt=&quot;% Speedup of CUTLASS Ping-Pong vs Triton FP8 with TMA.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; % Speedup of CUTLASS Ping-Pong vs Triton FP8 with TMA.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This speedup is purely kernel throughput, not including E2E launch overhead which we will discuss below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TMA Descriptor movement - a key difference between Triton and CUTLASS with E2E performance implications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As noted previously, creation of a 2D+ dimensional TMA descriptor takes place on the host and is then transferred to the device.  However, this transfer process takes place very differently depending on the implementation.&lt;/p&gt;

&lt;p&gt;Here we showcase the differences between how Triton transfers TMA descriptors compared with CUTLASS.&lt;/p&gt;

&lt;p&gt;Recall, TMA transfers require a special data structure, a tensor map to be created on CPU through the cuTensorMap API, which for an FP8 GEMM Kernel means creating three descriptors, one for each A, B and C. We see below that for both the Triton and CUTLASS Kernels the same CPU procedures are invoked.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg8.png&quot; alt=&quot;Calls to cuTensorMapEncodeTiled (Both Triton and CUTLASS use this path)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Calls to cuTensorMapEncodeTiled (Both Triton and CUTLASS use this path)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, for Triton, each descriptor is transferred in its own distinct copy kernel, which adds a significant amount of overhead and serves as a barrier to use this kernel in an end-to-end use inference scenario.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg9.png&quot; alt=&quot;Three H2D Copy Kernels are launched before the kernel execution, for A, B and C&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Three H2D Copy Kernels are launched before the kernel execution, for A, B and C&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These copies are not observed in the CUTLASS implementation, due to the way that TMA descriptors are passed to the kernel. We can see from the PTX below that with Cutlass, tensor maps are passed-by-value to the kernel.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.entry _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_6half_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEENS7_ILi128EEES9_EEENS6_IJNS7_ILi2EEENS7_ILi1EEESC_EEENS_4gemm32KernelTmaWarpSpecializedPingpongENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE(

.param .align 64 .b8 _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_6half_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEENS7_ILi128EEES9_EEENS6_IJNS7_ILi2EEENS7_ILi1EEESC_EEENS_4gemm32KernelTmaWarpSpecializedPingpongENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE_param_0[1024]


mov.b64 	%rd110, _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_10bfloat16_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEES8_NS7_ILi256EEEEEENS6_IJNS7_ILi1EEESB_SB_EEENS_4gemm24KernelTmaWarpSpecializedENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE_param_0;

add.s64 	%rd70, %rd110, 704;
cvta.param.u64 	%rd69, %rd70;

cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd69, {%r284, %r283}], [%r1880];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; CUTLASS kernel PTX showing pass-by-value&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By directly passing the TMA Descriptor as opposed to passing a global memory pointer, the CUTLASS kernel avoids the three extra H2D copy kernels and instead these copies are included in the single device kernel launch for the GEMM.&lt;/p&gt;

&lt;p&gt;Because of the difference in how descriptors are moved to the device, the kernel latencies including the time to prepare the tensors to be consumed by the TMA is drastically different.  For M=1-128, N=4096, K=4096 the CUTLASS pingpong kernel has an average latency of 10us Triton TMA kernels complete in an average of 4ms.  This is a factor of ~3330x slower and appears to be directly linked to the 3 independent kernel launches for TMA descriptor transfer by Triton.&lt;/p&gt;

&lt;p&gt;Cuda graphs may be one way to reduce this, but given the overhead created by the H2D copies the current Triton implementation when measured end to end is not competitive.  A rework of how the Triton compiler manages TMA descriptors would likely resolve this gap.  We thus focused on comparing the actual compute kernel throughput and not E2E in our data above.&lt;/p&gt;

&lt;h2 id=&quot;results-summary&quot;&gt;Results Summary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg10.png&quot; alt=&quot;Triton FP8 TMA GEMM TFLOPs Comparison&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; Triton FP8 TMA GEMM TFLOPs Comparison&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;mt-5 table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;M
   &lt;/td&gt;
   &lt;td&gt;Triton TMA
   &lt;/td&gt;
   &lt;td&gt;Triton Tutorial
   &lt;/td&gt;
   &lt;td&gt;Triton SplitK 
   &lt;/td&gt;
   &lt;td&gt;cuBLAS FP8 
   &lt;/td&gt;
   &lt;td&gt;cuBLAS FP16 
   &lt;/td&gt;
   &lt;td&gt;CUTLASS Ping-Pong FP8
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2.5
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2.4
   &lt;/td&gt;
   &lt;td&gt;1.5
   &lt;/td&gt;
   &lt;td&gt;1.8
   &lt;/td&gt;
   &lt;td&gt;3.57
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;5.1
   &lt;/td&gt;
   &lt;td&gt;2.5
   &lt;/td&gt;
   &lt;td&gt;4.8
   &lt;/td&gt;
   &lt;td&gt;3.1
   &lt;/td&gt;
   &lt;td&gt;3.6
   &lt;/td&gt;
   &lt;td&gt;5.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;10.3
   &lt;/td&gt;
   &lt;td&gt;7.21
   &lt;/td&gt;
   &lt;td&gt;9.6
   &lt;/td&gt;
   &lt;td&gt;6.1
   &lt;/td&gt;
   &lt;td&gt;7.2
   &lt;/td&gt;
   &lt;td&gt;14.3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;21.0
   &lt;/td&gt;
   &lt;td&gt;16.5
   &lt;/td&gt;
   &lt;td&gt;19.2
   &lt;/td&gt;
   &lt;td&gt;12.3
   &lt;/td&gt;
   &lt;td&gt;14.4
   &lt;/td&gt;
   &lt;td&gt;28.6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;44.5
   &lt;/td&gt;
   &lt;td&gt;41.0
   &lt;/td&gt;
   &lt;td&gt;37.2
   &lt;/td&gt;
   &lt;td&gt;24.5
   &lt;/td&gt;
   &lt;td&gt;27.7
   &lt;/td&gt;
   &lt;td&gt;55.1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;89.7
   &lt;/td&gt;
   &lt;td&gt;81.2
   &lt;/td&gt;
   &lt;td&gt;72.2
   &lt;/td&gt;
   &lt;td&gt;71.6
   &lt;/td&gt;
   &lt;td&gt;56.8
   &lt;/td&gt;
   &lt;td&gt;114.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;178.5
   &lt;/td&gt;
   &lt;td&gt;163.7
   &lt;/td&gt;
   &lt;td&gt;130.8
   &lt;/td&gt;
   &lt;td&gt;144.6
   &lt;/td&gt;
   &lt;td&gt;105.3
   &lt;/td&gt;
   &lt;td&gt;228.7
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;359.7
   &lt;/td&gt;
   &lt;td&gt;225.9
   &lt;/td&gt;
   &lt;td&gt;160.1
   &lt;/td&gt;
   &lt;td&gt;244.0
   &lt;/td&gt;
   &lt;td&gt;189.2
   &lt;/td&gt;
   &lt;td&gt;377.7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; Triton FP8 TMA GEMM TFLOPs Comparison Table&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The above chart and table summarize the gain we’ve been able to achieve on a single NVIDIA H100 for FP8 GEMM, by leveraging the TMA Hardware Unit, over non-TMA Triton kernels and high performance CUDA (cuBLAS) kernels. The key point to note is this kernel’s superior scaling (with the batch size) properties over the competition. The problem sizes we benchmarked on are representative of the matrix shapes found in small-to-medium batch size LLM inference. Thus, TMA GEMM kernel performance in the mid-M regime (M=32 to M=128) will be critical for those interested in leveraging this kernel for FP8 LLM deployment use cases, as the FP8 compressed data type can allow larger matrices to fit in GPUs memory.&lt;/p&gt;

&lt;p&gt;To summarize our analysis, the TMA implementation in Triton and CUTLASS differ in terms of full featureset support (multicast, prefetch etc.) and how the TMA Descriptor is passed to the GPU kernel. If this descriptor is passed in a manner that more closely matches the CUTLASS kernel (pass-by-value), the extraneous H2D copies could be avoided and thus the E2E performance would be greatly improved.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;For future research, we plan to improve upon these results, by working with the community to incorporate the CUTLASS architecture of TMA loads into Triton as well as investigating the Cooperative Kernel for FP8 GEMM, a modified strategy to the Ping-Pong Kernel.&lt;/p&gt;

&lt;p&gt;In addition, once features like thread block clusters and TMA atomic operations are enabled in Triton, we may be able to get further speedups by leveraging the SplitK strategy in the TMA GEMM Kernel, as atomic operations on Hopper can be performed in Distributed Shared Memory (DSMEM) as opposed to L2 Cache.  We also note the similarities of NVIDIA Hopper GPUs with other AI hardware accelerators like Google’s &lt;a href=&quot;https://people.csail.mit.edu/suvinay/pubs/2023.tpu.isca.pdf&quot;&gt;TPU&lt;/a&gt; and IBM’s &lt;a href=&quot;https://ieeexplore.ieee.org/document/9499865&quot;&gt;AIU&lt;/a&gt; which are dataflow architectures. On Hopper, data can now “flow” from GMEM to a network of connected SMs due to the additions of TMA, which we discussed extensively in this blog, and DSMEM, which we plan to cover in a future post.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Chih-Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</title>
      <link href="https://pytorch.org/blog/flashattention-3/" rel="alternate" type="text/html" title="FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision" />
      <published>2024-07-11T00:00:00-07:00</published>
      <updated>2024-07-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flashattention-3</id>
      <content type="html" xml:base="https://pytorch.org/blog/flashattention-3/">&lt;p&gt;Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;libraries&lt;/a&gt; to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (&lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k&quot;&gt;Llama 3&lt;/a&gt;). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.&lt;/p&gt;

&lt;p&gt;We’re excited to release FlashAttention-3 that incorporates these techniques. It’s 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.&lt;/p&gt;

&lt;p&gt;FlashAttention-3 is available at: &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;https://github.com/Dao-AILab/flash-attention&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://tridao.me/publications/flash3/flash3.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;flashattention-recap&quot;&gt;FlashAttention Recap&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. We use tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.&lt;/p&gt;

&lt;p&gt;Here we show a diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg1.png&quot; alt=&quot;math equations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-hardware-features-on-hopper-gpus---wgmma-tma-fp8&quot;&gt;New hardware features on Hopper GPUs - WGMMA, TMA, FP8&lt;/h2&gt;

&lt;p&gt;While FlashAttention-2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take advantage of new features on Hopper GPUs to maximize performance. We describe some of the new Hopper-specific features here, and why they are important.&lt;/p&gt;

&lt;p&gt;1. WGMMA (Warpgroup Matrix Multiply-Accumulate). This new feature makes use of the new Tensor Cores on Hopper, with much higher throughput&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; than the older mma.sync instruction in Ampere (image from the &lt;a href=&quot;https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid&quot;&gt;H100 white paper)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg2.png&quot; alt=&quot;image from the H100 white paper&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2. TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable resource to increase tile size and efficiency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg3.png&quot; alt=&quot;block diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3. Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg4.png&quot; alt=&quot;6x throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FlashAttention-3 makes use of all of these new features of Hopper, using powerful abstractions from &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;NVIDIA’s CUTLASS&lt;/a&gt; library. &lt;br /&gt;
 &lt;br /&gt;
By rewriting FlashAttention to use these new features, we can already significantly speed it up (e.g., from 350 TFLOPS in FlashAttention-2 FP16 forward pass to around 540-570 TFLOPS). However, the asynchronous nature of the new instructions on Hopper (WGMMA and TMA) opens up additional algorithmic opportunities to overlap operations and thereby extract even greater performance. For this blogpost, we’ll explain two such techniques specific to attention. The generic technique of warp specialization, with separate producer and consumer warps doing TMA and WGMMA, is &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization&quot;&gt;well-covered elsewhere&lt;/a&gt; in the context of GEMM and works the same here.&lt;/p&gt;

&lt;h2 id=&quot;asynchrony-overlapping-gemm-and-softmax&quot;&gt;Asynchrony: Overlapping GEMM and Softmax&lt;/h2&gt;

&lt;p&gt;Why overlap?&lt;/p&gt;

&lt;p&gt;Attention has GEMMs (those matmuls between Q and K and between attention probability P and V) and softmax as its two main operations. Why do we need to overlap them? Isn’t most of the FLOPS in the GEMMs anyway? As long as the GEMMs are fast (e.g., computed using WGMMA instructions), shouldn’t the &lt;a href=&quot;https://horace.io/brrr_intro.html&quot;&gt;GPU be going brrrr&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;The problem is that non-matmul operations are much slower than matmul operations on modern accelerators. Special functions such as exponential (for the softmax) have even lower throughput than floating point multiply-add; they are evaluated by the multi-function unit, a unit separate from floating point multiply-add or matrix multiply-add. As an example, the H100 GPU SXM5 has 989 TFLOPS of FP16 matrix multiply, but only 3.9 TFLOPS (256x less throughput) for special functions&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;! For head dimension 128, there are 512x more matmul FLOPS than exponential, which means that exponential can take 50% of the time compared to matmul. The situation is even worse for FP8, where the matmul FLOPS are twice as fast yet exponential FLOPS stay the same speed. Ideally we want matmul and softmax to operate in parallel. While the Tensor Cores are busy with matmul, the multi-function units should be calculating exponential!&lt;/p&gt;

&lt;h3 id=&quot;inter-warpgroup-overlapping-with-pingpong-scheduling&quot;&gt;Inter-warpgroup overlapping with pingpong scheduling&lt;/h3&gt;

&lt;p&gt;The first and easiest way to overlap GEMM and softmax is to do nothing at all! The warp schedulers already try to schedule warps so that if some warps are blocked (e.g., waiting for GEMM results), other warps can run. That is, the warp schedulers do some of this overlapping for us, for free.&lt;/p&gt;

&lt;p&gt;However, we can improve on this by doing some of the scheduling manually. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. This “pingpong” schedule is illustrated in the figure below, where the same color denotes the same iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg5.png&quot; alt=&quot;block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This would allow us to perform the softmax in the shadow of the GEMMs of the other warpgroup. Of course, this figure is just a caricature; in practice the scheduling is not really this clean. Nevertheless, pingpong scheduling can improve FP16 attention forward pass from around 570 TFLOPS to 620 TFLOPS (head dim 128, seqlen 8K).&lt;/p&gt;

&lt;h3 id=&quot;intra-warpgroup-overlapping-of-gemm-and-softmax&quot;&gt;Intra-warpgroup overlapping of GEMM and Softmax&lt;/h3&gt;

&lt;p&gt;Even within one warpgroup, we can have some part of softmax running while the GEMMs of that warpgroup is running. This is illustrated in this figure, where the same color denotes the same iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6.png&quot; alt=&quot;block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This pipelining increases throughput from around 620 TFLOPS to around 640-660 TFLOPS for FP16 attention forward, at the cost of higher register pressure. We need more registers to hold both accumulators of the GEMMs, and the input/output of softmax. Overall, we find this technique to offer a favorable tradeoff.&lt;/p&gt;

&lt;h2 id=&quot;low-precision-reduce-quantization-error-with-incoherent-processing&quot;&gt;Low-precision: reduce quantization error with incoherent processing&lt;/h2&gt;

&lt;p&gt;LLM activation can have &lt;a href=&quot;https://arxiv.org/abs/2208.07339&quot;&gt;outliers&lt;/a&gt; with much larger magnitude than the rest of the features. These outliers make it difficult to quantize, producing much larger quantization errors. We leverage incoherent processing, a technique used in the quantization literature (e.g. from &lt;a href=&quot;https://arxiv.org/abs/2307.13304&quot;&gt;QuIP&lt;/a&gt;) that multiplies the query and key with a random orthogonal matrix to “spread out” the outliers and reduce quantization error. In particular, we use the Hadamard transform (with random signs), which can be done per attention head in O(d log d) instead of O(d^2) time, where d is the head dimension. Since the Hadamard transform is memory-bandwidth bound, it can be fused with previous operations such as rotary embedding (also memory-bandwidth bound) “for free”.&lt;/p&gt;

&lt;p&gt;In our experiment where Q, K, V are generated from a standard normal distribution but 0.1% of the entries have large magnitudes (to simulate outliers), we found that incoherent processing can reduce the quantization error by 2.6x. We show numerical error comparison in the table below. Please see the paper for details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6a.png&quot; alt=&quot;text diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;attention-benchmark&quot;&gt;Attention benchmark&lt;/h2&gt;

&lt;p&gt;We show some results with FlashAttention-3, and compare it to FlashAttention-2, as well as the implementation in Triton and cuDNN (both of which already use new hardware features of Hopper GPUs).&lt;/p&gt;

&lt;p&gt;For FP16, we see about 1.6x-1.8x speedup over FlashAttention-2&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg7.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg8.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For FP8, we can reach close to 1.2 PFLOPS!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg9.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;This blogpost highlights some of the optimizations for FlashAttention available on Hopper GPUs. Other optimizations (e.g., variable length sequences, persistent kernel, and in-kernel transpose for FP8) are covered in the paper.&lt;/p&gt;

&lt;p&gt;We have seen that designing algorithms that take advantage of the hardware they run on can bring significant efficiency gains and unlock new model capabilities such as long context. We look forward to future work on optimization for LLM inference, as well as generalizing our techniques to other hardware architectures.&lt;/p&gt;

&lt;p&gt;We also look forward to FlashAttention-3 being integrated in a future release of PyTorch.&lt;/p&gt;

&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Without the wgmma instruction, the older mma.sync instruction can only reach about ⅔ the peak throughput of Hopper Tensor Cores: https://arxiv.org/abs/2402.13499v1 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;The CUDA programming guide specifies that the throughput for special functions is 16 operations per streaming multiprocessor (SM) per clock cycle. We multiply 16 by 132 SMs and 1830 Mhz (clock speed used to calculate 989 TFLOPS of FP16 matmul) to get 3.9 TFLOPS &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Jay Shah and Ganesh Bikshandi, Colfax Research, Ying Zhang, Meta, Vijay Thakkar and Pradeep Ramani, NVIDIA, Tri Dao, TogetherAI and Princeton University</name>
        
        
      </author>

      

      

      
        <summary type="html">Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Learn how to develop Android applications with ExecuTorch and Llama models</title>
      <link href="https://pytorch.org/blog/develop-android-applications/" rel="alternate" type="text/html" title="Learn how to develop Android applications with ExecuTorch and Llama models" />
      <published>2024-07-10T00:00:00-07:00</published>
      <updated>2024-07-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/develop-android-applications</id>
      <content type="html" xml:base="https://pytorch.org/blog/develop-android-applications/">&lt;p&gt;&lt;em&gt;This blog is courtesy of the PyTorch team at Arm. More details can be found &lt;a href=&quot;https://learn.arm.com/learning-paths/smartphones-and-mobile/build-llama3-chat-android-app-using-executorch-and-xnnpack/?utm_source=twitter&amp;amp;utm_medium=social-organic&amp;amp;utm_content=landingpage&amp;amp;utm_campaign=mk24_developer_na&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Arm’s compute platform is delivering GenAI applications on phones, laptops, and servers. Cost, privacy, performance, security, and energy efficiency are just some of the reasons developers are investigating on-device AI.&lt;/p&gt;

&lt;p&gt;A new Learning Path explaining how to leverage the capabilities of large language models (LLMs) on Android using ExecuTorch and XNNPACK is now available.&lt;/p&gt;

&lt;p&gt;Here’s a summary of what you’ll learn:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Development Environment setup&lt;/p&gt;

    &lt;p&gt;The Learning Path begins by guiding you through setting up your development environment, ensuring you have all the necessary tools installed, including Android Studio, the Android NDK, Java JDK, and Python.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ExecuTorch and XNNPACK&lt;/p&gt;

    &lt;p&gt;You’ll learn about the core technologies: ExecuTorch, a framework for deploying PyTorch models to edge devices, and XNNPACK, a high-performance library for executing neural networks on Arm-based platforms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Llama models&lt;/p&gt;

    &lt;p&gt;The Learning Path explores Llama, a family of powerful LLMs, focusing specifically on the 8B Llama 3 model. You’ll learn about quantization techniques, which are essential for optimizing model size and performance on mobile devices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prepare Llama models for ExecuTorch&lt;/p&gt;

    &lt;p&gt;You’ll be guided through the process of downloading, exporting, and evaluating Llama models, ensuring they are ready for deployment using ExecuTorch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check model performance on Android&lt;/p&gt;

    &lt;p&gt;The Learning Path walks you through cross-compiling the Llama runner binary for Android, allowing you to test your model’s performance on your phone.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Build and run an Android Chat App&lt;/p&gt;

    &lt;p&gt;Finally, you’ll learn how to build a native Android chat app using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaDemo&lt;/code&gt; application from the ExecuTorch repository. This hands-on experience allows you to put your knowledge into practice and create a real-world application.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Explore this Learning Path if you want to learn how to leverage the power of LLMs on your Android phone, and gain expertise in tools for on-device machine learning.&lt;/p&gt;

&lt;p&gt;Dig into the excitement of building Android chat apps and understand more about how they work on the &lt;a href=&quot;https://learn.arm.com/learning-paths/smartphones-and-mobile/build-llama3-chat-android-app-using-executorch-and-xnnpack/?utm_source=twitter&amp;amp;utm_medium=social-organic&amp;amp;utm_content=landingpage&amp;amp;utm_campaign=mk24_developer_na&quot;&gt;Arm Developer Hub&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">This blog is courtesy of the PyTorch team at Arm. More details can be found here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated PyTorch inference with torch.compile on AWS Graviton processors</title>
      <link href="https://pytorch.org/blog/accelerated-pytorch-inference/" rel="alternate" type="text/html" title="Accelerated PyTorch inference with torch.compile on AWS Graviton processors" />
      <published>2024-07-09T00:00:00-07:00</published>
      <updated>2024-07-09T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-pytorch-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-pytorch-inference/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Originally PyTorch, used an eager mode where each PyTorch operation that forms the model is run independently as soon as it’s reached. PyTorch 2.0 introduced &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;torch.compile&lt;/a&gt; to speed up PyTorch code over the default eager mode. In contrast to eager mode, the torch.compile pre-compiles the entire model into a single graph in a manner that’s optimal for running on a given hardware platform. AWS optimized the PyTorch torch.compile feature for &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2022/05/amazon-ec2-c7g-instances-powered-aws-graviton3-processors/&quot;&gt;AWS Graviton3 processors&lt;/a&gt;. This optimization results in up to 2x better performance for &lt;a href=&quot;https://huggingface.co/models&quot;&gt;Hugging Face&lt;/a&gt; model inference (based on geomean of performance improvement for 33 models) and up to 1.35x better performance for &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; model inference (geomean of performance improvement for 45 models) compared to the default eager mode inference across several natural language processing (NLP), computer vision (CV), and recommendation models on AWS Graviton3-based Amazon EC2 instances. Starting with PyTorch 2.3.1, the optimizations are available in torch Python &lt;a href=&quot;https://pypi.org/project/torch/2.3.1/&quot;&gt;wheels&lt;/a&gt; and AWS Graviton PyTorch &lt;a href=&quot;https://github.com/aws/deep-learning-containers/blob/master/available_images.md#ec2-framework-graviton-containers-ec2-ecs-and-eks-support-only&quot;&gt;deep learning container (DLC)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we show how we optimized torch.compile performance on AWS Graviton3-based EC2 instances, how to use the optimizations to improve inference performance, and the resulting speedups.&lt;/p&gt;

&lt;h2 id=&quot;why-torchcompile-and-whats-the-goal&quot;&gt;Why torch.compile and what’s the goal?&lt;/h2&gt;

&lt;p&gt;In eager mode, operators in a model are run immediately as they are encountered. It’s easier to use, more suitable for machine learning (ML) researchers, and hence is the default mode. However, eager mode incurs runtime overhead because of redundant kernel launch and memory read overhead. Whereas in torch compile mode, operators are first synthesized into a graph, wherein one operator is merged with another to reduce and localize memory reads and total kernel launch overhead.&lt;/p&gt;

&lt;p&gt;The goal for the AWS Graviton team was to optimize torch.compile backend for Graviton3 processors. PyTorch eager mode was already optimized for Graviton3 processors with &lt;a href=&quot;https://github.com/ARM-software/ComputeLibrary&quot;&gt;Arm Compute Library (ACL)&lt;/a&gt; kernels using oneDNN (also known as MKLDNN). So, the question was, how to reuse those kernels in torch.compile mode to get the best of graph compilation and the optimized kernel performance together?&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The AWS Graviton team extended the torch inductor and oneDNN primitives that reused the ACL kernels and optimized compile mode performance on Graviton3 processors. Starting with PyTorch 2.3.1, the optimizations are available in the torch Python wheels and AWS Graviton DLC. Please see the &lt;strong&gt;Running an inference&lt;/strong&gt; section that follows for the instructions on installation, runtime configuration, and how to run the tests.&lt;/p&gt;

&lt;p&gt;To demonstrate the performance improvements, we used NLP, CV, and recommendation models from &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; and the most downloaded NLP models from &lt;a href=&quot;https://huggingface.co/models&quot;&gt;Hugging Face&lt;/a&gt; across Question Answering, Text Classification, Token Classification, Translation, Zero-Shot Classification, Translation, Summarization, Feature Extraction, Text Generation, Text2Text Generation, Fill-Mask, and Sentence Similarity tasks to cover a wide variety of customer use cases.&lt;/p&gt;

&lt;p&gt;We started with measuring TorchBench model inference latency, in milliseconds (msec), for the eager mode, which is marked 1.0 with a red dotted line in the following graph. Then we compared the improvements from torch.compile for the same model inference, the normalized results are plotted in the graph. You can see that for the 45 models we benchmarked, there is a 1.35x latency improvement (geomean for the 45 models).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg1.png&quot; alt=&quot;PyTorch model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using TorchBench framework&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: PyTorch model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using TorchBench framework. The reference eager mode performance is marked as 1.0. (higher is better)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Similar to the preceding TorchBench inference performance graph, we started with measuring the Hugging Face NLP model inference latency, in msec, for the eager mode, which is marked 1.0 with a red dotted line in the following graph. Then we compared the improvements from torch.compile for the same model inference, the normalized results are plotted in the graph. You can see that for the 33 models we benchmarked, there is around 2x performance improvement (geomean for the 33 models).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg2.png&quot; alt=&quot;Hugging Face NLP model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using Hugging Face example scripts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Hugging Face NLP model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using Hugging Face example scripts. The reference eager mode performance is marked as 1.0. (higher is better)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-an-inference&quot;&gt;Running an inference&lt;/h2&gt;

&lt;p&gt;Starting with PyTorch 2.3.1, the optimizations are available in the torch Python wheel and in AWS Graviton PyTorch DLC. This section shows how to run inference in eager and torch.compile modes using torch Python wheels and benchmarking scripts from Hugging Face and TorchBench repos.&lt;/p&gt;

&lt;p&gt;To successfully run the scripts and reproduce the speedup numbers mentioned in this post, you need an instance from the Graviton3 family (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c7g/r7g/m7g/hpc7g&lt;/code&gt;) of hardware. For this post, we used the &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c7g/&quot;&gt;c7g.4xl (16 vcpu) instance&lt;/a&gt;. The instance, the AMI details, and the required torch library versions are mentioned in the following snippet.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Instance: c7g.4xl instance
Region: us-west-2
AMI: ami-05cc25bfa725a144a﻿ (Ubuntu 22.04/Jammy with 6.5.0-1017-aws kernel)

# Install Python
sudo apt-get update
sudo apt-get install -y python3 python3-pip

# Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install PyTorch and extensions
python3 -m pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The generic runtime tunings implemented for eager mode inference are equally applicable for the torch.compile mode, so, we set the following environment variables to further improve the torch.compile performance on AWS Graviton3 processors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Enable the fast math GEMM kernels, to accelerate fp32 inference with bfloat16 gemm
export DNNL_DEFAULT_FPMATH_MODE=BF16

# Enable Linux Transparent Huge Page (THP) allocations,
# to reduce the tensor memory allocation latency
export THP_MEM_ALLOC_ENABLE=1

# Set LRU Cache capacity to cache the primitives and avoid redundant
# memory allocations
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;torchbench-benchmarking-scripts&quot;&gt;TORCHBENCH BENCHMARKING SCRIPTS&lt;/h4&gt;

&lt;p&gt;TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance. We benchmarked 45 models using the scripts from the TorchBench repo. Following code shows how to run the scripts for the eager mode and the compile mode with inductor backend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Set OMP_NUM_THREADS to number of vcpus, 16 for c7g.4xl instance
export OMP_NUM_THREADS=16

# Install the dependencies
sudo apt-get install -y libgl1-mesa-glx
sudo apt-get install -y libpangocairo-1.0-0
python3 -m pip install psutil numpy transformers pynvml numba onnx onnxruntime scikit-learn timm effdet gym doctr opencv-python h5py==3.10.0 python-doctr 

# Clone pytorch benchmark repo
git clone https://github.com/pytorch/benchmark.git
cd benchmark
# PyTorch benchmark repo doesn't have any release tags. So,
# listing the commit we used for collecting the performance numbers
git checkout 9a5e4137299741e1b6fb7aa7f5a6a853e5dd2295

# Setup the models
python3 install.py 

# Colect eager mode performance using the following command. The results will be
# stored at .userbenchmark/cpu/metric-&amp;lt;timestamp&amp;gt;.json.
python3 run_benchmark.py cpu --model BERT_pytorch,hf_Bert,hf_Bert_large,hf_GPT2,hf_Albert,hf_Bart,hf_BigBird,hf_DistilBert,hf_GPT2_large,dlrm,hf_T5,mnasnet1_0,mobilenet_v2,mobilenet_v3_large,squeezenet1_1,timm_efficientnet,shufflenet_v2_x1_0,timm_regnet,resnet50,soft_actor_critic,phlippe_densenet,resnet152,resnet18,resnext50_32x4d,densenet121,phlippe_resnet,doctr_det_predictor,timm_vovnet,alexnet,doctr_reco_predictor,vgg16,dcgan,yolov3,pytorch_stargan,hf_Longformer,timm_nfnet,timm_vision_transformer,timm_vision_transformer_large,nvidia_deeprecommender,demucs,tts_angular,hf_Reformer,pytorch_CycleGAN_and_pix2pix,functorch_dp_cifar10,pytorch_unet --test eval --metrics=&quot;latencies,cpu_peak_mem&quot;

# Collect torch.compile mode performance with inductor backend
# and weights pre-packing enabled. The results will be stored at
# .userbenchmark/cpu/metric-&amp;lt;timestamp&amp;gt;.json
python3 run_benchmark.py cpu --model BERT_pytorch,hf_Bert,hf_Bert_large,hf_GPT2,hf_Albert,hf_Bart,hf_BigBird,hf_DistilBert,hf_GPT2_large,dlrm,hf_T5,mnasnet1_0,mobilenet_v2,mobilenet_v3_large,squeezenet1_1,timm_efficientnet,shufflenet_v2_x1_0,timm_regnet,resnet50,soft_actor_critic,phlippe_densenet,resnet152,resnet18,resnext50_32x4d,densenet121,phlippe_resnet,doctr_det_predictor,timm_vovnet,alexnet,doctr_reco_predictor,vgg16,dcgan,yolov3,pytorch_stargan,hf_Longformer,timm_nfnet,timm_vision_transformer,timm_vision_transformer_large,nvidia_deeprecommender,demucs,tts_angular,hf_Reformer,pytorch_CycleGAN_and_pix2pix,functorch_dp_cifar10,pytorch_unet --test eval --torchdynamo inductor --freeze_prepack_weights --metrics=&quot;latencies,cpu_peak_mem&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On successful completion of the inference runs, the script stores the results in JSON format. The following is the sample output:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
 &quot;name&quot;: &quot;cpu&quot;
 &quot;environ&quot;: {
     &quot;pytorch_git_version&quot;: &quot;d44533f9d073df13895333e70b66f81c513c1889&quot;
  },
  
  &quot;metrics&quot;: {
       &quot;BERT_pytorch-eval_latency&quot;: 56.3769865,
       &quot;BERT_pytorch-eval_cmem&quot;: 0.4169921875
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;hugging-face-benchmarking-scripts&quot;&gt;HUGGING FACE BENCHMARKING SCRIPTS&lt;/h4&gt;

&lt;p&gt;Google T5 Small Text Translation model is one of the around 30 Hugging Face models we benchmarked. We’re using it as a sample model to demonstrate how to run inference in eager and compile modes. The additional configurations and APIs required to run it in compile mode are highlighted in &lt;strong&gt;BOLD&lt;/strong&gt;. Save the following script as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google_t5_small_text_translation.py&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import argparse
from transformers import T5Tokenizer, T5Model
import torch
from torch.profiler import profile, record_function, ProfilerActivity
&lt;b&gt;import torch._inductor.config as config
config.cpp.weight_prepack=True
config.freezing=True&lt;/b&gt;

def test_inference(mode, num_iter):
    tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
    model = T5Model.from_pretrained(&quot;t5-small&quot;)

    input_ids = tokenizer(
        &quot;Studies have been shown that owning a dog is good for you&quot;, return_tensors=&quot;pt&quot;
    ).input_ids  # Batch size 1
    decoder_input_ids = tokenizer(&quot;Studies show that&quot;, return_tensors=&quot;pt&quot;).input_ids  # Batch size 1

    &lt;b&gt;if (mode == 'compile'):
        model = torch.compile(model)&lt;/b&gt;

    with torch.no_grad():
        for _ in range(50):
            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

        with profile(activities=[ProfilerActivity.CPU]) as prof:
            with record_function(&quot;model_inference&quot;):
                for _ in range(num_iter):
                    outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

    print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))

def main() -&amp;gt; None:
    global m, args
    parser = argparse.ArgumentParser(__doc__)
    parser.add_argument(
        &quot;-m&quot;,
        &quot;--mode&quot;,
        choices=[&quot;eager&quot;, &quot;compile&quot;],
        default=&quot;eager&quot;,
        help=&quot;Which test to run.&quot;,
    )
    parser.add_argument(
        &quot;-n&quot;,
        &quot;--number&quot;,
        type=int,
        default=100,
        help=&quot;how many iterations to run.&quot;,
    )
    args = parser.parse_args()
    test_inference(args.mode, args.number)

if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the script with the following steps:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Set OMP_NUM_THREADS to number of vcpus to 4 because
# the scripts are running inference in sequence, and
# they don't need large number of vcpus
export OMP_NUM_THREADS=4

# Install the dependencies
python3 -m pip install transformers

# Run the inference script in Eager mode
# using number of iterations as 1 just to show the torch profiler output
# but for the benchmarking, we used 1000 iterations.
python3 google_t5_small_text_translation.py -n 1 -m eager

# Run the inference script in torch compile mode
python3 google_t5_small_text_translation.py -n 1 -m compile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On successful completion of the inference runs, the script prints the torch profiler output with the latency breakdown for the torch operators. The following is the sample output from torch profiler:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Torch profiler output for the eager mode run on c7g.xl (4vcpu)
------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                aten::mm        40.71%      12.502ms        40.71%      12.502ms     130.229us            96  
         model_inference        26.44%       8.118ms       100.00%      30.708ms      30.708ms             1  
               aten::bmm         6.85%       2.102ms         9.47%       2.908ms      80.778us            36  
            aten::matmul         3.73%       1.146ms        57.26%      17.583ms     133.205us           132  
            aten::select         1.88%     576.000us         1.90%     583.000us       0.998us           584  
         aten::transpose         1.51%     464.000us         1.83%     563.000us       3.027us           186  
------------------------ ------------ ------------ ------------ ------------ ------------ -------------------
Self CPU time total: 30.708ms

# Torch profiler output for the compile mode run for the same model on the same instance
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
        mkldnn::_linear_pointwise        37.98%       5.461ms        45.91%       6.602ms      68.771us            96  
            Torch-Compiled Region        29.56%       4.251ms        98.53%      14.168ms      14.168ms             1  
                        aten::bmm        14.90%       2.143ms        21.73%       3.124ms      86.778us            36  
                     aten::select         4.51%     648.000us         4.62%     665.000us       1.155us           576  
                       aten::view         3.29%     473.000us         3.29%     473.000us       1.642us           288  
                      aten::empty         2.53%     364.000us         2.53%     364.000us       3.165us           115  
--------------------------------- ------------ ------------ ------------ ------------ ------------ --------------------
Self CPU time total: 14.379ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;technical-deep-dive-what-are-the-challenges-and-optimization-details&quot;&gt;Technical deep dive: What are the challenges and optimization details&lt;/h2&gt;

&lt;p&gt;Underpinning torch.compile are new technologies – TorchDynamo, AOTDispatcher, and TorchInductor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TorchDynamo&lt;/strong&gt; captures PyTorch programs safely using Python Frame Evaluation Hooks&lt;br /&gt;
&lt;strong&gt;AOTDispatcher&lt;/strong&gt; overloads PyTorch’s autograd engine as a tracing autodiff for generating ahead-of-time backward traces.&lt;br /&gt;
&lt;strong&gt;TorchInductor&lt;/strong&gt; is a deep learning compiler that generates fast code for multiple accelerators and backends.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg3.png&quot; alt=&quot;The PyTorch compilation process source&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: The PyTorch compilation process&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When torch.compile is invoked, torch dynamo rewrites Python bytecode to extract sequences of PyTorch operations into an &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;FX&lt;/a&gt; &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Graph&lt;/a&gt;, which is then compiled with inductor backend. For a typical inference scenario where the graph is frozen and gradient calculations are disabled, the inductor invokes platform specific optimizations like graph rewrite into more performant operators, operator fusion, and weights pre-packing.&lt;/p&gt;

&lt;p&gt;However, on Graviton3, the inductor wasn’t able to perform any of those optimizations because there was no aarch64 backend defined. To fix this, we extended the inductor’s FX passes to pick oneDNN operators for linear layer compilation on Graviton3 processors with ACL backend. The code snippet for this follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_weight_op = (
    mkldnn._reorder_linear_weight
    if (is_bf16_weight or mkldnn._is_mkldnn_acl_supported())
                    
packed_linear_inputs: Tuple[Any, ...] = (input, packed_weight_node)
if is_bf16_weight or mkldnn._is_mkldnn_acl_supported():
    packed_linear_inputs += (bias, &quot;none&quot;, [], &quot;&quot;)
    packed_linear_op = mkldnn._linear_pointwise.default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this was done, the FX pass was successful in compiling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul &lt;/code&gt;operators to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linear_pointwise &lt;/code&gt;. The following snippet highlights the matmul operator in the original model:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; %attention_scores   : [num_users=1] = call_function[target=torch.matmul](args = (%query_layer, %transpose), kwargs = {})
 %attention_scores_1 : [num_users=1] = call_function[target=operator.truediv](args = (%attention_scores, 8.0), kwargs = {})
 %attention_scores_2 : [num_users=1] = call_function[target=operator.add](args = (%attention_scores_1, %extended_attention_mask_3), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following snippet highlights the linear_pointwise operator in the compiled graph:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%_linear_pointwise_default_140 : [num_users=2] = call_function[target=torch.ops.mkldnn._linear_pointwise.default](args = (%add_7, %_frozen_param278, %_frozen_param16, none, [], ), kwargs = {})
%mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.5), kwargs = {})
%mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.7071067811865476), kwargs = {})
%erf   : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_6,), kwargs = {})
%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This completes the torch inductor changes required to compile the graph into optimized operators on AWS Graviton3 processors. Next comes the actual inference where the compiled graph is dispatched to be run. OneDNN with ACL was the backend we chose during the inductor compilation, so, the new operators were dispatched to oneDNN as expected, for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn._linear_pointwise&lt;/code&gt;. However, due to gaps in oneDNN ACL primitives, the operators were run with C++ reference kernels instead of the optimized ACL kernels. Hence, the compile performance was still significantly behind the eager mode performance.&lt;/p&gt;

&lt;p&gt;There were mainly three areas where oneDNN ACL primitives lack support for torch.compile mode. The following section talks about them in detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. ACL primitives didn’t have support for weights in blocked layout&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ACL primitives originally designed for eager mode supported weights only in the standard channels last (&lt;a href=&quot;https://oneapi-src.github.io/oneDNN/dev_guide_understanding_memory_formats.html#nhwc&quot;&gt;NHWC&lt;/a&gt;) format, without any pre-packing. Whereas weights pre-packing into blocked layout is one of the main optimizations in the inductor compilation passes where the weights are reordered into blocks specific to the runtime platform. This avoids the redundant and on-the-fly reorders when running the General Matrix Multiplication (GEMM), which otherwise would be the bottleneck for inference performance. But the ACL primitives didn’t have support for blocked layout and hence the operators were run with oneDNN C++ reference kernels instead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Mixed precision primitives weren’t supported in oneDNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS Graviton3 processors support &lt;a href=&quot;https://developer.arm.com/documentation/ddi0596/2020-12/SVE-Instructions/BFMMLA--BFloat16-floating-point-matrix-multiply-accumulate-&quot;&gt;bfloat16 MMLA instructions&lt;/a&gt; which can be used to accelerate fp32 inference with bfloat16 GEMM as a mixed precision compute. ACL supports bfloat16 mixed precision GEMM kernels, and are integrated into oneDNN as a fast math compute option for the existing fp32 operators. However, the fast math approach didn’t work for compile mode because of weights pre-packing optimization. The compile mode requires explicit mixed precision primitive implementation in oneDNN in order to use bfloat16 acceleration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. ACL primitives didn’t support fused kernels for some of the activation functions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In eager mode, operators are dispatched individually because the model is run independently as soon as it’s reached. Whereas in compile mode, operator fusion is another important optimization where the operators are fused for runtime efficiency. For example, Gaussian Error Linear Unit (&lt;a href=&quot;https://arxiv.org/pdf/1606.08415.pdf#%3A~%3Atext%3DWe%20propose%20the%20Gaussian%20Error%2Cstandard%20Gaussian%20cumulative%20distribution%20function&quot;&gt;GELU&lt;/a&gt;) is one of the most widely used activation functions in transformers-based neural network architectures. So, it’s typical to have a linear layer (with matrix multiplications) followed by GELU activation. As part of compiling the model into efficient operators, the torch inductor fuses matmul and GELU into a single linearpointwise+gelu operator. However, oneDNN ACL primitives didn’t have the support for fused kernels with GELU.&lt;/p&gt;

&lt;p&gt;We addressed these gaps by extending oneDNN primitives to handle the additional layouts and new primitive definitions. The following sections talk about the optimizations in detail.&lt;/p&gt;

&lt;h3 id=&quot;optimization-1-extended-acl-primitives-to-accept-weight-tensors-in-blocked-layout&quot;&gt;Optimization 1: Extended ACL primitives to accept weight tensors in blocked layout&lt;/h3&gt;

&lt;p&gt;We extended the ACL primitives to accept blocked layout in addition to the the standard NHWC format. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const bool is_weights_md_format_ok
                    = utils::one_of(weights_format_kind_received,
                      format_kind::any, format_kind::blocked);


const memory_desc_t weights_md_received = weights_md_;
acl_utils::reorder_to_weight_format(aip.wei_tensor_info,
             weights_md_, expected_weight_format, inner_dim, o_dim,
             remaining_dims, {});

ACL_CHECK_SUPPORT(
     (weights_format_kind_received == format_kind::blocked)
      &amp;amp;&amp;amp; !(dnnl_memory_desc_equal(
      &amp;amp;weights_md_received, &amp;amp;weights_md_)),
      &quot;specified blocked format not supported by ACL, use &quot;
      &quot;format_kind_t::any to find a supported blocked format for &quot;
      &quot;your platform&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;optimization-2-defined-new-acl-primitives-to-handle-mixed-precision-operators-weights-in-bfloat16-and-activations-in-fp32&quot;&gt;Optimization 2: Defined new ACL primitives to handle mixed precision operators (weights in bfloat16 and activations in fp32)&lt;/h3&gt;

&lt;p&gt;We defined mixed precision primitive definitions and updated the existing oneDNN ACL fp32 primitives to handle bfloat16 tensors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; /* With graph compilation, we are able to reorder and pre-pack the weights during the model load
  * and compilation phase itself so that redundant and on-the-fly reorders can be avoided.
  * This primitive definition is to support gemm fastmath mode for the compile scenario where src is
  * in fp32 and weights are in bf16
  */
 {{forward, f32, bf16, f32}, {
    CPU_INSTANCE_AARCH64_ACL(acl_inner_product_fwd_t)
    nullptr,
 }},
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;optimization-3-disabled-operator-fusion-pass-in-torch-inductor&quot;&gt;Optimization 3: Disabled operator fusion pass in torch inductor&lt;/h3&gt;

&lt;p&gt;We bypassed the operator fusion pass in torch inductor so that the compiled graph doesn’t contain GELU fused operators. This is a temporary solution to enable ACL kernels in torch.compile. There is a work in progress to enable operator fusion pass for the future PyTorch releases. With this workaround, we were able to successfully dispatch the linear layer to ACL. As shown in the following torch.profiler output, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten::addmm &lt;/code&gt;(one of the variants of the matmul operator) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten::gelu &lt;/code&gt;in the original model (as highlighted in &lt;em&gt;Image 4&lt;/em&gt;) was compiled to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn::_linear_pointwise &lt;/code&gt;without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gelu &lt;/code&gt;operator fusion (as highlighted in &lt;em&gt;Image 5&lt;/em&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                aten::addmm        73.32%      46.543ms        74.49%      47.287ms     647.767us            73  
            model_inference         9.92%       6.296ms       100.00%      63.479ms      63.479ms             1  
                  aten::bmm         4.37%       2.776ms         5.46%       3.467ms     144.458us            24  
                aten::copy_         1.74%       1.102ms         1.74%       1.102ms       8.103us           136  
                 aten::gelu         1.50%     950.000us         1.50%     950.000us      79.167us            12  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: torch.profiler output for Hugging Face bert base model inference in Eager mode, showing addmm and gelu operators&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;mt-3 mb-3&quot;&gt;&amp;nbsp;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                            mkldnn::_linear_pointwise        53.61%      15.529ms        57.53%      16.665ms     228.288us            73  
                                Torch-Compiled Region        36.95%      10.705ms        99.31%      28.769ms      28.769ms             1  
    aten::_scaled_dot_product_flash_attention_for_cpu         3.67%       1.064ms         4.43%       1.284ms     107.000us            12  
                                           aten::view         1.97%     572.000us         1.97%     572.000us       2.509us           228  
                                          aten::empty         1.38%     399.000us         1.38%     399.000us       3.270us           122 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 5&lt;/strong&gt;: torch.profiler output for Hugging Face Bert base model inference in torch.compile mode, showing linear_pointwise operator without gelu fusion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gelu &lt;/code&gt;operator was compiled into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;erf &lt;/code&gt;(error function) and was dispatched to an inductor auto vectorization backend. The following snippets show the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;erf &lt;/code&gt;operator in the compiled graph and running it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libm.so&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%_linear_pointwise_default_140 : [num_users=2] = call_function[target=torch.ops.mkldnn._linear_pointwise.default](args = (%add_7, %_frozen_param278, %_frozen_param16, none, [], ), kwargs = {})
%mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.5), kwargs = {})
%mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.7071067811865476), kwargs = {})
%erf   : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_6,), kwargs = {})
%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 6&lt;/strong&gt;: snippet after post grad pass showing erf function in the compiled graph&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;mt-3 mb-3&quot;&gt;&amp;nbsp;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     0.82%     0.40%  python3  libm.so.6            [.] erff32
     0.05%     0.00%  python3  libtorch_python.so   [.] torch::autograd::THPVariable_erf
     0.05%     0.00%  python3  libtorch_cpu.so      [.] at::_ops::erf::call
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 7&lt;/strong&gt;: Linux perf report showing erf dispatch to libm.so&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With this work, we were able to optimize &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile &lt;/code&gt;performance on Graviton3 processors by using inductor graph compilation along with the oneDNN+ACL backend.&lt;/p&gt;

&lt;h3 id=&quot;torchbench-enhancements&quot;&gt;TorchBench enhancements&lt;/h3&gt;

&lt;p&gt;To demonstrate the torch.compile performance improvements on AWS Graviton3 processors, we extended TorchBench framework to add a new argument to enable graph freeze and weights pre-packing and disable torch auto grad for eval test mode. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;parser.add_argument(
 &quot;—freeze_prepack_weights&quot;,
 action='store_true',
 help=&quot;set to freeze the graph and prepack weights&quot;,
 )

if args.freeze_prepack_weights:
 torch._inductor.config.freezing=True
 torch._inductor.config.cpp.weight_prepack=True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 8&lt;/strong&gt;: Added freeze_prepack_weights option for torchdynamo backend in TorchBench to demonstrate torch.compile performance improvements on AWS Graviton3 processors&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We have upstreamed all the optimizations, and starting with PyTorch 2.3.1, these are supported in torch Python wheels and AWS Graviton PyTorch DLC.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Next, we’re extending the torch inductor CPU backend support to compile Llama model, and adding support for fused GEMM kernels to enable torch inductor operator fusion optimization on AWS Graviton3 processors.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this tutorial, we covered how we optimized torch.compile performance on AWS Graviton3-based EC2 instances, how to use the optimizations to improve PyTorch model inference performance, and demonstrated the resulting speedups. We hope that you will give it a try! If you need any support with ML software on Graviton, please open an issue on the AWS Graviton Technical Guide &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank the PyTorch community for the baseline torch.compile framework and their continued efforts to optimize it further.&lt;/p&gt;

&lt;p&gt;References:  &lt;a href=&quot;https://pytorch.org/assets/pytorch2-2.pdf&quot;&gt;https://pytorch.org/assets/pytorch2-2.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;

&lt;p&gt;Sunita Nadampalli is a Software Development Manager and AI/ML expert at AWS. She leads AWS Graviton software performance optimizations for AI/ML and HPC workloads. She is passionate about open source software development and delivering high-performance and sustainable software solutions for SoCs based on the Arm ISA.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sunita Nadampalli</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing Hacker Cup AI Track at NeurIPS 2024</title>
      <link href="https://pytorch.org/blog/hacker-cup/" rel="alternate" type="text/html" title="Announcing Hacker Cup AI Track at NeurIPS 2024" />
      <published>2024-07-03T00:00:00-07:00</published>
      <updated>2024-07-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hacker-cup</id>
      <content type="html" xml:base="https://pytorch.org/blog/hacker-cup/">&lt;p&gt;The PyTorch team in partnership with Meta Hacker Cup, and Microsoft Research, are excited to announce the Hacker Cup AI Track at NeurIPS 2024. This will be the first AI track for the popular Meta Hacker Cup programming competition designed to assess the capabilities of Generative AI in performing autonomous code generation tasks. We aim to test the limits of AI in complex coding challenges and measure the performance gap between AI systems and human programmers. We will provide access to all Hacker Cup problems since 2011 alongside their respective solutions in a multimodal (image and text) format, and utilize the existing Hacker Cup infrastructure for competitor evaluation. Featuring both &lt;em&gt;open evaluation, open model&lt;/em&gt; and &lt;em&gt;open evaluation, closed model&lt;/em&gt; tracks, this competition invites diverse participation from research institutions of varied interests and resource constraints, including academic labs, AI startups, large technology companies, and AI enthusiasts. Our goal is to develop and democratize meaningful advancements in code automation with the very first open evaluation process for competitive AI programmers. Registration will begin in &lt;strong&gt;Early August&lt;/strong&gt;, with our first qualification round on &lt;strong&gt;September 20th.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more information please visit our website at &lt;a href=&quot;https://www.facebook.com/codingcompetitions/hacker-cup/&quot;&gt;https://www.facebook.com/codingcompetitions/hacker-cup/&lt;/a&gt; &lt;strong&gt;and join our Discord&lt;/strong&gt; at &lt;a href=&quot;https://discord.com/invite/wWeN9hTH32&quot;&gt;discord.gg/wWeN9hTH32&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch team in partnership with Meta Hacker Cup, and Microsoft Research, are excited to announce the Hacker Cup AI Track at NeurIPS 2024. This will be the first AI track for the popular Meta Hacker Cup programming competition designed to assess the capabilities of Generative AI in performing autonomous code generation tasks. We aim to test the limits of AI in complex coding challenges and measure the performance gap between AI systems and human programmers. We will provide access to all Hacker Cup problems since 2011 alongside their respective solutions in a multimodal (image and text) format, and utilize the existing Hacker Cup infrastructure for competitor evaluation. Featuring both open evaluation, open model and open evaluation, closed model tracks, this competition invites diverse participation from research institutions of varied interests and resource constraints, including academic labs, AI startups, large technology companies, and AI enthusiasts. Our goal is to develop and democratize meaningful advancements in code automation with the very first open evaluation process for competitive AI programmers. Registration will begin in Early August, with our first qualification round on September 20th.</summary>
      

      
      
    </entry>
  
</feed>



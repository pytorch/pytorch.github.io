<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2020-03-26T14:01:54-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">Introduction to Quantization on PyTorch</title><link href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/" rel="alternate" type="text/html" title="Introduction to Quantization on PyTorch" /><published>2020-03-26T00:00:00-07:00</published><updated>2020-03-26T00:00:00-07:00</updated><id>https://pytorch.org/blog/introduction-to-quantization-on-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.&lt;/p&gt;

&lt;p&gt;Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency) and can be the difference between a model achieving quality of service goals or even fitting into the resources available on a mobile device. Even when resources aren’t quite so constrained it may enable you to deploy a larger and more accurate model. Quantization is available in PyTorch starting in version 1.3 and with the release of PyTorch 1.4 we published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.&lt;/p&gt;

&lt;p&gt;This blog post provides an overview of the quantization support on PyTorch and its incorporation with the TorchVision domain library.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantization&quot;&gt;&lt;strong&gt;What is Quantization?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually int8 compared to floating point implementations. This enables performance gains in several important areas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;4x reduction in model size;&lt;/li&gt;
  &lt;li&gt;2-4x reduction in memory bandwidth;&lt;/li&gt;
  &lt;li&gt;2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization does not however come without additional cost. Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.&lt;/p&gt;

&lt;p&gt;We designed quantization to fit into the PyTorch framework. The means that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch has data types corresponding to &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;quantized tensors&lt;/a&gt;, which share many of the features of tensors.&lt;/li&gt;
  &lt;li&gt;One can write kernels with quantized tensors, much like kernels for floating point tensors to customize their implementation. PyTorch supports quantized modules for common operations as part of the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.quantized.dynamic&lt;/code&gt; name-space.&lt;/li&gt;
  &lt;li&gt;Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. The quantization method is virtually identical for both server and mobile backends. One can easily mix quantized and floating point operations in a model.&lt;/li&gt;
  &lt;li&gt;Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torch_stack1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We developed three techniques for quantizing neural networks in PyTorch as part of quantization tooling in the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization&lt;/code&gt; name-space.&lt;/p&gt;

&lt;h2 id=&quot;the-three-modes-of-quantization-supported-in-pytorch-starting-version-13&quot;&gt;&lt;strong&gt;The Three Modes of Quantization Supported in PyTorch starting version 1.3&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;dynamic-quantization&quot;&gt;&lt;strong&gt;Dynamic Quantization&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;The easiest method of quantization PyTorch supports is called &lt;strong&gt;dynamic quantization&lt;/strong&gt;. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;: we have a simple API for dynamic quantization in PyTorch. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.quantize_dynamic&lt;/code&gt; takes in a model, as well as a couple other arguments, and produces a quantized model! Our &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;end-to-end tutorial&lt;/a&gt; illustrates this for a BERT model; while the tutorial is long and contains sections on loading pre-trained models and other concepts unrelated to quantization, the part the quantizes the BERT model is simply:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.quantization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantize_dynamic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;See the documentation for the function &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic&quot;&gt;here&lt;/a&gt; an end-to-end example in our tutorials &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;post-training-static-quantization&quot;&gt;&lt;strong&gt;Post-Training Static Quantization&lt;/strong&gt;&lt;/h3&gt;

    &lt;p&gt;One can further improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time (a simple technique would be to simply divide the entire range of activations into 256 levels, but we support more sophisticated methods as well). Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.&lt;/p&gt;

    &lt;p&gt;With this release, we’re supporting several features that allow users to optimize their static quantization:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Observers: you can customize observer modules which specify how statistics are collected prior to quantization to try out more advanced methods to quantize your data.&lt;/li&gt;
      &lt;li&gt;Operator fusion: you can fuse multiple operations into a single operation, saving on memory access while also improving the operation’s numerical accuracy.&lt;/li&gt;
      &lt;li&gt;Per-channel quantization: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;h3 id=&quot;pytorch-api&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
        &lt;ul&gt;
          &lt;li&gt;To fuse modules, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.fuse_modules&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Observers are inserted using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Finally, quantization itself is done using &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;We have a tutorial with an end-to-end example of quantization (this same tutorial also covers our third quantization method, quantization-aware training), but because of our simple API, the three lines that perform post-training static quantization on the pre-trained model &lt;code class=&quot;highlighter-rouge&quot;&gt;myModel&lt;/code&gt; are:&lt;/p&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# set quantization config for server (x86)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deploymentmyModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# insert observers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Calibrate the model and collect statistics&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;quantization-aware-training&quot;&gt;&lt;strong&gt;Quantization Aware Training&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Quantization-aware training(QAT)&lt;/strong&gt; is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;pytorch-api-1&quot;&gt;&lt;strong&gt;PyTorch API&lt;/strong&gt;:&lt;/h3&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.prepare_qat&lt;/code&gt; inserts fake quantization modules to model quantization.&lt;/li&gt;
      &lt;li&gt;Mimicking the static quantization API, &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.quantization.convert&lt;/code&gt; actually quantizes the model once training is complete.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, in &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;the end-to-end example&lt;/a&gt;, we load in a pre-trained model as &lt;code class=&quot;highlighter-rouge&quot;&gt;qat_model&lt;/code&gt;, then we simply perform quantization-aware training using:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# specify quantization config for QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qat_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# prepare QAT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare_qat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# convert to quantized version, removing dropout, to check for accuracy on each&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochquantized_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qat_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;device-and-operator-support&quot;&gt;&lt;strong&gt;Device and Operator Support&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, please see the documentation at &lt;a href=&quot;https://pytorch.org/docs/stable/quantization.html&quot;&gt;https://pytorch.org/docs/stable/quantization.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. Currently quantized operators are supported only for CPU inference in the following backends: x86 and ARM. Both the quantization configuration (how tensors should be quantized and the quantized kernels (arithmetic with quantized tensors) are backend dependent. One can specify the backend by doing:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchbackend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fbgemm'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 'fbgemm' for server, 'qnnpack' for mobile&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qconfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_qconfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# prepare and convert model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Set the backend on which the quantized kernels need to be run&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backends&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, quantization aware training occurs in full floating point and can run on either GPU or CPU. Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn’t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).&lt;/p&gt;

&lt;h4 id=&quot;integration-in-torchvision&quot;&gt;&lt;strong&gt;Integration in torchvision&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We’ve also enabled quantization for some of the most popular models in &lt;a href=&quot;https://github.com/pytorch/vision/tree/master/torchvision/models/quantization&quot;&gt;torchvision&lt;/a&gt;: Googlenet, Inception, Resnet, ResNeXt, Mobilenet and Shufflenet. We have upstreamed these changes to torchvision in three forms:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre-trained quantized weights so that you can use them right away.&lt;/li&gt;
  &lt;li&gt;Quantization ready model definitions so that you can do post-training quantization or quantization aware training.&lt;/li&gt;
  &lt;li&gt;A script for doing quantization aware training — which is available for any of these model though, as you will learn below, we only found it necessary for achieving accuracy with Mobilenet.&lt;/li&gt;
  &lt;li&gt;We also have a &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;tutorial&lt;/a&gt; showing how you can do transfer learning with quantization using one of the torchvision models.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;choosing-an-approach&quot;&gt;&lt;strong&gt;Choosing an approach&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The choice of which scheme to use depends on multiple factors:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Model/Target requirements: Some models might be sensitive to quantization, requiring quantization aware training.&lt;/li&gt;
  &lt;li&gt;Operator/Backend support: Some backends require fully quantized operators.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, operator coverage is limited and may restrict the choices listed in the table below:
The table below provides a guideline.&lt;/p&gt;

&lt;style type=&quot;text/css&quot;&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:black;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;font-weight:bold;color:black;}
article.pytorch-article table tr th:first-of-type, article.pytorch-article table tr td:first-of-type{padding-left:5px}
&lt;/style&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Model Type&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Preferred scheme&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Why&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;LSTM/RNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;BERT/Transformer&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Dynamic Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput dominated by compute/memory bandwidth for weights&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Static Quantization&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Throughput limited by memory bandwidth for activations&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;CNN&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;Quantization Aware Training&lt;/td&gt;
    &lt;td class=&quot;tg-lboi&quot;&gt;In the case where accuracy can't be achieved with static quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;performance-results&quot;&gt;&lt;strong&gt;Performance Results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Quantization provides a 4x reduction in the model size and a speedup of 2x to 3x compared to floating point implementations depending on the hardware platform and the model being benchmarked. Some sample results are:&lt;/p&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Float Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantized Latency (ms)&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Inference Performance Gain&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Device&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Notes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;581&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;313&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;1.8x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Resnet-50&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;214&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;103&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;2x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Xeon-D2191 (1.6GHz)&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Single thread, x86-64, Static quantization&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Mobilenet-v2&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;97&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;17&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;5.7x&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Samsung S9&lt;/td&gt;
      &lt;td class=&quot;tg-lboi&quot;&gt;Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accuracy-results&quot;&gt;&lt;strong&gt;Accuracy results&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We also compared the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/run_glue.py&quot;&gt;compared&lt;/a&gt; the F1 score of BERT on the GLUE benchmark for MRPC.&lt;/p&gt;

&lt;h4 id=&quot;computer-vision-model-accuracy&quot;&gt;&lt;strong&gt;Computer Vision Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;table class=&quot;tg&quot;&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Float)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Top-1 Accuracy (Quantized)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Googlenet&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;69.7&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Inception-v3&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.5&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;77.1&lt;/td&gt;
    &lt;td class=&quot;tg-cly1&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNet-18&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.8&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Resnet-50&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;76.1&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;75.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;ResNext-101 32x8d&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79.3&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;79&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Mobilenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.9&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;71.6&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Quantization Aware Training&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Shufflenet-v2&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;69.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;68.4&lt;/td&gt;
    &lt;td class=&quot;tg-0lax&quot;&gt;Static post training quantization&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;speech-and-nlp-model-accuracy&quot;&gt;&lt;strong&gt;Speech and NLP Model accuracy&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;table-responsive&quot;&gt;
  &lt;table class=&quot;tg&quot;&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Model&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Float&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;F1 (GLUEMRPC) Quantized&lt;/td&gt;
      &lt;td class=&quot;tg-0pky&quot;&gt;Quantization scheme&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;BERT&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.902&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;0.895&lt;/td&gt;
      &lt;td class=&quot;tg-cly1&quot;&gt;Dynamic quantization&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To get started on quantizing your models in PyTorch, start with &lt;a href=&quot;https://pytorch.org/tutorials/#model-optimization&quot;&gt;the tutorials on the PyTorch website&lt;/a&gt;. If you are working with sequence data start with &lt;a href=&quot;https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html&quot;&gt;dynamic quantization for LSTM&lt;/a&gt;, or &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html&quot;&gt;BERT&lt;/a&gt;. If you are working with image data then we recommend starting with the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html&quot;&gt;transfer learning with quantization&lt;/a&gt; tutorial. Then you can explore &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;static post training quantization&lt;/a&gt;. If you find that the accuracy drop with post training quantization is too high, then try &lt;a href=&quot;https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html&quot;&gt;quantization aware training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you run into issues you can get community help by posting in at &lt;a href=&quot;discuss.pytorch.org&quot;&gt;discuss.pytorch.org&lt;/a&gt;, use the quantization category for quantization related issues.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is authored by Raghuraman Krishnamoorthi, James Reed, MinNi, and Seth Weidman. Special thanks to Jianyu Huan, Lingyi Liu and Haixin Liu for producing quantization metrics included in this post.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;PyTorch quantization presentation at Neurips: &lt;a href=&quot;https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx&quot;&gt;(https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantized Tensors &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor&quot;&gt;(https://github.com/pytorch/pytorch/wiki/
Introducing-Quantized-Tensor)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Quantization RFC on Github &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/18318&quot;&gt;(https://github.com/pytorch/pytorch/
issues/18318)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Raghuraman Krishnamoorthi, James Reed, Min Ni, and Seth Weidman</name></author><summary type="html">It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.</summary></entry><entry><title type="html">PyTorch 1.4 released, domain libraries updated</title><link href="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/" rel="alternate" type="text/html" title="PyTorch 1.4 released, domain libraries updated" /><published>2020-01-15T00:00:00-08:00</published><updated>2020-01-15T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-4-released-and-domain-libraries-updated/">&lt;p&gt;Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from &lt;a href=&quot;https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/&quot;&gt;NeurIPS 2019&lt;/a&gt;, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found &lt;a href=&quot;https://research.fb.com/neurips-2019-expo-workshops/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-14&quot;&gt;PyTorch 1.4&lt;/h2&gt;

&lt;p&gt;The 1.4 release of PyTorch adds new capabilities, including the ability to do fine grain build level customization for PyTorch Mobile, and new experimental features including support for model parallel training and Java language bindings.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-mobile---build-level-customization&quot;&gt;PyTorch Mobile - Build level customization&lt;/h3&gt;

&lt;p&gt;Following the open sourcing of &lt;a href=&quot;https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/&quot;&gt;PyTorch Mobile in the 1.3 release&lt;/a&gt;, PyTorch 1.4 adds additional mobile support including the ability to customize build scripts at a fine-grain level. This allows mobile developers to optimize library size by only including the operators used by their models and, in the process, reduce their on device footprint significantly. Initial results show that, for example, a customized MobileNetV2 is 40% to 50% smaller than the prebuilt PyTorch mobile library. You can learn more &lt;a href=&quot;https://pytorch.org/mobile/home/&quot;&gt;here&lt;/a&gt; about how to create your own custom builds and, as always, please engage with the community on the &lt;a href=&quot;https://discuss.pytorch.org/c/mobile&quot;&gt;PyTorch forums&lt;/a&gt; to provide any feedback you have.&lt;/p&gt;

&lt;p&gt;Example code snippet for selectively compiling only the operators needed for MobileNetV2:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Dump list of operators used by MobileNetV2:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.pt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export_opnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MobileNetV2.yaml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yaml&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch Android library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh arm64-v8a

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt; Build PyTorch iOS library customized &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;MobileNetV2:
&lt;span class=&quot;go&quot;&gt;SELECTED_OP_LIST=MobileNetV2.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 scripts/build_ios.sh
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;distributed-model-parallel-training-experimental&quot;&gt;Distributed model parallel training (Experimental)&lt;/h3&gt;

&lt;p&gt;With the scale of models, such as RoBERTa, continuing to increase into the billions of parameters, model parallel training has become ever more important to help researchers push the limits. This release provides a distributed RPC framework to support distributed model parallel training. It allows for running functions remotely and referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backwards and update parameters across RPC boundaries.&lt;/p&gt;

&lt;p&gt;To learn more about the APIs and the design of this feature, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/rpc.html&quot;&gt;API documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/distributed_autograd.html&quot;&gt;Distributed Autograd design doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/rref.html&quot;&gt;Remote Reference design doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the full tutorials, see the links below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/rpc_tutorial.html&quot;&gt;A full RPC tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/distributed/rpc&quot;&gt;Examples using model parallel training for reinforcement learning and with an LSTM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, you can connect with community members and discuss more on the &lt;a href=&quot;https://discuss.pytorch.org/c/distributed/distributed-rpc&quot;&gt;forums&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;java-bindings-experimental&quot;&gt;Java bindings (Experimental)&lt;/h3&gt;

&lt;p&gt;In addition to supporting Python and C++, this release adds experimental support for Java bindings. Based on the interface developed for Android in PyTorch Mobile, the new bindings allow you to invoke TorchScript models from any Java program. Note that the Java bindings are only available for Linux for this release, and for inference only. We expect support to expand in subsequent releases. See the code snippet below for how to use PyTorch within Java:&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;demo-model.pt1&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;fromBlob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// data&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// shape&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mod&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toTensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;shape: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Arrays&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getDataAsFloatArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more about how to use PyTorch from Java &lt;a href=&quot;https://github.com/pytorch/java-demo&quot;&gt;here&lt;/a&gt;, and see the full Javadocs API documentation &lt;a href=&quot;https://pytorch.org/javadoc/1.4.0/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the full 1.4 release notes, see &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;domain-libraries&quot;&gt;Domain Libraries&lt;/h2&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio complement PyTorch with common datasets, models, and transforms. We’re excited to share new releases for all three domain libraries alongside the PyTorch 1.4 core release.&lt;/p&gt;

&lt;h3 id=&quot;torchvision-05&quot;&gt;torchvision 0.5&lt;/h3&gt;

&lt;p&gt;The improvements to torchvision 0.5 mainly focus on adding support for production deployment including quantization, TorchScript, and ONNX. Some of the highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All models in torchvision are now torchscriptable making them easier to ship into non-Python production environments&lt;/li&gt;
  &lt;li&gt;ResNets, MobileNet, ShuffleNet, GoogleNet and InceptionV3 now have quantized counterparts with pre-trained models, and also include scripts for quantization-aware training.&lt;/li&gt;
  &lt;li&gt;In partnership with the Microsoft team, we’ve added ONNX support for all models including Mask R-CNN.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchvision 0.5 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchaudio-04&quot;&gt;torchaudio 0.4&lt;/h3&gt;

&lt;p&gt;Improvements in torchaudio 0.4 focus on enhancing the currently available transformations, datasets, and backend support. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SoX is now optional, and a new extensible backend dispatch mechanism exposes SoundFile as an alternative to SoX.&lt;/li&gt;
  &lt;li&gt;The interface for datasets has been unified. This enables the addition of two large datasets: LibriSpeech and Common Voice.&lt;/li&gt;
  &lt;li&gt;New filters such as biquad, data augmentation such as time and frequency masking, transforms such as MFCC, gain and dither, and new feature computation such as deltas, are now available.&lt;/li&gt;
  &lt;li&gt;Transformations now support batches and are jitable.&lt;/li&gt;
  &lt;li&gt;An interactive speech recognition demo with voice activity detection is available for experimentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchaudio 0.4 &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;torchtext-05&quot;&gt;torchtext 0.5&lt;/h3&gt;

&lt;p&gt;torchtext 0.5 focuses mainly on improvements to the dataset loader APIs, including compatibility with core PyTorch APIs, but also adds support for unsupervised text tokenization. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added bindings for SentencePiece for unsupervised text tokenization .&lt;/li&gt;
  &lt;li&gt;Added a new unsupervised learning dataset - enwik9.&lt;/li&gt;
  &lt;li&gt;Made revisions to PennTreebank, WikiText103, WikiText2, IMDb to make them compatible with torch.utils.data. Those datasets are in an experimental folder and we welcome your feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about torchtext 0.5 &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Today, we’re announcing the availability of PyTorch 1.4, along with updates to the PyTorch domain libraries. These releases build on top of the announcements from NeurIPS 2019, where we shared the availability of PyTorch Elastic, a new classification framework for image and video, and the addition of Preferred Networks to the PyTorch community. For those that attended the workshops at NeurIPS, the content can be found here.</summary></entry><entry><title type="html">PyTorch adds new tools and libraries, welcomes Preferred Networks to its community</title><link href="https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/" rel="alternate" type="text/html" title="PyTorch adds new tools and libraries, welcomes Preferred Networks to its community" /><published>2019-12-06T00:00:00-08:00</published><updated>2019-12-06T00:00:00-08:00</updated><id>https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-adds-new-tools-and-libraries-welcomes-preferred-networks-to-its-community/">&lt;p&gt;PyTorch continues to be used for the latest state-of-the-art research on display at the NeurIPS conference next week, making up nearly &lt;a href=&quot;https://chillee.github.io/pytorch-vs-tensorflow/&quot;&gt;70% of papers&lt;/a&gt; that cite a framework. In addition, we’re excited to welcome Preferred Networks, the maintainers of the Chainer framework, to the PyTorch community. Their teams are moving fully over to PyTorch for developing their ML capabilities and services.&lt;/p&gt;

&lt;p&gt;This growth underpins PyTorch’s focus on building for the needs of the research community, and increasingly, supporting the full workflow from research to production deployment. To further support researchers and developers, we’re launching a number of new tools and libraries for large scale computer vision and elastic fault tolerant training. Learn more on GitHub and at our NeurIPS booth.&lt;/p&gt;

&lt;h2 id=&quot;preferred-networks-joins-the-pytorch-community&quot;&gt;Preferred Networks joins the PyTorch community&lt;/h2&gt;

&lt;p&gt;Preferred Networks, Inc. (PFN) announced plans to move its deep learning framework from Chainer to PyTorch. As part of this change, PFN will collaborate with the PyTorch community and contributors, including people from Facebook, Microsoft, CMU, and NYU, to participate in the development of PyTorch.&lt;/p&gt;

&lt;p&gt;PFN developed Chainer, a deep learning framework that introduced the concept of define-by-run (also referred to as eager execution), to support and speed up its deep learning development. Chainer has been used at PFN since 2015 to rapidly solve real-world problems with the latest, cutting-edge technology. Chainer was also one of the inspirations for PyTorch’s initial design, as outlined in the &lt;a href=&quot;https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library&quot;&gt;PyTorch NeurIPS&lt;/a&gt; paper.&lt;/p&gt;

&lt;p&gt;PFN has driven innovative work with &lt;a href=&quot;https://cupy.chainer.org/&quot;&gt;CuPy&lt;/a&gt;, ImageNet in 15 minutes, &lt;a href=&quot;https://optuna.org/&quot;&gt;Optuna&lt;/a&gt;, and other projects that have pushed the boundaries of design and engineering. As part of the PyTorch community, PFN brings with them creative engineering capabilities and experience to help take the framework forward. In addition, PFN’s migration to PyTorch will allow it to efficiently incorporate the latest research results to accelerate its R&amp;amp;D activities, &lt;a href=&quot;https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/&quot;&gt;given PyTorch’s broad adoption with researchers&lt;/a&gt;, and to collaborate with the community to add support for PyTorch on MN-Core, a deep learning processor currently in development.&lt;/p&gt;

&lt;p&gt;We are excited to welcome PFN to the PyTorch community, and to jointly work towards the common goal of furthering advances in deep learning technology. Learn more about the PFN’s migration to PyTorch &lt;a href=&quot;https://preferred.jp/en/news/pr20191205/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tools-for-elastic-training-and-large-scale-computer-vision&quot;&gt;Tools for elastic training and large scale computer vision&lt;/h2&gt;

&lt;h3 id=&quot;pytorch-elastic-experimental&quot;&gt;PyTorch Elastic (Experimental)&lt;/h3&gt;

&lt;p&gt;Large scale model training is becoming commonplace with architectures like BERT and the growth of model parameter counts into the billions or even tens of billions. To achieve convergence at this scale in a reasonable amount of time, the use of distributed training is needed.&lt;/p&gt;

&lt;p&gt;The current PyTorch Distributed Data Parallel (DDP) module enables data parallel training where each process trains the same model but on different shards of data. It enables bulk synchronous, multi-host, multi-GPU/CPU execution of ML training. However, DDP has several shortcomings; e.g. jobs cannot start without acquiring all the requested nodes; jobs cannot continue after a node fails due to error or transient issue; jobs cannot incorporate a node that joined later; and lastly; progress cannot be made with the presence of a slow/stuck node.&lt;/p&gt;

&lt;p&gt;The focus of &lt;a href=&quot;https://github.com/pytorch/elastic&quot;&gt;PyTorch Elastic&lt;/a&gt;, which uses Elastic Distributed Data Parallelism, is to address these issues and build a generic framework/APIs for PyTorch to enable reliable and elastic execution of these data parallel training workloads. It will provide better programmability, higher resilience to failures of all kinds, higher-efficiency and larger-scale training compared with pure DDP.&lt;/p&gt;

&lt;p&gt;Elasticity, in this case, means both: 1) the ability for a job to continue after node failure (by running with fewer nodes and/or by incorporating a new host and transferring state to it); and 2) the ability to add/remove nodes dynamically due to resource availability changes or bottlenecks.&lt;/p&gt;

&lt;p&gt;While this feature is still experimental, you can try it out on AWS EC2, with the instructions &lt;a href=&quot;https://github.com/pytorch/elastic/tree/master/aws&quot;&gt;here&lt;/a&gt;. Additionally, the PyTorch distributed team is working closely with teams across AWS to support PyTorch Elastic training within services such as Amazon Sagemaker and Elastic Kubernetes Service (EKS). Look for additional updates in the near future.&lt;/p&gt;

&lt;h3 id=&quot;new-classification-framework&quot;&gt;New Classification Framework&lt;/h3&gt;

&lt;p&gt;Image and video classification are at the core of content understanding. To that end, you can now leverage a new end-to-end framework for large-scale training of state-of-the-art image and video classification models. It allows researchers to quickly prototype and iterate on large distributed training jobs at the scale of billions of images. Advantages include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ease of use - This framework features a modular, flexible design that allows anyone to train machine learning models on top of PyTorch using very simple abstractions. The system also has out-of-the-box integration with AWS on PyTorch Elastic, facilitating research at scale and making it simple to move between research and production.&lt;/li&gt;
  &lt;li&gt;High performance - Researchers can use the framework to train models such as Resnet50 on ImageNet in as little as 15 minutes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can learn more at the &lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=16&quot;&gt;NeurIPS Expo workshop&lt;/a&gt; on Multi-Modal research to production or get started with the PyTorch Elastic Imagenet example &lt;a href=&quot;https://github.com/pytorch/elastic/blob/master/examples/imagenet/main.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;come-see-us-at-neurips&quot;&gt;Come see us at NeurIPS&lt;/h2&gt;

&lt;p&gt;The PyTorch team will be hosting workshops at NeurIPS during the industry expo on 12/8. Join the sessions below to learn more, and visit the team at the PyTorch booth on the show floor and during the Poster Session. At the booth, we’ll be walking through an interactive demo of PyTorch running fast neural style transfer on a Cloud TPU - here’s a &lt;a href=&quot;https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference-xrt-1-15.ipynb&quot;&gt;sneak peek&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’re also publishing a &lt;a href=&quot;https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library&quot;&gt;paper that details the principles that drove the implementation of PyTorch&lt;/a&gt; and how they’re reflected in its architecture.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=16&quot;&gt;Multi-modal Research to Production&lt;/a&gt;&lt;/em&gt; - This workshop will dive into a number of modalities such as computer vision (large scale image classification and instance segmentation) and Translation and Speech (seq-to-seq Transformers) from the lens of taking cutting edge research to production. Lastly, we will also walk through how to use the latest APIs in PyTorch to take eager mode developed models into graph mode via Torchscript and quantize them for scale production deployment on servers or mobile devices. Libraries used include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Classification Framework - a newly open sourced PyTorch framework developed by Facebook AI for research on large-scale image and video classification. It allows researchers to quickly prototype and iterate on large distributed training jobs. Models built on the framework can be seamlessly deployed to production.&lt;/li&gt;
  &lt;li&gt;Detectron2 - the recently released object detection library built by the Facebook AI Research computer vision team. We will articulate the improvements over the previous version including: 1) Support for latest models and new tasks; 2) Increased flexibility, to enable new computer vision research; 3) Maintainable and scalable, to support production use cases.&lt;/li&gt;
  &lt;li&gt;Fairseq - general purpose sequence-to-sequence library, can be used in many applications, including (unsupervised) translation, summarization, dialog and speech recognition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://nips.cc/ExpoConferences/2019/schedule?workshop_id=14&quot;&gt;Responsible and Reproducible AI&lt;/a&gt;&lt;/em&gt; - This workshop on Responsible and Reproducible AI will dive into important areas that are shaping the future of how we interpret, reproduce research, and build AI with privacy in mind. We will cover major challenges, walk through solutions, and finish each talk with a hands-on tutorial.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reproducibility: As the number of research papers submitted to arXiv and conferences skyrockets, scaling reproducibility becomes difficult. We must address the following challenges: aid extensibility by standardizing code bases, democratize paper implementation by writing hardware agnostic code, facilitate results validation by documenting “tricks” authors use to make their complex systems function. To offer solutions, we will dive into tool like PyTorch Hub and PyTorch Lightning which are used by some of the top researchers in the world to reproduce the state of the art.&lt;/li&gt;
  &lt;li&gt;Interpretability: With the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. To get hands on, we will use the recently released Captum library that provides state-of-the-art algorithms to provide researchers and developers with an easy way to understand the importance of neurons/layers and the predictions made by our models.`&lt;/li&gt;
  &lt;li&gt;Private AI: Practical applications of ML via cloud-based or machine-learning-as-a-service platforms pose a range of security and privacy challenges. There are a number of technical approaches being studied including: homomorphic encryption, secure multi-party computation, trusted execution environments, on-device computation, and differential privacy. To provide an immersive understanding of how some of these technologies are applied, we will use the CrypTen project which provides a community based research platform to take the field of Private AI forward.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch continues to be used for the latest state-of-the-art research on display at the NeurIPS conference next week, making up nearly 70% of papers that cite a framework. In addition, we’re excited to welcome Preferred Networks, the maintainers of the Chainer framework, to the PyTorch community. Their teams are moving fully over to PyTorch for developing their ML capabilities and services.</summary></entry><entry><title type="html">OpenMined and PyTorch partner to launch fellowship funding for privacy-preserving ML community</title><link href="https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml/" rel="alternate" type="text/html" title="OpenMined and PyTorch partner to launch fellowship funding for privacy-preserving ML community" /><published>2019-12-06T00:00:00-08:00</published><updated>2019-12-06T00:00:00-08:00</updated><id>https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml</id><content type="html" xml:base="https://pytorch.org/blog/openmined-and-pytorch-launch-fellowship-funding-for-privacy-preserving-ml/">&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/openmined-pytorch.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Many applications of machine learning (ML) pose a range of security and privacy challenges. In particular, users may not be willing or allowed to share their data, which prevents them from taking full advantage of ML platforms like PyTorch. To take the field of privacy-preserving ML (PPML) forward, OpenMined and PyTorch are announcing plans to jointly develop a combined platform to accelerate PPML research as well as new funding for fellowships.&lt;/p&gt;

&lt;p&gt;There are many techniques attempting to solve the problem of privacy in ML, each at various levels of maturity. These include (1) homomorphic encryption, (2) secure multi-party computation, (3) trusted execution environments, (4) on-device computation, (5) federated learning with secure aggregation, and (6) differential privacy. Additionally, a number of open source projects implementing these techniques were created with the goal of enabling research at the intersection of privacy, security, and ML. Among them, PySyft and CrypTen have taken an “ML-first” approach by presenting an API that is familiar to the ML community, while masking the complexities of privacy and security protocols. We are excited to announce that these two projects are now collaborating closely to build a mature PPML ecosystem around PyTorch.&lt;/p&gt;

&lt;p&gt;Additionally, to bolster this ecosystem and take the field of privacy preserving ML forward, we are also calling for contributions and supporting research efforts on this combined platform by providing funding to support the OpenMined community and the researchers that contribute, build proofs of concepts and desire to be on the cutting edge of how privacy-preserving technology is applied. We will provide funding through the &lt;a href=&quot;https://www.raais.org/&quot;&gt;RAAIS Foundation&lt;/a&gt;, a non-profit organization with a mission to advance education and research in artificial intelligence for the common good. We encourage interested parties to apply to one or more of the fellowships listed below.&lt;/p&gt;

&lt;h2 id=&quot;tools-powering-the-future-of-privacy-preserving-ml&quot;&gt;Tools Powering the Future of Privacy-Preserving ML&lt;/h2&gt;

&lt;p&gt;The next generation of privacy-preserving open source tools enable ML researchers to easily experiment with ML models using secure computing techniques without needing to be cryptography experts. By integrating with PyTorch, PySyft and CrypTen offer familiar environments for ML developers to research and apply these techniques as part of their work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PySyft&lt;/strong&gt; is a Python library for secure and private ML developed by the OpenMined community. It is a flexible, easy-to-use library that makes secure computation techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Secure_multi-party_computation&quot;&gt;multi-party computation (MPC)&lt;/a&gt; and privacy-preserving techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;&gt;differential privacy&lt;/a&gt; accessible to the ML community. It prioritizes ease of use and focuses on integrating these techniques into end-user use cases like federated learning with mobile phones and other edge devices, encrypted ML as a service, and privacy-preserving data science.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CrypTen&lt;/strong&gt; is a framework built on PyTorch that enables private and secure ML for the PyTorch community. It is the first step along the journey towards a privacy-preserving mode in PyTorch that will make secure computing techniques accessible beyond cryptography researchers. It currently implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Secure_multi-party_computation&quot;&gt;secure multiparty computation&lt;/a&gt; with the goal of offering other secure computing backends in the near future. Other benefits to ML researchers include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is &lt;strong&gt;ML first&lt;/strong&gt; and presents secure computing techniques via a CrypTensor object that looks and feels exactly like a PyTorch Tensor. This allows the user to use automatic differentiation and neural network modules akin to those in PyTorch.&lt;/li&gt;
  &lt;li&gt;The framework focuses on &lt;strong&gt;scalability and performance&lt;/strong&gt; and is built with real-world challenges in mind.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The focus areas for CrypTen and PySyft are naturally aligned and complement each other. The former focuses on building support for various secure and privacy preserving techniques on PyTorch through an encrypted tensor abstraction, while the latter focuses on end user use cases like deployment on edge devices and a user friendly data science platform.&lt;/p&gt;

&lt;p&gt;Working together will enable PySyft to use CrypTen as a backend for encrypted tensors. This can lead to an increase in performance for PySyft and the adoption of CrypTen as a runtime by PySyft’s userbase. In addition to this, PyTorch is also adding cryptography friendly features such as support for cryptographically secure random number generation. Over the long run, this allows each library to focus exclusively on its core competencies while enjoying the benefits of the synergistic relationship.&lt;/p&gt;

&lt;h2 id=&quot;new-funding-for-openmined-contributors&quot;&gt;New Funding for OpenMined Contributors&lt;/h2&gt;

&lt;p&gt;We are especially excited to announce that the PyTorch team has invested $250,000 to support OpenMined in furthering the development and proliferation of privacy-preserving ML. This gift will be facilitated via the &lt;a href=&quot;https://www.raais.org/&quot;&gt;RAAIS Foundation&lt;/a&gt; and will be available immediately to support paid fellowship grants for the OpenMined community.&lt;/p&gt;

&lt;h2 id=&quot;how-to-get-involved&quot;&gt;How to get involved&lt;/h2&gt;

&lt;p&gt;Thanks to the support from the PyTorch team, OpenMined is able to offer three different opportunities for you to participate in the project’s development. Each of these fellowships furthers our shared mission to lower the barrier-to-entry for privacy-preserving ML and to create a more privacy-preserving world.&lt;/p&gt;

&lt;h3 id=&quot;core-pysyft-crypten-integration-fellowships&quot;&gt;Core PySyft CrypTen Integration Fellowships&lt;/h3&gt;

&lt;p&gt;During these fellowships, we will integrate CrypTen as a supported backend for encrypted computation in PySyft. This will allow for the high-performance, secure multi-party computation capabilities of CrypTen to be used alongside other important tools in PySyft such as differential privacy and federated learning. For more information on the roadmap and how to apply for a paid fellowship, check out the project’s &lt;a href=&quot;https://blog.openmined.org/openmined-pytorch-fellowship-crypten-project&quot;&gt;call for contributors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;federated-learning-on-mobile-web-and-iot-devices&quot;&gt;Federated Learning on Mobile, Web, and IoT Devices&lt;/h3&gt;

&lt;p&gt;During these fellowships, we will be extending PyTorch with the ability to perform federated learning across mobile, web, and IoT devices. To this end, a PyTorch front-end will be able to coordinate across federated learning backends that run in Javascript, Kotlin, Swift, and Python. Furthermore, we will also extend PySyft with the ability to coordinate these backends using peer-to-peer connections, providing low latency and the ability to run secure aggregation as a part of the protocol. For more information on the roadmap and how to apply for a paid fellowship, check out the project’s &lt;a href=&quot;https://blog.openmined.org/announcing-the-pytorch-openmined-federated-learning-fellowships&quot;&gt;call for contributors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;development-challenges&quot;&gt;Development Challenges&lt;/h3&gt;

&lt;p&gt;Over the coming months, we will issue regular open competitions for increasing the performance and security of the PySyft and PyGrid codebases. For performance-related challenges, contestants will compete (for a cash prize) to make a specific PySyft demo (such as federated learning) as fast as possible. For security-related challenges, contestants will compete to hack into a PyGrid server. The first to demonstrate their ability will win the cash bounty! For more information on the challenges and to sign up to receive emails when each challenge is opened, &lt;a href=&quot;http://blog.openmined.org/announcing-the-openmined-pytorch-development-challenges&quot;&gt;sign up here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To apply, select one of the above projects and identify a role that matches your strengths!&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Andrew, Laurens, Joe, and Shubho&lt;/p&gt;</content><author><name>Andrew Trask (OpenMined/U.Oxford), Shubho Sengupta, Laurens van der Maaten, Joe Spisak</name></author><summary type="html"></summary></entry><entry><title type="html">PyTorch 1.3 adds mobile, privacy, quantization, and named tensors</title><link href="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/" rel="alternate" type="text/html" title="PyTorch 1.3 adds mobile, privacy, quantization, and named tensors" /><published>2019-10-10T00:00:00-07:00</published><updated>2019-10-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/">&lt;p&gt;PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv &lt;a href=&quot;https://www.oreilly.com/ideas/one-simple-graphic-researchers-love-pytorch-and-tensorflow?fbclid=IwAR3kYmlyD7zky37IYFu0cafQn7yemhl8P-7MNyB30z0q5RDzxcTOrP8kxDk&quot;&gt;grew 194 percent in the first half of 2019 alone, as noted by O’Reilly&lt;/a&gt;, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.&lt;/p&gt;

&lt;p&gt;We are now advancing the platform further with the release of PyTorch 1.3, which includes experimental support for features such as seamless model deployment to mobile devices, model quantization for better performance at inference time, and front-end improvements, like the ability to name tensors and create clearer code with less need for inline comments. We’re also launching a number of additional tools and libraries to support model interpretability and bringing multimodal research to production.&lt;/p&gt;

&lt;p&gt;Additionally, we’ve collaborated with Google and Salesforce to add broad support for Cloud Tensor Processing Units, providing a significantly accelerated option for training large-scale deep neural networks. &lt;a href=&quot;https://data.aliyun.com/bigdata/pai-pytorch?spm=5176.12825654.a9ylfrljh.d112.7b652c4ayuOO4M&amp;amp;scm=20140722.1068.1.1098&amp;amp;aly_as=-PvJ5e4c&quot;&gt;Alibaba Cloud&lt;/a&gt; also joins Amazon Web Services, Microsoft Azure, and Google Cloud as supported cloud platforms for PyTorch users. You can get started now at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-13&quot;&gt;PyTorch 1.3&lt;/h1&gt;

&lt;p&gt;The 1.3 release of PyTorch brings significant new features, including experimental support for mobile device deployment, eager mode quantization at 8-bit integer, and the ability to name tensors. With each of these enhancements, we look forward to additional contributions and improvements from the PyTorch community.&lt;/p&gt;

&lt;h2 id=&quot;named-tensors-experimental&quot;&gt;Named tensors (experimental)&lt;/h2&gt;

&lt;p&gt;Cornell University’s &lt;a href=&quot;http://nlp.seas.harvard.edu/NamedTensor&quot;&gt;Sasha Rush has argued&lt;/a&gt; that, despite its ubiquity in deep learning, the traditional implementation of tensors has significant shortcomings, such as exposing private dimensions, broadcasting based on absolute position, and keeping type information in documentation. He proposed named tensors as an alternative approach.&lt;/p&gt;

&lt;p&gt;Today, we name and access dimensions by comment:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Tensor[N, C, H, W]&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But naming explicitly leads to more readable and maintainable code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;quantization-experimental&quot;&gt;Quantization (experimental)&lt;/h2&gt;

&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing ML applications. To support more efficient deployment on servers and edge devices, PyTorch 1.3 now supports 8-bit model quantization using the familiar eager mode Python API. Quantization refers to techniques used to perform computation and storage at reduced precision, such as 8-bit integer. This currently experimental feature includes support for post-training quantization, dynamic quantization, and quantization-aware training. It leverages the &lt;a href=&quot;https://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/QNNPACK&quot;&gt;QNNPACK&lt;/a&gt; state-of-the-art quantized kernel back ends, for x86 and ARM CPUs, respectively, which are integrated with PyTorch and now share a common API.&lt;/p&gt;

&lt;p&gt;To learn more about the design and architecture, check out the API docs &lt;a href=&quot;https://pytorch.org/docs/master/quantization.html&quot;&gt;here&lt;/a&gt;, and get started with any of the supported techniques using the tutorials available &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-mobile-experimental&quot;&gt;PyTorch mobile (experimental)&lt;/h2&gt;

&lt;p&gt;Running ML on edge devices is growing in importance as applications continue to demand lower latency. It is also a foundational element for privacy-preserving techniques such as federated learning. To enable more efficient on-device ML, PyTorch 1.3 now supports an end-to-end workflow from Python to deployment on iOS and Android.&lt;/p&gt;

&lt;p&gt;This is an early, experimental release, optimized for end-to-end development. Coming releases will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimization for size: Build level optimization and selective compilation depending on the operators needed for user applications (i.e., you pay binary size for only the operators you need)&lt;/li&gt;
  &lt;li&gt;Performance: Further improvements to performance and coverage on mobile CPUs and GPUs&lt;/li&gt;
  &lt;li&gt;High level API: Extend mobile native APIs to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications. e.g. Computer vision and NLP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more or get started on Android or iOS &lt;a href=&quot;http://pytorch.org/mobile&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;new-tools-for-model-interpretability-and-privacy&quot;&gt;New tools for model interpretability and privacy&lt;/h1&gt;

&lt;h2 id=&quot;captum&quot;&gt;Captum&lt;/h2&gt;

&lt;p&gt;As models become ever more complex, it is increasingly important to develop new methods for model interpretability. To help address this need, we’re launching Captum, a tool to help developers working in PyTorch understand why their model generates a specific output. Captum provides state-of-the-art tools to understand how the importance of specific neurons and layers and affect predictions made by the models. Captum’s algorithms include integrated gradients, conductance, SmoothGrad and VarGrad, and DeepLift.&lt;/p&gt;

&lt;p&gt;The example below shows how to apply model interpretability algorithms on a pretrained ResNet model and then visualize the attributions for each pixel by overlaying them on the image.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoiseTunnel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;integrated_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nt_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'smoothgrad_sq'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_label_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;viz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize_image_attr_multiple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;original_image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;heat_map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;all&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;positive&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformed_img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;show_colorbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 1.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Learn more about Captum at &lt;a href=&quot;https://www.captum.ai/&quot;&gt;captum.ai&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;crypten&quot;&gt;CrypTen&lt;/h2&gt;

&lt;p&gt;Practical applications of ML via cloud-based or machine-learning-as-a-service (MLaaS) platforms pose a range of security and privacy challenges. In particular, users of these platforms may not want or be able to share unencrypted data, which prevents them from taking full advantage of ML tools. To address these challenges, the ML community is exploring a number of technical approaches, at various levels of maturity. These include homomorphic encryption, secure multiparty computation, trusted execution environments, on-device computation, and differential privacy.&lt;/p&gt;

&lt;p&gt;To provide a better understanding of how some of these technologies can be applied, we are releasing CrypTen, a new community-based research platform for taking the field of privacy-preserving ML forward. Learn more about CrypTen &lt;a href=&quot;https://ai.facebook.com/blog/crypten-a-new-research-tool-for-secure-machine-learning-with-pytorch&quot;&gt;here&lt;/a&gt;. It is available on GitHub &lt;a href=&quot;https://github.com/facebookresearch/CrypTen&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;tools-for-multimodal-ai-systems&quot;&gt;Tools for multimodal AI systems&lt;/h1&gt;

&lt;p&gt;Digital content is often made up of several modalities, such as text, images, audio, and video. For example, a single public post might contain an image, body text, a title, a video, and a landing page. Even one particular component may have more than one modality, such as a video that contains both visual and audio signals, or a landing page that is composed of images, text, and HTML sources.&lt;/p&gt;

&lt;p&gt;The ecosystem of tools and libraries that work with PyTorch offer enhanced ways to address the challenges of building multimodal ML systems. Here are some of the latest libraries launching today:&lt;/p&gt;

&lt;h2 id=&quot;detectron2&quot;&gt;Detectron2&lt;/h2&gt;

&lt;p&gt;Object detection and segmentation are used for tasks ranging from autonomous vehicles to content understanding for platform integrity. To advance this work, Facebook AI Research (FAIR) is releasing Detectron2, an object detection library now implemented in PyTorch. Detectron2 provides support for the latest models and tasks, increased flexibility to aid computer vision research, and improvements in maintainability and scalability to support production use cases.&lt;/p&gt;

&lt;p&gt;Detectron2 is available &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;here&lt;/a&gt; and you can learn more &lt;a href=&quot;https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;speech-extensions-to-fairseq&quot;&gt;Speech extensions to fairseq&lt;/h2&gt;

&lt;p&gt;Language translation and audio processing are critical components in systems and applications such as search, translation, speech, and assistants. There has been tremendous progress in these fields recently thanks to the development of new architectures like transformers, as well as large-scale pretraining methods. We’ve extended fairseq, a framework for sequence-to-sequence applications such as language translation, to include support for end-to-end learning for speech and audio recognition tasks.These extensions to fairseq enable faster exploration and prototyping of new speech research ideas while offering a clear path to production.&lt;/p&gt;

&lt;p&gt;Get started with fairseq &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;cloud-provider-and-hardware-ecosystem-support&quot;&gt;Cloud provider and hardware ecosystem support&lt;/h1&gt;

&lt;p&gt;Cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud provide extensive support for anyone looking to develop ML on PyTorch and deploy in production. We’re excited to share the general availability of Google Cloud TPU support and a newly launched integration with Alibaba Cloud. We’re also expanding hardware ecosystem support.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google Cloud TPU support now broadly available. To accelerate the largest-scale machine learning (ML) applications deployed today and enable rapid development of the ML applications of tomorrow, Google created custom silicon chips called Tensor Processing Units (&lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;TPUs&lt;/a&gt;). When assembled into multi-rack ML supercomputers called &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-pods-break-ai-training-records&quot;&gt;Cloud TPU Pods&lt;/a&gt;, these TPUs can complete ML workloads in minutes or hours that previously took days or weeks on other systems. Engineers from Facebook, Google, and Salesforce worked together to enable and pilot Cloud TPU support in PyTorch, including experimental support for Cloud TPU Pods. PyTorch support for Cloud TPUs is also available in Colab. Learn more about how to get started with PyTorch on Cloud TPUs &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Alibaba adds support for PyTorch in Alibaba Cloud. The initial integration involves a one-click solution for PyTorch 1.x, Data Science Workshop notebook service, distributed training with Gloo/NCCL, as well as seamless integration with Alibaba IaaS such as OSS, ODPS, and NAS. Together with the toolchain provided by Alibaba, we look forward to significantly reducing the overhead necessary for adoption, as well as helping Alibaba Cloud’s global customer base leverage PyTorch to develop new AI applications.&lt;/li&gt;
  &lt;li&gt;ML hardware ecosystem expands. In addition to key GPU and CPU partners, the PyTorch ecosystem has also enabled support for dedicated ML accelerators. Updates from &lt;a href=&quot;https://www.intel.ai/nnpi-glow-pytorch/&quot;&gt;Intel&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@HabanaLabs/unlocking-ai-scaling-through-software-and-hardware-interface-standardization-77561cb7598b&quot;&gt;Habana&lt;/a&gt; showcase how PyTorch, connected to the Glow optimizing compiler, enables developers to utilize these market-specific solutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;growth-in-the-pytorch-community&quot;&gt;Growth in the PyTorch community&lt;/h1&gt;

&lt;p&gt;As an open source, community-driven project, PyTorch benefits from wide range of contributors bringing new capabilities to the ecosystem. Here are some recent examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mila SpeechBrain aims to provide an open source, all-in-one speech toolkit based on PyTorch. The goal is to develop a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art systems for speech recognition (both end to end and HMM-DNN), speaker recognition, speech separation, multi-microphone signal processing (e.g., beamforming), self-supervised learning, and many others. &lt;a href=&quot;https://speechbrain.github.io/&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SpaCy is a new wrapping library with consistent and easy-to-use interfaces to several models, in order to extract features to power NLP pipelines. Support is provided for via spaCy’s standard training API. The library also calculates an alignment so the transformer features can be related back to actual words instead of just wordpieces. &lt;a href=&quot;https://explosion.ai/blog/spacy-pytorch-transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;HuggingFace PyTorch-Transformers (formerly known as pytorch-pretrained-bert is a library of state-of-the-art pretrained models for Natural Language Processing (NLP). The library currently contains PyTorch implementations, pretrained model weights, usage scripts, and conversion utilities for models such as BERT, GPT-2, RoBERTa, and DistilBERT. It has also grown quickly, with more than 13,000 GitHub stars and a broad set of users. &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PyTorch Lightning is a Keras-like ML library for PyTorch. It leaves core training and validation logic to you and automates the rest. Reproducibility is a crucial requirement for many fields of research, including those based on ML techniques. As the number of research papers submitted to arXiv and conferences skyrockets into the tens of thousands, scaling reproducibility becomes difficult. &lt;a href=&quot;https://github.com/williamFalcon/pytorch-lightning&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We recently held the first online Global PyTorch Summer Hackathon, where researchers and developers around the world were invited to build innovative new projects with PyTorch. Nearly 1,500 developers participated, submitting projects ranging from livestock disease detection to AI-powered financial assistants. The winning projects were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torchmeta, which provides extensions for PyTorch to simplify the development of meta-learning algorithms in PyTorch. It features a unified interface inspired by TorchVision for both few-shot classification and regression problems, to allow easy benchmarking on multiple data sets to aid with reproducibility.&lt;/li&gt;
  &lt;li&gt;Open-Unmix, a system for end-to-end music demixing with PyTorch. Demixing separates the individual instruments or vocal track from any stereo recording.&lt;/li&gt;
  &lt;li&gt;Endless AI-Generated Tees, a store featuring AI-generated T-shirt designs that can be purchased and delivered worldwide. The system uses a state-of-the-art generative model (StyleGAN) that was built with PyTorch and then trained on modern art.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Visit &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt; to learn more and get started with PyTorch 1.3 and the latest libraries and ecosystem projects. We look forward to the contributions, exciting research advancements, and real-world applications that the community builds with PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv grew 194 percent in the first half of 2019 alone, as noted by O’Reilly, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.</summary></entry><entry><title type="html">New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4</title><link href="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/" rel="alternate" type="text/html" title="New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4" /><published>2019-08-08T00:00:00-07:00</published><updated>2019-08-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.2-and-domain-api-release</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/">&lt;p&gt;Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.&lt;/p&gt;

&lt;p&gt;From a core perspective, PyTorch has continued to add features to support both research and production usage, including the ability to bridge these two worlds via &lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;TorchScript&lt;/a&gt;. Today, we are excited to announce that we have four new releases including PyTorch 1.2, torchvision 0.4, torchaudio 0.3, and torchtext 0.4. You can get started now with any of these releases at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-12&quot;&gt;PyTorch 1.2&lt;/h1&gt;

&lt;p&gt;With PyTorch 1.2, the open source ML framework takes a major step forward for production usage with the addition of an improved and more polished TorchScript environment. These improvements make it even easier to ship production models, expand support for exporting ONNX formatted models, and enhance module level support for Transformers. In addition to these new features, &lt;a href=&quot;https://pytorch.org/docs/stable/tensorboard.html&quot;&gt;TensorBoard&lt;/a&gt; is now no longer experimental - you can simply type &lt;code class=&quot;highlighter-rouge&quot;&gt;from torch.utils.tensorboard import SummaryWriter&lt;/code&gt; to get started.&lt;/p&gt;

&lt;h2 id=&quot;torchscript-improvements&quot;&gt;TorchScript Improvements&lt;/h2&gt;

&lt;p&gt;Since its release in PyTorch 1.0, TorchScript has provided a path to production for eager PyTorch models. The TorchScript compiler converts PyTorch models to a statically typed graph representation, opening up opportunities for
optimization and execution in constrained environments where Python is not available. You can incrementally convert your model to TorchScript, mixing compiled code seamlessly with Python.&lt;/p&gt;

&lt;p&gt;PyTorch 1.2 significantly expands TorchScript’s support for the subset of Python used in PyTorch models and delivers a new, easier-to-use API for compiling your models to TorchScript. See the &lt;a href=&quot;https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api&quot;&gt;migration guide&lt;/a&gt; for details. Below is an example usage of the new API:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Compile the model code to a static representation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Save the compiled code and model data so it can be loaded elsewhere&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_script_module.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To learn more, see our &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a
PyTorch Model in C++&lt;/a&gt; tutorials.&lt;/p&gt;

&lt;h2 id=&quot;expanded-onnx-export&quot;&gt;Expanded ONNX Export&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; community continues to grow with an open &lt;a href=&quot;https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!&quot;&gt;governance structure&lt;/a&gt; and additional steering committee members, special interest groups (SIGs), and working groups (WGs). In collaboration with Microsoft, we’ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We’ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. ScriptModule has also been improved including support for multiple outputs, tensor factories, and tuples as inputs and outputs. Additionally, users are now able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for multiple Opsets including the ability to export dropout, slice, flip, and interpolate in Opset 10.&lt;/li&gt;
  &lt;li&gt;Improvements to ScriptModule including support for multiple outputs, tensor factories, and tuples as inputs and outputs.&lt;/li&gt;
  &lt;li&gt;More than a dozen additional PyTorch operators supported including the ability to export a custom operator.&lt;/li&gt;
  &lt;li&gt;Many big fixes and test infra improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out the latest tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;here&lt;/a&gt;, contributed by @lara-hdr at Microsoft. A big thank you to the entire Microsoft team for all of their hard work to make this release happen!&lt;/p&gt;

&lt;h2 id=&quot;nntransformer&quot;&gt;nn.Transformer&lt;/h2&gt;

&lt;p&gt;In PyTorch 1.2, we now include a standard &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer&quot;&gt;nn.Transformer&lt;/a&gt; module, based on the paper “&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;”. The &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module relies entirely on an &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention&quot;&gt;attention mechanism&lt;/a&gt; to draw global dependencies between input and output.  The individual components of the &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module are designed so they can be adopted independently. For example, the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&quot;&gt;nn.TransformerEncoder&lt;/a&gt; can be used by itself, without the larger &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;. The new APIs include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoderLayer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoderLayer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/transformer.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#transformer-layers&quot;&gt;Transformer Layers&lt;/a&gt; documentation for more information. See &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt; for the full PyTorch 1.2 release notes.&lt;/p&gt;

&lt;h1 id=&quot;domain-api-library-updates&quot;&gt;Domain API Library Updates&lt;/h1&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio provide convenient access to common datasets, models, and transforms that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. Since research domains have distinct requirements, an ecosystem of specialized libraries called domain APIs (DAPI) has emerged around PyTorch to simplify the development of new and existing algorithms in a number of fields. We’re excited to release three updated DAPI libraries for text, audio, and vision that compliment the PyTorch 1.2 core release.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio-03-with-kaldi-compatibility-new-transforms&quot;&gt;Torchaudio 0.3 with Kaldi Compatibility, New Transforms&lt;/h2&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/spectrograms.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Torchaudio specializes in machine understanding of audio waveforms. It is an ML library that provides relevant signal processing functionality (but is not a general signal processing library). It leverages PyTorch’s GPU support to provide many tools and transformations for waveforms to make data loading and standardization easier and more readable. For example, it offers data loaders for waveforms using sox, and transformations such as spectrograms, resampling, and mu-law encoding and decoding.&lt;/p&gt;

&lt;p&gt;We are happy to announce the availability of torchaudio 0.3.0, with a focus on standardization and complex numbers, a transformation (resample) and two new functionals (phase_vocoder, ISTFT), Kaldi compatibility, and a new tutorial. Torchaudio was redesigned to be an extension of PyTorch and a part of the domain APIs (DAPI) ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;Significant effort in solving machine learning problems goes into data preparation. In this new release, we’ve updated torchaudio’s interfaces for its transformations to standardize around the following vocabulary and conventions.&lt;/p&gt;

&lt;p&gt;Tensors are assumed to have channel as the first dimension and time as the last dimension (when applicable). This makes it consistent with PyTorch’s dimensions. For size names, the prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;n_&lt;/code&gt; is used (e.g. “a tensor of size (&lt;code class=&quot;highlighter-rouge&quot;&gt;n_freq&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;n_mel&lt;/code&gt;)”) whereas dimension names do not have this prefix (e.g. “a tensor of dimension (channel, time)”). The input of all transforms and functions now assumes channel first. This is done to be consistent with PyTorch, which has channel followed by the number of samples. The channel parameter of all transforms and functions is now deprecated.&lt;/p&gt;

&lt;p&gt;The output of &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; is (channel, frequency, time, 2), meaning for each channel, the columns are the Fourier transform of a certain window, so as we travel horizontally we can see each column (the Fourier transformed waveform) change over time. This matches the output of librosa so we no longer need to transpose in our test comparisons with &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;. Moreover, because of these new conventions, we deprecated &lt;code class=&quot;highlighter-rouge&quot;&gt;LC2CL&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;BLC2CBL&lt;/code&gt; which were used to transfer from one shape of signal to another.&lt;/p&gt;

&lt;p&gt;As part of this release, we’re also introducing support for complex numbers via tensors of dimension (…, 2), and providing &lt;code class=&quot;highlighter-rouge&quot;&gt;magphase&lt;/code&gt; to convert such a tensor into its magnitude and phase, and similarly &lt;code class=&quot;highlighter-rouge&quot;&gt;complex_norm&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;angle&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The details of the standardization are provided in the &lt;a href=&quot;https://github.com/pytorch/audio/blob/v0.3.0/README.md#Conventions&quot;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;functionals-transformations-and-kaldi-compatibility&quot;&gt;Functionals, Transformations, and Kaldi Compatibility&lt;/h3&gt;

&lt;p&gt;Prior to the standardization, we separated state and computation into &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.transforms&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.functional&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As part of the transforms, we’re adding a new transformation in 0.3.0: &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt; can upsample or downsample a waveform to a different frequency.&lt;/p&gt;

&lt;p&gt;As part of the functionals, we’re introducing: &lt;code class=&quot;highlighter-rouge&quot;&gt;phase_vocoder&lt;/code&gt;, a phase vocoder to change the speed of a waveform without changing its pitch, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ISTFT&lt;/code&gt;, the inverse &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; implemented to be compatible with STFT provided by PyTorch. This separation allows us to make functionals weak scriptable and to utilize JIT in 0.3.0. We thus have JIT and CUDA support for the following transformations: &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;AmplitudeToDB&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;SpectrogramToDB&lt;/code&gt;), &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawEncoding&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawDecoding&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawExpanding&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;We now also provide a compatibility interface with Kaldi to ease onboarding and reduce a user’s code dependency on Kaldi. We now have an interface for &lt;code class=&quot;highlighter-rouge&quot;&gt;spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fbank&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;resample_waveform&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-tutorial&quot;&gt;New Tutorial&lt;/h3&gt;

&lt;p&gt;To showcase the new conventions and transformations, we have a &lt;a href=&quot;https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html&quot;&gt;new tutorial&lt;/a&gt; demonstrating how to preprocess waveforms using torchaudio. This tutorial walks through an example of loading a waveform and applying some of the available transformations to it.&lt;/p&gt;

&lt;p&gt;We are excited to see an active community around torchaudio and eager to further grow and support it. We encourage you to go ahead and experiment for yourself with this tutorial and the two datasets that are available: VCTK and YESNO! They have an interface to download the datasets and preprocess them in a convenient format. You can find the details in the release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext-04-with-supervised-learning-datasets&quot;&gt;Torchtext 0.4 with supervised learning datasets&lt;/h2&gt;

&lt;p&gt;A key focus area of torchtext is to provide the fundamental elements to help accelerate NLP research. This includes easy access to commonly used datasets and basic preprocessing pipelines for working on raw text based data. The torchtext 0.4.0 release includes several popular supervised learning baselines with “one-command” data loading. A &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial&lt;/a&gt; is included to show how to use the new datasets for text classification analysis. We also added and improved on a few functions such as &lt;a href=&quot;https://pytorch.org/text/data.html?highlight=get_tokenizer#torchtext.data.get_tokenizer&quot;&gt;get_tokenizer&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/text/vocab.html#build-vocab-from-iterator&quot;&gt;build_vocab_from_iterator&lt;/a&gt; to make it easier to implement future datasets. Additional examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Text classification is an important task in Natural Language Processing with many applications, such as sentiment analysis. The new release includes several popular &lt;a href=&quot;https://pytorch.org/text/datasets.html?highlight=textclassification#torchtext.datasets.TextClassificationDataset&quot;&gt;text classification datasets&lt;/a&gt; for supervised learning including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AG_NEWS&lt;/li&gt;
  &lt;li&gt;SogouNews&lt;/li&gt;
  &lt;li&gt;DBpedia&lt;/li&gt;
  &lt;li&gt;YelpReviewPolarity&lt;/li&gt;
  &lt;li&gt;YelpReviewFull&lt;/li&gt;
  &lt;li&gt;YahooAnswers&lt;/li&gt;
  &lt;li&gt;AmazonReviewPolarity&lt;/li&gt;
  &lt;li&gt;AmazonReviewFull&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each dataset comes with two parts (train vs. test), and can be easily loaded with a single command. The datasets also support an ngrams feature to capture the partial information about the local word order. Take a look at the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;here&lt;/a&gt; to learn more about how to use the new datasets for supervised problems such as text classification analysis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.datasets.text_classification&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AG_NEWS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrams&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the domain library, PyTorch provides many tools to make data loading easy. Users now can load and preprocess the text classification datasets with some well supported tools, like &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html&quot;&gt;torch.utils.data.DataLoader&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset&quot;&gt;torch.utils.data.IterableDataset&lt;/a&gt;. Here are a few lines to wrap the data with DataLoader. More examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collate_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check out the release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; to learn more and try out the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision-04-with-support-for-video&quot;&gt;Torchvision 0.4 with Support for Video&lt;/h2&gt;

&lt;p&gt;Video is now a first-class citizen in torchvision, with support for data loading, datasets, pre-trained models, and transforms. The 0.4 release of torchvision includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Efficient IO primitives for reading/writing video files (including audio), with support for arbitrary encodings and formats.&lt;/li&gt;
  &lt;li&gt;Standard video datasets, compatible with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Pre-trained models built on the Kinetics-400 dataset for action classification on videos (including the training scripts).&lt;/li&gt;
  &lt;li&gt;Reference training scripts for training your own video models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wanted working with video data in PyTorch to be as straightforward as possible, without compromising too much on performance.
As such, we avoid the steps that would require re-encoding the videos beforehand, as it would involve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A preprocessing step which duplicates the dataset in order to re-encode it.&lt;/li&gt;
  &lt;li&gt;An overhead in time and space because this re-encoding is time-consuming.&lt;/li&gt;
  &lt;li&gt;Generally, an external script should be used to perform the re-encoding.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, we provide APIs such as the utility class, &lt;code class=&quot;highlighter-rouge&quot;&gt;VideoClips&lt;/code&gt;, that simplifies the task of enumerating all possible clips of fixed size in a list of video files by creating an index of all clips in a set of videos. It also allows you to specify a fixed frame-rate for the videos. An example of the API is provided below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets.video_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyVideoDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;clip_length_in_frames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frames_between_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frame_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_clips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Most of the user-facing API is in Python, similar to PyTorch, which makes it easily extensible. Plus, the underlying implementation is fast — torchvision decodes as little as possible from the video on-the-fly in order to return a clip from the video.&lt;/p&gt;

&lt;p&gt;Check out the torchvision 0.4 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes here&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;We look forward to continuing our collaboration with the community and hearing your feedback as we further improve and expand the PyTorch deep learning platform.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all of the contributions to this work!&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.</summary></entry><entry><title type="html">Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm</title><link href="https://pytorch.org/blog/mapillary-research/" rel="alternate" type="text/html" title="Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm" /><published>2019-07-23T00:00:00-07:00</published><updated>2019-07-23T00:00:00-07:00</updated><id>https://pytorch.org/blog/mapillary-research</id><content type="html" xml:base="https://pytorch.org/blog/mapillary-research/">&lt;p&gt;With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.&lt;/p&gt;

&lt;p&gt;Today, people and organizations all over the world have contributed more than 600 million images toward Mapillary’s mission of helping people understand the world’s places through images and making this data available, with clients and partners including the World Bank, HERE, and Toyota Research Institute.&lt;/p&gt;

&lt;p&gt;Mapillary’s computer vision technology brings intelligence to maps in an unprecedented way, increasing our overall understanding of the world. &lt;a href=&quot;https://www.mapillary.com/&quot;&gt;Mapillary&lt;/a&gt; runs state-of-the-art semantic image analysis and image-based 3d modeling at scale and on all its images. In this post we discuss two recent works from Mapillary Research and their implementations in PyTorch - Seamless Scene Segmentation [1] and In-Place Activated BatchNorm [2] - generating Panoptic segmentation results and saving up to 50% of GPU memory during training, respectively.&lt;/p&gt;

&lt;h2 id=&quot;seamless-scene-segmentation&quot;&gt;Seamless Scene Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/seamseg/&quot;&gt;https://github.com/mapillary/seamseg/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/seamless.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The objective of Seamless Scene Segmentation is to predict a “panoptic” segmentation [3] from an image, that is a complete labeling where each pixel is assigned with a class id and, where possible, an instance id. Like many modern CNNs dealing with instance detection and segmentation, we adopt the Mask R-CNN framework [4], using ResNet50 + FPN [5] as a backbone. This architecture works in two stages: first, the “Proposal Head” selects a set of candidate bounding boxes on the image (i.e. the proposals) that could contain an object; then, the “Mask Head” focuses on each proposal, predicting its class and segmentation mask. The output of this process is a “sparse” instance segmentation, covering only the parts of the image that contain countable objects (e.g. cars and pedestrians).&lt;/p&gt;

&lt;p&gt;To complete our panoptic approach coined Seamless Scene Segmentation, we add a third stage to Mask R-CNN. Stemming from the same backbone, the “Semantic Head” predicts a dense semantic segmentation over the whole image, also accounting for the uncountable or amorphous classes (e.g. road and sky). The outputs of the Mask and Semantic heads are finally fused using a simple non-maximum suppression algorithm to generate the final panoptic prediction. All details about the actual network architecture, used losses and underlying math can be found at the &lt;a href=&quot;https://research.mapillary.com/publication/cvpr19a&quot;&gt;project website&lt;/a&gt; for our CVPR 2019 paper [1].&lt;/p&gt;

&lt;p&gt;While several versions of Mask R-CNN are publicly available, including an &lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;official implementation&lt;/a&gt; written in Caffe2, at Mapillary we decided to build Seamless Scene Segmentation from scratch using PyTorch, in order to have full control and understanding of the whole pipeline. While doing so we encountered a couple of main stumbling blocks, and had to come up with some creative workarounds we are going to describe next.&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-variable-sized-tensors&quot;&gt;Dealing with variable-sized tensors&lt;/h2&gt;

&lt;p&gt;Something that sets aside panoptic segmentation networks from traditional CNNs is the prevalence of variable-sized data. In fact, many of the quantities we are dealing with cannot be easily represented with fixed sized tensors: each image contains a different number of objects, the Proposal head can produce a different number of proposals for each image, and the images themselves can have different sizes. While this is not a problem per-se – one could just process images one at a time – we would still like to exploit batch-level parallelism as much as possible. Furthermore, when performing distributed training with multiple GPUs, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; expects its inputs to be batched, uniformly-sized tensors.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/packed_sequence.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Our solution to these issues is to wrap each batch of variable-sized tensors in a &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; is little more than a glorified list class for tensors, tagging its contents as “related”, ensuring that they all share the same type, and providing useful methods like moving all the tensors to a particular device, etc. When performing light-weight operations that wouldn’t be much faster with batch-level parallelism, we simply iterate over the contents of the &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; in a for loop. When performance is crucial, e.g. in the body of the network, we simply concatenate the contents of the PackedSequence, adding zero padding as required (like in RNNs with variable-length inputs), and keeping track of the original dimensions of each tensor.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;s also help us deal with the second problem highlighted above. We slightly modify &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; to recognize &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; inputs, splitting them in equally sized chunks and distributing their contents across the GPUs.&lt;/p&gt;

&lt;h2 id=&quot;asymmetric-computational-graphs-with-distributed-data-parallel&quot;&gt;Asymmetric computational graphs with Distributed Data Parallel&lt;/h2&gt;

&lt;p&gt;Another, perhaps more subtle, peculiarity of our network is that it can generate asymmetric computational graphs across GPUs. In fact, some of the modules that compose the network are “optional”, in the sense that they are not always computed for all images. As an example, when the Proposal head doesn’t output any proposal, the Mask head is not traversed at all. If we are training on multiple GPUs with &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt;, this results in one of the replicas not computing gradients for the Mask head parameters.&lt;/p&gt;

&lt;p&gt;Prior to PyTorch 1.1, this resulted in a crash, so we had to develop a workaround. Our simple but effective solution was to compute a “fake forward pass” when no actual forward is required, i.e. something like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fake_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_correctly_shaped_fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we generate a batch of bogus data, pass it through the Mask head, and return a loss that always back-progates zeros to all parameters.&lt;/p&gt;

&lt;p&gt;Starting from PyTorch 1.1 this workaround is no longer required: by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;find_unused_parameters=True&lt;/code&gt; in the constructor, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; is told to identify parameters whose gradients have not been computed by all replicas and correctly handle them. This leads to some substantial simplifications in our code base!&lt;/p&gt;

&lt;h2 id=&quot;in-place-activated-batchnorm&quot;&gt;In-place Activated BatchNorm&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Most researchers would probably agree that there are always constraints in terms of available GPU resources, regardless if their research lab has access to only a few or multiple thousands of GPUs. In a time where at Mapillary we still worked at rather few and mostly 12GB Titan X - style prosumer GPUs, we were searching for a solution that virtually enhances the usable memory during training, so we would be able to obtain and push state-of-the-art results on dense labeling tasks like semantic segmentation. In-place activated BatchNorm is enabling us to use up to 50% more memory (at little computational overhead) and is therefore deeply integrated in all our current projects (including Seamless Scene Segmentation described above).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/inplace_abn.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When processing a BN-Activation-Convolution sequence in the forward pass, most deep learning frameworks (including PyTorch) need to store two big buffers, i.e. the input x of BN and the input z of Conv. This is necessary because the standard implementations of the backward passes of BN and Conv depend on their inputs to calculate the gradients. Using InPlace-ABN to replace the BN-Activation sequence, we can safely discard x, thus saving up to 50% GPU memory at training time. To achieve this, we rewrite the backward pass of BN in terms of its output y, which is in turn reconstructed from z by inverting the activation function.&lt;/p&gt;

&lt;p&gt;The only limitation of InPlace-ABN is that it requires using an invertible activation function, such as leaky relu or elu. Except for this, it can be used as a direct, drop-in replacement for BN+activation modules in any network. Our native CUDA implementation offers minimal computational overhead compared to PyTorch’s standard BN, and is available for anyone to use from here: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;synchronized-bn-with-asymmetric-graphs-and-unbalanced-batches&quot;&gt;Synchronized BN with asymmetric graphs and unbalanced batches&lt;/h2&gt;

&lt;p&gt;When training networks with synchronized SGD over multiple GPUs and/or multiple nodes, it’s common practice to compute BatchNorm statistics separately on each device. However, in our experience working with semantic and panoptic segmentation networks, we found that accumulating mean and variance across all workers can bring a substantial boost in accuracy. This is particularly true when dealing with small batches, like in Seamless Scene Segmentation where we train with a single, super-high resolution image per GPU.&lt;/p&gt;

&lt;p&gt;InPlace-ABN supports synchronized operation over multiple GPUs and multiple nodes, and, since version 1.1, this can also be achieved in the standard PyTorch library using &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#syncbatchnorm&quot;&gt;SyncBatchNorm&lt;/a&gt;. Compared to SyncBatchNorm, however, we support some additional functionality which is particularly important for Seamless Scene Segmentation: unbalanced batches and asymmetric graphs.&lt;/p&gt;

&lt;p&gt;As mentioned before, Mask R-CNN-like networks naturally give rise to variable-sized tensors. Thus, in InPlace-ABN we calculate synchronized statistics using a variant of the parallel algorithm described &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm&quot;&gt;here&lt;/a&gt;, which properly takes into account the fact that each GPU can hold a different number of samples. PyTorch’s SyncBatchNorm is currently being revised to support this, and the improved functionality will be available in a future release.&lt;/p&gt;

&lt;p&gt;Asymmetric graphs (in the sense mentioned above) are another complicating factor one has to deal with when creating a synchronized BatchNorm implementation. Luckily, PyTorch’s distributed group functionality allows us to restrict distributed communication to a subset of workers, easily excluding those that are currently inactive. The only missing piece is that, in order to create a distributed group, each process needs to know the ids of all processes that will participate in the group, and even processes that are not part of the group need to call the &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; function. In InPlace-ABN we handle it with a function like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;active_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Initialize a distributed group where each process can independently decide whether to participate or not&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Gather active status from all workers&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Create group&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First each process, including inactive ones, communicates its status to all others through an &lt;code class=&quot;highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; call, then it creates the distributed group with the shared information. In the actual implementation we also include a caching mechanism for groups, since &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; is usually too expensive to call at each batch.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Seamless Scene Segmentation; Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[2] In-place Activated BatchNorm for Memory-Optimized Training of DNNs; Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2018&lt;/p&gt;

&lt;p&gt;[3] Panoptic Segmentation; Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollar; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[4] Mask R-CNN; Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick; International Conference on Computer Vision (ICCV), 2017&lt;/p&gt;

&lt;p&gt;[5] Feature Pyramid Networks for Object Detection; Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie; Computer Vision and Pattern Recognition (CVPR), 2017&lt;/p&gt;</content><author><name>Lorenzo Porzi, Mapillary</name></author><summary type="html">With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.</summary></entry><entry><title type="html">PyTorch Adds New Ecosystem Projects for Encrypted AI and Quantum Computing, Expands PyTorch Hub</title><link href="https://pytorch.org/blog/pytorch-ecosystem/" rel="alternate" type="text/html" title="PyTorch Adds New Ecosystem Projects for Encrypted AI and Quantum Computing, Expands PyTorch Hub" /><published>2019-07-18T00:00:00-07:00</published><updated>2019-07-18T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-ecosystem</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-ecosystem/">&lt;p&gt;The PyTorch ecosystem includes projects, tools, models and libraries from a broad community of researchers in academia and industry, application developers, and ML engineers. The goal of this ecosystem is to support, accelerate, and aid in your exploration with PyTorch and help you push the state of the art, no matter what field you are exploring. Similarly, we are expanding the recently launched PyTorch Hub to further help you discover and reproduce the latest research.&lt;/p&gt;

&lt;p&gt;In this post, we’ll highlight some of the projects that have been added to the PyTorch ecosystem this year and provide some context on the criteria we use to evaluate community projects. We’ll also provide an update on the fast-growing PyTorch Hub and share details on our upcoming PyTorch Summer Hackathon.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-ecosystem.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;recently-added-ecosystem-projects&quot;&gt;Recently added ecosystem projects&lt;/h2&gt;

&lt;p&gt;From private AI to quantum computing, we’ve seen the community continue to expand into new and interesting areas. The latest projects include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/BorealisAI/advertorch&quot;&gt;Advertorch&lt;/a&gt;: A Python toolbox for adversarial robustness research. The primary functionalities are implemented in PyTorch. Specifically, AdverTorch contains modules for generating adversarial perturbations and defending against adversarial examples, as well as scripts for adversarial training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://botorch.org/&quot;&gt;botorch&lt;/a&gt;: A modular and easily extensible interface for composing Bayesian optimization primitives, including probabilistic models, acquisition functions, and optimizers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/skorch-dev/skorch&quot;&gt;Skorch&lt;/a&gt;: A high-level library for PyTorch that provides full scikit-learn compatibility.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/rusty1s/pytorch_geometric&quot;&gt;PyTorch Geometric&lt;/a&gt;: A library for deep learning on irregular input data such as graphs, point clouds, and manifolds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/OpenMined/PySyft&quot;&gt;PySyft&lt;/a&gt;: A Python library for encrypted, privacy preserving deep learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pennylane.ai/&quot;&gt;PennyLane&lt;/a&gt;: A library for quantum ML, automatic differentiation, and optimization of hybrid quantum-classical computations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/zalandoresearch/flair&quot;&gt;Flair&lt;/a&gt;: A very simple framework for state-of-the-art natural language processing (NLP).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-makes-a-great-project&quot;&gt;What makes a great project?&lt;/h3&gt;

&lt;p&gt;When we review project submissions for the PyTorch ecosystem, we take into account a number of factors that we feel are important and that we would want in the projects we use ourselves. Some of these criteria include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Well-tested:&lt;/em&gt; Users should be confident that ecosystem projects will work well with PyTorch, and include support for CI to ensure that testing is occurring on a continuous basis and the project can run on the latest version of PyTorch.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Clear utility:&lt;/em&gt; Users should understand where each project fits within the PyTorch ecosystem and the value it brings.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Permissive licensing:&lt;/em&gt; Users must be able to utilize ecosystem projects without licensing concerns. e.g. BSD-3, Apache-2 and MIT licenses&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Easy onboarding:&lt;/em&gt; Projects need to have support for binary installation options (pip/Conda), clear documentation and a rich set of tutorials (ideally built into Jupyter notebooks).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ongoing maintenance:&lt;/em&gt; Project authors need to be committed to supporting and maintaining their projects.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Community:&lt;/em&gt; Projects should have (or be on track to building) an active, broad-based community.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you would like to have your project included in the PyTorch ecosystem and featured on &lt;a href=&quot;http://pytorch.org/ecosystem&quot;&gt;pytorch.org/ecosystem&lt;/a&gt;, please complete the form &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;here&lt;/a&gt;. If you’ve previously submitted a project for consideration and haven’t heard back, we promise to get back to you as soon as we can - we’ve received a lot of submissions!&lt;/p&gt;

&lt;h2 id=&quot;pytorch-hub-for-reproducible-research--new-models&quot;&gt;PyTorch Hub for reproducible research | New models&lt;/h2&gt;

&lt;p&gt;Since &lt;a href=&quot;https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/&quot;&gt;launching&lt;/a&gt; the PyTorch Hub in beta, we’ve received a lot of interest from the community including the contribution of many new models. Some of the latest include &lt;a href=&quot;https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/&quot;&gt;U-Net for Brain MRI&lt;/a&gt; contributed by researchers at Duke University, &lt;a href=&quot;https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/&quot;&gt;Single Shot Detection&lt;/a&gt; from NVIDIA and &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_transformerXL/&quot;&gt;Transformer-XL&lt;/a&gt; from HuggingFace.&lt;/p&gt;

&lt;p&gt;We’ve seen organic integration of the PyTorch Hub by folks like &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode&lt;/a&gt;, making it even easier for you to try out the state of the art in AI research. In addition, companies like &lt;a href=&quot;https://github.com/axsaucedo/seldon-core/tree/pytorch_hub/examples/models/pytorchhub&quot;&gt;Seldon&lt;/a&gt; provide production-level support for PyTorch Hub models on top of Kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-benefits-of-contributing-a-model-in-the-pytorch-hub&quot;&gt;What are the benefits of contributing a model in the PyTorch Hub?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Compatibility:&lt;/em&gt; PyTorch Hub models are prioritized first for testing by the TorchScript and Cloud TPU teams, and used as baselines for researchers across a number of fields.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Visibility:&lt;/em&gt; Models in the Hub will be promoted on &lt;a href=&quot;http://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt; as well as on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ease of testing and reproducibility:&lt;/em&gt; Each model comes with code, clear preprocessing requirements, and methods/dependencies to run. There is also tight integration with &lt;a href=&quot;https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_WSL-Images_resnext.ipynb#scrollTo=LM_l7vXJvnDM&quot;&gt;Google Colab&lt;/a&gt;, making it a true single click to get started.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-hub-contributions-welcome&quot;&gt;PyTorch Hub contributions welcome!&lt;/h3&gt;

&lt;p&gt;We are actively looking to grow the PyTorch Hub and welcome contributions. You don’t need to be an original paper author to contribute, and we’d love to see the number of domains and fields broaden. So what types of contributions are we looking for?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Artifacts of a published or an arXiv paper (or something of a similar nature that serves a different audience — such as ULMFit) that a large audience would need.&lt;/p&gt;

    &lt;p&gt;AND&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reproduces the published results (or better)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall these models are aimed at researchers either trying to reproduce a baseline, or trying to build downstream research on top of the model (such as feature-extraction or fine-tuning) as well as researchers looking for a demo of the paper for subjective evaluation. Please keep this audience in mind when contributing.&lt;/p&gt;

&lt;p&gt;If you are short on inspiration or would just like to find out what the SOTA is an any given field or domain, checkout the Paperswithcode &lt;a href=&quot;https://paperswithcode.com/sota&quot;&gt;state-of-the-art gallery&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-summer-hackathon&quot;&gt;PyTorch Summer Hackathon&lt;/h2&gt;

&lt;p&gt;We’ll be hosting the first PyTorch Summer Hackathon next month. We invite you to apply to participate in the in-person hackathon on  August 8th to 9th at Facebook’s Menlo Park campus. We’ll be bringing the community together to work on innovative ML projects that can solve a broad range of complex challenges.&lt;/p&gt;

&lt;p&gt;Applications will be reviewed and accepted on a rolling basis until spaces are filled. For those who cannot join this Hackathon in person, we’ll be following up soon with other ways to participate.&lt;/p&gt;

&lt;p&gt;Please visit &lt;a href=&quot;https://www.eventbrite.com/e/pytorch-summer-hackathon-in-menlo-park-registration-63756668913&quot;&gt;this link to apply&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you for being part of the PyTorch community!&lt;/p&gt;

&lt;p&gt;-Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">The PyTorch ecosystem includes projects, tools, models and libraries from a broad community of researchers in academia and industry, application developers, and ML engineers. The goal of this ecosystem is to support, accelerate, and aid in your exploration with PyTorch and help you push the state of the art, no matter what field you are exploring. Similarly, we are expanding the recently launched PyTorch Hub to further help you discover and reproduce the latest research.</summary></entry><entry><title type="html">Towards Reproducible Research with PyTorch Hub</title><link href="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/" rel="alternate" type="text/html" title="Towards Reproducible Research with PyTorch Hub" /><published>2019-06-10T00:00:00-07:00</published><updated>2019-06-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub</id><content type="html" xml:base="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/">&lt;p&gt;Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.&lt;/p&gt;

&lt;p&gt;We are excited to announce the availability of PyTorch Hub, a simple API and workflow that provides the basic building blocks for improving machine learning research reproducibility. PyTorch Hub consists of a pre-trained model repository designed specifically to facilitate research reproducibility and enable new research. It also has built-in support for &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Colab&lt;/a&gt;, integration with &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;&lt;em&gt;Papers With Code&lt;/em&gt;&lt;/a&gt; and currently contains a broad set of models that include Classification and Segmentation, Generative, Transformers, etc.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-header-1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;owner-publishing-models&quot;&gt;[Owner] Publishing models&lt;/h2&gt;

&lt;p&gt;PyTorch Hub supports the publication of pre-trained models (model definitions and pre-trained weights) to a GitHub repository by adding a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; file.
This provides an enumeration of which models are to be supported and a list of dependencies needed to run the models.
Examples can be found in the &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;torchvision&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/hubconf.py&quot;&gt;huggingface-bert&lt;/a&gt; and &lt;a href=&quot;https://github.com/facebookresearch/pytorch_GAN_zoo&quot;&gt;gan-model-zoo&lt;/a&gt; repositories.&lt;/p&gt;

&lt;p&gt;Let us look at the simplest case: &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;’s &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Optional list of dependencies required by the package&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.alexnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.densenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet169&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet161&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.inception&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inception_v3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.resnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet152&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
&lt;span class=&quot;n&quot;&gt;resnext50_32x4d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnext101_32x8d&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.squeezenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_1&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.vgg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19_bn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.segmentation&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fcn_resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deeplabv3_resnet101&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.googlenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;googlenet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.shufflenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x0_5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x1_0&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.mobilenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;, the models have the following properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each model file can function and be executed independently&lt;/li&gt;
  &lt;li&gt;They dont require any package other than PyTorch (encoded in &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;dependencies['torch']&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;They dont need separate entry-points, because the models when created, work seamlessly out of the box&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minimizing package dependencies reduces the friction for users to load your model for immediate experimentation.&lt;/p&gt;

&lt;p&gt;A more involved example is HuggingFace’s BERT models. Here is their &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tqdm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'boto3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'requests'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'regex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hubconfs.bert_hubconf&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForNextSentencePrediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForPreTraining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMultipleChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForQuestionAnswering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForTokenClassification&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each model then requires an entrypoint to be created. Here is a code snippet to specify an entrypoint of the &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt; model, which returns the pre-trained model weights.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    BertForMaskedLM includes the BertModel Transformer followed by the
    pre-trained masked language modeling head.
    Example:
      ...
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMaskedLM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These entry-points can serve as wrappers around complex model factories. They can give a clean and consistent help docstring, have logic to support downloading of pretrained weights (for example via &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt;) or have additional hub-specific functionality such as visualization.&lt;/p&gt;

&lt;p&gt;With a &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; in place, you can send a pull request based on the template &lt;a href=&quot;https://github.com/pytorch/hub/blob/master/docs/template.md&quot;&gt;here&lt;/a&gt;.
Our goal is to curate high-quality, easily-reproducible, maximally-beneficial models for research reproducibility.
Hence, we may work with you to refine your pull request and in some cases reject some low-quality models to be published.
Once we accept your pull request, your model will soon appear on &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;Pytorch hub webpage&lt;/a&gt; for all users to explore.&lt;/p&gt;

&lt;h2 id=&quot;user-workflow&quot;&gt;[User] Workflow&lt;/h2&gt;

&lt;p&gt;As a user, PyTorch Hub allows you to follow a few simple steps and do things like: 1) explore available models; 2) load a model; and 3) understand what methods are available for any given model. Let’s walk through some examples of each.&lt;/p&gt;

&lt;h3 id=&quot;explore-available-entrypoints&quot;&gt;Explore available entrypoints.&lt;/h3&gt;

&lt;p&gt;Users can list all available entrypoints in a repo using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.list()&lt;/code&gt; API.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alexnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'densenet121'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg19'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'vgg19_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that PyTorch Hub also allows auxillary entrypoints (other than pretrained models), e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;bertTokenizer&lt;/code&gt; for preprocessing in the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; models, to make the user workflow smoother.&lt;/p&gt;

&lt;h3 id=&quot;load-a-model&quot;&gt;Load a model&lt;/h3&gt;

&lt;p&gt;Now that we know which models are available in the Hub, users can load a model entrypoint using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.load()&lt;/code&gt; API. This only requires a single command without the need to install a wheel. In addition the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.help()&lt;/code&gt; API can provide useful information about how to instantiate the model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is also common that repo owners will want to continually add bug fixes or performance improvements. PyTorch Hub makes it super simple for users to get the latest update by calling:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;force_reload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We believe this will help to alleviate the burden of repetitive package releases by repo owners and instead allow them to focus more on their research.
It also ensures that, as a user, you are getting the freshest available models.&lt;/p&gt;

&lt;p&gt;On the contrary, stability is important for users. Hence, some model owners serve them from a specificed branch or tag, rather than the &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; branch, to ensure stability of the code.
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch_GAN_zoo&lt;/code&gt; serves them from the &lt;code class=&quot;highlighter-rouge&quot;&gt;hub&lt;/code&gt; branch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'facebookresearch/pytorch_GAN_zoo:hub'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'DCGAN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;useGPU&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;**kwargs&lt;/code&gt; passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.load()&lt;/code&gt; are used to &lt;em&gt;instantiate&lt;/em&gt; a model. In the above example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;useGPU=False&lt;/code&gt; are given to the model’s entrypoint.&lt;/p&gt;

&lt;h3 id=&quot;explore-a-loaded-model&quot;&gt;Explore a loaded model&lt;/h3&gt;

&lt;p&gt;Once you have a model from PyTorch Hub loaded, you can use the following workflow to find out the available methods that are supported as well as understand better what arguments are requires to run it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dir(model)&lt;/code&gt; to see all available methods of the model. Let’s take a look at &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt;’s available methods.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'forward'&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'to'&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'state_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;help(model.forward)&lt;/code&gt; provides a view into what arguments are required to make your loaded model run&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Help&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_pretrained_bert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modeling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masked_lm_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Have a closer look at the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&quot;&gt;DeepLabV3&lt;/a&gt; pages, where you can see how these models can be used once loaded.&lt;/p&gt;

&lt;h3 id=&quot;other-ways-to-explore&quot;&gt;Other ways to explore&lt;/h3&gt;

&lt;p&gt;Models available in PyTorch Hub also support both &lt;a href=&quot;https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorch-gan-zoo_pgan.ipynb&quot;&gt;Colab&lt;/a&gt; and are directly linked on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;Papers With Code&lt;/a&gt; and you can get started with a single click. &lt;a href=&quot;https://paperswithcode.com/paper/densely-connected-convolutional-networks&quot;&gt;Here&lt;/a&gt; is a good example to get started with (shown below).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-pwc.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional resources:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Hub API documentation can be found &lt;a href=&quot;https://pytorch.org/docs/stable/hub.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Submit a model &lt;a href=&quot;https://github.com/pytorch/hub&quot;&gt;here&lt;/a&gt; for publication in PyTorch Hub.&lt;/li&gt;
  &lt;li&gt;Go to &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;https://pytorch.org/hub&lt;/a&gt; to learn more about the available models.&lt;/li&gt;
  &lt;li&gt;Look for more models to come on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode.com&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A BIG thanks to the folks at HuggingFace, the PapersWithCode team, fast.ai and Nvidia as well as Morgane Riviere (FAIR Paris) and lots of others for helping bootstrap this effort!!&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: If we would like to contribute a model that is already in the Hub but perhaps mine has better accuracy, should I still contribute?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: Yes!! A next step for Hub is to implement an upvote/downvote system to surface the best models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Who hosts the model weights for PyTorch Hub?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: You, as the contributor, are responsible to host the model weights. You can host your model in your favorite cloud storage or, if it fits within the limits, on GitHub. If it is not within your means to host the weights, check with us via opening an issue on the hub repository.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if my model is trained on private data? Should I still contribute this model?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: No! PyTorch Hub is centered around open research and that extends to the usage of open datasets to train these models on. If a pull request for a proprietary model is submitted, we will kindly ask that you resubmit a model trained on something open and available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Where are my downloaded models saved?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: We follow the &lt;a href=&quot;https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html&quot;&gt;XDG Base Directory Specification&lt;/a&gt; and adhere to common standards around cached files and directories.&lt;/p&gt;

&lt;p&gt;The locations are used in the order of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$TORCH_HOME/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;TORCH_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$XDG_CACHE_HOME/torch/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;XDG_CACHE_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.cache/torch/hub&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.</summary></entry><entry><title type="html">torchvision 0.3: segmentation, detection models, new datasets and more..</title><link href="https://pytorch.org/blog/torchvision03/" rel="alternate" type="text/html" title="torchvision 0.3: segmentation, detection models, new datasets and more.." /><published>2019-05-22T00:00:00-07:00</published><updated>2019-05-22T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision03</id><content type="html" xml:base="https://pytorch.org/blog/torchvision03/">&lt;p&gt;PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torchvision_0.3_headline.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;new-features-include&quot;&gt;New features include:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reference training / evaluation scripts:&lt;/strong&gt; torchvision now provides, under the references/ folder, scripts for training and evaluation of the following tasks: classification, semantic segmentation, object detection, instance segmentation and person keypoint detection. These serve as a log of how to train a specific model and provide baseline training and evaluation scripts to quickly bootstrap research.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;torchvision ops:&lt;/strong&gt; torchvision now contains custom C++ / CUDA operators. Those operators are specific to computer vision, and make it easier to build object detection models. These operators currently do not support PyTorch script mode, but support for it is planned for in the next release. Some of the ops supported include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;roi_pool (and the module version RoIPool)&lt;/li&gt;
  &lt;li&gt;roi_align (and the module version RoIAlign)&lt;/li&gt;
  &lt;li&gt;nms, for non-maximum suppression of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_iou, for computing the intersection over union metric between two sets of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_area, for computing the area of a set of bounding boxes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are a few examples on using torchvision ops:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create 10 random boxes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# they need to be in [x0, y0, x1, y1] format&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# create a random image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# extract regions in `image` defined in `boxes`, rescaling&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# them to have a size of 3x3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi_align&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# check the size&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 3, 3, 3])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or compute the intersection over union between&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# all pairs of boxes&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box_iou&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 10])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;New models and datasets:&lt;/strong&gt; torchvision now adds support for object detection, instance segmentation and person keypoint detection models. In addition, several popular datasets have been added. Note: The API is currently experimental and might change in future versions of torchvision. New models include:&lt;/p&gt;

&lt;h3 id=&quot;segmentation-models&quot;&gt;Segmentation Models&lt;/h3&gt;

&lt;p&gt;The 0.3 release also contains models for dense pixelwise prediction on images.
It adds FCN and DeepLabV3 segmentation models, using a ResNet50 and ResNet101 backbones.
Pre-trained weights for ResNet101 backbone are available, and have been trained on a subset of COCO train2017, which contains the same 20 categories as those from Pascal VOC.&lt;/p&gt;

&lt;p&gt;The pre-trained models give the following results on the subset of COCO val2017 which contain the same 20 categories as those present in Pascal VOC:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;mean IoU&lt;/th&gt;
      &lt;th&gt;global pixelwise acc&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet101&lt;/td&gt;
      &lt;td&gt;63.7&lt;/td&gt;
      &lt;td&gt;91.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet101&lt;/td&gt;
      &lt;td&gt;67.4&lt;/td&gt;
      &lt;td&gt;92.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;detection-models&quot;&gt;Detection Models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;box AP&lt;/th&gt;
      &lt;th&gt;mask AP&lt;/th&gt;
      &lt;th&gt;keypoint AP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.9&lt;/td&gt;
      &lt;td&gt;34.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;54.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;65.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The implementations of the models for object detection, instance segmentation and keypoint detection are fast, specially during training.&lt;/p&gt;

&lt;p&gt;In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.&lt;/p&gt;

&lt;p&gt;For test time, we report the time for the model evaluation and post-processing (including mask pasting in image), but not the time for computing the precision-recall.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;train time (s / it)&lt;/th&gt;
      &lt;th&gt;test time (s / it)&lt;/th&gt;
      &lt;th&gt;memory (GB)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2288&lt;/td&gt;
      &lt;td&gt;0.0590&lt;/td&gt;
      &lt;td&gt;5.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2728&lt;/td&gt;
      &lt;td&gt;0.0903&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.3789&lt;/td&gt;
      &lt;td&gt;0.1242&lt;/td&gt;
      &lt;td&gt;6.8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can load and use pre-trained detection and segmentation models with a few lines of code&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maskrcnn_resnet50_fpn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# set it to evaluation mode, as the model behaves differently&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# during training and during evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/an/image.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pass a list of (potentially different sized) tensors&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to the model, in 0-1 range. The model will take care of&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# batching them together and normalizing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output is a list of dict, containing the postprocessed predictions&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;classification-models&quot;&gt;Classification Models&lt;/h3&gt;

&lt;p&gt;The following classification models were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GoogLeNet (Inception v1)&lt;/li&gt;
  &lt;li&gt;MobileNet V2&lt;/li&gt;
  &lt;li&gt;ShuffleNet v2&lt;/li&gt;
  &lt;li&gt;ResNeXt-50 32x4d and ResNeXt-101 32x8d&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;The following datasets were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Caltech101, Caltech256, and CelebA&lt;/li&gt;
  &lt;li&gt;ImageNet dataset (improving on ImageFolder, provides class-strings)&lt;/li&gt;
  &lt;li&gt;Semantic Boundaries Dataset&lt;/li&gt;
  &lt;li&gt;VisionDataset as a base class for all datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we’ve added more image transforms, general improvements and bug fixes, as well as improved documentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;See the full release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; as well as this getting started tutorial &lt;a href=&quot;https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb&quot;&gt;on Google Colab here&lt;/a&gt;, which describes how to fine tune your own instance segmentation model on a custom dataset.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Francisco Massa</name></author><summary type="html">PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.</summary></entry></feed>
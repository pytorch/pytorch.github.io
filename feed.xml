<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-12-14T17:47:43-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Understanding GPU Memory 1: Visualizing All Allocations over Time</title>
      <link href="https://pytorch.org/blog/understanding-gpu-memory-1/" rel="alternate" type="text/html" title="Understanding GPU Memory 1: Visualizing All Allocations over Time" />
      <published>2023-12-14T00:00:00-08:00</published>
      <updated>2023-12-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/understanding-gpu-memory-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/understanding-gpu-memory-1/">&lt;p&gt;During your time with PyTorch on GPUs, you may be familiar with this common error message:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 401.56 MiB is free.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this series, we show how to use memory tooling, including the Memory Snapshot, the Memory Profiler, and the Reference Cycle Detector to debug out of memory errors and improve memory usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig1.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Memory Snapshot&lt;/strong&gt; tool provides a fine-grained GPU memory visualization for debugging GPU OOMs. Captured memory snapshots will show memory events including allocations, frees and OOMs, along with their stack traces.&lt;/p&gt;

&lt;p&gt;In a snapshot, each tensor’s memory allocation is color coded separately. The x axis is over time, and the y axis is the amount of GPU memory in MB. The snapshot is interactive, so we can observe the stack trace for any allocation by mousing over.&lt;/p&gt;

&lt;p&gt;In this snapshot, there are 3 peaks showing the memory allocations over 3 training iterations. When looking at the peaks, it is &lt;strong&gt;easy to see the rise of memory in the forward&lt;/strong&gt; &lt;strong&gt;pass&lt;/strong&gt; and the &lt;strong&gt;fall during the backward pass&lt;/strong&gt; as the gradients are computed. It is also possible to see that the program has the &lt;strong&gt;same pattern of memory use iteration to iteration&lt;/strong&gt;. One thing that stands out is the many &lt;strong&gt;tiny spikes in memory&lt;/strong&gt;, by mousing over them, we see that they are buffers used temporarily by convolution operators.&lt;/p&gt;

&lt;h3 id=&quot;capturing-memory-snapshots&quot;&gt;Capturing Memory Snapshots&lt;/h3&gt;

&lt;p&gt;The API to capture memory snapshots is fairly simple and available in torch.cuda.memory:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Start:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._record_memory_history(max_entries=100000)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Save:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._dump_snapshot(file_name)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stop:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._record_memory_history(enabled=None)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Code Snippet&lt;/strong&gt; (for full code sample, see &lt;strong&gt;Appendix A&lt;/strong&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   # Start recording memory snapshot history, initialized with a buffer
   # capacity of 100,000 memory events, via the `max_entries` field.
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

   # Run your PyTorch Model.
   # At any point in time, save a snapshot to file for later.
   for _ in range(5):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # In this sample, we save the snapshot after running 5 iterations.
   #   - Save as many snapshots as you'd like.
   #   - Snapshots will save last `max_entries` number of memory events
   #     (100,000 in this example).
   try:
       torch.cuda.memory._dump_snapshot(f&quot;{file_prefix}.pickle&quot;)
   except Exception as e:
       logger.error(f&quot;Failed to capture memory snapshot {e}&quot;)

   # Stop recording memory snapshot history.
   torch.cuda.memory._record_memory_history(enabled=None)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To visualize the snapshot file, we have a tool hosted at &lt;a href=&quot;https://pytorch.org/memory_viz&quot;&gt;https://pytorch.org/memory_viz&lt;/a&gt;. There, you can drag and drop your saved snapshot file and it will plot each allocation over time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig2.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, you can generate an HTML from a .pickle by using the script at pytorch/torch/cuda/_memory_viz.py, here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python torch/cuda/_memory_viz.py trace_plot snapshot.pickle -o snapshot.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;debugging-cuda-ooms&quot;&gt;Debugging CUDA OOMs&lt;/h2&gt;

&lt;p&gt;Let’s look at how we can use the memory snapshot tool to answer:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Why did a &lt;strong&gt;CUDA OOM&lt;/strong&gt; happen?&lt;/li&gt;
  &lt;li&gt;Where is the &lt;strong&gt;GPU Memory being used&lt;/strong&gt;?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;resnet50-with-a-bug&quot;&gt;ResNet50 with a bug&lt;/h3&gt;

&lt;p&gt;We’ve taken a look at a properly working model in the first snapshot. Now, let’s take a look at a training example with a bug, see snapshot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig3.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the &lt;strong&gt;second iteration uses far more memory&lt;/strong&gt; than the first iteration. If this model were much larger, it could have &lt;strong&gt;CUDA OOM’d in the second iteration&lt;/strong&gt; without much more insight into why.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig4.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When examining this snapshot further, we can clearly see that several tensors are staying alive from the first iteration to the second and later iterations. If we mouse over one of these tensors, it would show a &lt;strong&gt;stack trace suggesting that these were gradient tensors&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;And indeed if we go to the code, we can see that &lt;strong&gt;it doesn’t clear the gradient tensors&lt;/strong&gt;, when it could have &lt;strong&gt;cleared them before the forward&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        for _ in range(num_iters):
          pred = model(inputs)
          loss_fn(pred, labels).backward()
          optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        for _ in range(num_iters):
          pred = model(inputs)
          loss_fn(pred, labels).backward()
          optimizer.step()
          # Add this line to clear grad tensors
          optimizer.zero_grad(set_to_none=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can simply add an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimizer.zero_grad(set_to_none=True)&lt;/code&gt; instruction to clear the gradient tensors from iteration to iteration (more details about why we need to zero the gradients here: &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html&quot;&gt;https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This is a simplification of a bug we’ve found in more complicated programs using this tool. We encourage you to try out the Memory Snapshot on your GPU memory problems and let us know how it goes.&lt;/p&gt;

&lt;h3 id=&quot;resnet50-after-bug-fix&quot;&gt;ResNet50 after bug fix&lt;/h3&gt;

&lt;p&gt;After applying the fix, the snapshot seems to be clearing the gradients now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig5.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now have the snapshot of a properly working ResNet50 model. Try out the code yourself (see code sample in &lt;strong&gt;Appendix A&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;But you may be wondering, &lt;strong&gt;why is there still an increase in memory after the first iteration?&lt;/strong&gt; To answer this, let’s visit the &lt;strong&gt;Memory Profiler&lt;/strong&gt; in the next section.&lt;/p&gt;

&lt;h2 id=&quot;categorized-memory-usage&quot;&gt;Categorized Memory Usage&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Memory Profiler&lt;/strong&gt; is an added feature of the PyTorch Profiler that &lt;strong&gt;categorizes&lt;/strong&gt; memory usage over time. We still rely on the Memory Snapshot for stack traces for deep dives into memory allocations.&lt;/p&gt;

&lt;p&gt;To generate a memory timeline, here is a code snippet (full code sample in &lt;strong&gt;Appendix B&lt;/strong&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   # Initialize the profiler context with record_shapes, profile_memory,
   # and with_stack set to True.
   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       # Run the PyTorch Model inside the profile context.
       for _ in range(5):
           prof.step()
           with record_function(&quot;## forward ##&quot;):
               pred = model(inputs)

           with record_function(&quot;## backward ##&quot;):
               loss_fn(pred, labels).backward()

           with record_function(&quot;## optimizer ##&quot;):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

   # Construct the memory timeline HTML plot.
   prof.export_memory_timeline(f&quot;{file_prefix}.html&quot;, device=&quot;cuda:0&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For further reference, see &lt;a href=&quot;https://pytorch.org/docs/main/profiler.html&quot;&gt;https://pytorch.org/docs/main/profiler.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Memory Profiler automatically generates categories based on the graph of tensor operations recorded during profiling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig6.jpg&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this Memory Timeline collected using the Memory Profiler, we have the same training example as before. We can observe the &lt;strong&gt;gradients in blue are now being cleared&lt;/strong&gt; from iteration to iteration. We can also notice that the &lt;strong&gt;optimizer state in yellow is allocated after the first iteration&lt;/strong&gt;, and is kept constant for the rest of the job.&lt;/p&gt;

&lt;p&gt;This optimizer state is the reason behind the increase of GPU memory from the first iteration to the second. Try out the code yourself (see code sample in &lt;strong&gt;Appendix B&lt;/strong&gt;). The Memory Profiler helps to improve training &lt;strong&gt;memory understanding&lt;/strong&gt; so that model authors can figure out which categories are using the most GPU memory.&lt;/p&gt;

&lt;h2 id=&quot;where-can-i-find-these-tools&quot;&gt;Where can I find these tools?&lt;/h2&gt;

&lt;p&gt;We hope that these tools will greatly improve your ability to debug CUDA OOMs and to understand your memory usage by category.&lt;/p&gt;

&lt;p&gt;The Memory Snapshot and the Memory Profiler are available in the v2.1 release of PyTorch as experimental features.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More information about the Memory Snapshot can be found in the &lt;a href=&quot;https://pytorch.org/docs/main/torch_cuda_memory.html&quot;&gt;PyTorch Memory docs here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;More details about the Memory Profiler can be found in the &lt;a href=&quot;https://pytorch.org/docs/main/profiler.html&quot;&gt;PyTorch Profiler docs here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;We look forward to hearing from you about any enhancements, bugs or memory stories that our tools helped to solve! As always, please feel free to open new issues on PyTorch’s Github page.&lt;/p&gt;

&lt;p&gt;We are also open to contributions from the OSS community, feel free to tag &lt;a href=&quot;https://github.com/aaronenyeshi&quot;&gt;Aaron Shi&lt;/a&gt; and &lt;a href=&quot;https://github.com/zdevito&quot;&gt;Zachary DeVito&lt;/a&gt; in any Github PRs for reviews.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Really appreciate the content reviewers, &lt;a href=&quot;mailto:marksaroufim@meta.com&quot;&gt;Mark Saroufim&lt;/a&gt;, &lt;a href=&quot;mailto:gchanan@meta.com&quot;&gt;Gregory Chanan&lt;/a&gt;, and &lt;a href=&quot;mailto:adnanaziz@meta.com&quot;&gt;Adnan Aziz&lt;/a&gt; for reviewing this post and improving its readability.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;appendix-a---resnet50-memory-snapshot-code-example&quot;&gt;Appendix A - ResNet50 Memory Snapshot Code Example&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# (c) Meta Platforms, Inc. and affiliates. 
import logging
import socket
from datetime import datetime, timedelta

import torch

from torchvision import models

logging.basicConfig(
   format=&quot;%(levelname)s:%(asctime)s %(message)s&quot;,
   level=logging.INFO,
   datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = &quot;%b_%d_%H_%M_%S&quot;

# Keep a max of 100,000 alloc/free events in the recorded history
# leading up to the snapshot.
MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000

def start_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Starting snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

def stop_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Stopping snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(enabled=None)

def export_memory_snapshot() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not exporting memory snapshot&quot;)
       return

   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f&quot;{host_name}_{timestamp}&quot;

   try:
       logger.info(f&quot;Saving snapshot to local file: {file_prefix}.pickle&quot;)
       torch.cuda.memory._dump_snapshot(f&quot;{file_prefix}.pickle&quot;)
   except Exception as e:
       logger.error(f&quot;Failed to capture memory snapshot {e}&quot;)
       return

# Simple Resnet50 example to demonstrate how to capture memory visuals.
def run_resnet50(num_iters=5, device=&quot;cuda:0&quot;):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   # Start recording memory snapshot history
   start_record_memory_history()

   for _ in range(num_iters):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # Create the memory snapshot file
   export_memory_snapshot()

   # Stop recording memory snapshot history
   stop_record_memory_history()

if __name__ == &quot;__main__&quot;:
    # Run the resnet50 model
    run_resnet50()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;appendix-b---resnet50-memory-profiler-code-example&quot;&gt;Appendix B - ResNet50 Memory Profiler Code Example&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# (c) Meta Platforms, Inc. and affiliates. 
import logging
import socket
from datetime import datetime, timedelta

import torch

from torch.autograd.profiler import record_function
from torchvision import models

logging.basicConfig(
   format=&quot;%(levelname)s:%(asctime)s %(message)s&quot;,
   level=logging.INFO,
   datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = &quot;%b_%d_%H_%M_%S&quot;

def trace_handler(prof: torch.profiler.profile):
   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f&quot;{host_name}_{timestamp}&quot;

   # Construct the trace file.
   prof.export_chrome_trace(f&quot;{file_prefix}.json.gz&quot;)

   # Construct the memory timeline file.
   prof.export_memory_timeline(f&quot;{file_prefix}.html&quot;, device=&quot;cuda:0&quot;)

def run_resnet50(num_iters=5, device=&quot;cuda:0&quot;):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       for _ in range(num_iters):
           prof.step()
           with record_function(&quot;## forward ##&quot;):
               pred = model(inputs)

           with record_function(&quot;## backward ##&quot;):
               loss_fn(pred, labels).backward()

           with record_function(&quot;## optimizer ##&quot;):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

if __name__ == &quot;__main__&quot;:
    # Warm up
    run_resnet50()
    # Run the resnet50 model
    run_resnet50()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Aaron Shi, Zachary DeVito</name>
        
        
      </author>

      

      

      
        <summary type="html">During your time with PyTorch on GPUs, you may be familiar with this common error message:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">From PyTorch Conference 2023: From Dinosaurs to Seismic Imaging with Intel</title>
      <link href="https://pytorch.org/blog/dinosaurs-to-seismic-imaging/" rel="alternate" type="text/html" title="From PyTorch Conference 2023: From Dinosaurs to Seismic Imaging with Intel" />
      <published>2023-12-12T00:00:00-08:00</published>
      <updated>2023-12-12T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/dinosaurs-to-seismic-imaging</id>
      <content type="html" xml:base="https://pytorch.org/blog/dinosaurs-to-seismic-imaging/">&lt;p&gt;&lt;img src=&quot;/assets/images/hunting-dinosaurs-with-intel-ai-fig1.jpeg&quot; alt=&quot;Dinosaur fossil&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lightning-talk-1-seismic-data-to-subsurface-models-with-openfwi&quot;&gt;Lightning Talk 1: Seismic Data to Subsurface Models with OpenFWI&lt;/h2&gt;

&lt;p&gt;Speaker: Benjamin Consolvo, AI Software Engineering Manager, Intel, &lt;a href=&quot;https://linkedin.com/in/bconsolvo&quot;&gt;LinkedIn&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;session-overview&quot;&gt;Session Overview&lt;/h3&gt;

&lt;p&gt;In this session, Ben begins with an overview of seismic imaging and full waveform inversion (FWI). Seismic imaging and FWI helps us to explore land for important subsurface minerals necessary for human thriving. To find those crucial subsurface minerals, we need to image the subsurface with a high degree of accuracy at a low cost, which involves two main challenges. He explains the solutions for those challenges using AI, which are summarized below.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Challenges&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Solutions using AI&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Traditional physics based FWI requires an accurate starting model.
   &lt;/td&gt;
   &lt;td&gt;Data-driven deep learning solutions do not require an accurate starting model.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPUs are typically used for fine-tuning neural networks but are often unavailable and expensive.
   &lt;/td&gt;
   &lt;td&gt;CPUs are highly available, inexpensive, and viable for AI fine-tuning. The new 4&lt;sup&gt;th&lt;/sup&gt; Gen Intel® Xeon® Scalable processor has the built-in AI accelerator engine called Intel® AMX (Intel® Advanced Matrix Extensions) that helps to accelerate AI training and inference performance. 
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Next, he shows the wave propagation for the subsurface model and corresponding seismic shot gathers. In his example, the shot gathers are synthetically generated time-sampled records of sounds recordings from a shot (like a dynamite explosion or vibroseis truck) recorded by geophones spread across a large area. For this application, the training data consists of a pair of subsurface model image and seismic shot gather images, where the model from the shot gather is predicted.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Number of Seismic Shot Images&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Number of subsurface model images&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Train&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;120,000
   &lt;/td&gt;
   &lt;td&gt;24,000
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Test&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;25,000
   &lt;/td&gt;
   &lt;td&gt;5,000
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Validation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;5,000
   &lt;/td&gt;
   &lt;td&gt;1,000
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In this application, the algorithm used during training was InversionNET (encoder-decoder convolutional neural network). Check out the implementation details for InversionNET architecture in &lt;a href=&quot;https://arxiv.org/abs/2111.02926&quot;&gt;Deng et al. (2021)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;He then shows the results:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prediction versus ground truth model after one epoch and at 50 epochs. After training InversionNET, the predicted model is much closer to the ground truth image.&lt;/li&gt;
  &lt;li&gt;Training loss and validation loss curves decreasing over time across 50 epochs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, Ben concludes his talk by highlighting that he was able to successfully fine-tune a deep neural network without an accurate starting model to obtain subsurface model on a 4th generation Intel® Xeon® Scalable processor.&lt;/p&gt;

&lt;p&gt;Watch the &lt;a href=&quot;https://www.youtube.com/watch?v=TPp_Zyco6X4&amp;amp;list=PL_lsbAsL_o2BivkGLiDfHY9VqWlaNoZ2O&amp;amp;index=56&quot;&gt;full video recording here&lt;/a&gt; and download the &lt;a href=&quot;https://static.sched.com/hosted_files/pytorch2023/57/20231017_Consolvo_Seismic_PyTorchConf.pdf&quot;&gt;presentation&lt;/a&gt;. More details can be found in this &lt;a href=&quot;https://medium.com/better-programming/seismic-data-to-subsurface-models-with-openfwi-bcca0218b4e8&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;about-the-speaker&quot;&gt;About the Speaker&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ben-consolvo.jpg&quot; alt=&quot;Ben Consolvo&quot; style=&quot;max-width:220px;float:right;margin-left: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ben Consolvo is an AI Solutions Engineering Manager at Intel. He has been building a team and a program around Intel’s AI technology paired with Intel’s hardware offerings. He brings a background and passion in data science, particularly in deep learning (DL) and computer vision. He has applied his skills in DL in the cybersecurity industry to automatically identify phishing websites, as well as to the oil and gas industry to identify subsurface features for geophysical imaging.&lt;/p&gt;

&lt;h2 id=&quot;lightning-talk-2-dinosaur-bone-hunt&quot;&gt;Lightning Talk 2: Dinosaur Bone Hunt&lt;/h2&gt;

&lt;p&gt;Speaker: Bob Chesebrough, Sr Solution Architect, Intel, &lt;a href=&quot;https://www.linkedin.com/in/robertchesebrough/&quot;&gt;LinkedIn&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;session-overview-1&quot;&gt;Session Overview&lt;/h3&gt;

&lt;p&gt;In this session, Bob starts the presentation by explaining his interest in collecting dinosaur bones and gives an overview of &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Software portfolio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;He then explains the steps to create a dinosaur site treasure map or dinosaur bone likelihood map:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Collect data and create training data (New Mexico aerial photos of the Morrison Formation - a famous dinosaur bone bed in the Western United States and the GPS coordinates for small bone fragments discovered)&lt;/li&gt;
  &lt;li&gt;Train a simple ResNet 18 model using &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.1jggir&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Score the model on Utah photos and create a heat map&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally, Bob shows the results that dinosaur bones were discovered in Utah using dinosaur bone likelihood map. Go to the &lt;a href=&quot;https://github.com/intelsoftware/jurassic&quot;&gt;GitHub repository&lt;/a&gt; to access the code sample and try out the sample using Intel Extension for PyTorch.&lt;/p&gt;

&lt;p&gt;Watch the &lt;a href=&quot;https://www.youtube.com/watch?v=Q_soyAhduKk&amp;amp;list=PL_lsbAsL_o2BivkGLiDfHY9VqWlaNoZ2O&amp;amp;index=67&quot;&gt;full video recording here&lt;/a&gt; and download the &lt;a href=&quot;https://static.sched.com/hosted_files/pytorch2023/86/PyTorch_Conf_Chesebrough_2023_PPT.pdf&quot;&gt;presentation&lt;/a&gt;. More details can be found in this &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-step-by-step-guide-for-hunting-dinosaurs.html&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;about-the-speaker-1&quot;&gt;About the Speaker&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bob-chesebrough.jpg&quot; alt=&quot;Bob Chesebrough&quot; style=&quot;max-width:220px;float:right;margin-left: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bob Chesebrough’s industry experience is software development/AI solution engineering for fortune 100 companies and national laboratories for over three decades. He is also a hobbyist who has logged over 800 miles and 1000 hours in the field finding dinosaur bones. He and his sons discovered an important fossil of the only known crocodilian from the Jurassic in New Mexico, they have also discovered and logged into the museum 2000+ bones localities and described a new mass bone bed in New Mexico.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Ramya Ravi, Susan Kahler at Intel</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Snowflake Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/snowflake-joins-pytorch/" rel="alternate" type="text/html" title="Snowflake Joins the PyTorch Foundation as a General Member" />
      <published>2023-12-05T00:00:00-08:00</published>
      <updated>2023-12-05T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/snowflake-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/snowflake-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/snowflake-logo.svg&quot; alt=&quot;Snowflake logo&quot; style=&quot;max-width:350px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Snowflake has joined as a general member.&lt;/p&gt;

&lt;p&gt;Snowflake enables thousands of organizations to unite siloed data, discover and securely share data, power data applications, and execute diverse AI/ML and analytic workloads across multiple clouds and geographies.&lt;/p&gt;

&lt;p&gt;“By joining the PyTorch community, we know that Snowflake will help accelerate data warehousing solutions and cutting-edge AI frameworks. This showcases the commitment to advancing innovation for data and artificial intelligence,” said Ibrahim Haddad, Executive Director, PyTorch Foundation. “We are thrilled to have Snowflake join the PyTorch Foundation, marking a significant stride in the convergence of data management and deep learning technologies.”&lt;/p&gt;

&lt;p&gt;Snowflake enables collaboration with AI technologies to handle the storage and analysis of large datasets generated by machine learning and AI applications through scalability and SQL support.&lt;/p&gt;

&lt;p&gt;With the integrated repository of Python libraries from Anaconda in Snowpark, Snowflake users have always had a streamlined experience to deploy pre-trained PyTorch models in Snowflake to easily and securely make them a part of applications. Now with the addition of GPU instances in Snowpark Container Services (in private preview), training and other computationally intensive processing using PyTorch will also be streamlined, providing teams with an end-to-end solution for AI development and deployment.&lt;/p&gt;

&lt;p&gt;“Most if not all of our customers incorporate open source software as part of their data stacks, so it is critical for us to work with open source ecosystems like the PyTorch Foundation, alongside incorporating open source to meet the needs of our customers,” said Adrien Treuille, Co-Founder of Streamlit, Director of Product Management at Snowflake. “As AI developers continue to integrate their models as part of applications, the power of Snowflake and PyTorch — coupled with Streamlit as the powerful front-end — creates near-limitless innovation for developers looking to build next-generation apps and unlock even more use cases.”&lt;/p&gt;

&lt;p&gt;To learn more about the power of Snowflake and PyTorch, tune into Snowflake’s developer conference for AI and apps, BUILD.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-snowflake&quot;&gt;About Snowflake&lt;/h2&gt;

&lt;p&gt;Snowflake enables every organization to mobilize their data with Snowflake’s Data Cloud. Customers use the Data Cloud to unite siloed data, discover and securely share data, power data applications, and execute diverse AI/ML and analytic workloads. Wherever data or users live, Snowflake delivers a single data experience that spans multiple clouds and geographies. Thousands of customers across many industries, including 639 of the 2023 Forbes Global 2000 (G2K) as of July 31, 2023, use Snowflake Data Cloud to power their businesses. Learn more at &lt;a href=&quot;https://www.snowflake.com/&quot;&gt;snowflake.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Generative AI with PyTorch II: GPT, Fast</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai-2/" rel="alternate" type="text/html" title="Accelerating Generative AI with PyTorch II: GPT, Fast" />
      <published>2023-11-30T00:00:00-08:00</published>
      <updated>2023-11-30T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai-2/">&lt;p&gt;This post is the second part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;Segment Anything over 8x&lt;/a&gt; using only pure, native PyTorch. In this blog we’ll focus on LLM optimization.&lt;/p&gt;

&lt;p&gt;Over the past year, generative AI use cases have exploded in popularity. Text generation has been one particularly popular area, with lots of innovation among open-source projects such as &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;, and &lt;a href=&quot;https://github.com/mlc-ai/mlc-llm&quot;&gt;MLC-LLM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While these projects are performant, they often come with tradeoffs in ease of use, such as requiring model conversion to specific formats or building and shipping new dependencies. This begs the question: &lt;strong&gt;how fast can we run transformer inference with only pure, native PyTorch?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As announced during our recent &lt;a href=&quot;https://www.youtube.com/watch?v=IWpM_9AsC-U&quot;&gt;PyTorch Developer Conference&lt;/a&gt;, the PyTorch team wrote a from-scratch LLM &lt;strong&gt;almost 10x faster than baseline,&lt;/strong&gt; with no loss of accuracy, all using native PyTorch optimizations. We leverage a breadth of optimizations including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;Torch.compile&lt;/a&gt;&lt;/strong&gt;: A compiler for PyTorch models&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao/tree/main#torchao&quot;&gt;GPU quantization&lt;/a&gt;&lt;/strong&gt;: Accelerate models with reduced precision operations&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L76&quot;&gt;Speculative Decoding&lt;/a&gt;&lt;/strong&gt;: Accelerate LLMs using a small “draft” model to predict large “target” model’s output&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py&quot;&gt;Tensor Parallelism&lt;/a&gt;&lt;/strong&gt;: Accelerate models by running them across multiple devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And, even better, we can do it in &lt;strong&gt;less than 1000 lines of native PyTorch code&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If this excites you enough to jump straight into the code, check it out at &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;https://github.com/pytorch-labs/gpt-fast&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/screen-recording.gif&quot; alt=&quot;Screen recording&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: We will be focusing on latency (i.e. batch size=1) for all of these benchmarks. Unless otherwise specified, all benchmarks are run on an A100-80GB, power limited to 330W.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;starting-point-255-toks&quot;&gt;Starting Point (25.5 tok/s)&lt;/h2&gt;

&lt;p&gt;Let’s start off with an extremely basic and simple implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image23.png&quot; alt=&quot;simple implementation&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sadly, this does not perform very well. But why? Looking at a trace reveals the answer - it’s heavily &lt;strong&gt;CPU overhead bound&lt;/strong&gt;! What this means is that our CPU is not able to tell the GPU what to do fast enough for the GPU to be fully utilized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image14.png&quot; alt=&quot;trace&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine the GPU as this super massive factory with a ridiculous amount of compute available. Then, imagine the CPU as some messenger shuttling instructions back and forth to the GPU. Remember, in large scale deep learning systems, the GPU is responsible for doing 100% of the work! In such systems, the only role of the CPU is to tell the GPU what work it should be doing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image16.png&quot; alt=&quot;factory&quot; style=&quot;width:100%;display: block;max-width:500px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, the CPU runs over and tells the GPU to do an “add”, but by the time the CPU can give the GPU another chunk of work, the GPU has long finished the previous chunk of work.&lt;/p&gt;

&lt;p&gt;Despite the fact that the GPU needs to perform thousands of computations while the CPU only needs to do orchestration work, this is surprisingly common! There’s a variety of reasons for this, ranging from the fact that the CPU is likely running some single-threaded Python to the fact that GPUs are just incredibly fast nowadays.&lt;/p&gt;

&lt;p&gt;Regardless of the reason, we now find ourselves in the &lt;strong&gt;overhead-bound regime&lt;/strong&gt;. So, what can we do? One, we could rewrite our implementation in C++, perhaps even eschew frameworks entirely and write raw CUDA. Or…. we could just send more work to the GPU at once.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image3.png&quot; alt=&quot;factory&quot; style=&quot;width:100%;display: block;max-width:500px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By just sending a massive chunk of work at once, we can keep our GPU busy! Although during training, this may just be accomplished by increasing your batch size, how do we do this during inference?&lt;/p&gt;

&lt;p&gt;Enter torch.compile.&lt;/p&gt;

&lt;h2 id=&quot;step-1-reducing-cpu-overhead-through-torchcompile-and-a-static-kv-cache-1070-toks&quot;&gt;Step 1: Reducing CPU overhead through torch.compile and a static kv-cache (107.0 tok/s)&lt;/h2&gt;

&lt;p&gt;Torch.compile allows us to capture a larger region into a single compiled region, and particularly when run with mode=”reduce-overhead”, is very effective at reducing CPU overhead. Here, we also specify fullgraph=True, which validates that there are no “graph breaks” in your model (i.e. portions that torch.compile cannot compile). In other words, it ensures that torch.compile is running to its fullest potential.&lt;/p&gt;

&lt;p&gt;To apply it, we &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L296&quot;&gt;simply wrap a function (or a module) with it&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.compile(decode_one_token, mode=&quot;reduce-overhead&quot;, fullgraph=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, there are a couple of nuances here that make it somewhat nontrivial for folks to get significant performance boosts from applying torch.compile to text generation.&lt;/p&gt;

&lt;p&gt;The first obstacle is the kv-cache. The kv-cache is an inference-time optimization that caches the activations computed for the previous tokens (see &lt;a href=&quot;https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/&quot;&gt;here&lt;/a&gt; for a more in-depth explanation). However, as we generate more tokens, the “logical length” of the kv-cache grows. This is problematic for two reasons. One is that reallocating (and copying!) the kv-cache every time the cache grows is simply expensive. The other one is that this dynamism makes it harder to reduce the overhead, as we are no longer able to leverage approaches like cudagraphs.&lt;/p&gt;

&lt;p&gt;To resolve this, we use a&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/0afae1ace441ce4c5d02ef11a72da28cf7ca4795/generate.py#L154&quot;&gt; “static” kv-cache&lt;/a&gt;, which means that we statically allocate the maximum size of the kv-cache, and then mask out the unused values in the attention portion of the computation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image2.png&quot; alt=&quot;code&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second obstacle is the prefill phase. Transformer text generation is best thought of as a two phase process: 1. The prefill where the entire prompt is processed, and 2. Decoding where each token is generated autoregressively.&lt;/p&gt;

&lt;p&gt;Although decoding can be made entirely static once the kv-cache is made static, the prefill stage still requires significantly more dynamism, due to having a variable prompt length. Thus, we actually need to compile the two stages with separate compilation strategies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image9.png&quot; alt=&quot;compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although these details are a bit tricky, the actual implementation is not very difficult at all (see gpt-fast)! And the performance boost is dramatic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image28.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All of a sudden, our performance improves by more than 4x! Such performance gains are often common when one’s workload is overhead bound.&lt;/p&gt;

&lt;h2 id=&quot;sidenote-how-is-torchcompile-helping&quot;&gt;Sidenote: How is torch.compile helping?&lt;/h2&gt;

&lt;p&gt;It is worth disentangling how exactly torch.compile is improving performance. There’s 2 main factors leading to torch.compile’s performance.&lt;/p&gt;

&lt;p&gt;The first factor, like mentioned above, is overhead reduction. Torch.compile is able to reduce overhead through a variety of optimizations, but one of the most effective ones is called &lt;a href=&quot;https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/&quot;&gt;CUDAGraphs&lt;/a&gt;. Although torch.compile applies this automatically for you when “reduce-overhead” is set, saving the extra work and code you need to write when doing this yourself manually  without torch.compile.&lt;/p&gt;

&lt;p&gt;The second factor, however, is that torch.compile simply generates faster kernels. In the decoding benchmark above, torch.compile actually generates every single kernel from scratch, including both the matrix multiplications and the attention! And even cooler, these kernels are actually faster than the built in alternatives (CuBLAS and FlashAttention2)!&lt;/p&gt;

&lt;p&gt;This may sound implausible to many of you, considering how hard it is to write efficient matrix multiplication/attention kernels, and how much manpower has been put into CuBLAS and FlashAttention. The key here, however, is that transformer decoding has very unusual computational properties. In particular, because of the KV-cache, for BS=1 &lt;em&gt;every single matrix multiplication in a transformer is actually a matrix vector multiplication&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This means that the computations are completely &lt;em&gt;memory-bandwidth bound&lt;/em&gt;, and as such, are well within the range of compilers to automatically generate. And in fact, when we benchmark torch.compile’s matrix-vector multiplications against CuBLAS, we find that torch.compile’s kernels are actually quite a bit faster!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image24.png&quot; alt=&quot;code&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image17.png&quot; alt=&quot;code&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-2-alleviating-memory-bandwidth-bottleneck-through-int8-weight-only-quantization-1574-toks&quot;&gt;Step 2: Alleviating memory bandwidth bottleneck through int8 weight-only quantization (157.4 tok/s)&lt;/h2&gt;

&lt;p&gt;So, given that we’ve already seen massive speedups from applying torch.compile, is it possible to do even better? One way to think about this problem is to compute how close we are to the theoretical peak. In this case, the largest bottleneck is the cost of loading the weights from GPU global memory to registers. In other words, each forward pass requires us to “touch” every single parameter on the GPU. So, how fast can we theoretically “touch” every single parameter in a model?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image11.png&quot; alt=&quot;weights&quot; style=&quot;width:100%;display: block;max-width:500px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To measure this, we can use &lt;strong&gt;Model Bandwidth Utilization (MBU).&lt;/strong&gt; This measures what percentage of our memory bandwidth we’re able to use during inference.&lt;/p&gt;

&lt;p&gt;Computing it is pretty simple. We simply take the total size of our model (# params * bytes per param) and multiply it by the number of inferences we can do per second. Then, we divide this by the peak bandwidth of the GPU to get our MBU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image8.png&quot; alt=&quot;MBU&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, for our above case, we have a 7B parameter model. Each parameter is stored in fp16 (2 bytes per parameter), and we achieved 107 tokens/s. Finally, our A100-80GB has a theoretical 2 TB/s of memory bandwidth.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image25.png&quot; alt=&quot;MBU&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Putting this all together, we get **72% MBU! **This is quite good, considering that even just copying memory struggles to break 85%.&lt;/p&gt;

&lt;p&gt;But… it does mean that we’re pretty close to the theoretical limit here, and that we’re clearly bottlenecked on just loading our weights from memory. It doesn’t matter what we do - without changing the problem statement in some manner, we might only be able to eek out another 10% in performance.&lt;/p&gt;

&lt;p&gt;Let’s take another look at the above equation. We can’t really change the number of parameters in our model. We can’t really change the memory bandwidth of our GPU (well, without paying more money). But, we &lt;strong&gt;can&lt;/strong&gt; change how many bytes each parameter is stored in!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image18.png&quot; alt=&quot;MBU&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus, we arrive at our next technique - int8 quantization. The idea here is simple. If loading our weights from memory is our main bottleneck, why don’t we just make the weights smaller?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image7.png&quot; alt=&quot;MBU&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that this is quantizing &lt;em&gt;only&lt;/em&gt; the weights - the computation itself is still done in bf16. This makes this form of quantization easy to apply with very little to no accuracy degradation.&lt;/p&gt;

&lt;p&gt;Moreover, torch.compile can also easily generate efficient code for int8 quantization. Let’s look again at the above benchmark, this time with int8 weight-only quantization included.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image1.png&quot; alt=&quot;code&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image27.png&quot; alt=&quot;code&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see from the dark blue line (torch.compile + int8), there is a significant performance improvement when using torch.compile + int8 weight-only quantization! Moreover, the light-blue line (no torch.compile + int8) is actually much worse than even the fp16 performance! This is because in order to take advantage of the perf benefits of int8 quantization, we need the kernels to be fused. This shows one of the benefits of torch.compile - these kernels can be automatically generated for the user!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/quantize.py#L314&quot;&gt;Applying int8 quantization to our model&lt;/a&gt;, we see a nice 50% performance improvement, bringing us up to 157.4 tokens/s!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image19.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-3-reframing-the-problem-using-speculative-decoding&quot;&gt;Step 3: Reframing the problem using speculative decoding&lt;/h2&gt;

&lt;p&gt;Even after using techniques like quantization, we’re still faced with another problem. In order to generate 100 tokens, we must load our weights 100 times.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image5.png&quot; alt=&quot;diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even if the weights are quantized, we still must load our weights over and over, once for each token we generate! Is there any way around this?&lt;/p&gt;

&lt;p&gt;At first glance, the answer might seem like no - there’s a strict serial dependency in our autoregressive generation. However, as it turns out, by utilizing &lt;a href=&quot;https://arxiv.org/abs/2211.17192&quot;&gt;speculative decoding&lt;/a&gt;, we’re able to break this strict serial dependency and obtain speedups!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image21.png&quot; alt=&quot;engineers&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine you had a senior engineer (called Verity), who makes the right technical decisions but is rather slow at writing code. However, you also have a junior engineer (called Drake), who doesn’t always make the right technical decisions but can write code much faster (and cheaper!) than Verity. How can we take advantage of Drake (the junior engineer) to write code faster while ensuring that we are still making the right technical decisions?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image6.png&quot; alt=&quot;engineers&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, Drake goes through the labor-intensive process of writing the code, making technical decisions along the way. Next, we give the code to Verity to review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image15.png&quot; alt=&quot;engineers&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon reviewing the code, Verity might decide that the first 3 technical decisions Drake made are correct, but the last 2 need to be redone. So, Drake goes back, throws away his last 2 decisions, and restarts coding from there.&lt;/p&gt;

&lt;p&gt;Notably, although Verity (the senior engineer) has only looked at the code once, we are able to generate 3 pieces of validated code identical to what she would have written! Thus, assuming Verity is able to review the code faster than it would have taken her to write those 3 pieces herself, this approach comes out ahead.&lt;/p&gt;

&lt;p&gt;In the context of transformer inference, Verity would be played by the role of the larger model whose outputs we want for our task, called the &lt;strong&gt;verifier model&lt;/strong&gt;. Similarly, Drake would be played by a smaller model that’s able to generate text much faster than the larger model, called the &lt;strong&gt;draft model&lt;/strong&gt;. So, we would generate 8 tokens using the draft model, and then process all eight tokens in parallel using the verifier model, throwing out the ones that don’t match.&lt;/p&gt;

&lt;p&gt;Like mentioned above, one crucial property of speculative decoding is that &lt;strong&gt;it does not change the quality of the output&lt;/strong&gt;. As long as the time it takes for generating the tokens using the draft model + verifying the tokens is less than it would have taken to generate those tokens, we come out ahead.&lt;/p&gt;

&lt;p&gt;One of the great things about doing this all in native PyTorch is that this technique is actually really easy to implement! Here’s the &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L76&quot;&gt;entirety of the implementation&lt;/a&gt;, in about 50 lines of native PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image10.png&quot; alt=&quot;code&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although speculative decoding guarantees that we have mathematically identical results compared to regular generation, it does have the property that the runtime performance varies depending on the generated text, as well as how aligned the draft and verifier model are. For example, when running CodeLlama-34B + CodeLlama-7B, we’re able to obtain a 2x boost in tokens/s for generating code. On the other hand, when using Llama-7B + TinyLlama-1B, we’re only able to obtain about a 1.3x boost in tokens/s.&lt;/p&gt;

&lt;h2 id=&quot;sidenote-running-this-on-amd&quot;&gt;Sidenote: Running this on AMD&lt;/h2&gt;

&lt;p&gt;Like mentioned above, every single kernel in decoding is generated from scratch by torch.compile, and is converted into OpenAI Triton. As AMD has a &lt;a href=&quot;https://pytorch.org/blog/experience-power-pytorch-2.0/&quot;&gt;torch.compile backend&lt;/a&gt; (and also a Triton backend), we can simply go through all of the optimizations above… but on an AMD GPU! With int8 quantization, we’re able to achieve 102.5 tokens/s with one GCD (i.e. one half) of a MI250x!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image4.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-4-reducing-the-size-of-the-weights-even-more-with-int4-quantization-and-gptq-2021-toks&quot;&gt;Step 4: Reducing the size of the weights even more with int4 quantization and GPTQ (202.1 tok/s)&lt;/h2&gt;

&lt;p&gt;Of course, if reducing the weights down from 16 bits to 8 bits allows for speedups by reducing the number of bytes we need to load, reducing the weights down to 4 bits would result in even larger speedups!&lt;/p&gt;

&lt;p&gt;Unfortunately, when reducing weights down to 4-bits, the accuracy of the model starts to become a much larger concern. From our preliminary evals, we see that although using int8 weight-only quantization has no perceptible accuracy degradation, using int4 weight-only quantization does.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image13.png&quot; alt=&quot;table&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 2 main tricks we can use to limit the accuracy degradation of int4 quantization.&lt;/p&gt;

&lt;p&gt;The first one is to have a more granular scaling factor. One way to think about the scaling factor is that when we have a quantized tensor representation, it is on a sliding scale between a floating point tensor (each value has a scaling factor) and an integer tensor (no values have a scaling factor). For example, with int8 quantization, we had one scaling factor per row. If we want higher accuracy, however, we can change that to “one scaling factor per 32 elements”. We choose a group size of 32 to minimize accuracy degradation, and this is also a common choice among the community.&lt;/p&gt;

&lt;p&gt;The other one is to use a more advanced quantization strategy than simply rounding the weights. For example, approaches like &lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot;&gt;GPTQ&lt;/a&gt; leverage example data in order to calibrate the weights more accurately. In this case, we prototype an implementation of GPTQ in the repository based off of PyTorch’s recently released &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html&quot;&gt;torch.export&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, we need kernels that fuse int4 dequantize with the matrix vector multiplication. In this case, torch.compile is unfortunately not able to generate these kernels from scratch, so we leverage some handwritten CUDA kernels in PyTorch.&lt;/p&gt;

&lt;p&gt;These techniques require some additional work, but putting them all together results in even better performance!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image12.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-5-combining-everything-together-2447-toks&quot;&gt;Step 5: Combining everything together (244.7 tok/s)&lt;/h2&gt;

&lt;p&gt;Finally, we can compose all of the techniques together to achieve even better performance!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image22.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-6-using-tensor-parallelism&quot;&gt;Step 6: Using Tensor Parallelism&lt;/h2&gt;

&lt;p&gt;So far, we’ve been restricting ourselves to minimizing latency while on a single GPU. In many settings, however, we have access to multiple GPUs. This allows us to improve our latency further!&lt;/p&gt;

&lt;p&gt;To get an intuitive sense of why this would allow us to improve our latency, let’s take a look at the prior equation for MBU, particularly the denominator. Running on multiple GPUs gives us access to more memory bandwidth, and thus, higher potential performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image8.png&quot; alt=&quot;MBU&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As for which parallelism strategy to pick, note that in order to reduce our latency for one example, we need to be able to leverage our memory bandwidth across more devices simultaneously. This means that we need to split the processing of one token across multiple devices. In other words, we need to use tensor parallelism.&lt;/p&gt;

&lt;p&gt;Luckily, PyTorch also provides low-level tools for tensor-parallelism that compose with torch.compile. We are also working on higher-level APIs for expressing tensor parallelism, stay tuned for those!&lt;/p&gt;

&lt;p&gt;However, even without a higher-level API, it’s actually still quite easy to add tensor parallelism. Our implementation comes in at &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py&quot;&gt;150 lines of code&lt;/a&gt;, and doesn’t require any model changes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image20.png&quot; alt=&quot;code&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are still able to take advantage of all the optimizations mentioned previously, which all can continue to compose with tensor parallelism. Combining these together, we’re able to serve Llama-70B at 55 tokens/s with int8 quantization!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-2/image26.png&quot; alt=&quot;chart&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Let’s take a look at what we’re able to accomplish.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Simplicity: Ignoring quantization, &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/model.py&quot;&gt;model.py&lt;/a&gt; (244 LOC) + &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py&quot;&gt;generate.py&lt;/a&gt; (371 LOC) + &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py&quot;&gt;tp.py&lt;/a&gt; (151 LOC) comes out to 766 LOC to implement fast inference + speculative decoding + tensor-parallelism.&lt;/li&gt;
  &lt;li&gt;Performance: With Llama-7B, we’re able to use compile + int4 quant + speculative decoding to reach 241 tok/s. With llama-70B, we’re able to also throw in tensor-parallelism to reach 80 tok/s. These are both close to or surpassing SOTA performance numbers!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PyTorch has always allowed for simplicity, ease of use, and flexibility. However, with torch.compile, we can throw in performance as well.&lt;/p&gt;

&lt;p&gt;The code can be found here: &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;https://github.com/pytorch-labs/gpt-fast&lt;/a&gt;. We hope that the community finds it useful. Our goal with this repo is not to provide another library or framework for people to import. Instead, we encourage users to copy-paste, fork, and modify the code in the repo.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank the vibrant open source community for their continual support of scaling LLMs, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lightning AI for supporting pytorch and work in flash attention, int8 quantization, and LoRA fine-tuning.&lt;/li&gt;
  &lt;li&gt;GGML for driving forward fast, on device inference of LLMs&lt;/li&gt;
  &lt;li&gt;Andrej Karpathy for spearheading simple, interpretable and fast LLM implementations&lt;/li&gt;
  &lt;li&gt;MLC-LLM for pushing 4-bit quantization performance on heterogenous hardware&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is the second part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In this blog we’ll focus on LLM optimization.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.1 Contains New Performance Features for AI Developers</title>
      <link href="https://pytorch.org/blog/new-features-for-ai/" rel="alternate" type="text/html" title="PyTorch 2.1 Contains New Performance Features for AI Developers" />
      <published>2023-11-29T00:00:00-08:00</published>
      <updated>2023-11-29T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/new-features-for-ai</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-features-for-ai/">&lt;p&gt;We are excited to see the release of PyTorch 2.1. In this blog, we discuss the five features for which Intel made significant contributions to PyTorch 2.1:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TorchInductor-CPU optimizations including Bfloat16 inference path for torch.compile&lt;/li&gt;
  &lt;li&gt;CPU dynamic shape inference path for torch.compile&lt;/li&gt;
  &lt;li&gt;C++ wrapper (prototype)&lt;/li&gt;
  &lt;li&gt;Flash-attention-based scaled dot product algorithm for CPU&lt;/li&gt;
  &lt;li&gt;PyTorch 2 export post-training auantization with an x86 back end through an inductor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At Intel, we are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at Meta* as we co-developed these features.&lt;/p&gt;

&lt;p&gt;Let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;torchinductor-cpu-optimizations&quot;&gt;TorchInductor-CPU Optimizations&lt;/h2&gt;

&lt;p&gt;This feature optimizes bfloat16 inference performance for TorchInductor. The 3rd and 4th generation Intel® Xeon® Scalable processors have built-in hardware accelerators for speeding up dot-product computation with the bfloat16 data type. Figure 1 shows a code snippet of how to specify the BF16 inference path.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user_model = ...

user_model.eval()
with torch.no_grad(), torch.autocast(&quot;cpu&quot;):
	compiled_model = torch.compile(user_model)
	y = compiled_model(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Figure 1. Code snippet showing the use of BF16 inference with TorchInductor \&lt;/p&gt;

&lt;p&gt;We measured the performance on three TorchInductor benchmark suites—TorchBench, Hugging Face&lt;em&gt;, and TIMM—and the results are as follows in Table 1. Here we see that performance in graph mode (TorchInductor) outperforms eager mode by factors ranging from 1.25x to 2.35x.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Table 1. Bfloat16 performance geometric mean speedup in graph mode, compared with eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Bfloat16 Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.81x
   &lt;/td&gt;
   &lt;td&gt;
1.25x
   &lt;/td&gt;
   &lt;td&gt;
2.35x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Bfloat16 Geometric Mean Speedup (Single-Core Single Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.74x
   &lt;/td&gt;
   &lt;td&gt;
1.28x
   &lt;/td&gt;
   &lt;td&gt;
1.29x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Developers can fully deploy their models on 4th generation Intel Xeon processors to take advantage of the Intel® Advanced Matrix Extensions (Intel® AMX) feature to get peak performance for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. Intel AMX has two primary components: tiles and tiled matrix multiplication (TMUL). The tiles store large amounts of data in eight two-dimensional registers, each one kilobyte in size. TMUL is an accelerator engine attached to the tiles that contain instructions to compute larger matrices in a single operation.&lt;/p&gt;

&lt;h2 id=&quot;cpu-dynamic-shapes-inference-path-for-torchcompile&quot;&gt;CPU Dynamic Shapes Inference Path for torch.compile&lt;/h2&gt;

&lt;p&gt;Dynamic shapes is one of the key features in PyTorch 2.0. PyTorch 2.0 assumes everything is static by default. If we recompile because a size changed, we will instead attempt to recompile that size as being dynamic (sizes that have changed are likely to change in the future). Dynamic shapes support is required for popular models like large language models (LLM). Dynamic shapes that provide support for a broad scope of models can help users get more benefit from torch.compile. For dynamic shapes, we provide the post-op fusion for conv/gemm operators and vectorization code-gen for non-conv/gemm operators.&lt;/p&gt;

&lt;p&gt;Dynamic shapes is supported by both the inductor Triton back end for CUDA* and the C++ back end for CPU. The scope covers improvements for both functionality (as measured by model passing rate) and performance (as measured by inference latency/throughput). Figure 2 shows a code snippet for the use of dynamic shape inference with TorchInductor.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user_model = ...

# Training example
compiled_model = torch.compile(user_model)
y = compiled_model(x_size1)
# Here trigger the recompile because the input size changed
y = compiled_model(x_size2)


# Inference example
user_model.eval()
compiled_model = torch.compile(user_model)
with torch.no_grad():
	y = compiled_model(x_size1)
 # Here trigger the recompile because the input size changed
 y = compiled_model(x_size2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Figure 2. Code snippet showing the use of dynamic shape inference with TorchInductor&lt;/p&gt;

&lt;p&gt;We again measured the performance on the three TorchInductor benchmark suites—TorchBench, Hugging Face, and TIMM—and the results are in Table 2. Here we see that performance in graph mode outperforms eager mode by factors ranging from 1.15x to 1.79x.&lt;/p&gt;

&lt;p&gt;Table 2. Dynamic shape geometric mean speedup compared with Eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Dynamic Shape Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.35x
   &lt;/td&gt;
   &lt;td&gt;
1.15x
   &lt;/td&gt;
   &lt;td&gt;
1.79x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Dynamic Shape Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.48x
   &lt;/td&gt;
   &lt;td&gt;
1.15x
   &lt;/td&gt;
   &lt;td&gt;
1.48x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;c-wrapper-prototype&quot;&gt;C++ Wrapper (Prototype)&lt;/h2&gt;

&lt;p&gt;The feature generates C++ code instead of Python* code to invoke the generated kernels and external kernels in TorchInductor to reduce Python overhead. It is also an intermediate step to support deployment in environments without Python.&lt;/p&gt;

&lt;p&gt;To enable this feature, use the following configuration:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch._inductor.config as config
config.cpp_wrapper = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For light workloads where the overhead of the Python wrapper is more dominant, C++ wrapper demonstrates a higher performance boost ratio. We grouped the models in TorchBench, Hugging Face, and TIMM per the average inference time of one iteration and categorized them into small, medium, and large categories. Table 3 shows the geometric mean speedups achieved by the C++ wrapper in comparison to the default Python wrapper.&lt;/p&gt;

&lt;p&gt;Table 3. C++ wrapper geometric mean speedup compared with Eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Static Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.06x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
   &lt;td&gt;
1.00x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Static Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.13x
   &lt;/td&gt;
   &lt;td&gt;
1.02x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Dynamic Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.05x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
   &lt;td&gt;
1.00x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Dynamic Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.14x
   &lt;/td&gt;
   &lt;td&gt;
1.02x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;BF16 Static Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.09x
   &lt;/td&gt;
   &lt;td&gt;
1.03x
   &lt;/td&gt;
   &lt;td&gt;
1.04x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;BF16 Static Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.17x
   &lt;/td&gt;
   &lt;td&gt;
1.04x
   &lt;/td&gt;
   &lt;td&gt;
1.03x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;flash-attention-based-scaled-dot-product-algorithm-for-cpu&quot;&gt;Flash-Attention-Based Scaled Dot Product Algorithm for CPU&lt;/h2&gt;

&lt;p&gt;Scaled dot product attention (SDPA) is one of the flagship features of PyTorch 2.0 that helps speed up transformer models. It is accelerated with optimal CUDA kernels while still lacking optimized CPU kernels. This flash-attention implementation targets both training and inference, with both FP32 and Bfloat16 data types supported. There is no front-end use change for users to leverage this SDPA optimization. When calling SDPA, a specific implementation will be chosen automatically, including this new implementation.&lt;/p&gt;

&lt;p&gt;We have measured the SDPA-related models in Hugging Face, and they are proven effective when compared to the unfused SDPA. Shown in Table 4 are the geometric mean speedups for SDPA optimization. \&lt;/p&gt;

&lt;p&gt;Table 4. SDPA optimization performance geometric mean speedup&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;3&quot;&gt;
&lt;strong&gt;SDPA Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup FP32
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup BF16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.15x, 20/20
   &lt;/td&gt;
   &lt;td&gt;
1.07x, 20/20
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;3&quot;&gt;
&lt;strong&gt;SDPA Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup FP32
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup BF16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.02x, 20/20
   &lt;/td&gt;
   &lt;td&gt;
1.04x, 20/20
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;pytorch-2-export-post-training-quantization-with-x86-back-end-through-inductor&quot;&gt;PyTorch 2 Export Post-Training Quantization with x86 Back End through Inductor&lt;/h2&gt;

&lt;p&gt;PyTorch provides a new quantization flow in the PyTorch 2.0 export. This feature uses TorchInductor with an x86 CPU device as the back end for post-training static quantization with this new quantization flow. An example code snippet is shown in Figure 3.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch._dynamo as torchdynamo
from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e
import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq

model = ... 

model.eval()
with torch.no_grad():
 # Step 1: Trace the model into an FX graph of flattened ATen operators
 exported_graph_module, guards = torchdynamo.export(
	 model,
	 *copy.deepcopy(example_inputs),
	 aten_graph=True,
 )

 # Step 2: Insert observers or fake quantize modules
 quantizer = xiq.X86InductorQuantizer()
 operator_config = xiq.get_default_x86_inductor_quantization_config()
 quantizer.set_global(operator_config)
 prepared_graph_module = prepare_pt2e(exported_graph_module, quantizer)

 # Doing calibration here.

 # Step 3: Quantize the model
 convert_graph_module = convert_pt2e(prepared_graph_module)

 # Step 4: Lower Quantized Model into the backend
 compile_model = torch.compile(convert_graph_module)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Figure 3. Code snippet showing the use of Inductor as back end for PyTorch 2 export post-training quantization&lt;/p&gt;

&lt;p&gt;All convolutional neural networks (CNN) models from the TorchBench test suite have been measured and proven effective when compared with the Inductor FP32 inference path. Performance metrics are shown in Table 5.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Compiler&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Geometric Speedup&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Geometric Related Accuracy Loss&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
3.25x, 12/12
   &lt;/td&gt;
   &lt;td&gt;
0.44%, 12/12
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;Try out &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.1.0&quot;&gt;PyTorch 2.1&lt;/a&gt; and realize the performance benefits for yourself from these features contributed by Intel.&lt;/p&gt;

&lt;p&gt;We encourage you to check out Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about the 4th generation Intel Xeon Scalable processor, visit the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI platform&lt;/a&gt; where you can learn how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h3&gt;

&lt;p&gt;1 Amazon EC2* m7i.16xlarge: 1-node, Intel Xeon Platinum 8488C processor with 256 GB memory (1 x 256 GB DDR5 4800 MT/s), microcode 0x2b000461, hyperthreading on, turbo on, Ubuntu* 22.04.3 LTS, kernel 6.2.0-1011-aws, GCC* 11.3.0, Amazon Elastic Block Store 200 GB, BIOS Amazon EC2 1.0 10/16/2017; Software: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1&quot;&gt;PyTorch 2.1.0_rc4&lt;/a&gt;, &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/tree/v3.1.1&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN) version 3.1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/commit/ffbbebb9&quot;&gt;TorchBench&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision/commit/8636bf3&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text/commit/142d029&quot;&gt;TorchText&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/audio/commit/475b6ae&quot;&gt;TorchAudio&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/data/commit/eb9bf61&quot;&gt;TorchData&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1/benchmarks/dynamo&quot;&gt;TorchDynamo Benchmarks&lt;/a&gt;, tested by Intel on 9/12/2023.&lt;/p&gt;

&lt;p&gt;2 Amazon EC2 c6i.16xlarge: 1-node, Intel Xeon Platinum 8375C processor with 128 GB memory (1 x 128 GB DDR4 3200 MT/s), microcode 0xd0003a5, hyperthreading on, turbo on, Ubuntu 22.04.2 LTS, kernel 6.2.0-1011-aws, gcc 11.3.0, Amazon Elastic Block Store 200 GB, BIOS Amazon EC2 1.010/16/2017; Software: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1&quot;&gt;PyTorch 2.1.0_rc4&lt;/a&gt;, &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/tree/v3.1.1&quot;&gt;oneDNN version 3.1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/commit/ffbbebb9&quot;&gt;TorchBench&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision/commit/8636bf3&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text/commit/142d029&quot;&gt;TorchText&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/audio/commit/475b6ae&quot;&gt;TorchAudio&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/data/commit/eb9bf61&quot;&gt;TorchData&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1/benchmarks/dynamo&quot;&gt;TorchDynamo Benchmarks&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/tree/chuanqiw/inductor_quant/userbenchmark/cpu&quot;&gt;TorchBench cpu userbenchmark&lt;/a&gt;, tested by Intel on 9/12/2023.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to see the release of PyTorch 2.1. In this blog, we discuss the five features for which Intel made significant contributions to PyTorch 2.1:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Generative AI with PyTorch: Segment Anything, Fast</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai/" rel="alternate" type="text/html" title="Accelerating Generative AI with PyTorch: Segment Anything, Fast" />
      <published>2023-11-16T00:00:00-08:00</published>
      <updated>2023-11-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai/">&lt;p&gt;This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.&lt;/p&gt;

&lt;p&gt;As announced during the &lt;a href=&quot;https://www.youtube.com/watch?v=IWpM_9AsC-U&quot;&gt;PyTorch Developer Conference 2023&lt;/a&gt;, the PyTorch team &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;rewrote Meta’s Segment Anything (“SAM”) Model&lt;/a&gt; &lt;strong&gt;resulting in 8x faster code&lt;/strong&gt; than &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;the original implementation&lt;/a&gt;, with no loss of accuracy, all using native PyTorch optimizations. We leverage a breadth of new PyTorch features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;Torch.compile&lt;/a&gt;: A compiler for PyTorch models&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao/tree/main#torchao&quot;&gt;GPU quantization&lt;/a&gt;: Accelerate models with reduced precision operations&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;Scaled Dot Product Attention (SDPA)&lt;/a&gt;: Memory efficient attention implementations&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/semi_structured_sparse.html&quot;&gt;Semi-Structured (2:4) Sparsity:&lt;/a&gt; A GPU optimized sparse memory format&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;Nested Tensor:&lt;/a&gt; Batch together non-uniformly sized data into a single Tensor, such as images of different sizes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Custom operators with Triton:&lt;/strong&gt; Write GPU operations using Triton Python DSL and easily integrate it into PyTorch’s various components with custom operator registration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We encourage readers to copy-paste code from &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast&quot;&gt;our implementation of SAM on Github&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/issues&quot;&gt;ask us questions&lt;/a&gt; on Github.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_7.png&quot; alt=&quot;A quick glimpse of increasing throughput and decreasing memory overhead&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A quick glimpse of increasing throughput and decreasing memory overhead with our newly released, PyTorch native, features. Benchmarks run on p4d.24xlarge instance (8x A100s).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;segmentanything-model&quot;&gt;SegmentAnything Model&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;SAM&lt;/a&gt; is a zero-shot vision model for generating promptable image masks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/intro_image.jpg&quot; alt=&quot;sam image masks&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The SAM architecture [described&lt;a href=&quot;https://arxiv.org/abs/2304.02643&quot;&gt; in its paper&lt;/a&gt;] includes multiple prompt and image encoders based on the Transformer architecture. Of this, we measured performance across the smallest and largest vision transformer backbones: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&quot;&gt;ViT-B&lt;/a&gt; and &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&quot;&gt;ViT-H&lt;/a&gt;. And for simplicity, we only show traces for the ViT-B model.&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;Below we tell the story of optimizing SAM: profiling, identifying bottlenecks, and building new features into PyTorch that solve these problems. Throughout, we showcase our new PyTorch features: &lt;strong&gt;torch.compile, SDPA, Triton kernels, Nested Tensor and semi-structured sparsity.&lt;/strong&gt; The following sections are progressively built upon each other, ending with our SAM-fast, now &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast&quot;&gt;available on Github&lt;/a&gt;. We motivate each feature using real kernel and memory traces, using fully PyTorch native tooling, and visualize these traces with &lt;a href=&quot;https://perfetto.dev/&quot;&gt;Perfetto UI&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;p&gt;Our SAM baseline is Facebook Research’s &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;unmodified model&lt;/a&gt;, using float32 dtype and a batch size of 1. After some initial warmup, we can look at a kernel trace using the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;PyTorch Profiler&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/baseline_trace.jpg&quot; alt=&quot;kernel trace&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We notice two areas ripe for optimization.&lt;/p&gt;

&lt;p&gt;The first is long calls to aten::index, the underlying call resulting from a Tensor index operation (e.g., []). While the actual GPU time spent on aten::index is relatively low. aten::index is launching two kernels, and a blocking cudaStreamSynchronize is happening in between. This means the CPU is waiting for the GPU to finish processing until it launches the second kernel. To optimize SAM, we should aim to remove blocking GPU syncs causing idle time.&lt;/p&gt;

&lt;p&gt;The second is significant time spent on GPU in matrix multiplication (dark green on stream 7 7 above). This is common in Transformers. We can significantly speed up SAM if we can reduce the amount of GPU time spent on matrix multiplication.&lt;/p&gt;

&lt;p&gt;We can measure the throughput (img/s) and memory overhead (GiB) from out of the box SAM to establish a baseline:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_0.png&quot; alt=&quot;throughput (img/s) and memory overhead (GiB) from out of the box SAM&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bfloat16-half-precision-gpu-syncs-and-batching&quot;&gt;Bfloat16 Half precision (+GPU syncs and batching)&lt;/h3&gt;

&lt;p&gt;To address the first issue of less time spent in matrix multiplication, we can turn to &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format&quot;&gt;bfloat16&lt;/a&gt;. Bfloat16 is a commonly used half-precision type. Through less precision per parameter and activations, we can save significant time and memory in computation. With reducing precision of parameters, it’s critical to validate end to end model accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bfloat16_snippet.jpg&quot; alt=&quot;replacing padding dtypes with half precision, bfloat16&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shown here is an example of replacing padding dtypes with half precision, bfloat16. &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blame/main/segment_anything_fast/modeling/prompt_encoder.py#L86&quot;&gt;Code is here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next to simply setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.to(torch.bfloat16)&lt;/code&gt; we have to change a few small places that assume the default dtype.&lt;/p&gt;

&lt;p&gt;Now, in order to remove GPU syncs we need to audit operations that cause them. We can find these pieces of code by searching the GPU traces for calls to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cudaStreamSynchronize&lt;/code&gt;. In fact, we found two locations that we were able to rewrite to be sync-free.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/code1.jpg&quot; alt=&quot;code sample 1&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bfloat16_snippet2.jpg&quot; alt=&quot;replacing padding dtypes with half precision, bfloat16&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Specifically, we see that within SAM’s image encoder, there are variables acting as coordinate scalers, q_coords and k_coords. These are both allocated and processed on the CPU. However, once these variables are used to index in rel_pos_resized, the index operation automatically moves these variables to the GPU. This copy over causes the GPU sync we’ve observed above. We notice a second call to index in SAM’s prompt encoder: We can use torch.where to rewrite this as shown above.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel trace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After applying these changes, we begin to see significant time between individual kernel calls. This is typically observed with small batch sizes (1 here) due to the GPU overhead of launching kernels. To get a closer look at practical areas for optimization, we can start to profile SAM inference with batch size 8:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bfloat16_trace.jpg&quot; alt=&quot;profile SAM inference with batch size 8&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the time spent per-kernel, we obverse most of SAM’s GPU time spent on elementwise kernels and softmax operation. With this we now see that matrix multiplications have become a much smaller relative overhead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bfloat16_kernels.jpg&quot; alt=&quot;matrix multiplications have become a much smaller relative overhead&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Taken the GPU sync and bfloat16 optimizations together, we have now pushed SAM performance by up to 3x&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_1.png&quot; alt=&quot;SAM performance by up to 3x&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;torchcompile-graph-breaks-and-cuda-graphs&quot;&gt;Torch.compile (+graph breaks and CUDA graphs)&lt;/h3&gt;

&lt;p&gt;When observing a large number of small operations, such as the elementwise kernels profiled above, turning to a compiler to fuse operations can have strong benefits. PyTorch’s recently released &lt;strong&gt;torch.compile&lt;/strong&gt; does a great job optimizing by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fusing together sequences of operations such as nn.LayerNorm or nn.GELU into a single GPU kernel that is called and&lt;/li&gt;
  &lt;li&gt;Epilogues: fusing operations that immediately follow matrix multiplication kernels to reduce the number of GPU kernel calls.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through these optimizations, we reduce the number of GPU global memory roundtrips, thus speeding up inference. We can now try torch.compile on SAM’s &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/3bd74614fe7285de4de3d763d8ec2e951c4c589c/experiments/eval_combo.py#L196-L201&quot;&gt;image encoder&lt;/a&gt;. To maximize performance we use a few advanced compile techniques such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;using torch.compile’s max-autotune mode enables &lt;a href=&quot;https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/&quot;&gt;CUDA graphs&lt;/a&gt; and shape-specific kernels with custom epilogues&lt;/li&gt;
  &lt;li&gt;By setting TORCH_LOGS=”graph_breaks,recompiles” we can manually verify that we are not running into &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_faq.html#graph-breaks&quot;&gt;graph breaks&lt;/a&gt; or recompiles.&lt;/li&gt;
  &lt;li&gt;Padding the batch of images input to the encoder with zeros ensures compile accepts static shapes thus being able to always use shape-specific optimized kernels with custom epilogues without recompilations.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;predictor.model.image_encoder = \
    torch.compile(predictor.model.image_encoder, mode=use_compile)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Kernel trace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/compile_trace.jpg&quot; alt=&quot;Kernel trace&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;torch.compile is working beautifully. We launch a single CUDA graph, which makes up a significant portion of GPU time within the timed region. Let’s run our profile again and look at the percentage of GPU time spent in specific kernels:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/compile_kernels.jpg&quot; alt=&quot;the percentage of GPU time spent in specific kernels&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now see softmax makes up a significant portion of the time followed by various GEMM variants. In summary we observe the following measurements for batch size 8 and above changes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_2.png&quot; alt=&quot;measurements for batch size 8 and above&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sdpa-scaled_dot_product_attention&quot;&gt;SDPA: scaled_dot_product_attention&lt;/h3&gt;

&lt;p&gt;Next up, we can tackle one of the most common areas for transformer performance overhead: the attention mechanism. Naive attention implementations scale quadratically in time and memory with sequence length. PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention&quot;&gt;scaled_dot_product_attention&lt;/a&gt; operation built upon the principles of &lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;Flash Attention&lt;/a&gt;, &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttentionV2&lt;/a&gt; and &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormer’s memory efficient attention&lt;/a&gt; can significantly speed up GPU attention. Combined with torch.compile, this operation allows us to express and fuse a common pattern within variants of MultiheadAttention. After &lt;a href=&quot;https://github.com/facebookresearch/segment-anything/compare/50cb459d080bcd783a4b481d3bde4150d35ac497...7dc75fdf283693f73606f2fe7fdcb693afcb16b9&quot;&gt;a small set of changes&lt;/a&gt; we can adapt the model to use scaled_dot_product_attention.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/sdpa_snippet.jpg&quot; alt=&quot;PyTorch native attention implementation&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PyTorch native attention implementation, &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L236&quot;&gt;see code here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel trace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can now see that in particular the memory efficient attention kernel is taking up a large amount of computational time on the GPU:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/sdpa_kernels.jpg&quot; alt=&quot;memory efficient attention kernel is taking up a large amount of computational time on the GPU&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using PyTorch’s native scaled_dot_product_attention, we can significantly increase the batch size. We now observe the following measurements for batch size 32 and above changes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_3.png&quot; alt=&quot;batch size 32 and above&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;triton-custom-sdpa-for-fused-relative-positional-encoding&quot;&gt;Triton: Custom SDPA for fused relative positional encoding&lt;/h3&gt;

&lt;p&gt;Transitioning away from inference throughput for a moment, we started profiling overall SAM memory. Within the image encoder, we saw significant spikes in memory allocation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/triton_trace.png&quot; alt=&quot;spikes in memory allocation&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Zooming in, we see this allocation happens within add_decomposed_rel_pos, &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L373&quot;&gt;on the following line:&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/triton_snippet.jpg&quot; alt=&quot;we see this allocation happens within add_decomposed_rel_pos&quot; style=&quot;width:100%;display: block;max-width:500px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The attn variable here is the addition of two smaller tensors: rel_h of shape (B, q_h, q_w, k_h, 1) and rel_w of shape (B, q_h, q_w, 1, k_w).&lt;/p&gt;

&lt;p&gt;It’s not surprising that the memory efficient attention kernel (used via SDPA) is taking a long time with an attention bias size over 3.0GiB. If instead of allocating this large attn tensor, we thread into SDPA the two smaller rel_h and rel_w tensors, and only construct attn as needed, we’d anticipate significant performance gain.&lt;/p&gt;

&lt;p&gt;Unfortunately this is not a trivial modification; SDPA kernels are highly optimized and written in CUDA. We can turn to Triton, with their easy to understand and use &lt;a href=&quot;https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html&quot;&gt;tutorial on a FlashAttention implementation&lt;/a&gt;. After some significant digging and in close collaboration with xFormer’s Daniel Haziza we found one case of input shapes where it is relatively straightforward to implement a fused version of the kernel. The &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/flash_4.py&quot;&gt;details have been added to the repository&lt;/a&gt;. Surprisingly this can be done in under 350 lines of code for the inference case.&lt;/p&gt;

&lt;p&gt;This is a great example of extending PyTorch with a new kernel, straightforwardly built with Triton code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel trace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/triton_kernels.jpg&quot; alt=&quot;kernel trace&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With our custom positional Triton kernel we observe the following measurements for batch size 32.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_4.png&quot; alt=&quot;we observe the following measurements for batch size 32&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;nt-nestedtensor-and-batching-predict_torch&quot;&gt;NT: NestedTensor and batching predict_torch&lt;/h3&gt;

&lt;p&gt;We have spent a lot of time on the image encoder. This makes sense, since it takes up the most amount of computational time. At this point however it is fairly well optimized and the operator that takes the most time would require significant additional investment to be improved.&lt;/p&gt;

&lt;p&gt;We discovered an interesting observation with the &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/7cd6ba3cea451602acb7d36d176da06c70ac68f1/experiments/eval_combo.py#L137-L157&quot;&gt;mask prediction pipeline&lt;/a&gt;: for each image we have there is an associated size, coords, and fg_labels Tensor. Each of these tensors are of different batch sizes. Each image itself is also of a different size. This representation of data looks like &lt;a href=&quot;https://en.wikipedia.org/wiki/Jagged_array&quot;&gt;Jagged Data&lt;/a&gt;. With PyTorch’s recently released &lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;NestedTensor&lt;/a&gt;, we can modify our data pipeline batch coords and fg_labels Tensors into a single NestedTensor. This can have significant performance benefits for the prompt encoder and mask decoder that follow the image encoder. Invoking:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.nested.nested_tensor(data, dtype=dtype, layout=torch.jagged)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Kernel trace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/trace1.jpg&quot; alt=&quot;Kernel trace&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/nt_kernel.jpg&quot; alt=&quot;we can launch kernels much faster from the CPU than the GPU can process&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see now that we can launch kernels much faster from the CPU than the GPU can process and that it spends a long time waiting at the end of our timed region for the GPU to finish (cudaDeviceSynchronize). We also don’t see any more idle time (white space) between kernels on the GPU.&lt;/p&gt;

&lt;p&gt;With Nested Tensor, we observe the following measurements for batch size 32 and above changes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_5.png&quot; alt=&quot;batch size 32 and above changes&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;int8-quantization-and-approximating-matmul&quot;&gt;int8: quantization and approximating matmul&lt;/h3&gt;

&lt;p&gt;We notice in the above trace, that significant time is now spent in GEMM kernels. We’ve optimized enough that we now see matrix multiplication account for more time in inference than scaled dot product attention.&lt;/p&gt;

&lt;p&gt;Building on earlier learnings going from fp32 to bfloat16, let’s go a step further, emulating even lower precision with int8 quantization. Looking at quantization methods, we focus on &lt;a href=&quot;https://pytorch.org/tutorials/recipes/quantization.html&quot;&gt;Dynamic quantization&lt;/a&gt; wherein our model observes the range of possible inputs and weights of a layer, and subdivides the expressible int8 range to uniformly “spread out” observed values. Ultimately each float input will be mapped to a single integer in the range [-128, 127]. For more information see PyTorch’s &lt;a href=&quot;https://pytorch.org/tutorials/recipes/quantization.html&quot;&gt;tutorial on quantization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Reducing precision can immediately lead to peak memory savings, but to realize inference speedups, we have to make full use of int8 through SAM’s operations. This requires building an efficient int8@int8 matrix multiplication kernel, as well as casting logic to translate from high to low precision (quantization) as well as reversing back from low to high (dequantization). Utilizing the power of torch.compile, we can compile and fuse together these quantization and dequantization routines into efficient single kernels and epilogues of our matrix multiplication. The resulting implementation is &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/blob/21b0208ae46eefc5659f7f200a2bf447add8765b/segment_anything_fast/dynamic_quant.py&quot;&gt;fairly short and less than 250 lines of code&lt;/a&gt;. For more information on the APIs and usage, see &lt;a href=&quot;https://github.com/pytorch-labs/ao/tree/main#torchao&quot;&gt;pytorch-labs/ao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While it’s common to see some accuracy regression when quantizing models at inference time, SAM has been particularly robust to lower precision inference with minimal loss of accuracy. With quantization added, we now observe the following measurements for &lt;strong&gt;batch size 32&lt;/strong&gt; and above changes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_6.png&quot; alt=&quot;batch size 32 and above changes&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sparse-semi-structured-24-sparsity&quot;&gt;sparse: Semi-structured (2:4) sparsity&lt;/h3&gt;

&lt;p&gt;Matrix multiplications are still our bottleneck. We can turn to the model acceleration playbook with another classic method to approximate matrix multiplication: sparsification. By sparsifying our matrices (i.e., zeroing out values), we could theoretically use fewer bits to store weight and activation tensors. The process by which we decide which weights in the tensor to set to zero is called pruning. The idea behind pruning is that small weights in a weight tensor contribute little to the net output of a layer, typically the product of weights with activations. Pruning away small weights can potentially reduce model size without significant loss of accuracy.&lt;/p&gt;

&lt;p&gt;Methods for pruning are varied, from completely unstructured, wherein weights are greedily pruned to highly structured, wherein large sub-components of a tensor are pruned a time. Choice of method is not trivial. While unstructured pruning may have the theoretically least impact on accuracy, GPUs are also highly efficient with multiplying large, dense matrices and may suffer significant performance degradation in sparse regimes. One recent pruning method supported in PyTorch seeks to strike a balance, called semi-structured (or 2:4) sparsity. This sparse storage reduces the original tensor by a significant 50%, while simultaneously resulting in a dense tensor output that can leverage highly performant, 2:4 GPU kernels. See the following picture for an illustration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/sparse_image.png&quot; alt=&quot;dense tensor output that can leverage highly performant, 2:4 GPU kernels&quot; style=&quot;width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt&quot;&gt;developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In order to use this sparse storage format and the associated fast kernels we need to prune our weights such that they adhere to the constraints for the format. We pick the two smallest weights to prune in a 1 by 4 region, measuring the performance vs accuracy tradeoff. It is easy to change a weight from its default PyTorch (“strided”) layout to this new, semi-structured sparse layout. To implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apply_sparse(model)&lt;/code&gt; we only require 32 lines of Python code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor

# Sparsity helper functions
def apply_fake_sparsity(model):
    &quot;&quot;&quot;
    This function simulates 2:4 sparsity on all linear layers in a model.
    It uses the torch.ao.pruning flow.
    &quot;&quot;&quot;
    # torch.ao.pruning flow
    from torch.ao.pruning import WeightNormSparsifier
    sparse_config = []
    for name, mod in model.named_modules():
        if isinstance(mod, torch.nn.Linear):
            sparse_config.append({&quot;tensor_fqn&quot;: f&quot;{name}.weight&quot;})

    sparsifier = WeightNormSparsifier(sparsity_level=1.0,
                                      sparse_block_shape=(1,4),
                                      zeros_per_block=2)
    sparsifier.prepare(model, sparse_config)
    sparsifier.step()

    sparsifier.step()
    sparsifier.squash_mask()


def apply_sparse(model):
    apply_fake_sparsity(model)
    for name, mod in model.named_modules():
        if isinstance(mod, torch.nn.Linear):
            mod.weight = torch.nn.Parameter(to_sparse_semi_structured(mod.weight))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With 2:4 sparsity, we observe peak performance on SAM with vit_b and batch size 32:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai/bar_chart_7.png&quot; alt=&quot;With 2:4 sparsity, we observe peak performance on SAM with vit_b and batch size 32&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Wrapping up, we are excited to have&lt;a href=&quot;https://www.youtube.com/watch?v=IWpM_9AsC-U&quot;&gt; announced&lt;/a&gt; our fastest implementation of &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;Segment Anything&lt;/a&gt; to date. We rewrote Meta’s original SAM in pure PyTorch with no loss of accuracy using a breadth of newly released features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Torch.compile&lt;/strong&gt; PyTorch’s native JIT compiler, providing fast, automated fusion of PyTorch operations [&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;tutorial&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPU quantization&lt;/strong&gt; accelerate models with reduced precision operations [&lt;a href=&quot;https://github.com/pytorch-labs/ao/tree/main#torchao&quot;&gt;api&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scaled Dot Product Attention (SDPA)&lt;/strong&gt; a new, memory efficient implementation of Attention [&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;tutorial&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semi-Structured (2:4) Sparsity&lt;/strong&gt; accelerate models with fewer bits to store weights and activations [&lt;a href=&quot;https://pytorch.org/tutorials/prototype/semi_structured_sparse.html&quot;&gt;tutorial&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nested Tensor&lt;/strong&gt; Highly optimized, ragged array handling for non-uniform batch and image sizes [&lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;tutorial&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Triton kernels.&lt;/strong&gt; Custom GPU operations, easily built and optimized via Triton&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details on how to reproduce the data presented in this blog post, check out &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/tree/main/experiments&quot;&gt;the experiments folder of segment-anything-fast&lt;/a&gt;. Please don’t hesitate to contact us or &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast/issues/new&quot;&gt;open an issue&lt;/a&gt; if you run into any technical issues.&lt;/p&gt;

&lt;p&gt;In our next post, we are excited to share similar performance gains with our PyTorch natively authored LLM!&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;We would like to thank Meta’s &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; team including Daniel Haziza and Francisco Massa for authoring SDPA kernels and helping us design our custom one-off Triton kernel.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">🎉 PyTorch Docathon H2 2023 Wrap-up 🎉</title>
      <link href="https://pytorch.org/blog/pytorch-docathon-h2-2023-wrap/" rel="alternate" type="text/html" title="🎉 PyTorch Docathon H2 2023 Wrap-up 🎉" />
      <published>2023-11-16T00:00:00-08:00</published>
      <updated>2023-11-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch-docathon-h2-2023-wrap</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-docathon-h2-2023-wrap/">&lt;p&gt;We are thrilled to announce the successful completion of the Fall 2023 PyTorch Docathon! The event was a resounding success, and we want to extend our heartfelt gratitude to all the participants who made it possible. Dedication, expertise, and tireless efforts of our open-source contributors have once again helped us to improve PyTorch documentation.&lt;/p&gt;

&lt;p&gt;This Docathon ran from Nov 1 through Nov 15 with more than 170 registrants. The energy and enthusiasm were palpable, and entrants were judged on the difficulty of submissions that resulted in over TBA merged pull requests. We have fixed the PyTorch docstrings and made them compatible with the PEP 257 Python Docstring Conventions guidelines. We also have fixed multiple bugs in the pytorch/tutorials repo.&lt;/p&gt;

&lt;p&gt;We want to give a special shout-out to our top contributors, who went above and beyond during this event. Your dedication and expertise have been invaluable in enhancing the PyTorch documentation and empowering developers worldwide.&lt;/p&gt;

&lt;p&gt;Meet the top contributors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First place: &lt;a href=&quot;https://github.com/ahoblitz&quot;&gt;ahoblitz&lt;/a&gt;, &lt;a href=&quot;https://github.com/spzala&quot;&gt;spzala&lt;/a&gt;, &lt;a href=&quot;https://github.com/alperenunlu&quot;&gt;alperenunlu&lt;/a&gt;, &lt;a href=&quot;https://github.com/ChanBong&quot;&gt;ChanBong&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Second place: &lt;a href=&quot;https://github.com/nvs-abhilash&quot;&gt;nvs-abhilash&lt;/a&gt;, &lt;a href=&quot;https://github.com/bjhargrave&quot;&gt;bjhargrave&lt;/a&gt;, &lt;a href=&quot;https://github.com/zabboud&quot;&gt;zabboud&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Third place: &lt;a href=&quot;https://github.com/guptaaryan16&quot;&gt;guptaaryan16&lt;/a&gt;, &lt;a href=&quot;https://github.com/min-jean-cho&quot;&gt;min-jean-cho&lt;/a&gt;, &lt;a href=&quot;https://github.com/markstur&quot;&gt;markstur&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Honorable mentions: &lt;a href=&quot;https://github.com/RustyGrackle&quot;&gt;RustyGrackle&lt;/a&gt;, &lt;a href=&quot;https://github.com/Viditagarwal7479&quot;&gt;Viditagarwal7479&lt;/a&gt;, &lt;a href=&quot;https://github.com/Skylion007&quot;&gt;Skylion007&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the full docathon leaderboard published &lt;a href=&quot;https://github.com/pytorch/tutorials/blob/main/docathon-leaderboard.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As we bring this Docathon to a close, we encourage each and every one of you to stay inspired and keep contributing to PyTorch documentation and code, and pushing the boundaries of what’s possible with PyTorch. Your collective efforts are shaping the landscape of deep learning and fostering innovation in the PyTorch community.&lt;/p&gt;

&lt;p&gt;Thank you again for your participation and support. We look forward to seeing what you will achieve next!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are thrilled to announce the successful completion of the Fall 2023 PyTorch Docathon! The event was a resounding success, and we want to extend our heartfelt gratitude to all the participants who made it possible. Dedication, expertise, and tireless efforts of our open-source contributors have once again helped us to improve PyTorch documentation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch compile to speed up inference on Llama 2</title>
      <link href="https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/" rel="alternate" type="text/html" title="PyTorch compile to speed up inference on Llama 2" />
      <published>2023-11-07T00:00:00-08:00</published>
      <updated>2023-11-07T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch-compile-to-speed-up-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/">&lt;p&gt;In this blog, we discuss how to improve the inference latencies of the Llama 2 family of models using  PyTorch native optimizations such as native fast kernels, compile transformations from torch compile, and tensor parallel for distributed inference. Our approach results in 29ms/token latency for single user requests on the 70B LLaMa model (as measured on 8 A100 GPUs). We are excited to share our findings with the community and make our code available &lt;a href=&quot;https://github.com/foundation-model-stack/foundation-model-stack&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;We are amid a generative AI revolution with large language models of tens of billions of parameters becoming commoditized and available for use. However, it is well recognized in the community that  deploying these large models in a cost-efficient manner remains a key challenge. Many different approaches have been attempted with varying degrees of success and offering different trade-offs. Hardware-specific optimizations (e.g., Faster Transformer from NVIDIA) are restricted to specific target hardware whereas approaches that rely on layers of abstraction (e.g., ONNX) enable arbitrary models but suffer from loss of efficiency.  With the introduction of PyTorch compile last year, IBM and the PyTorch team started exploring the use of model compilation for inference optimizations with the goal of reducing the latency per token for generative models.&lt;/p&gt;

&lt;h2 id=&quot;model-choice&quot;&gt;Model Choice&lt;/h2&gt;

&lt;p&gt;We choose to benchmark on the Llama 2 family of models, given their popularity. The models that we are interested in, and their hyper parameters relevant for this blog are given in the below table:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Hidden dimension&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Num heads&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Num layers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Attention type&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;7B
   &lt;/td&gt;
   &lt;td&gt;4096
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;MHA
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;13B
   &lt;/td&gt;
   &lt;td&gt;5120
   &lt;/td&gt;
   &lt;td&gt;40
   &lt;/td&gt;
   &lt;td&gt;40
   &lt;/td&gt;
   &lt;td&gt;MHA
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;8192
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;80
   &lt;/td&gt;
   &lt;td&gt;GQA
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;These models are decoder only, which means that tokens get generated in a serialized manner, which is typically sped up using KV caching. We take a similar approach in our latency and throughput measurements.&lt;/p&gt;

&lt;h2 id=&quot;inference-approach&quot;&gt;Inference Approach&lt;/h2&gt;

&lt;p&gt;Our goal for inference is to provide a path for achieving the best possible latencies rapidly, to keep up with the velocity with which new model architectures are emerging in the community. A PyTorch native approach is appealing as it allows for the maximum flexibility in terms of “coverage” of models. We note that there are four orthogonal techniques that provide acceleration in inference: (a) Kernel fusion using compile, (b) Faster kernels, (c) Tensor parallel for larger models, and (d) Quantization. In our approach, we use  the first three of these four levers -  compile natively working with faster kernels from SDPA and a custom tensor parallel implementation that all work hand-in-glove to achieve inference latencies of 29ms/token on a 70B model as measured on 8 NVIDIA A100 GPUs with single user.&lt;/p&gt;

&lt;h3 id=&quot;compile-all-the-way&quot;&gt;Compile all the way!&lt;/h3&gt;

&lt;p&gt;PyTorch Compile leverages tracing and graph capture to reduce the CPU overhead and in an ideal scenario results in a single graph execution/instruction from CPU to GPU. However, often compile introduces graph breaks due to model architecture and ops unsupported by compile. For example, complex operations such as einops are not supported by compile today.  Similarly, tensor parallel inference can introduce graph breaks at each layer, since compile requires the tensor parallel  implementation to use traceable communication collectives. If these graph breaks are not removed, the performance of the compiled artifacts will be hampered and could even be lower compared to eager mode execution. To get full benefit of the compiled artifacts, the graph breaks need to be removed.&lt;/p&gt;

&lt;p&gt;Below, we describe how we went about doing this for the 70b Llama 2 model and the challenges we had to overcome to get compile to work all the way through.&lt;/p&gt;

&lt;p&gt;Our first attempt was to try using torch.compile to compile the out-of-box Llama 2 model, but it failed because complex ops were not  supported.  Using TORCH_COMPILE_DEBUG = 1 we identified the RoPE positional encodings was using complex number functions resulting in graph breaks and significant slowdowns. We rewrote the RoPE function to bypass torch.einsum (Original  implementation uses torch.polar that also conflicts with compile) and use torch.cos and torch.sin instead.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.cached_freqs[dev_idx][alpha] = torch.stack(
            [
                torch.cos(freqs),
                -torch.sin(freqs),
                torch.sin(freqs),
                torch.cos(freqs),
            ],
            dim=2,
        ).view(*freqs.shape, 2, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Our implementation of the frequencies computation&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;mt-5&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
t = t / self.scaling_factor

freqs = torch.einsum(&quot;i,j-&amp;gt;ij&quot;, t, self.inv_freq)
# Different from paper, but it uses a different permutation in order to obtain the same calculation
emb = torch.cat((freqs, freqs), dim=-1)
self.register_buffer(&quot;cos_cached&quot;, emb.cos()[None, None, :, :].to(dtype), persistent=False)
self.register_buffer(&quot;sin_cached&quot;, emb.sin()[None, None, :, :].to(dtype), persistent=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Hugging Face implementation of the frequencies computation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once RoPE was fixed, we were able to get 7B and 13B models to compile without ANY graph breaks on a single A100 GPU.&lt;/p&gt;

&lt;p&gt;We used SDPA, the PyTorch native implementation of efficient attention computation with tracing enabled (for compile). To avoid graph breaks related to forcing a single algorithm choice using a Python context, the recommended way, we had to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.cuda.enable_*_sdp &lt;/code&gt;functions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;attn = torch.nn.functional.scaled_dot_product_attention(
            queries,
            keys_e,
            values_e,
            attn_mask=attn_mask,
            dropout_p=self.p_dropout if self.training else 0.0,
            is_causal=is_causal_mask,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Attention computation using SDPA&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next we ran the same steps for the larger 70B model and found that even with half precision, the model does not fit in a single GPU and requires tensor parallel inference. Using torch.compile for the 70B model resulted in 162 graph breaks due to two all-reduces per layer, one all-gather for forward embedding, and one all-gather for reverse embedding. Due to this, we saw no significant improvement in inference latencies. We could not use the distributed tensor implementation from PyTorch at the time of writing this blog as it did not support compile. We rewrote the tensor parallel code from scratch so that it  only depends on traceable collectives to make it work with compile. After this last change, PyTorch compiler did not introduce any graph breaks and we saw a significant speedup in inference latencies. Specifically, we measured latencies for the Llama 70B model at 29ms/token when using 8 A100 GPUs, a 2.4x improvement over unoptimized inference.&lt;/p&gt;

&lt;h3 id=&quot;serving-aspects&quot;&gt;Serving aspects&lt;/h3&gt;

&lt;p&gt;Finally, a point to note here is that simply performing compile on a model is not sufficient to serve the model in a production setting. To realize the above performance with high throughput, we need to support dynamic batching, nested tensors, as well as have a warm up phase where we pre-compile for bucketized sequence lengths. We are working on these aspects to realize such performance in a production setting.&lt;/p&gt;

&lt;h2 id=&quot;experiments-and-measurements&quot;&gt;Experiments and Measurements&lt;/h2&gt;

&lt;p&gt;We use nodes with 8 A100 NVIDIA GPUs with 80G cards for all our measurements in two different environments (IBM Cloud and AWS, both running OpenShift). First, we compare the various techniques – eager mode, with SDPA Flash kernel, with Compile, and with Compile and SDPA. For the 70B model, we run it in Tensor Parallel mode with compile and SDPA. For this experiment, we use 512 tokens as input length with 50 token generation.  For 7 and 13B models, we use single A100 for measurement of latencies, whereas we use 8 A100s for the 70B model. In addition, for the 70B model we use the reduce-overhead option in PyTorch compile that uses CudaGraphs to reduce CPU to GPU kernel launching overheads; the use of CudaGraphs in the 7B and 13B models did not show any benefits (and are thus not reported here). We observe from Figure 1 that compile and SDPA provide very low latencies, with 70B Llama 2 model at 29ms/token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-compile-to-speed-up-inference/fig1.jpg&quot; alt=&quot;Figure 1. Median latency across different techniques with sequence length 512 (measured on IBM Cloud A100 servers)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: Median latency across different techniques with sequence length 512 (measured on IBM Cloud A100 servers)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Next, we examine the impact of sequence length, where we increase it from 1024 to 4096 and observe that the median latency per token increases sub-linearly, demonstrating that when we increase context to large documents, we do not sacrifice response times.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-compile-to-speed-up-inference/fig2.jpg&quot; alt=&quot;Figure 2. Median latency for compile+SDPA with different sequence lengths (Measured on A100s on AWS)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Median latency for compile+SDPA with different sequence lengths (Measured on A100s on AWS)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Finally, with increased batch sizes, we observe that the response latencies increase sub-linearly. For the 13B model, at batch size 8, we encounter an OOM. For the 70B model, given that it is running on 8 GPUs with tensor parallel, we do not see any such OOM issues.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-compile-to-speed-up-inference/fig3.jpg&quot; alt=&quot;Figure 3. Median latency for compile+SDPA with different batch sizes and sequence length fixed at 4096 (Measured on A100s on AWS)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 3&lt;/strong&gt;: Median latency for compile+SDPA with different batch sizes and sequence length fixed at 4096 (Measured on A100s on AWS)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;We have demonstrated how a PyTorch compile pathway for inference demonstrates ultra low latencies for 70B model inference. The next steps are to enable dynamic batching and nested tensors with the above levers.&lt;/p&gt;

&lt;p&gt;Special thanks to Edward Yang, Elias Ellison, Driss Guessous, Will Feng, Will Constable, Horace He, Less Wright, and Andrew Gu from Team PyTorch, whose PRs reviews and code contributions made it possible for us to realize the latencies using PyTorch native approach. We thank the broader Team PyTorch that have been tirelessly working to make PyTorch better, special shout outs to the SDPA team for enabling tracing and compile on fast kernels, the compile team that has been closely guiding us on how to work around as well as fix issues (including identifying and raising NVIDIA driver bugs in CUDA graphs).&lt;/p&gt;

&lt;p&gt;Inference latency has been one of the roadblocks for LLM adoption in critical enterprise workflows, but another major one is the need for safety, trustworthiness and governance. IBM’s guide for AI safety and LLM risk can be found &lt;a href=&quot;https://www.ibm.com/downloads/cas/E5KE5KRZ&quot;&gt;here&lt;/a&gt; and Meta’s responsible user guide for LLaMa can be found &lt;a href=&quot;https://ai.meta.com/llama/responsible-use-guide/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub resources: &lt;a href=&quot;https://ibm.biz/fm-stack&quot;&gt;https://ibm.biz/fm-stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;The Path to Achieve Ultra-Low Inference Latency With LLaMa 65B on PyTorch/XLA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.fireworks.ai/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning-353bf6241248&quot;&gt;Speed, Python: Pick Two. How CUDA Graphs Enable Fast Python Code for Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;IBM’s resources on AI Ethics and Trust: &lt;a href=&quot;https://www.ibm.com/downloads/cas/E5KE5KRZ&quot;&gt;https://www.ibm.com/downloads/cas/E5KE5KRZ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Meta LLaMa responsible user guide: &lt;a href=&quot;https://ai.meta.com/llama/responsible-use-guide/&quot;&gt;https://ai.meta.com/llama/responsible-use-guide/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>IBM Research: Antoni Viros i Martin, Brian Vaughan, Davis Wertheimer, Joshua Rosenkranz, Mudhakar Srivatsa, Nelson Mimura Gonzalez, Raghu Ganti, Supriyo Chakraborty, Zhuoran Liu Meta: Geeta Chauhan, Hamid Shojanazeri</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we discuss how to improve the inference latencies of the Llama 2 family of models using PyTorch native optimizations such as native fast kernels, compile transformations from torch compile, and tensor parallel for distributed inference. Our approach results in 29ms/token latency for single user requests on the 70B LLaMa model (as measured on 8 A100 GPUs). We are excited to share our findings with the community and make our code available here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs</title>
      <link href="https://pytorch.org/blog/high-performance-llama-2/" rel="alternate" type="text/html" title="High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs" />
      <published>2023-11-06T00:00:00-08:00</published>
      <updated>2023-11-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/high-performance-llama-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/high-performance-llama-2/">&lt;p&gt;In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;Llama&lt;/a&gt; family of open sourced large language models (LLMs) stands out as a notable breakthrough. &lt;a href=&quot;https://ai.meta.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Llama&lt;/a&gt; marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. &lt;a href=&quot;https://about.fb.com/news/2023/07/llama-2/&quot;&gt;Llama 2&lt;/a&gt; further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.&lt;/p&gt;

&lt;p&gt;Shortly after the announcement of Llama, we published a &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;blog post&lt;/a&gt; showcasing ultra-low inference latency for Llama using PyTorch/XLA on Cloud TPU v4. Building on these results, today, we are proud to share Llama 2 training and inference performance using &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;PyTorch/XLA&lt;/a&gt; on Cloud TPU v4 and our newest AI supercomputer, &lt;a href=&quot;https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga&quot;&gt;Cloud TPU v5e&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we use Llama 2 as an example model to demonstrate the power of PyTorch/XLA on Cloud TPUs for LLM training and inference. We discuss the computation techniques and optimizations used to improve inference throughput and training model FLOPs utilization (MFU). &lt;strong&gt;For Llama 2 70B parameters, we deliver 53% training MFU, 17 ms/token inference latency, 42 tokens/s/chip throughput powered by PyTorch/XLA on Google Cloud TPU.&lt;/strong&gt; We offer a &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md&quot;&gt;training user guide&lt;/a&gt; and an &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/TORCH_XLA_USER_GUIDE.md&quot;&gt;inference user guide&lt;/a&gt; for reproducing the results in this article. Additionally, you may find our &lt;a href=&quot;https://www.youtube.com/watch?v=PSpmRtWuMs8&quot;&gt;Google Next 2023 presentation here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h2&gt;

&lt;p&gt;Llama 2 comes in various sizes, ranging from 7B to 70B parameters, catering to different needs, computational resources, and training / inference budgets. Whether it’s small-scale projects or large-scale deployments, Llama models offer versatility and scalability to accommodate a wide range of applications.&lt;/p&gt;

&lt;p&gt;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The largest, 70B model, uses grouped-query attention, which speeds up inference without sacrificing quality. &lt;a href=&quot;https://arxiv.org/pdf/2307.09288.pdf&quot;&gt;Llama 2 is trained on 2 trillion tokens&lt;/a&gt; (40% more data than Llama) and has the context length of 4,096 tokens for inference (double the context length of Llama), which enables more accuracy, fluency, and creativity for the model.&lt;/p&gt;

&lt;p&gt;Llama 2 is a state-of-the-art LLM that outperforms many other open source language models on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. The model’s scale and complexity place many demands on AI accelerators, making it an ideal benchmark for LLM training and inference performance of PyTorch/XLA on Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;performance-challenge-of-llms&quot;&gt;Performance Challenge of LLMs&lt;/h2&gt;

&lt;p&gt;Large-scale distributed training for LLMs such as Llama 2 introduces technical challenges that require practical solutions to make the most efficient use of TPUs. Llama’s size can strain both memory and processing resources of TPUs. To address this, we use model sharding, which involves breaking down the model into smaller segments, each fitting within the capacity of a single TPU core. This enables parallelism across multiple TPUs, improving training speed while reducing communication overhead.&lt;/p&gt;

&lt;p&gt;Another challenge is managing the large datasets required for training Llama 2 efficiently, which requires effective data distribution and synchronization methods. Additionally, optimizing factors like learning rate schedules, gradient aggregation, and weight synchronization across distributed TPUs is crucial for achieving convergence.&lt;/p&gt;

&lt;p&gt;After pretraining or fine-tuning Llama 2, running inference on the model checkpoint creates additional technical challenges. All of the challenges discussed in our &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previous blog post&lt;/a&gt;, such as autoregressive decoding, variable input prompt lengths, and the need for model sharding and quantization still apply for Llama 2. In addition, Llama 2 introduced two new capabilities: grouped-query attention and early stopping. We discuss how PyTorch/XLA handles these challenges to enable high-performance, cost-efficient training and inference of Llama 2 on Cloud TPU v4 and v5e.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-distributed-training&quot;&gt;Large-Scale Distributed Training&lt;/h2&gt;

&lt;p&gt;PyTorch/XLA offers two major ways of doing large-scale distributed training: &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;SPMD&lt;/a&gt;, which utilizes the XLA compiler to transform and partition a single-device program into a multi-device distributed program; and &lt;a href=&quot;https://pytorch.org/blog/large-scale-training-hugging-face/&quot;&gt;FSDP&lt;/a&gt;, which implements the widely-adopted &lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;Fully Sharded Data Parallel&lt;/a&gt; algorithm.&lt;/p&gt;

&lt;p&gt;In this blog post, we show how to use the SPMD API to annotate the &lt;a href=&quot;https://huggingface.co/blog/llama2&quot;&gt;HuggingFace (HF) Llama 2&lt;/a&gt; implementation to maximize performance. For comparison, we also show our FSDP results with the same configurations; read about &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP API here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;spmd-overview&quot;&gt;SPMD Overview&lt;/h3&gt;

&lt;p&gt;Let’s briefly review the fundamentals of SPMD. For details, please refer to our &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;blog post&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;mesh&quot;&gt;Mesh&lt;/h4&gt;

&lt;p&gt;A multidimensional array that describes the logical topology of the TPU devices:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' and 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h4&gt;

&lt;p&gt;A tuple that describes how the corresponding tensor’s dimensions are sharded across the mesh:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;partition_spec = ('x', 'y')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;mark-sharding&quot;&gt;Mark Sharding&lt;/h4&gt;

&lt;p&gt;An API that takes a mesh and a partition_spec, and then generates a sharding annotation for the XLA compiler.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor = torch.randn(4, 4).to('xla')
# Let's resue the above mesh and partition_spec.
# It means the tensor's 0th dim is sharded 4 way and 1th dim is sharded 2 way.
xs.mark_sharding(tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2d-sharding-with-spmd&quot;&gt;2D Sharding with SPMD&lt;/h3&gt;

&lt;p&gt;In our &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;SPMD blog post&lt;/a&gt;, we demonstrated using 1D FSDP style sharding. Here, we introduce a more powerful sharding strategy, called &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;2D sharding&lt;/a&gt;, where both the parameters and activations are sharded. This new sharding strategy not only allows fitting a larger model but also boosts the MFU to up to &lt;strong&gt;54.3%&lt;/strong&gt;. For more details, read the Benchmarks section.&lt;/p&gt;

&lt;p&gt;This section introduces a set of general rules that applies to most LLMs, and for convenience we directly reference the variable names and configuration names from &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py&quot;&gt;HF Llama&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, let’s create a 2D Mesh with corresponding axis names: data and model. The data axis is usually where we distribute the input data, and the model axis is where we further distribute the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mesh = Mesh(device_ids, mesh_shape, ('data', 'model'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt; can be a hyper-parameter that is tuned for different model sizes and hardware configurations. The same mesh will be reused in all following sharding annotations. In the next few sections, we will cover how to use the mesh to shard parameters, activations and input data.&lt;/p&gt;

&lt;h4 id=&quot;parameter-sharding&quot;&gt;Parameter Sharding&lt;/h4&gt;

&lt;p&gt;Below is a table that summarizes all parameters of HF Llama 2 and corresponding partition specifications. Example HF code can be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L572&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Parameter Name&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Explanation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Parameter Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Partition Spec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;embed_tokens&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;embedding layer
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;vocab_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;q_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;num_heads&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;head_dim&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;k_proj / v_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;num_key_value_heads&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;head_dim&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;o_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;hidden_size&lt;/code&gt;, &lt;code&gt;num_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;gate_proj / up_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;intermediate_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;down_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;hidden_size&lt;/code&gt;, &lt;code&gt;intermediate_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;lm_head&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;HF output embedding 
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;vocab_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: SPMD 2D Sharding Parameter Partition Spec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rule is to shard the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidden_size&lt;/code&gt; dim of any weights except QKVO projections according to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, then shard the other dim with the remaining &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; axis. For QKVO, do the opposite. This model-data axis rotation methodology is similar to that of &lt;a href=&quot;https://arxiv.org/pdf/1909.08053.pdf&quot;&gt;Megatron-LM&lt;/a&gt; to reduce communication overhead. For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;layernorm&lt;/code&gt; weights, we implicitly mark them as replicated across different devices given they are 1D tensors.&lt;/p&gt;

&lt;h4 id=&quot;activation-sharding&quot;&gt;Activation Sharding&lt;/h4&gt;

&lt;p&gt;In order to better utilize the device memory, very often we need to annotate the output of some memory bound ops. That way the compiler is forced to only keep partial output on devices instead of the full output. In Llama 2, we explicitly annotate all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Linear&lt;/code&gt; outputs. Table 2 summarizes the corresponding annotations; the example HF code can be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py#L235&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Output Name&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Explanation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Output Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Partition Spec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;inputs_embeds&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;embedding layer output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;query_states&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention nn.Linear output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;num_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;key_states / value_states&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention nn.Linear output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;num_key_value_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;attn_weights&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;num_attention_heads&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model, None, None)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;attn_output&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention layer output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;up_proj / gate_proj / down_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP &lt;code&gt;nn.Linear&lt;/code&gt; outputs
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;intermediate_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;logits&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;HF output embedding output 
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 2: SPMD 2D Sharding Activation Partition Spec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rule is to shard the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; dim of any outputs according to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, then replicate the length dims of any outputs, and finally shard the last dim along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; axis.&lt;/p&gt;

&lt;h4 id=&quot;input-sharding&quot;&gt;Input Sharding&lt;/h4&gt;

&lt;p&gt;For input sharding, the rule is to shard the batch dim along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, and replicate the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence_length&lt;/code&gt; dim. Below is the example code, and the corresponding HF change may be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/trainer.py#L1456&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;partition_spec = ('data', None)
sharding_spec = xs.ShardingSpec(mesh, partition_spec)
# MpDeviceLoader will shard the input data before sending to the device.
pl.MpDeviceLoader(dataloader, self.args.device, input_sharding=sharding_spec, ...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, all the data and model tensors that require sharding are covered!&lt;/p&gt;

&lt;h4 id=&quot;optimizer-states--gradients&quot;&gt;Optimizer States &amp;amp; Gradients&lt;/h4&gt;

&lt;p&gt;You may be wondering whether it is necessary to shard the optimizer states and gradients as well. Great news: the sharding propagation feature of the XLA compiler automates the sharding annotation in these two scenarios, without needing more hints to improve performance.&lt;/p&gt;

&lt;p&gt;It is important to note that optimizer states are typically initialized within the first iteration of the training loop. From the standpoint of the XLA compiler, the optimizer states are the outputs of the first graph, and therefore have the sharding annotation propagated. For subsequent iterations, the optimizer states become inputs to the second graph, with the sharding annotation propagated from the first one. This is also why PyTorch/XLA typically produces two graphs for the training loops. If the optimizer states are somehow initialized before the first iteration, users will have to manually annotate them, just like the model weights.&lt;/p&gt;

&lt;p&gt;Again, all concrete examples of the above sharding annotation can be found in our fork of HF Transformers &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/llama2-google-next-training&quot;&gt;here&lt;/a&gt;. The repo also contains code for our experimental feature &lt;a href=&quot;https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads&quot;&gt;MultiSlice&lt;/a&gt;, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HybridMesh&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcn&lt;/code&gt; axis, which follows the same principles mentioned above.&lt;/p&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;p&gt;While using SPMD for training, there are a few important things to pay attention to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.einsum&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt;; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt; usually flattens tensors and does a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.mm&lt;/code&gt; at the end, and that’s bad for SPMD when the combined axes are sharded. The XLA compiler will have a hard time determining how to propagate the sharding.&lt;/li&gt;
  &lt;li&gt;PyTorch/XLA provides patched &lt;code&gt;[nn.Linear](https://github.com/pytorch/xla/blob/master/torch_xla/experimental/xla_sharding.py#L570)&lt;/code&gt; to overcome the above constraint:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch_xla.experimental.xla_sharding as xs
from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear

 model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Always reuse the same mesh across all shardings&lt;/li&gt;
  &lt;li&gt;Always specify &lt;code&gt;--dataloader_drop_last yes&lt;/code&gt;. The last smaller data is hard to annotate.&lt;/li&gt;
  &lt;li&gt;Large models which are initialized on the host can induce host-side OOM. One way to avoid this issue is to initialize parameters on the &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L501&quot;&gt;meta device&lt;/a&gt;, then create and shard real tensors layer-by-layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;infrastructure-improvements&quot;&gt;Infrastructure Improvements&lt;/h3&gt;

&lt;p&gt;Besides the above modeling techniques, we have developed additional features and improvements to maximize performance, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We enable asynchronous collective communication. This requires enhancements on the XLA compiler’s latency hiding scheduler to better optimize for the Llama 2 PyTorch code.&lt;/li&gt;
  &lt;li&gt;We now allow sharding annotations in the middle of the IR graph, just like JAX’s &lt;a href=&quot;https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html&quot;&gt;jax.lax.with_sharding_constraint&lt;/a&gt;. Previously, only graph inputs were annotated.&lt;/li&gt;
  &lt;li&gt;We also propagate replicated sharding spec from the compiler to the graph outputs. This allows us to shard the optimizer states automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inference-optimizations&quot;&gt;Inference Optimizations&lt;/h2&gt;

&lt;p&gt;All the PyTorch/XLA &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;optimizations&lt;/a&gt; implemented for Llama inference are applied to Llama 2 as well. That includes &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#fairscale-sharding&quot;&gt;Tensor Parallelism + Dynamo (torch.compile) using torch-xla collective ops&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#autoregressive-decoding-on-pytorchxla&quot;&gt;autoregressive decoding logic improvement to avoid recompilation&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#input-prompt-optimization&quot;&gt;bucketized prompt length&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#kv-cache-optimization&quot;&gt;KV-cache with compilation friendly index ops&lt;/a&gt;. Llama 2 introduces two new changes: Grouped Query Attention, and Early Stopping when eos is reached for all prompts. We applied corresponding changes to promote better performance and flexibility with PyTorch/XLA.&lt;/p&gt;

&lt;h3 id=&quot;grouped-query-attention&quot;&gt;Grouped Query Attention&lt;/h3&gt;

&lt;p&gt;Llama 2 enables &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;&gt;Grouped Query Attention&lt;/a&gt; for the 70B models. It allows the number of Key and Value heads to be smaller than the number of Query heads, while still supporting KV-cache sharding up to the number of KV heads. For the 70B models, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_kv_heads&lt;/code&gt; is 8, which limits the tensor parallelism to be less or equal to 8. In order to shard the model checkpoint to run on more devices, the K, V projection weights need to be replicated first, and then split into multiple pieces. For example, to shard the 70B model checkpoint from 8 pieces to 16 pieces, the K, V projection weights are duplicated and split into 2 pieces for each shard. We provide a &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/reshard_checkpoints.py&quot;&gt;reshard_checkpoints.py&lt;/a&gt; script to handle that, and to make sure the sharded checkpoint performs mathematically identical to the original checkpoint.&lt;/p&gt;

&lt;h3 id=&quot;eos-early-stopping&quot;&gt;EOS Early Stopping&lt;/h3&gt;

&lt;p&gt;The Llama 2 generation code added &lt;a href=&quot;https://github.com/facebookresearch/llama/blob/ea9f33d6d3ea8ed7d560d270986407fd6c2e52b7/llama/generation.py#L159&quot;&gt;the early stopping logic&lt;/a&gt;. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; tensor is used to track the completion of all the prompt generations, and if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos&lt;/code&gt; token is reached for all the prompts in the batch, the generation would stop early. The similar change is incorporated in the PyTorch/XLA optimized version as well, with some minor tweaks.&lt;/p&gt;

&lt;p&gt;In PyTorch/XLA, checking the value of a tensor like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; as part of the control flow condition would invoke a blocking device-to-host transfer. The tensor would be transferred from device memory to CPU memory to evaluate its value, while all other logics are waiting. This introduced a delay on the scale of ms after every new token generation. As a trade-off, we reduce the rate of checking the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; value to be &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/b89dd0f2351c42fef367670d9d2c5b65cd0ae932/llama/generation.py#L268C13-L270C26&quot;&gt;once every 10 new token generations&lt;/a&gt;. With this change, the impact of the blocking device-to-host transfer would be reduced by 10x, while the early stopping would still be effective, and at most 9 unnecessary tokens would be generated after each sequence reaches the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos&lt;/code&gt; token.&lt;/p&gt;

&lt;h3 id=&quot;model-serving&quot;&gt;Model Serving&lt;/h3&gt;

&lt;p&gt;PyTorch/XLA is working on a serving strategy to enable the PyTorch community to serve their deep learning applications via &lt;a href=&quot;https://pytorch.org/docs/stable/export.html&quot;&gt;Torch.Export&lt;/a&gt;, &lt;a href=&quot;https://github.com/openxla/stablehlo&quot;&gt;StableHLO&lt;/a&gt;, and &lt;a href=&quot;https://www.tensorflow.org/guide/saved_model&quot;&gt;SavedModel&lt;/a&gt;. PyTorch/XLA Serving is an experimental feature in &lt;a href=&quot;https://github.com/pytorch/xla/releases&quot;&gt;PyTorch/XLA 2.1 release&lt;/a&gt;; for details visit our &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.1/docs/stablehlo.md#convert-saved-stablehlo-for-serving&quot;&gt;serving user guide&lt;/a&gt;. Users can take advantage of TorchServe to run their single-host workloads.&lt;/p&gt;

&lt;h2 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h2&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;To measure training performance, we use the industry-standard metric: &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;Model FLOPS Utilization (MFU)&lt;/a&gt;. Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPs are hardware and implementation independent and only depend on the underlying model. MFU measures how effectively the model is using the actual hardware during training. Achieving 100% MFU means that the model is using the hardware perfectly.&lt;/p&gt;

&lt;p&gt;To measure inference performance, we use the industry-standard metric of throughput. First, we measure latency per token when the model has been compiled and loaded. Then, we calculate throughput by dividing batch size (BS) over latency per chip. As a result, throughput measures how the model is performing in production environments regardless of how many chips are used.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;h4 id=&quot;training-evaluation&quot;&gt;Training Evaluation&lt;/h4&gt;

&lt;p&gt;Figure 1 shows Llama 2 SPMD 2D sharding training results on a range of Google TPU v4 hardware with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt; as the baseline. We increased MFU by &lt;strong&gt;28%&lt;/strong&gt; across all sizes of Llama 2 compared to FSDP running on the same hardware configuration. This performance improvement is largely due to: 1) 2D Sharding has less communication overhead than FSDP, and 2) asynchronous collective communication is enabled in SPMD which allows communication and computation overlapping. Also note that as the model size scales, we maintain the high MFU. Table 3 shows all the hardware configurations plus some hyperparameters used in the training benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig1.jpg&quot; alt=&quot;Figure 1. Llama 2 Training MFU on TPU v4 Hardware&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: Llama 2 Training MFU on TPU v4 Hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The results in Figure 1 are produced with sequence length 1,024. Figure 2 shows how the performance behaves with larger sequence lengths. It shows our performance also scales linearly with sequence lengths. The MFU is expected to decrease a little as a smaller per device batch size is needed to accommodate the additional memory pressure introduced by the larger sequence length since the sequence length axis is not sharded in 2D sharding. And TPU is very sensitive to batch size. For Llama 2, 70B parameters, the performance decrease is as low as &lt;strong&gt;4%&lt;/strong&gt;. At the time of preparing these results, &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/tokenization_llama.py#L48&quot;&gt;Hugging Face Llama 2 tokenizer&lt;/a&gt; limits the max model input to 2,048, preventing us from evaluating larger sequence lengths.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig2.jpg&quot; alt=&quot;Figure 2. Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model Size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;13B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;70B&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-32
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-64
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mesh Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(16, 1)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(32, 1)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(32, 4)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Seq Len&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Global Batch&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Per Device Batch&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 3: Llama 2 SPMD Training Benchmark TPU Configurations and Hyperparameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One last thing to call out is that we use &lt;a href=&quot;https://arxiv.org/abs/1804.04235&quot;&gt;adafactor&lt;/a&gt; as the optimizer for better memory utilization. And once again, here is the &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md&quot;&gt;user guide&lt;/a&gt; to reproduce the benchmark results listed above.&lt;/p&gt;

&lt;h4 id=&quot;inference-evaluation&quot;&gt;Inference Evaluation&lt;/h4&gt;

&lt;p&gt;In this section, we extend our &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previous evaluation of Llama on Cloud v4 TPU&lt;/a&gt;. Here, we demonstrate the performance properties of TPU v5e for inference applications.&lt;/p&gt;

&lt;p&gt;We define inference throughput as the number of tokens produced by a model per second per TPU chip. Figure 3 shows Llama 2 70B throughput on a v5e-16 TPU node. Given Llama is a memory bound application, we see that applying weight-only quantization unblocks extending the model batch size to 32. Higher throughput results would be possible on larger TPU v5e hardware up to the point where the ICI network bandwidth between chips throttle the TPU slice from delivering higher throughput. Exploring the upper bound limits of TPU v5e on Llama 2 was outside of the scope of this work. Notice, to make the Llama 2 70B model run on v5e-16, we replicated the attention heads to have one head per chip as discussed in the Inference section above. As discussed &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previously&lt;/a&gt;, with increasing model batch size, per-token latency grows proportionally; quantization improves overall latency by reducing memory I/O demand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig3.jpg&quot; alt=&quot;Figure 3. Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 3&lt;/strong&gt;: Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 4 shows inference throughput results across different model sizes. These results highlight the largest throughput given the hardware configuration when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; precision. With weight only quantization, this throughput reaches 42 on the 70B model. As mentioned above, increasing hardware resources may lead to performance gains.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig4.jpg&quot; alt=&quot;Figure 4. Llama 2 Inference Per-Chip Throughput on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 4&lt;/strong&gt;: Llama 2 Inference Per-Chip Throughput on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 5 shows the cost of serving Llama 2 models (from Figure 4) on Cloud TPU v5e. We report the TPU v5e per-chip cost based on the 3-year commitment (reserved) price in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-west4&lt;/code&gt; region. All model sizes use maximum sequence length of 2,048 and maximum generation length of 1,000 tokens. Note that with quantization, the cost for the 70B model drops to &lt;strong&gt;$0.0036 per 1,000 tokens&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig5.jpg&quot; alt=&quot;Figure 5. Llama 2 Inference Per-Chip Cost on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 5&lt;/strong&gt;: Llama 2 Inference Per-Chip Cost on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 6 summarizes our best Llama 2 inference latency results on TPU v5e. Llama 2 7B results are obtained from our non-quantized configuration (BF16 Weight, BF16 Activation) while the 13B and 70B results are from the quantized (INT8 Weight, BF16 Activation) configuration. We attribute this observation to the inherent memory saving vs. compute overhead tradeoff of quantization; as a result, for smaller models, quantization may not lead to lower inference latency.&lt;/p&gt;

&lt;p&gt;Additionally, prompt length has a strong effect on the memory requirements of LLMs. For instance, we observe a latency of 1.2ms / token (i.e. 201 tokens / second / chip) when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len=256&lt;/code&gt; at batch size of 1 with no quantization on v5e-4 running Llama2 7B.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig6.jpg&quot; alt=&quot;Figure 6. Llama 2 Inference Latency on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 6&lt;/strong&gt;: Llama 2 Inference Latency on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;The recent wave of AI innovation has been nothing short of transformative, with breakthroughs in LLMs at the forefront. Meta’s Llama and Llama 2 models stand as notable milestones in this wave of progress. PyTorch/XLA uniquely enables high-performance, cost-efficient training and inference for Llama 2 and other LLMs and generative AI models on Cloud TPUs, including the new Cloud TPU v5e. Looking forward, PyTorch/XLA will continue to push the performance limits on Cloud TPUs in both throughput and scalability and at the same time maintain the same PyTorch user experience.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;We would like to extend our special thanks to Marcello Maggioni, Tongfei Guo, Andy Davis, Berkin Ilbeyi for their support and collaboration in this effort.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jiewen Tan, Jon Bolin, Yeounoh Chung, Liyang Lu, Siyuan Liu, Wonjoo Lee, Manfei Bai, Meghan Cowan, Jack Cao, Milad Mohammadi, Shauheen Zahirazami, Alex Spiridonov</name>
        
        
      </author>

      

      

      
        <summary type="html">In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s Llama family of open sourced large language models (LLMs) stands out as a notable breakthrough. Llama marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. Llama 2 further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Inference on x86-64 Machines with oneDNN Graph</title>
      <link href="https://pytorch.org/blog/accelerating-inference/" rel="alternate" type="text/html" title="Accelerating Inference on x86-64 Machines with oneDNN Graph" />
      <published>2023-11-02T00:00:00-07:00</published>
      <updated>2023-11-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-inference/">&lt;p&gt;&lt;em&gt;Supported in PyTorch 2.0 as a beta feature, oneDNN Graph leverages aggressive fusion patterns to accelerate inference on x86-64 machines, especially Intel® Xeon® Scalable processors.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;In PyTorch 2.0 and beyond, oneDNN Graph can help accelerate inference on x86-64 CPUs (primarily, Intel Xeon processor-based machines) with Float32 and BFloat16 (with PyTorch’s Automatic Mixed Precision support) datatypes. With BFloat16, speedup is limited to machines that support AVX512_BF16 ISA (Instruction Set Architecture), as well as machines that also support AMX_BF16 ISA.&lt;/p&gt;

&lt;h2 id=&quot;onednn-graph-usage&quot;&gt;oneDNN Graph Usage&lt;/h2&gt;

&lt;p&gt;From a user’s perspective, the usage is quite simple and intuitive, &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;with the only change in code being an API invocation&lt;/a&gt;. To leverage oneDNN Graph with &lt;a href=&quot;http://pytorch.org/docs/stable/generated/torch.jit.trace.html&quot;&gt;JIT-tracing&lt;/a&gt;, a model is profiled with an example input as shown below in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-onednn-graph-api-code-snippet.png&quot; alt=&quot;Figure 1. A code-snippet that demonstrates using oneDNN Graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: A code-snippet that demonstrates using oneDNN Graph&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;oneDNN Graph receives the model’s graph and identifies candidates for operator-fusion with respect to the input shape of the example input. Currently, only static shapes are supported. This means that any other input shape would neither be supported nor receive any performance-benefit.&lt;/p&gt;

&lt;h2 id=&quot;measurements&quot;&gt;Measurements&lt;/h2&gt;

&lt;p&gt;To ensure reproducibility of results, we used a &lt;a href=&quot;http://github.com/sanchitintel/benchmark/tree/onednn-graph-preview2&quot;&gt;fork&lt;/a&gt; of &lt;a href=&quot;http://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; to measure inference speed-up of some Vision models on an &lt;a href=&quot;http://aws.amazon.com/ec2/instance-types/m7i/&quot;&gt;AWS m7i.16xlarge&lt;/a&gt; instance, which uses 4th Gen Intel® Xeon® Scalable processors.&lt;/p&gt;

&lt;p&gt;The baseline for comparison was &lt;a href=&quot;http://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html&quot;&gt;torch.jit.optimize_for_inference&lt;/a&gt; which only supports Float32 datatype. The batch-size for each model was based on the respective batch size being used for them in TorchBench.&lt;/p&gt;

&lt;p&gt;In Figure 2, we depict the inference speedup of using oneDNN Graph over PyTorch alone. The geomean speedup with oneDNN Graph &lt;strong&gt;for Float32 datatype was 1.24x&lt;/strong&gt;, and the geomean speedup &lt;strong&gt;for BFloat16 datatype was 3.31x&lt;/strong&gt;1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-inference-speedup-with-onednn-graph.png&quot; alt=&quot;Figure 2. Inference speedup with oneDNN Graph over default CPU JIT Fuser (which only uses Float32 datatype)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Inference speedup with oneDNN Graph over default CPU JIT Fuser (which only uses Float32 datatype)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;oneDNN Graph is currently supported in PyTorch through TorchScript, but work is already underway by Intel to integrate it with the Inductor-CPU backend as a prototype feature in a future PyTorch release and Dynamo make supporting dynamic shapes easier with PyTorch, and we would like to introduce Dynamic shape support with Inductor-CPU. We also plan to add int8 quantization support.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The results presented in this blog are a joint effort between Meta and the Intel PyTorch team. Special thanks to Elias Ellison from Meta who spent precious time thoroughly reviewing the PRs and gave us helpful feedback.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Supported in PyTorch 2.0 as a beta feature, oneDNN Graph leverages aggressive fusion patterns to accelerate inference on x86-64 machines, especially Intel® Xeon® Scalable processors.</summary>
      

      
      
    </entry>
  
</feed>



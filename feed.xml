<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-12-19T14:43:02-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">docTR joins PyTorch Ecosystem: From Pixels to Data, Building a Recognition Pipeline with PyTorch and docTR</title>
      <link href="https://pytorch.org/blog/doctr-joins-pytorch-ecosystem/" rel="alternate" type="text/html" title="docTR joins PyTorch Ecosystem: From Pixels to Data, Building a Recognition Pipeline with PyTorch and docTR" />
      <published>2024-12-18T00:00:00-08:00</published>
      <updated>2024-12-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/doctr-joins-pytorch-ecosystem</id>
      <content type="html" xml:base="https://pytorch.org/blog/doctr-joins-pytorch-ecosystem/">&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg1.png&quot; alt=&quot;docTR logo&quot; style=&quot;width:100%;display: block;max-width:400px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the docTR project has been integrated into the PyTorch ecosystem! This integration ensures that docTR aligns with PyTorch’s standards and practices, giving developers a reliable, community-backed solution for powerful OCR workflows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For more information on what it means to be a PyTorch ecosystem project, see the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem Tools page&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;about-doctr&quot;&gt;About docTR&lt;/h2&gt;

&lt;p&gt;docTR is an Apache 2.0 project developed and distributed by &lt;a href=&quot;https://www.mindee.com/&quot;&gt;Mindee&lt;/a&gt; to help developers integrate OCR capabilities into applications with no prior knowledge required.&lt;/p&gt;

&lt;p&gt;To quickly and efficiently extract text information, docTR uses a two-stage approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, it performs text &lt;strong&gt;detection&lt;/strong&gt; to localize words.&lt;/li&gt;
  &lt;li&gt;Then, it conducts text &lt;strong&gt;recognition&lt;/strong&gt; to identify all characters in a word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Detection&lt;/strong&gt; and &lt;strong&gt;recognition&lt;/strong&gt; are performed by state-of-the-art models written in PyTorch. To learn more about this approach, you can refer &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;to the docTR documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;docTR enhances the user experience in PyTorch projects by providing high-performance OCR capabilities right out of the box. Its specially designed models require minimal to no fine-tuning for common use cases, allowing developers to quickly integrate advanced document analysis features.&lt;/p&gt;

&lt;h2 id=&quot;local-installation&quot;&gt;Local installation&lt;/h2&gt;

&lt;p&gt;docTR requires Python &amp;gt;= 3.10 and supports Windows, Mac and Linux. Please refer to our &lt;a href=&quot;https://github.com/mindee/doctr?tab=readme-ov-file#installation&quot;&gt;README&lt;/a&gt; for necessary dependencies for MacBook with the M1 chip.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install -U pip
pip3 install &quot;python-doctr[torch,viz]&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will install docTR along with the latest version of PyTorch.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Note: docTR also provides docker images for an easy deployment, such as a part of Kubernetes cluster.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;text-recognition&quot;&gt;Text recognition&lt;/h2&gt;

&lt;p&gt;Now, let’s try docTR’s OCR recognition on this sample:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg2.jpg&quot; alt=&quot;OCR sample&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The OCR recognition model expects an image with only one word on it and will output the predicted word with a confidence score. You can use the following snippet to test OCR capabilities from docTR:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python
from doctr.io import DocumentFile
from doctr.models import recognition_predictor

doc = DocumentFile.from_images(&quot;/path/to/image&quot;)

# Load the OCR model
# This will download pre-trained models hosted by Mindee
model = recognition_predictor(pretrained=True)

result = model(doc)
print(result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the most important line of code is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = recognition_predictor(pretrained=True)&lt;/code&gt;. This will load a default text recognition model, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;crnn_vgg16_bn&lt;/code&gt;, but you can select other models through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch&lt;/code&gt; parameter. You can check out the &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;available architectures&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When run on the sample, the recognition predictor retrieves the following data: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[('MAGAZINE', 0.9872216582298279)]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Note: using the DocumentFile object docTR provides an easy way to manipulate PDF or Images.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;text-detection&quot;&gt;Text detection&lt;/h2&gt;

&lt;p&gt;The last example was a crop on a single word. Now, what about an image with several words on it, like this one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg3.jpg&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A text detection model is used before the text recognition to output a segmentation map representing the location of the text. Following that, the text recognition is applied on every detected patch.&lt;/p&gt;

&lt;p&gt;Below is a snippet to run only the detection part:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from doctr.io import DocumentFile
from doctr.models import detection_predictor
from matplotlib import pyplot as plt
from doctr.utils.geometry import detach_scores
from doctr.utils.visualization import draw_boxes

doc = DocumentFile.from_images(&quot;path/to/my/file&quot;)
model = detection_predictor(pretrained=True)

result = model(doc)

draw_boxes(detach_scores([result[0][&quot;words&quot;]])[0][0], doc[0])
plt.axis('off')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running it on the full sample yields the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg4.png&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly to the text recognition, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detection_predictor&lt;/code&gt; will load a default model (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast_base&lt;/code&gt; here). You can also load another one by providing it through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arch&lt;/code&gt; parameter.&lt;/p&gt;

&lt;h2 id=&quot;the-full-implementation&quot;&gt;The full implementation&lt;/h2&gt;

&lt;p&gt;Now, let’s plug both components into the same pipeline.&lt;/p&gt;

&lt;p&gt;Conveniently, docTR provides a wrapper that does exactly that for us:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from doctr.io import DocumentFile
from doctr.models import ocr_predictor

doc = DocumentFile.from_images(&quot;/path/to/image&quot;)

model = ocr_predictor(pretrained=True, assume_straight_pages=False)

result = model(doc)
result.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg5.png&quot; alt=&quot;photo of magazines&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The last line should display a matplotlib window which shows the detected patches. Hovering the mouse over them will display their contents.&lt;/p&gt;

&lt;p&gt;You can also do more with this output, such as reconstituting a synthetic document like so:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import matplotlib.pyplot as plt

synthetic_pages = result.synthesize()
plt.imshow(synthetic_pages[0])
plt.axis('off')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doctr-joins-pytorch-ecosystem/fg6.png&quot; alt=&quot;black text on white&quot; style=&quot;width:100%;display: block;max-width:300px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pipeline is highly customizable, where you can modify the detection or recognition model behaviors by passing arguments to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ocr_predictor&lt;/code&gt;. Please refer to the &lt;a href=&quot;https://mindee.github.io/doctr/using_doctr/using_models.html&quot;&gt;documentation&lt;/a&gt; to learn more about it.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re excited to welcome docTR into the PyTorch Ecosystem, where it seamlessly integrates with PyTorch pipelines to deliver state-of-the-art OCR capabilities right out of the box.&lt;/p&gt;

&lt;p&gt;By empowering developers to quickly extract text from images or PDFs using familiar tooling, docTR simplifies complex document analysis tasks and enhances the overall PyTorch experience.&lt;/p&gt;

&lt;p&gt;We invite you to explore the &lt;a href=&quot;https://github.com/mindee/doctr&quot;&gt;docTR GitHub repository&lt;/a&gt;, join the &lt;a href=&quot;https://slack.mindee.com/&quot;&gt;docTR community on Slack&lt;/a&gt;, and reach out at contact@mindee.com for inquiries or collaboration opportunities.&lt;/p&gt;

&lt;p&gt;Together, we can continue to push the boundaries of document understanding and develop even more powerful, accessible tools for everyone in the PyTorch community.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Olivier Dulcy &amp; Sebastian Olivera, Mindee</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">torchcodec: Easy and Efficient Video Decoding for PyTorch</title>
      <link href="https://pytorch.org/blog/torchcodec/" rel="alternate" type="text/html" title="torchcodec: Easy and Efficient Video Decoding for PyTorch" />
      <published>2024-12-11T00:00:00-08:00</published>
      <updated>2024-12-11T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/torchcodec</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchcodec/">&lt;p&gt;We are pleased to officially announce &lt;a href=&quot;https://github.com/pytorch/torchcodec&quot;&gt;torchcodec&lt;/a&gt;, a library for decoding videos into PyTorch tensors. It is fast, accurate, and easy to use. When running PyTorch models on videos, torchcodec is our recommended way to turn those videos into data your model can use.&lt;/p&gt;

&lt;p&gt;Highlights of torchcodec include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An intuitive decoding API that treats a video file as a Python sequence of frames. We support both index-based and presentation-time-based frame retrieval.&lt;/li&gt;
  &lt;li&gt;An emphasis on accuracy: we ensure you get the frames you requested, even if your video has variable frame rates.&lt;/li&gt;
  &lt;li&gt;A rich sampling API that makes it easy and efficient to retrieve batches of frames.&lt;/li&gt;
  &lt;li&gt;Best-in-class CPU decoding performance.&lt;/li&gt;
  &lt;li&gt;CUDA accelerated decoding that enables high throughput when decoding many videos at once.&lt;/li&gt;
  &lt;li&gt;Support for all codecs available in your installed version of FFmpeg.&lt;/li&gt;
  &lt;li&gt;Simple binary installs for Linux and Mac.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;easy-to-use&quot;&gt;Easy to Use&lt;/h2&gt;

&lt;p&gt;A simple, intuitive API was one of our main design principles. We start with simple decoding and extracting specific frames of a video:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchcodec.decoders import VideoDecoder
from torch import Tensor

decoder = VideoDecoder(&quot;my_video.mp4&quot;)

# Index based frame retrieval.
first_ten_frames: Tensor = decoder[10:]
last_ten_frames: Tensor = decoder[-10:]

# Multi-frame retrieval, index and time based.
frames = decoder.get_frames_at(indices=[10, 0, 15])
frames = decoder.get_frames_played_at(seconds=[0.2, 3, 4.5])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All decoded frames are already PyTorch tensors, ready to be fed into models for training.&lt;/p&gt;

&lt;p&gt;Of course, more common in ML training pipelines is sampling multiple clips from videos. A clip is just a sequence of frames in presentation order—but the frames are often &lt;em&gt;not&lt;/em&gt; consecutive. Our sampling API makes this easy:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchcodec.samplers import clips_at_regular_timestamps

clips = clips_at_regular_timestamps(
  decoder,
  seconds_between_clip_starts=10,
  num_frames_per_clip=5,
  seconds_between_frames=0.2,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above call yields a batch of clips where each clip starts 10 seconds apart, each clip has 5 frames, and those frames are 0.2 seconds apart. See our tutorials on &lt;a href=&quot;https://pytorch.org/torchcodec/0.1.0/generated_examples/basic_example.html&quot;&gt;decoding&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/torchcodec/0.1.0/generated_examples/sampling.html&quot;&gt;sampling&lt;/a&gt; for more!&lt;/p&gt;

&lt;h2 id=&quot;fast-performance&quot;&gt;Fast Performance&lt;/h2&gt;

&lt;p&gt;Performance was our other main design principle. Decoding videos for ML training has different performance requirements than decoding videos for playback. A typical ML video training pipeline will process many different videos (sometimes in the millions!), but only sample a small number of frames (dozens to hundreds) from each video.&lt;/p&gt;

&lt;p&gt;For this reason, we’ve paid particular attention to our decoder’s performance when seeking multiple times in a video, decoding a small number of frames after each seek. We present experiments with the following four scenarios:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Decoding and transforming frames from multiple videos at once, inspired by what we have seen in data loading for large-scale training pipelines:&lt;/p&gt;

    &lt;p&gt;a. Ten threads decode batches of 50 videos in parallel.&lt;br /&gt;
b. For each video, decode 10 frames at evenly spaced times.&lt;br /&gt;
c. For each frame, resize it to a 256x256 resolution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Decoding 10 frames at random locations in a single video.&lt;/li&gt;
  &lt;li&gt;Decoding 10 frames at evenly spaced times of a single video.&lt;/li&gt;
  &lt;li&gt;Decoding the first 100 frames of a single video.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We compare the following video decoders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/stable/index.html&quot;&gt;Torchaudio&lt;/a&gt;, CPU decoding only.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/vision/stable/index.html&quot;&gt;Torchvision&lt;/a&gt;, using the &lt;a href=&quot;https://pytorch.org/vision/stable/index.html#torchvision.set_video_backend&quot;&gt;video_reader&lt;/a&gt; backend which is CPU decoding only.&lt;/li&gt;
  &lt;li&gt;Torchcodec, GPU decoding with CUDA.&lt;/li&gt;
  &lt;li&gt;Torchcodec, CPU decoding only.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the following three videos:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A synthetically generated video using FFmpeg’s &lt;a href=&quot;https://ffmpeg.org/ffmpeg-filters.html#mandelbrot&quot;&gt;mandelbrot&lt;/a&gt; generation pattern. The video is 10 seconds long, 60 frames per second and 1920x1080.&lt;/li&gt;
  &lt;li&gt;Same as above, except the video is 120 seconds long.&lt;/li&gt;
  &lt;li&gt;A &lt;a href=&quot;https://download.pytorch.org/torchaudio/tutorial-assets/stream-api/NASAs_Most_Scientifically_Complex_Space_Observatory_Requires_Precision-MP4_small.mp4&quot;&gt;promotional video from NASA&lt;/a&gt; that is 206 seconds long, 29.7 frames per second and 960x540.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/torchcodec/blob/b0de66677bac322e628f04ec90ddeeb0304c6abb/benchmarks/decoders/generate_readme_data.py&quot;&gt;experimental script&lt;/a&gt; is in our repo. Our experiments run on a Linux system with an Intel processor that has 22 available cores and an NVIDIA GPU. For CPU decoding, all libraries were instructed to automatically determine the best number of threads to use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/benchmark_readme_chart.png&quot; alt=&quot;Benchmark chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From our experiments, we draw several conclusions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torchcodec is consistently the best-performing library for the primary use case we designed it for: decoding many videos at once as a part of a training data loading pipeline. In particular, high-resolution videos see great gains with CUDA where decoding and transforms both happen on the GPU.&lt;/li&gt;
  &lt;li&gt;Torchcodec is competitive on the CPU with seek-heavy use cases such as random and uniform sampling. Currently, torchcodec’s performance is better with shorter videos that have a smaller file size. This performance is due to torchcodec’s emphasis on seek-accuracy, which involves an initial linear scan.&lt;/li&gt;
  &lt;li&gt;Torchcodec is not as competitive when there is no seeking; that is, opening a video file and decoding from the beginning. This is again due to our emphasis on seek-accuracy and the initial linear scan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implementing an &lt;a href=&quot;https://github.com/pytorch/torchcodec/issues/427&quot;&gt;approximate seeking mode&lt;/a&gt; in torchcodec should resolve these performance gaps, and it’s our highest priority feature for video decoding.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;As the name implies, the long-term future for torchcodec is more than just video decoding. Our next big feature is audio support—both decoding audio streams from video, and from audio-only media. In the long term, we want torchcodec to be the media decoding library for PyTorch. That means as we implement functionality in torchcodec, we will deprecate and eventually remove complementary features from torchaudio and torchvision.&lt;/p&gt;

&lt;p&gt;We also have video decoding improvements lined up, such as the previously mentioned approximate seeking mode for those who are willing to sacrifice accuracy for performance.&lt;/p&gt;

&lt;p&gt;Most importantly, we’re looking for feedback from the community! We’re most interested in working on features that the community finds valuable. Come &lt;a href=&quot;https://github.com/pytorch/torchcodec/issues&quot;&gt;share your needs&lt;/a&gt; and influence our future direction!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are pleased to officially announce torchcodec, a library for decoding videos into PyTorch tensors. It is fast, accurate, and easy to use. When running PyTorch models on videos, torchcodec is our recommended way to turn those videos into data your model can use.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">vLLM Joins PyTorch Ecosystem: Easy, Fast, and Cheap LLM Serving for Everyone</title>
      <link href="https://pytorch.org/blog/vllm-joins-pytorch/" rel="alternate" type="text/html" title="vLLM Joins PyTorch Ecosystem: Easy, Fast, and Cheap LLM Serving for Everyone" />
      <published>2024-12-09T00:00:00-08:00</published>
      <updated>2024-12-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/vllm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/vllm-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/vllm.png&quot; alt=&quot;vllm logo&quot; style=&quot;width:100%;display: block;max-width:400px; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM project&lt;/a&gt; has become a PyTorch ecosystem project, and joined the PyTorch ecosystem family!&lt;/p&gt;

&lt;p&gt;For more information on what it means to be a PyTorch ecosystem project, see the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem Tools page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Running large language models (LLMs) is both resource-intensive and complex, especially as these models scale to hundreds of billions of parameters. That’s where vLLM comes in — a high-throughput, memory-efficient inference and serving engine designed for LLMs.&lt;/p&gt;

&lt;p&gt;Originally built around the innovative &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;PagedAttention algorithm&lt;/a&gt;, vLLM has grown into a comprehensive, state-of-the-art inference engine. A thriving community is also continuously adding new features and optimizations to vLLM, including pipeline parallelism, chunked prefill, speculative decoding, and disaggregated serving.&lt;/p&gt;

&lt;p&gt;Since its release, vLLM has garnered significant attention, achieving over 31,000 GitHub stars—a testament to its popularity and thriving community. This milestone marks an exciting chapter for vLLM as we continue to empower developers and researchers with cutting-edge tools for efficient and scalable AI deployment. Welcome to the next era of LLM inference!&lt;/p&gt;

&lt;p&gt;vLLM has always had a strong connection with the PyTorch project. It is deeply integrated into PyTorch, leveraging it as a unified interface to support a wide array of hardware backends. These include NVIDIA GPUs, AMD GPUs, Google Cloud TPUs, Intel GPUs, Intel CPUs, Intel Gaudi HPUs, and AWS Neuron, among others. This tight coupling with PyTorch ensures seamless compatibility and performance optimization across diverse hardware platforms.&lt;/p&gt;

&lt;p&gt;Do you know you can experience the power of vLLM right from your phone? During this year’s Amazon Prime Day, vLLM played a crucial role in &lt;a href=&quot;https://aws.amazon.com/cn/blogs/machine-learning/scaling-rufus-the-amazon-generative-ai-powered-conversational-shopping-assistant-with-over-80000-aws-inferentia-and-aws-trainium-chips-for-prime-day/&quot;&gt;delivering lightning-fast responses to millions of users&lt;/a&gt;. Across three regions, over 80,000 Trainium and Inferentia chips powered an average of 3 million tokens per minute, all while maintaining a P99 latency of less than 1 second for the first response. That means when customers opened the Amazon app and chatted with Rufus, they were seamlessly interacting with vLLM in action!&lt;/p&gt;

&lt;p&gt;vLLM also collaborates tightly with leading model vendors to ensure support for popular models. This includes tight integration with Meta LLAMA, Mistral, QWen, and DeepSeek models, plus many others. One particularly memorable milestone was the &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;release of LLAMA 3.1 (405B)&lt;/a&gt;. As the launching partner, vLLM was the first to enable running this very large model, showcasing vLLM’s capability to handle the most complex and resource-intensive language models.&lt;/p&gt;

&lt;p&gt;To install vLLM, simply run:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;vLLM is designed for both researchers and production-grade serving.&lt;/p&gt;

&lt;p&gt;To run vLLM as an OpenAI API compatible server, just use the Huggingface model ID:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vllm serve meta-llama/Llama-3.1-8B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run vLLM as a simple function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
   &quot;Hello, my name is&quot;,
   &quot;The president of the United States is&quot;,
   &quot;The capital of France is&quot;,
   &quot;The future of AI is&quot;,
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model=&quot;meta-llama/Llama-3.1-8B&quot;)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
   prompt = output.prompt
   generated_text = output.outputs[0].text
   print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Open-source innovation is part of the vLLM’s DNA. Born out of a Berkeley academic project, it follows the legacy of other pioneering open-source initiatives such as BSD, which revolutionized operating systems in the 1980s. Other innovations from the same organization include &lt;a href=&quot;https://github.com/apache/spark&quot;&gt;Apache Spark&lt;/a&gt; and &lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;Ray&lt;/a&gt;, now the standard for big data and AI systems. In the Gen AI era, vLLM serves as a platform dedicated to democratizing AI inference.&lt;/p&gt;

&lt;p&gt;The vLLM team remains steadfast in its mission to keep the project “of the community, by the community, and for the community.” Collaboration and inclusivity lie at the heart of everything we do.&lt;/p&gt;

&lt;p&gt;If you have collaboration requests or inquiries, feel free to reach out at &lt;a href=&quot;mailto:vllm-questions@lists.berkeley.edu&quot;&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;. To join the active and growing vLLM community, explore our &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;GitHub repository&lt;/a&gt; or connect with us on the &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;vLLM Slack&lt;/a&gt;. Together, we can push the boundaries of AI innovation and make it accessible to all.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>vLLM Team</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton</title>
      <link href="https://pytorch.org/blog/accelerating-gemms-triton/" rel="alternate" type="text/html" title="Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton" />
      <published>2024-12-06T00:00:00-08:00</published>
      <updated>2024-12-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-gemms-triton</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-gemms-triton/">&lt;p&gt;2D block quantization for Float8 (FP8) holds the promise of improving the accuracy of Float8 quantization while also accelerating GEMM’s for both inference and training.  In this blog, we showcase advances using Triton for the two main phases involved in doing block quantized Float8 GEMMs.&lt;/p&gt;

&lt;p&gt;For the incoming quantization of A and B tensors from high precision (BFloat16) to Float8, we showcase GridQuant which leverages a mini-grid stride loop style of processing with nearly &lt;strong&gt;2x&lt;/strong&gt; speedups (99.31%) over a current 2D block quantization kernel.&lt;/p&gt;

&lt;p&gt;For the Float8 GEMM, we showcase 3 new developments for Triton - Warp Specialization, TMA and a persistent kernel to effectively create a cooperative style kernel (an alternative to the &lt;a href=&quot;https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/&quot;&gt;Ping-Pong schedule&lt;/a&gt;).  As a result, we achieve ~&lt;strong&gt;1.2x&lt;/strong&gt; speedup over our best-performing SplitK kernel from last year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg1.png&quot; alt=&quot;Figure 1: A comparison of the 2D quantization speedup over a current baseline, across a range of sizes.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; A comparison of the 2D quantization speedup over a current baseline, across a range of sizes. &lt;strong&gt;&lt;em&gt;(lower-is-better)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-2d-blockwise-quantization-for-fp8&quot;&gt;Why 2D Blockwise Quantization for FP8?&lt;/h2&gt;

&lt;p&gt;Generally speaking, the accuracy of fp8 quantization improves as we move from tensor-wise scaling, to row-wise scaling, to 2D block-wise, and then finally to column-wise scaling.  This is because features for a given token are stored in each column, and thus each column in that tensor is more similarly scaled.&lt;/p&gt;

&lt;p&gt;To minimize the number of outliers of a given numerical set, we want to find commonality so that numbers are being scaled in a similar fashion.  For transformers, this means column based quantization could be optimal…however, columnar memory access is massively inefficient due to the data being laid out in memory in a rowwise contiguous manner.  Thus columnwise loading would require memory access involving large strides in memory to pull isolated values, contrary to the core tenets of efficient memory access.&lt;/p&gt;

&lt;p&gt;However, 2D is the next best option as it includes some aspects of columnar while being more memory efficient to pull since we can vectorize these loads with 2D vectorization.  Therefore, we want to find ways to improve the speed for 2D block quantization which is why we developed the GridQuant kernel.&lt;/p&gt;

&lt;p&gt;For the quantization process, we need to 2D block quantize both the higher precision BF16 incoming tensors (A = input activations, B = weights) and then proceed to do the Float8 matmul using the quantized tensors and their 2D block scaling values, and return an output C tensor in BF16.&lt;/p&gt;

&lt;h2 id=&quot;how-does-gridquant-improve-2d-block-quantization-efficiency&quot;&gt;How does GridQuant improve 2D block quantization efficiency?&lt;/h2&gt;

&lt;p&gt;The GridQuant kernel has several improvements over the initial baseline quantization implementation which was a standard tile based implementation.  The GridQuant kernel has two full passes through the entire input tensor and works as follows:&lt;/p&gt;

&lt;h2 id=&quot;phase-1---determine-the-max-abs-value-for-each-256x256-sub-block-from-the-incoming-high-precision-tensor&quot;&gt;Phase 1 - Determine the max abs value for each 256x256 sub block from the incoming high precision tensor.&lt;/h2&gt;

&lt;p&gt;1 - We divide the BF16 tensor into 256 x 256 sub blocks.  This quantization size is configurable, but 256x256 is the default as it provides a blend of quantization precision and processing efficiency.&lt;/p&gt;

&lt;p&gt;2 - Each 256x256 sub-block is subdivided into 64 sub-blocks arranged in an 8x8 pattern, with each sub-block processing a 32x32 element block. A single warp (32 threads) handles the computation for all elements within its assigned 32x32 block.&lt;/p&gt;

&lt;p&gt;3 - We declare a 32x32 max_vals array in shared memory.  This will store the current max val for each position i,j as the 2d vector block moves across the entire 256x256 sub_block.&lt;/p&gt;

&lt;p&gt;This is an important improvement because it means we can do vectorized, rather than scalar, updates to the max vals scoring system and allows for much more efficient updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg2.png&quot; alt=&quot;Figure 2: The Fractionalized layout of an incoming tensor - a grid of 256x256 is created across the tensor, and within each 256x256 block, it is further refined into 32x32 sub blocks. A 32x32 max_vals is created for each 256x256 block.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; The Fractionalized layout of an incoming tensor - a grid of 256x256 is created across the tensor, and within each 256x256 block, it is further refined into 32x32 sub blocks. A 32x32 max_vals is created for each 256x256 block.&lt;/p&gt;

&lt;p&gt;4 - Each warp processes a 32x32 chunk and because we are using 4 warps, we ensure the Triton compiler can pipeline the memory loads for the next 32x32 chunk with the actual processing of absmax calculations for the current chunk.  This ensures that the warp scheduler is able to toggle warps loading data with those processing and keep the SM continuously busy.&lt;/p&gt;

&lt;p&gt;5 - The 32x32 2D vector block processing is moved across and through the entire 256x256 subblock in a grid stride looping fashion, with each warp updating the shared memory 32x32 max_vals against its current 32x32 sub-block. Thus max_vals[i,j] holds the latest max value as each sub block is processed.&lt;/p&gt;

&lt;p&gt;After completing the 256x256 block grid stride loop, the maxvals matrix is then itself reduced to find the absolute single max value for that entire 256 block.&lt;/p&gt;

&lt;p&gt;This gives us our final scaling factor value for this 2D 256 x 256 block.&lt;/p&gt;

&lt;h2 id=&quot;phase-2---quantize-the-256x256-block-values-to-float8--by-using-the-single-max-value-scaling-factor-found-during-phase-1&quot;&gt;Phase 2 - Quantize the 256x256 block values to Float8,  by using the single max value scaling factor found during Phase 1.&lt;/h2&gt;

&lt;p&gt;Next, we make a second pass through the entire 256x256 block to rescale all the numbers using this max value found in phase 1 to convert them to the float 8 format.&lt;/p&gt;

&lt;p&gt;Because we know we need to do 2 complete passes, for the loads during the phase 1 portion we instruct the triton compiler to keep these values in cache at higher priority (evict policy = last).&lt;/p&gt;

&lt;p&gt;This means that during the second pass, we can get a high hit rate from the L2 cache which provides much faster memory access than going all the way to HBM.&lt;/p&gt;

&lt;p&gt;With the 2D block quantization processing complete when all 256 x256 blocks are processed, we can return the new Float8 quantized tensor along with it’s scaling factor matrix, which we’ll use in the next phase of the GEMM processing.   This input quantization is repeated for the second input tensor as well, meaning we end up with A_Float 8, A_scaling_matrix, and B_Float8 and B_scaling matrix.&lt;/p&gt;

&lt;h2 id=&quot;gridquant---gemm-kernel&quot;&gt;GridQuant - GEMM Kernel&lt;/h2&gt;

&lt;p&gt;The GridQuant-GEMM kernel takes in the four outputs from the quantization above for processing. Our high-performance GEMM kernel features several new Triton developments to achieve SOTA performance for matrix shape profiles relevant in LLM inference during the decoding phase.&lt;/p&gt;

&lt;p&gt;These new features are commonly found in Hopper optimized kernels like &lt;a href=&quot;https://arxiv.org/abs/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt; and &lt;a href=&quot;https://neuralmagic.com/blog/introducing-machete-a-mixed-input-gemm-kernel-optimized-for-nvidia-hopper-gpus/&quot;&gt;Machete&lt;/a&gt;, built using &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;CUTLASS 3.x&lt;/a&gt;. Here, we discuss these methods and showcase the performance benefits that can be achieved leveraging them in Triton.&lt;/p&gt;

&lt;h2 id=&quot;tensor-memory-accelerator-tma&quot;&gt;Tensor Memory Accelerator (TMA)&lt;/h2&gt;

&lt;p&gt;The TMA unit on NVIDIA Hopper GPUs, is a dedicated hardware unit for load/store operations that act on multidimensional tensors commonly found in AI workloads. This has several important benefits.&lt;/p&gt;

&lt;p&gt;Transferring data from global and shared memory can occur without involving other resources on GPU SMs, freeing up registers and CUDA Cores. Further, when used in warp-specialized kernels, light-weight TMA operations can be assigned to a producer warp allowing for a high degree of overlap of memory transfers and computation.&lt;/p&gt;

&lt;p&gt;For more details on how TMA is used in Triton see our &lt;a href=&quot;https://pytorch.org/blog/hopper-tma-unit/&quot;&gt;previous blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;warp-specialization-cooperative-persistent-kernel-design&quot;&gt;Warp-Specialization (Cooperative Persistent Kernel Design)&lt;/h2&gt;

&lt;p&gt;Warp Specialization is a technique to leverage pipeline parallelism on GPUs. This experimental feature enables the expression of specialized threads through a &lt;a href=&quot;https://github.com/facebookexperimental/triton/tree/ws&quot;&gt;tl.async_task API&lt;/a&gt;, allowing the user to specify how operations in a Triton program should be “split” amongst warps. The cooperative Triton kernel performs different types of computation and loads that each take place on their own dedicated hardware. Having dedicated hardware for each of these specialized tasks makes it possible to realize parallelism efficiently for operations that have no data dependency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg3.png&quot; alt=&quot;Figure 3. Logical view of dedicated HW units in NVIDIA H100 SM&quot; style=&quot;width:100%; max-width:400px; display: block; margin-left:auto; margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Logical view of dedicated HW units in NVIDIA H100 SM&lt;/p&gt;

&lt;p&gt;The operations in our kernel that create the pipeline are:&lt;/p&gt;

&lt;p&gt;A - Load per-block scale from GMEM into SMEM (cp.async engine)&lt;/p&gt;

&lt;p&gt;B - Load activation (A) and Weight (B) tiles from GMEM into SMEM (TMA)&lt;/p&gt;

&lt;p&gt;C - Matrix-Multiplication of A tile and B tile = C tile  (Tensor Core)&lt;/p&gt;

&lt;p&gt;D - Scale C tile with per-block scale from A and per-block scale from B (CUDA core)&lt;/p&gt;

&lt;p&gt;These steps can be assigned to “tasks” which are carried out by specialized warp groups in a threadblock. The cooperative strategy has three warp groups. A producer warp group that is responsible for feeding the compute units and 2 consumer warp groups that perform the computation. The two consumer warp groups each work on half of the same output tile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg4.png&quot; alt=&quot;Figure 4. Warp-Specialized Persistent Cooperative kernel&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Warp-Specialized Persistent Cooperative kernel (source: &lt;a href=&quot;https://drive.google.com/file/d/18sthk6IUOKbdtFphpm_jZNXoJenbWR8m/view&quot;&gt;NVIDIA&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This is different from the ping-pong schedule we discussed in our &lt;a href=&quot;https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/&quot;&gt;previous blog&lt;/a&gt;, where each consumer warp group works on &lt;em&gt;different&lt;/em&gt; output tiles. We note that the Tensor Core ops are not overlapped with the epilogue computation. Decreased utilization of the Tensor Core pipeline during the epilogue phase of the computation will reduce register pressure for the consumer warp group compared to ping-pong which always keeps the Tensor Core busy, thus allowing for larger tile sizes.&lt;/p&gt;

&lt;p&gt;Lastly, our kernel is designed to be persistent when the grid size exceeds the number of available compute units on H100 GPUs (132). Persistent kernels remain active on the GPU for an extended period and compute multiple output tiles during its lifetime. Our kernel leverages TMA async shared to global memory stores, while continuing to do work on the next output tile as opposed to incurring the cost of scheduling multiple threadblocks.&lt;/p&gt;

&lt;h2 id=&quot;microbenchmarks&quot;&gt;Microbenchmarks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-gemms-triton/fg5.png&quot; alt=&quot;Figure 5: Latency comparison (us) of Gridquant-GEMM vs our best performing SplitK kernel for small batch regime and Llama3 8192 N,K sizing.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Latency comparison (us) of Gridquant-GEMM vs our best performing SplitK kernel for small batch regime and Llama3 8192 N,K sizing. &lt;strong&gt;&lt;em&gt;(lower-is-better)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Warp-Specialized Triton kernel achieves SOTA performance at the above small-M and square matrix shapes, achieving a nearly &lt;strong&gt;1.2x&lt;/strong&gt; speedup over the SplitK Triton kernel, which was the previous best performing strategy for Triton GEMMs in this low arithmetic intensity regime. For future work, we plan to tune our kernel performance for the medium-to-large M regime and non-square matrices.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;Future work includes benchmarking gridquant on end to end workflows. In addition, we plan to run more extensive benchmarks on non-square (rectangular) matrices as well as medium-to-large M sizes. Finally, we plan to explore ping-pong style warp-specialization in Triton versus the current cooperative implementation.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta: Less Wright, IBM: Adnan Hoque</name>
        
        
      </author>

      

      

      
        <summary type="html">2D block quantization for Float8 (FP8) holds the promise of improving the accuracy of Float8 quantization while also accelerating GEMM’s for both inference and training. In this blog, we showcase advances using Triton for the two main phases involved in doing block quantized Float8 GEMMs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">HadaCore: Tensor Core Accelerated Hadamard Transform Kernel</title>
      <link href="https://pytorch.org/blog/hadacore/" rel="alternate" type="text/html" title="HadaCore: Tensor Core Accelerated Hadamard Transform Kernel" />
      <published>2024-12-02T00:00:00-08:00</published>
      <updated>2024-12-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/hadacore</id>
      <content type="html" xml:base="https://pytorch.org/blog/hadacore/">&lt;p&gt;&lt;strong&gt;IBM&lt;/strong&gt;: Krish Agarwal, Rishi Astra, Adnan Hoque, Mudhakar Srivatsa, Raghu Ganti&lt;br /&gt;
&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Sijia Chen&lt;/p&gt;

&lt;p&gt;Quantization is a method for improving model inference speeds by compressing model weights and performing (faster) computation in lower precision data types. However, quantization can result in accuracy loss due to the presence of outliers. Recent works like &lt;a href=&quot;https://arxiv.org/abs/2404.00456&quot;&gt;QuaRot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2405.16406&quot;&gt;SpinQuant&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt; introduce methods to increase the numerical accuracy of INT4, INT8 and FP8 quantization in LLMs. These methods rely on &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadamard_transform&quot;&gt;Hadamard Transforms&lt;/a&gt;. In this blog, we present HadaCore, a Hadamard Transform CUDA kernel that achieves state-of-the-art performance on NVIDIA A100 and H100 GPUs. Our kernel achieves speedups of &lt;strong&gt;1.1–1.4x&lt;/strong&gt; and &lt;strong&gt;1.0–1.3x&lt;/strong&gt;, with a peak gain of &lt;strong&gt;3.5x&lt;/strong&gt; and &lt;strong&gt;3.6x&lt;/strong&gt; respectively, over Dao AI Lab’s &lt;a href=&quot;https://github.com/Dao-AILab/fast-hadamard-transform&quot;&gt;Fast Hadamard Transform Kernel&lt;/a&gt;. We leverage a hardware-aware work decomposition that benefits from Tensor Core acceleration while maintaining quantization error reduction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg1.png&quot; alt=&quot;Figure 1: Speedup of HadaCore vs Dao AI Hadamard CUDA kernel. A peak gain of 3.46x on the A100 is achieved using 128 rotation by 8.4M elements.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Speedup of HadaCore vs Dao AI Hadamard CUDA kernel. A peak gain of 3.46x on the A100 is achieved using 128 rotation by 8.4M elements.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/cuda/inference/hadamard_transform&quot;&gt;HadaCore Kernel is publicly available&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.00456&quot;&gt;QuaRot&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2405.16406&quot;&gt;SpinQuant&lt;/a&gt; both propose methods to increase the numerical accuracy of INT4 and INT8 quantization in LLMs. Both methods rotate model activations since rotations are statistically likely to reduce the magnitude of outliers, as it “distributes” extreme values among other (less extreme) dimensions, and rotation is also an easily invertible operation using the inverse of the rotation matrix. These methods can also improve FP8 inference accuracy, such as in &lt;a href=&quot;https://arxiv.org/pdf/2407.08608&quot;&gt;FlashAttention-3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg2.png&quot; alt=&quot;Figure 2. Transformer block showing online (red) and offline rotations (blue) in QuaRot&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. Transformer block showing online (red) and offline rotations (blue) in QuaRot&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Applying these rotation matrices introduces model runtime overhead due to the online operations shown in Figure 2. These rotations can be applied through matrix multiplication, but the added overhead would diminish the benefits from quantization. Therefore, QuaRot and SpinQuant opt to use Walsh-Hadamard matrices, a special type of rotation matrix that can be applied faster than matrix multiplication using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fast_Walsh%E2%80%93Hadamard_transform&quot;&gt;Fast Walsh-Hadamard Transform&lt;/a&gt; algorithm. HadaCore is an optimized implementation of this algorithm for NVIDIA GPUs that support Tensor Cores.&lt;/p&gt;

&lt;h2 id=&quot;tensor-core-accelerated-hadamard-transform&quot;&gt;Tensor Core Accelerated Hadamard Transform&lt;/h2&gt;

&lt;p&gt;HadaCore leverages &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;NVIDIA Tensor Cores&lt;/a&gt;, which are specialized compute units on NVIDIA GPUs optimized for matrix multiplication. To achieve this, our kernel performs a hardware-aware work decomposition of the Fast Walsh-Hadamard algorithm. This work decomposition ensures that we can utilize the &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma#multiply-and-accumulate-instruction-mma&quot;&gt;MMA PTX instructions&lt;/a&gt; that execute on the Tensor Core chip. HadaCore applies a 16×16 Hadamard transform to chunks of the input data. The computation can then be offloaded to the FP16 Tensor Core with usage of the &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=mma#matrix-fragments-for-mma-m16n8k16-with-floating-point-type&quot;&gt;mma.m16n8k16&lt;/a&gt; instruction. The warp-level parallelism for HadaCore is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg3.png&quot; alt=&quot;Figure 3: HadaCore Parallelization, 1x256 vectors (rows) being rotated by a size 256 Hadamard.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: HadaCore Parallelization, 1x256 vectors (rows) being rotated by a size 256 Hadamard.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We process fragments of 256 elements in parallel using warp-level Tensor Core operations to achieve up to a 256-size Hadamard transform. For further sizes, we shuffle data between warps and repeat.&lt;/p&gt;

&lt;h2 id=&quot;microbenchmarks&quot;&gt;Microbenchmarks&lt;/h2&gt;

&lt;p&gt;We benchmark HadaCore against the&lt;a href=&quot;https://github.com/Dao-AILab&quot;&gt; Dao AI Lab Hadamard Kernel&lt;/a&gt; on both NVIDIA H100 and A100 GPUs across varying Hadamard and input tensor sizes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg4.png&quot; alt=&quot;Figure 4:  HadaCore Kernel Speedup on NVIDIA A100 over Dao AI Lab Fast Hadamard Kernel&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4:  HadaCore Kernel Speedup on NVIDIA A100 over Dao AI Lab Fast Hadamard Kernel&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg5.png&quot; alt=&quot;Color coded Speedup Table for NVIDIA A100, Green = Speedup over Baseline&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Color coded Speedup Table for NVIDIA A100, Green = Speedup over Baseline&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg6.png&quot; alt=&quot;Figure 5:  HadaCore Kernel Speedup on NVIDIA H100 over Dao AI Lab Fast Hadamard Kernel&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5:  HadaCore Kernel Speedup on NVIDIA H100 over Dao AI Lab Fast Hadamard Kernel&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hadacore/fg7.png&quot; alt=&quot;Color coded Speedup Table for NVIDIA H100, Green = Speedup over Baseline&quot; style=&quot;width:100%; margin-top: 35px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Color coded Speedup Table for NVIDIA H100, Green = Speedup over Baseline&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We showcase our speedup as the input tensor size (labeled element count) in our charts increase. Element count is the number of elements in the target matrix we are rotating. For example, in multi-head attention:&lt;/p&gt;

&lt;p&gt;The queries (Q), keys (K) and values (V) tensors are 4D tensors of size:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(batch_size, seq_len, n_heads, head_dim)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A Hadamard matrix of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head_dim&lt;/code&gt; is applied to these activation tensors, so we refer to this as using a Hadamard size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head_dim&lt;/code&gt; with an element count of:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size*seq_len*n_heads*head_dim.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Common element counts for query rotations in an attention block:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model \ Tokens&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prefill&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Decoding&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Llama-2 70b&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;33,554,432 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;

(1 batch * 64 heads * 4096 tokens * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
   &lt;td&gt;8192 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 64 heads * 1 token * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Llama-3 8b&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;33,554,432 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 32 heads * 8192 tokens * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
   &lt;td&gt;4,096 elements
&lt;br /&gt;
128 Hadamard size
&lt;br /&gt;
(1 batch * 32 heads * 1 token * 128 dimensional embeddings per head per token)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;HadaCore achieves &lt;strong&gt;1.1–1.4x&lt;/strong&gt; speedup on A100 and &lt;strong&gt;1.0–1.3x&lt;/strong&gt; speedup on H100 over Dao AI Lab’s Fast Hadamard kernel, with a peak gain of &lt;strong&gt;3.5x and 3.6x&lt;/strong&gt;, respectively. For smaller sizes on H100, HadaCore’s gain decreases. For future work, we plan to incorporate usage of Hopper specific features like TMA and WGMMA for improved H100 performance.&lt;/p&gt;

&lt;h2 id=&quot;mmlu-benchmarks&quot;&gt;MMLU Benchmarks&lt;/h2&gt;

&lt;p&gt;We evaluated MMLU scores on a &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-3.1-8B&quot;&gt;Llama 3.1-8B&lt;/a&gt; inference workload where the FlashAttention computation was performed in FP8. Newer generation &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/&quot;&gt;NVIDIA Hopper GPUs &lt;/a&gt;come equipped with FP8 Tensor Cores that deliver substantial compute gain over FP16.&lt;/p&gt;

&lt;p&gt;Our results show the benefit of using HadaCore for accuracy preservation when combined with optimizations such as FP8 FlashAttention.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Format&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama3.1-8B&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;Avg. 5-Shot MMLU Accuracy&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP16&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP16&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;65.38
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP16&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;No Hadamard
   &lt;/td&gt;
   &lt;td&gt;64.40
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP8&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;HadaCore
   &lt;/td&gt;
   &lt;td&gt;65.09
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Q, K, V: FP8&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;FlashAttention: FP8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;Dao AI Fast Hadamard Kernel
   &lt;/td&gt;
   &lt;td&gt;65.45
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: MMLU scores for Llama3.1 8B with FP16 baseline and FP8 attention using Hadamard transforms, comparing an implementation with explicit Hadamard matrix multiplications vs. HadaCore (&lt;strong&gt;higher is better&lt;/strong&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From the above MMLU scores, we note that for Llama3.1-8B inference with FP8 attention, HadaCore improves the quantization error introduced from computing attention in a lower precision.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We showcased our speedups achieved by moving the Fast-Walsh Hadamard algorithm into a CUDA kernel that leverages Tensor Core acceleration and achieves a peak speedup of &lt;strong&gt;3.5x&lt;/strong&gt; and &lt;strong&gt;3.6x&lt;/strong&gt; over the Dao AI Fast-Hadamard kernel on NVIDIA A100 and H100, respectively.&lt;/p&gt;

&lt;p&gt;Further, we showed on the MMLU benchmark that rotating with HadaCore maintains similar quantization error reduction to the Fast-Hadamard kernel, while providing computational acceleration.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;We plan to implement a Triton version of our kernel and experiment with more advanced techniques such as kernel fusion to support fused Hadamard transform and quantization. Further, we plan to extend our kernel to support BF16 Tensor Core compute.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>IBM and Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">Quantization is a method for improving model inference speeds by compressing model weights and performing (faster) computation in lower precision data types. However, quantization can result in accuracy loss due to the presence of outliers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Supercharging Training using float8 and FSDP2</title>
      <link href="https://pytorch.org/blog/training-using-float8-fsdp2/" rel="alternate" type="text/html" title="Supercharging Training using float8 and FSDP2" />
      <published>2024-11-25T00:00:00-08:00</published>
      <updated>2024-11-25T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/training-using-float8-fsdp2</id>
      <content type="html" xml:base="https://pytorch.org/blog/training-using-float8-fsdp2/">&lt;p&gt;&lt;strong&gt;IBM&lt;/strong&gt;: Tuan Hoang Trong, Alexei Karve, Yan Koyfman, Linsong Chu, Divya Kumari, Shweta Salaria, Robert Walkup, Praneet Adusumilli, Nirmit Desai, Raghu Ganti, Seetharami Seelam&lt;br /&gt;
&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Wei Feng, Vasiliy Kuznetsov, Driss Guesseous&lt;/p&gt;

&lt;p&gt;In this blog, we will demonstrate how we achieve up to &lt;strong&gt;50% throughput speedup&lt;/strong&gt; while achieving loss and evaluation benchmark parity in training over &lt;a href=&quot;https://pytorch.org/blog/maximizing-training-throughput/&quot;&gt;FSDP1 bf16 training&lt;/a&gt;. We achieve this speedup by leveraging FSDP2, DTensor, and torch.compile with torchao’s float8 via linear layer updates (compute), and float8 all_gathers for weight communication. We showcase these improvements across a spectrum of Meta LLaMa model architecture sizes, ranging from small 1.8B model size all the way to 405B model size, making training faster than ever.&lt;/p&gt;

&lt;p&gt;We demonstrate these improvements using the Meta Llama3 architecture, and then perform model quality studies at two scales: 100B tokens at 8B model size, and 50B tokens at 70B model size, which provide an exact comparison of float8 and bf16 training loss curves. We demonstrate that the loss curves result in identical loss convergence across these model training runs compared to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; counterpart. Further, we train a 3B model to 1T tokens using the FineWeb-edu dataset and run standard evaluation benchmarks to ensure that the model quality is intact and comparable to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; run.&lt;/p&gt;

&lt;p&gt;At IBM Research, we plan to adopt these capabilities for our data ablations to improve the number of experiments we can perform in a given GPU budget. Longer term, we will follow up with a larger scale model run to demonstrate the end-to-end feasibility of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; training.&lt;/p&gt;

&lt;h2 id=&quot;what-is-float8&quot;&gt;What is Float8?&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; format for training models was introduced by NVIDIA, ARM, and Intel in a &lt;a href=&quot;https://arxiv.org/abs/2209.05433&quot;&gt;2022 paper&lt;/a&gt; which demonstrated the feasibility of training using lower precision float8, without sacrificing model quality. With the introduction of newer GPUs like the NVIDIA Hopper series, FP8 training became feasible with the potential of more than 2x improvement in training throughput due to native float8 tensor core support. There are a few challenges to realize this promise:  &lt;br /&gt;
(i) Enable the core model operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attention&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;,  &lt;br /&gt;
(ii) Enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; training in a distributed framework, and  &lt;br /&gt;
(iii) Enable weight communication between GPUs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;.  &lt;br /&gt;
While the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; was enabled by NVIDIA libraries, the latter two were provided in recent updates to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSDP2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchao&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this blog, we are using &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;torchtitan&lt;/a&gt; as the entry point for training, IBM’s deterministic data loader, the &lt;code&gt;float8&lt;/code&gt; linear layer implementation from &lt;a href=&quot;https://www.google.com/url?q=https://github.com/pytorch/ao/tree/main/torchao/float8&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1730743084184771&amp;amp;usg=AOvVaw21FdkNG452P-nDIO-hIwcW&quot;&gt;torchao&lt;/a&gt;, and the &lt;code&gt;float8 all gather&lt;/code&gt; from the latest PyTorch nightlies in conjunction with FSDP2. For this training, we are using the float8  per tensor (tensorwise) scaling granularity rather than rowwise. We leverage &lt;code&gt;torch.compile&lt;/code&gt; to ensure that we get maximum performance gains. We are computing &lt;code&gt;attention&lt;/code&gt; in &lt;code&gt;bf16&lt;/code&gt; using SDPA and are currently working on moving this to float8 as well.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;We perform various experiments to demonstrate the benefits of float8 training. The first is to ensure that model quality is not sacrificed. To verify this, we train an 8B model and 70B model for a few thousand steps and compare the loss curves between both the float8 and bf16 training run. Our experiments are performed on three different H100 clusters with 128, 256, and 512 H100 GPU configurations in very different environments to demonstrate reproducibility. The first cluster is customized on &lt;a href=&quot;https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/&quot;&gt;Grand Teton&lt;/a&gt; in Meta with 400Gbps custom interconnect, the second is an IBM research cluster with 3.2Tbps Infiniband interconnect, and the third is an IBM Cloud cluster with 3.2Tbps RoCE interconnect for GPU-to-GPU communication.&lt;/p&gt;

&lt;p&gt;First, we plot the loss curve comparisons for both these models in the below figures to demonstrate loss parity for a few thousand steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-using-float8-fsdp2/fg1.png&quot; alt=&quot;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-using-float8-fsdp2/fg2.png&quot; alt=&quot;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: (a) 8B model loss parity for 2k steps, (b) 70B loss parity for 1k steps&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe that across these different models and in different environments, we obtain loss parity for the small scale of tokens. Next, we characterize the throughput gains for four different model sizes ranging from 1.8B to 405B. We explored the best batch size and activation checkpointing schemes for both the float8 and bf16 training runs to determine the tokens/sec/GPU (wps) metric and report the performance gain. For the 405B model, we leveraged &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTensor&lt;/code&gt; for tensor parallel training with FSDP2. We use a sequence length of 8K for all our measurements.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (bf16) &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percent gain&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.8B
   &lt;/td&gt;
   &lt;td&gt;29K
   &lt;/td&gt;
   &lt;td&gt;35K
   &lt;/td&gt;
   &lt;td&gt;18%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;8B
   &lt;/td&gt;
   &lt;td&gt;8K
   &lt;/td&gt;
   &lt;td&gt;10K
   &lt;/td&gt;
   &lt;td&gt;28%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;956
   &lt;/td&gt;
   &lt;td&gt;1430
   &lt;/td&gt;
   &lt;td&gt;50%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;405B (TP4)
   &lt;/td&gt;
   &lt;td&gt;149
   &lt;/td&gt;
   &lt;td&gt;227
   &lt;/td&gt;
   &lt;td&gt;52%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: Performance gains over bf16 (both bf16 and float8 use torch.compile)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe from Table 1 that the gains for larger models (70B and 405B) reach up to 50%, the smaller models see gains between roughly 20 and 30%. In further experiments, we observed that the addition of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; enables a boost of ~5% beyond the compute itself in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt;, which is inline with the observations in this &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/efficient-pre-training-of-llama-3-like-model-architectures-using-torchtitan-on-amazon-sagemaker/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Second, to demonstrate the effectiveness of an FP8 model, we trained a 3B model following the Llama3 architecture for 1T tokens using the FineWeb-edu dataset from Hugging Face. We performed evaluations using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lm-eval-harness&lt;/code&gt; framework and present a small portion of these results in the below table. We observe that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; performance is marginally better than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8&lt;/code&gt; scores (about one percent). While some scores are significantly better with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; (e.g., MMLU is 3 pts higher), we expect these gaps to vanish when the right hyper parameters are chosen and across larger scale training runs (e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; run had half the batch size and it is well known that smaller batch size runs can improve evaluation scores).&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Score (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Score (bf16)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (5-shot)
   &lt;/td&gt;
   &lt;td&gt;0.26
   &lt;/td&gt;
   &lt;td&gt;0.29
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ARC-e
   &lt;/td&gt;
   &lt;td&gt;0.73
   &lt;/td&gt;
   &lt;td&gt;0.73
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ARC-c
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
   &lt;td&gt;0.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hellaswag
   &lt;/td&gt;
   &lt;td&gt;0.65
   &lt;/td&gt;
   &lt;td&gt;0.67
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;sciq
   &lt;/td&gt;
   &lt;td&gt;0.89
   &lt;/td&gt;
   &lt;td&gt;0.88
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;OpenBook QA
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;PIQA
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Winogrande
   &lt;/td&gt;
   &lt;td&gt;0.60
   &lt;/td&gt;
   &lt;td&gt;0.65
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.59&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 2: Benchmark scores for float8 trained model running in FP16 for eval (at 1T tokens of FineWeb pre-training).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally, we scale our experiments to 512 H100 GPUs on the IBM Cloud cluster. We were able to recreate the results and speedups that we observed even at 512 GPU scale. We summarize these results only for the large models in the below table (70B and 405B).&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (bf16) &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;wps (float8)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percent gain&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;960
   &lt;/td&gt;
   &lt;td&gt;1448
   &lt;/td&gt;
   &lt;td&gt;51%
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;405B (TP4)
   &lt;/td&gt;
   &lt;td&gt;152
   &lt;/td&gt;
   &lt;td&gt;217
   &lt;/td&gt;
   &lt;td&gt;43%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 3: Performance gains over bf16 (both bf16 and float8 use torch.compile) for 512 GPU scale&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;We are also working on evaluating other forms of parallelism such as Context Parallelism. We plan to evaluate all of these features to demonstrate the composability and ability to make choices for training large scale models.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank Davis Wertheimer from IBM Research for enabling the data loader for torchtitan runs enabling us to replay data in the same order across multiple runs. We also thank IBM Cloud for enabling us with early test access to the H100 cluster.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>IBM and Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we will demonstrate how we achieve up to 50% throughput speedup while achieving loss and evaluation benchmark parity in training over FSDP1 bf16 training</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Rebellions Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/rebellions/" rel="alternate" type="text/html" title="Rebellions Joins the PyTorch Foundation as a General Member" />
      <published>2024-11-21T00:00:00-08:00</published>
      <updated>2024-11-21T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/rebellions</id>
      <content type="html" xml:base="https://pytorch.org/blog/rebellions/">&lt;p&gt;&lt;img src=&quot;/assets/images/rebellions-logo.svg&quot; alt=&quot;Rebellions logo&quot; style=&quot;max-width:350px;width:100%;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Rebellions has joined as a general member.&lt;/p&gt;

&lt;p&gt;Rebellions is a South Korea-based semiconductor company specializing in the design and development of AI chips for data centers and edge devices. Their innovative hardware and software solutions aim to accelerate generative AI and machine learning workloads, focusing on high energy efficiency and performance. The company successfully launched and deployed its AI chip ‘ATOM’ targeting data centers in 2023 and is developing its next-generation AI accelerator ‘REBEL’.&lt;/p&gt;

&lt;p&gt;“We’re thrilled to welcome Rebellions as a new general member of the PyTorch Foundation,” said Matt White, Executive Director of the PyTorch Foundation. “Rebellions brings a unique perspective to the PyTorch ecosystem with their focus on advancing the integration of NPU architectures for AI acceleration with PyTorch. Their expertise will play a vital role in ensuring PyTorch continues to evolve as a versatile framework, accommodating the diverse needs of modern AI workloads. We look forward to collaborating with Rebellions to drive innovation and strengthen the PyTorch ecosystem for developers worldwide.”&lt;/p&gt;

&lt;p&gt;Rebellions has introduced native support for PyTorch 2.0 in their RBLN SDK. This integration includes compatibility with torch.compile, a pivotal feature of PyTorch 2.0 that enhances model performance. Through this development, Rebellions has empowered developers to seamlessly harness the full potential of their AI accelerator lineup within the environment.&lt;/p&gt;

&lt;p&gt;Rebellions is also deeply committed to advancing the PyTorch ecosystem through collaborative innovation starting in Korea. The company has established a Special Interest Group (SIG) focusing on Pytorch Core within the PyTorch Korea community and is actively working with volunteers recruited through MODULABS, an open research institute, to integrate native support for the deep learning framework into their Neural Processing Unit (NPU).&lt;/p&gt;

&lt;p&gt;In addition, Rebellions is collaborating with academic institutions, such as Yonsei University, Hanyang University, University of Science &amp;amp; Technology (UST)  and national agencies, such as the Electronics and Telecommunications Research Institute (ETRI), to offer undergraduate and graduate courses on PyTorch and enable them to leverage Pytorch as their research platform.&lt;/p&gt;

&lt;p&gt;These initiatives highlight Rebellions’ dedication to optimizing the PyTorch experience for developers and researchers alike, while also fostering education and innovation in the field.&lt;/p&gt;

&lt;p&gt;“By integrating our hardware innovations with PyTorch, we’re building Native NPU support to accelerate diverse AI workloads.” said Hong-seok Kim, the Chief Software Architect at Rebellions. “We’re excited to contribute to the PyTorch community by community-driven initiatives and partnerships, advancing NPU architecture support for next-generation AI solutions. Together with the PyTorch community, we aim to pioneer new possibilities in AI acceleration and empower developers worldwide with efficient computing solutions.”&lt;/p&gt;

&lt;p&gt;To learn more about how your organization can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-rebellions&quot;&gt;About Rebellions&lt;/h2&gt;

&lt;p&gt;Rebellions is a South Korea-based semiconductor company specializing in the design and development of AI chips for data centers and edge devices. Their innovative hardware and software solutions aim to accelerate generative AI and machine learning workloads, focusing on high energy efficiency and performance. The company successfully launched and deployed its AI chip ‘ATOM’ targeting data centers in 2023 and is developing its next-generation AI accelerator ‘REBEL’ incorporating a scalable chiplet architecture and high-bandwidth memory.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Rebellions has joined as a general member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Distilling Llama3.1 8B into 1B in torchtune</title>
      <link href="https://pytorch.org/blog/llama-into-torchtune/" rel="alternate" type="text/html" title="Distilling Llama3.1 8B into 1B in torchtune" />
      <published>2024-11-18T00:00:00-08:00</published>
      <updated>2024-11-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/llama-into-torchtune</id>
      <content type="html" xml:base="https://pytorch.org/blog/llama-into-torchtune/">&lt;p&gt;In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.&lt;/p&gt;

&lt;h2 id=&quot;what-is-knowledge-distillation&quot;&gt;What is Knowledge Distillation?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.02531&quot;&gt;Knowledge Distillation&lt;/a&gt; is a widely used compression technique that transfers knowledge from a larger (teacher) model to a smaller (student) model. Larger models have more parameters and capacity for knowledge, however, this larger capacity is also more computationally expensive to deploy. Knowledge distillation can be used to compress the knowledge of a larger model into a smaller model. The idea is that performance of smaller models can be improved by learning from larger model’s outputs.&lt;/p&gt;

&lt;h2 id=&quot;how-does-knowledge-distillation-work&quot;&gt;How does Knowledge Distillation work?&lt;/h2&gt;

&lt;p&gt;Knowledge is transferred from the teacher to student model by training on a transfer set where the student is trained to imitate the token-level probability distributions of the teacher. The assumption is that the teacher model distribution is similar to the transfer dataset. The diagram below is a simplified representation of how KD works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/llama-into-torchtune/fg1.png&quot; alt=&quot;Figure 1: Simplified representation of knowledge transfer from teacher to student model&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: Simplified representation of knowledge transfer from teacher to student model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As knowledge distillation for LLMs is an active area of research, there are papers, such as &lt;a href=&quot;https://arxiv.org/pdf/2306.08543&quot;&gt;MiniLLM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2402.03898&quot;&gt;DistiLLM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2404.02657&quot;&gt;AKL&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2306.13649&quot;&gt;Generalized KD&lt;/a&gt;, investigating different loss approaches. In this case study, we focus on the standard cross-entropy (CE) loss with the forward &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback-Leibler (KL) divergence&lt;/a&gt; loss as the baseline. Forward KL divergence aims to minimize the difference by forcing the student’s distribution to align with all of the teacher’s distributions.&lt;/p&gt;

&lt;h2 id=&quot;why-is-knowledge-distillation-useful&quot;&gt;Why is Knowledge Distillation useful?&lt;/h2&gt;

&lt;p&gt;The idea of knowledge distillation is that a smaller model can achieve better performance using a teacher model’s outputs as an additional signal than it could training from scratch or with supervised fine-tuning. For instance, &lt;a href=&quot;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&quot;&gt;Llama 3.2 lightweight 1B and 3B text models&lt;/a&gt; incorporated logits from Llama 3.1 8B and 70B to recover performance after pruning. In addition, for fine-tuning on instruction-following tasks, research in LLM distillation demonstrates that knowledge distillation methods can outperform supervised fine-tuning (SFT) alone.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DollyEval&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Self-Inst&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;S-NI&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPT-4 Eval
   &lt;/td&gt;
   &lt;td&gt;GPT-4 Eval
   &lt;/td&gt;
   &lt;td&gt;Rouge-L
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Llama 7B
   &lt;/td&gt;
   &lt;td&gt;SFT
   &lt;/td&gt;
   &lt;td&gt;73.0
   &lt;/td&gt;
   &lt;td&gt;69.2
   &lt;/td&gt;
   &lt;td&gt;32.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD
   &lt;/td&gt;
   &lt;td&gt;73.7
   &lt;/td&gt;
   &lt;td&gt;70.5
   &lt;/td&gt;
   &lt;td&gt;33.7
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MiniLLM
   &lt;/td&gt;
   &lt;td&gt;76.4
   &lt;/td&gt;
   &lt;td&gt;73.1
   &lt;/td&gt;
   &lt;td&gt;35.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Llama 1.1B
   &lt;/td&gt;
   &lt;td&gt;SFT
   &lt;/td&gt;
   &lt;td&gt;22.1
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;27.8
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD
   &lt;/td&gt;
   &lt;td&gt;22.2
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;28.1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AKL
   &lt;/td&gt;
   &lt;td&gt;24.4
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;31.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt;OpenLlama 3B
   &lt;/td&gt;
   &lt;td&gt;SFT
   &lt;/td&gt;
   &lt;td&gt;47.3
   &lt;/td&gt;
   &lt;td&gt;41.7
   &lt;/td&gt;
   &lt;td&gt;29.3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD
   &lt;/td&gt;
   &lt;td&gt;44.9
   &lt;/td&gt;
   &lt;td&gt;42.1
   &lt;/td&gt;
   &lt;td&gt;27.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;SeqKD
   &lt;/td&gt;
   &lt;td&gt;48.1
   &lt;/td&gt;
   &lt;td&gt;46.0
   &lt;/td&gt;
   &lt;td&gt;29.1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;DistiLLM
   &lt;/td&gt;
   &lt;td&gt;59.9
   &lt;/td&gt;
   &lt;td&gt;53.3
   &lt;/td&gt;
   &lt;td&gt;37.6
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: Comparison of knowledge distillation approaches to supervised fine-tuning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Below is a simplified example of how knowledge distillation differs from supervised fine-tuning.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;th&gt;Supervised fine-tuning
   &lt;/th&gt;
   &lt;th&gt;Knowledge distillation
   &lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;pre class=&quot;highlight&quot;&gt;
   &lt;code&gt;
model = llama3_2_1b()
ce_loss = CrossEntropyLoss()
kd_loss = ForwardKLLoss()

tokens, labels = batch[&quot;tokens&quot;], batch[&quot;labels&quot;]
logits = model(tokens, ...)

loss = ce_loss(logits, labels)
loss.backward()

   &lt;/code&gt;
   &lt;/pre&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;pre class=&quot;highlight&quot;&gt;
   &lt;code&gt;
model = llama3_2_1b()
teacher_model = llama3_1_8b()
ce_loss = CrossEntropyLoss()
kd_loss = ForwardKLLoss()

tokens, labels = batch[&quot;tokens&quot;], batch[&quot;labels&quot;]
logits = model(tokens, ...)
teacher_logits = teacher_model(tokens, ...)
loss = ce_loss(logits, labels) + kd_loss(logits, teacher_logits, labels)
loss.backward()
   &lt;/code&gt;
   &lt;/pre&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;kd-recipe-in-torchtune&quot;&gt;KD recipe in torchtune&lt;/h2&gt;

&lt;p&gt;With torchtune, we can easily apply knowledge distillation to Llama3, as well as other LLM model families, using torchtune’s &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/4234b78b914af23384ce0348f564e2119d107a96/recipes/knowledge_distillation_single_device.py&quot;&gt;KD recipe&lt;/a&gt;. The objective for this recipe is to fine-tune Llama3.2-1B on the Alpaca instruction-following dataset by distilling from Llama3.1-8B. This recipe focuses on post-training and assumes the teacher and student models have already been pre-trained.&lt;/p&gt;

&lt;p&gt;First, we have to download the model weights. To be consistent with other torchtune fine-tuning configs, we will use the instruction tuned models of Llama3.1-8B as teacher and Llama3.2-1B as student.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns &quot;original/consolidated.00.pth&quot; --hf_token &amp;lt;HF_TOKEN&amp;gt;

tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns &quot;original/consolidated.00.pth&quot; --hf_token &amp;lt;HF_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order for the teacher model distribution to be similar to the Alpaca dataset, we will fine-tune the teacher model using LoRA. Based on our experiments, shown in the next section, we’ve found that KD performs better when the teacher model is already fine-tuned on the target dataset.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we can run the following command to distill the fine-tuned 8B model into the 1B model on a single GPU. For this case study, we used a single A100 80GB GPU. We also have a &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/09c2619f713e771b4159f7b83bac8971c7053bd3/recipes/knowledge_distillation_distributed.py&quot;&gt;distributed recipe&lt;/a&gt; for running on multiple devices.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation studies&lt;/h2&gt;

&lt;p&gt;In this section, we demonstrate how changing configurations and hyperparameters can affect performance. By default, our configuration uses the LoRA fine-tuned 8B teacher model,  downloaded 1B student model, learning rate of 3e&lt;sup&gt;-4&lt;/sup&gt; and KD loss ratio of 0.5. For this case study, we fine-tuned on the &lt;a href=&quot;https://pytorch.org/torchtune/main/generated/torchtune.datasets.alpaca_cleaned_dataset.html#torchtune.datasets.alpaca_cleaned_dataset&quot;&gt;alpaca_cleaned_dataset&lt;/a&gt; and evaluated the models on &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness/tree/feff1b55c57993c4d42c8f913a22eeec395cd690/lm_eval/tasks/truthfulqa&quot;&gt;truthfulqa_mc2&lt;/a&gt;, &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness/tree/517aadc/lm_eval/tasks/hellaswagd&quot;&gt;hellaswag&lt;/a&gt; and &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness/tree/b62b9bd/lm_eval/tasks/commonsense_qa&quot;&gt;commonsense_qa&lt;/a&gt; tasks through the EleutherAI &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness/tree/main&quot;&gt;LM evaluation harness&lt;/a&gt;. Let’s take a look at the effects of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using a fine-tuned teacher model&lt;/li&gt;
  &lt;li&gt;Using a fine-tuned student model&lt;/li&gt;
  &lt;li&gt;Hyperparameter tuning of KD loss ratio and learning rate&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;using-a-fine-tuned-teacher-model&quot;&gt;Using a fine-tuned teacher model&lt;/h3&gt;

&lt;p&gt;The default settings in the config uses the fine-tuned teacher model. Now, let’s take a look at the effects of not fine-tuning the teacher model first.&lt;/p&gt;

&lt;p&gt;Taking a loss at the losses, using the baseline 8B as teacher results in a higher loss than using the fine-tuned teacher model. The KD loss also remains relatively constant, suggesting that the teacher model should have the same distributions as the transfer dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/llama-into-torchtune/fg2.png&quot; alt=&quot;Figure 2: (left to right) KD loss from forward KL divergence, class loss from cross entropy, total loss: even combination of KD and class loss.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: (left to right) KD loss from forward KL divergence, class loss from cross entropy, total loss: even combination of KD and class loss.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In our benchmarks, we can see that supervised fine-tuning of the 1B model achieves better accuracy than the baseline 1B model. By using the fine-tuned 8B teacher model, we see comparable results for truthfulqa and improvement for hellaswag and commonsense. When using the baseline 8B as a teacher, we see improvement across all metrics, but lower than the other configurations.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;TruthfulQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;hellaswag&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;commonsense&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;mc2
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
   &lt;td&gt;acc_norm
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.1 8B
   &lt;/td&gt;
   &lt;td&gt;0.5401
   &lt;/td&gt;
   &lt;td&gt;0.5911
   &lt;/td&gt;
   &lt;td&gt;0.7915
   &lt;/td&gt;
   &lt;td&gt;0.7707
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.1 8B using LoRA
   &lt;/td&gt;
   &lt;td&gt;0.5475
   &lt;/td&gt;
   &lt;td&gt;0.6031
   &lt;/td&gt;
   &lt;td&gt;0.7951
   &lt;/td&gt;
   &lt;td&gt;0.7789
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.2 1B
   &lt;/td&gt;
   &lt;td&gt;0.4384
   &lt;/td&gt;
   &lt;td&gt;0.4517
   &lt;/td&gt;
   &lt;td&gt;0.6064
   &lt;/td&gt;
   &lt;td&gt;0.5536
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.2 1B using LoRA
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4492&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.4595
   &lt;/td&gt;
   &lt;td&gt;0.6132
   &lt;/td&gt;
   &lt;td&gt;0.5528
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using baseline 8B as teacher
   &lt;/td&gt;
   &lt;td&gt;0.444
   &lt;/td&gt;
   &lt;td&gt;0.4576
   &lt;/td&gt;
   &lt;td&gt;0.6123
   &lt;/td&gt;
   &lt;td&gt;0.5561
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B as teacher
   &lt;/td&gt;
   &lt;td&gt;0.4481
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4603&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.6157&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.5569&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 2: Comparison between using baseline and fine-tuned 8B as teacher model&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;using-a-fine-tuned-student-model&quot;&gt;Using a fine-tuned student model&lt;/h3&gt;

&lt;p&gt;For these experiments, we look at the effects of KD when the student model is already fine-tuned. We analyze the effects using different combinations of baseline and fine-tuned 8B and 1B models.&lt;/p&gt;

&lt;p&gt;Based on the loss graphs, using a fine-tuned teacher model results in a lower loss irrespective of whether the student model is fine-tuned or not. It’s also interesting to note that the class loss starts to increase when using a fine-tuned student model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/llama-into-torchtune/fg3.png&quot; alt=&quot;Figure 3: Comparing losses of different teacher and student model initializations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: Comparing losses of different teacher and student model initializations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using the fine-tuned student model boosts accuracy even further for truthfulqa, but the accuracy drops for hellaswag and commonsense. Using a fine-tuned teacher model and baseline student model achieved the best results on hellaswag and commonsense dataset. Based on these findings, the best configuration will change depending on which evaluation dataset and metric you are optimizing for.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;TruthfulQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;hellaswag&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;commonsense&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;mc2
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
   &lt;td&gt;acc_norm
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.1 8B
   &lt;/td&gt;
   &lt;td&gt;0.5401
   &lt;/td&gt;
   &lt;td&gt;0.5911
   &lt;/td&gt;
   &lt;td&gt;0.7915
   &lt;/td&gt;
   &lt;td&gt;0.7707
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.1 8B using LoRA
   &lt;/td&gt;
   &lt;td&gt;0.5475
   &lt;/td&gt;
   &lt;td&gt;0.6031
   &lt;/td&gt;
   &lt;td&gt;0.7951
   &lt;/td&gt;
   &lt;td&gt;0.7789
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.2 1B
   &lt;/td&gt;
   &lt;td&gt;0.4384
   &lt;/td&gt;
   &lt;td&gt;0.4517
   &lt;/td&gt;
   &lt;td&gt;0.6064
   &lt;/td&gt;
   &lt;td&gt;0.5536
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.2 1B using LoRA
   &lt;/td&gt;
   &lt;td&gt;0.4492
   &lt;/td&gt;
   &lt;td&gt;0.4595
   &lt;/td&gt;
   &lt;td&gt;0.6132
   &lt;/td&gt;
   &lt;td&gt;0.5528
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using baseline 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;0.444
   &lt;/td&gt;
   &lt;td&gt;0.4576
   &lt;/td&gt;
   &lt;td&gt;0.6123
   &lt;/td&gt;
   &lt;td&gt;0.5561
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using baseline 8B and fine-tuned 1B
   &lt;/td&gt;
   &lt;td&gt;0.4508
   &lt;/td&gt;
   &lt;td&gt;0.448
   &lt;/td&gt;
   &lt;td&gt;0.6004
   &lt;/td&gt;
   &lt;td&gt;0.5274
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;0.4481
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4603&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.6157&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.5569&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and fine-tuned 1B
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4713&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.4512
   &lt;/td&gt;
   &lt;td&gt;0.599
   &lt;/td&gt;
   &lt;td&gt;0.5233
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 3: Comparison using baseline and fine-tuned teacher and student models&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter-tuning-learning-rate&quot;&gt;Hyperparameter tuning: learning rate&lt;/h3&gt;

&lt;p&gt;By default, the recipe has a learning rate of 3e-4. For these experiments, we changed the learning rate from as high as 1e-3 to as low as 1e-5.&lt;/p&gt;

&lt;p&gt;Based on the loss graphs, all learning rates result in similar losses except for 1e-5, which has a higher KD and class loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/llama-into-torchtune/fg4.png&quot; alt=&quot;Figure 4: Comparing losses of different learning rates&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Comparing losses of different learning rates&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Based on our benchmarks, the optimal learning rate changes depending on which metric and tasks you are optimizing for.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;learning rate&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;TruthfulQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;hellaswag&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;commonsense&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;mc2
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
   &lt;td&gt;acc_norm
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.1 8B
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.5401
   &lt;/td&gt;
   &lt;td&gt;0.5911
   &lt;/td&gt;
   &lt;td&gt;0.7915
   &lt;/td&gt;
   &lt;td&gt;0.7707
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.1 8B using LoRA
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.5475
   &lt;/td&gt;
   &lt;td&gt;0.6031
   &lt;/td&gt;
   &lt;td&gt;0.7951
   &lt;/td&gt;
   &lt;td&gt;0.7789
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.2 1B
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.4384
   &lt;/td&gt;
   &lt;td&gt;0.4517
   &lt;/td&gt;
   &lt;td&gt;0.6064
   &lt;/td&gt;
   &lt;td&gt;0.5536
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.2 1B using LoRA
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.4492
   &lt;/td&gt;
   &lt;td&gt;0.4595
   &lt;/td&gt;
   &lt;td&gt;0.6132
   &lt;/td&gt;
   &lt;td&gt;0.5528
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;3e-4
   &lt;/td&gt;
   &lt;td&gt;0.4481
   &lt;/td&gt;
   &lt;td&gt;0.4603
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.6157&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.5569
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;1e-3
   &lt;/td&gt;
   &lt;td&gt;0.4453
   &lt;/td&gt;
   &lt;td&gt;0.4535
   &lt;/td&gt;
   &lt;td&gt;0.6071
   &lt;/td&gt;
   &lt;td&gt;0.5258
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;1e-4
   &lt;/td&gt;
   &lt;td&gt;0.4489
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4606&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.6156
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.5586&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;1e-5
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4547&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.4548
   &lt;/td&gt;
   &lt;td&gt;0.6114
   &lt;/td&gt;
   &lt;td&gt;0.5487
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 4: Effects of tuning learning rate&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter-tuning-kd-ratio&quot;&gt;Hyperparameter tuning: KD ratio&lt;/h3&gt;

&lt;p&gt;By default, the KD ratio is set to 0.5, which gives even weighting to both the class and KD loss. In these experiments, we look at the effects of different KD ratios, where 0 only uses the class loss and 1 only uses the KD loss.&lt;/p&gt;

&lt;p&gt;Overall, the benchmark results show that for these tasks and metrics, higher KD ratios perform slightly better.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;kd_ratio (lr=3e-4)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;TruthfulQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;hellaswag&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;commonsense&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;mc2
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
   &lt;td&gt;acc_norm
   &lt;/td&gt;
   &lt;td&gt;acc
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.1 8B
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.5401
   &lt;/td&gt;
   &lt;td&gt;0.5911
   &lt;/td&gt;
   &lt;td&gt;0.7915
   &lt;/td&gt;
   &lt;td&gt;0.7707
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.1 8B using LoRA
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.5475
   &lt;/td&gt;
   &lt;td&gt;0.6031
   &lt;/td&gt;
   &lt;td&gt;0.7951
   &lt;/td&gt;
   &lt;td&gt;0.7789
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseline Llama 3.2 1B
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.4384
   &lt;/td&gt;
   &lt;td&gt;0.4517
   &lt;/td&gt;
   &lt;td&gt;0.6064
   &lt;/td&gt;
   &lt;td&gt;0.5536
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Fine-tuned Llama 3.2 1B using LoRA
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;0.4492
   &lt;/td&gt;
   &lt;td&gt;0.4595
   &lt;/td&gt;
   &lt;td&gt;0.6132
   &lt;/td&gt;
   &lt;td&gt;0.5528
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;0.25
   &lt;/td&gt;
   &lt;td&gt;0.4485
   &lt;/td&gt;
   &lt;td&gt;0.4595
   &lt;/td&gt;
   &lt;td&gt;0.6155
   &lt;/td&gt;
   &lt;td&gt;0.5602
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;0.5
   &lt;/td&gt;
   &lt;td&gt;0.4481
   &lt;/td&gt;
   &lt;td&gt;0.4603
   &lt;/td&gt;
   &lt;td&gt;0.6157
   &lt;/td&gt;
   &lt;td&gt;0.5569
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;0.75
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4543&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.463
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.6189&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.5643
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;KD using fine-tuned 8B and baseline 1B
   &lt;/td&gt;
   &lt;td&gt;1.0
   &lt;/td&gt;
   &lt;td&gt;0.4537
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.4641&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.6177
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.5717&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 5: Effects of tuning KD ratio&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;In this blog, we presented a study on how to distill LLMs through torchtune using the forward KL divergence loss on Llama 3.1 8B and Llama 3.2 1B logits. There are many directions for future exploration to further improve performance and offer more flexibility in distillation methods.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expand KD loss offerings&lt;/strong&gt;. The KD recipe uses the forward KL divergence loss. However, aligning the student distribution to the whole teacher distribution may not be effective, as mentioned above. There are multiple papers, such as &lt;a href=&quot;https://arxiv.org/pdf/2306.08543&quot;&gt;MiniLLM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2402.03898&quot;&gt;DistiLLM&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2306.13649&quot;&gt;Generalized KD&lt;/a&gt;, that introduce new KD losses and policies to address the limitation and have shown to outperform the standard use of cross entropy with forward KL divergence loss. For instance, MiniLLM uses reverse KL divergence to prevent the student from over-estimating low-probability regions of the teacher. DistiLLM introduces a skewed KL loss and an adaptive training policy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Enable cross-tokenizer distillation&lt;/strong&gt;. The current recipe requires the teacher and student model to use the same tokenizer, which limits the ability to distill across different LLM families. There has been research on cross-tokenizer approaches (e.g. &lt;a href=&quot;https://arxiv.org/pdf/2402.12030&quot;&gt;Universal Logit Distillation&lt;/a&gt;) that we could explore.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Expand distillation to multimodal LLMs and encoder models&lt;/strong&gt;. A natural extension of the KD recipe is to expand to multimodal LLMs. Similar to deploying more efficient LLMs, there’s also a need to deploy smaller and more efficient multimodal LLMs. In addition, there has been work in demonstrating LLMs as encoder models (e.g. &lt;a href=&quot;https://arxiv.org/pdf/2404.05961&quot;&gt;LLM2Vec&lt;/a&gt;). Distillation from LLMs as encoders to smaller encoder models may also be a promising direction to explore.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Linda Wang, Evan Smothers, Kartikay Khandelwal</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Dive on CUTLASS Ping-Pong GEMM Kernel</title>
      <link href="https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/" rel="alternate" type="text/html" title="Deep Dive on CUTLASS Ping-Pong GEMM Kernel" />
      <published>2024-11-01T00:00:00-07:00</published>
      <updated>2024-11-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel</id>
      <content type="html" xml:base="https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/">&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg1.png&quot; alt=&quot;Figure 1. FP8 GEMM Throughput Comparison CUTLASS vs Triton&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. FP8 GEMM Throughput Comparison CUTLASS vs Triton&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post, we provide an overview, with relevant FP8 inference kernel benchmarking, of the CUTLASS Ping-Pong GEMM kernel.&lt;/p&gt;

&lt;p&gt;Ping-Pong is one of the fastest matmul (GEMM) kernel architectures available for the Hopper GPU architecture. Ping-Pong is a member of the Warp Group Specialized Persistent Kernels family, which includes both Cooperative and Ping-Pong variants. Relative to previous GPUs, Hopper’s substantial tensor core compute capability requires deep asynchronous software pipelining in order to achieve peak performance.&lt;/p&gt;

&lt;p&gt;The Ping-Pong and Cooperative kernels exemplify this paradigm, as the key design patterns are persistent kernels to amortize launch and prologue overhead, and ‘async everything’ with specialized warp groups with two consumers and one producer, to create a highly overlapped processing pipeline that is able to continuously supply data to the tensor cores.&lt;/p&gt;

&lt;p&gt;When the H100 (Hopper) GPU was launched, Nvidia billed it as the first truly asynchronous GPU. That statement highlights the need for H100 specific kernel architectures to also be asynchronous in order to fully maximize computational/GEMM throughput.&lt;/p&gt;

&lt;p&gt;The pingpong GEMM, introduced in CUTLASS 3.x, exemplifies this by moving all aspects of the kernel to a ‘fully asynchronous’ processing paradigm.  In this blog, we’ll showcase the core features of the ping-pong kernel design as well as showcase its performance on inference workloads vs cublas and triton split-k kernels.&lt;/p&gt;

&lt;h2 id=&quot;ping-pong-kernel-design&quot;&gt;Ping-Pong Kernel Design&lt;/h2&gt;

&lt;p&gt;Ping-Pong (or technically ‘sm90_gemm_tma_warpspecialized_pingpong’) operates with an asynchronous pipeline, leveraging warp specialization. Instead of the more classical homogeneous kernels, “warp groups” take on specialized roles. Note that a warp group consists of 4 warps of 32 threads each, or 128 total threads.&lt;/p&gt;

&lt;p&gt;On earlier architectures, latency was usually hidden by running multiple thread blocks per SM. However, with Hopper, the Tensor Core throughput is so high that it necessitates moving to deeper pipelines. These deeper pipelines then hinder running multiple thread blocks per SM. Thus, persistent thread blocks now issue collective main loops across multiple tiles and multiple warp groups. Thread block clusters are allocated based on the total SM count.&lt;/p&gt;

&lt;p&gt;For Ping-Pong, each warp group takes on a specialized role of either Data producer or Data consumer.&lt;/p&gt;

&lt;p&gt;The producer warp group focuses on producing data movement to fill the shared memory buffers (via TMA). Two other warp groups are dedicated consumers that process the math (MMA) portion with tensor cores, and then do any follow up work and write their results back to global memory (epilogue).&lt;/p&gt;

&lt;p&gt;Producer warp groups work with TMA (Tensor Memory Accelerator), and are deliberately kept as lightweight as possible. In fact, in Ping-Pong, they deliberately reduce their register resources to improve occupancy. Producers will reduce their max register counts by 40, vs consumers will increase their max register count by 232, an effect we can see in the CUTLASS source and corresponding SASS:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg2.png&quot; alt=&quot;source code&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unique to Ping-Pong, each consumer works on separate C output tiles. (For reference, the cooperative kernel is largely equivalent to Ping-Pong, but both consumer groups work on the same C output tile). Further, the two consumer warp groups then split their work between the main loop MMA and epilogue.&lt;/p&gt;

&lt;p&gt;This is shown in the below image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg3.png&quot; alt=&quot;Figure 2: An overview of the Ping-Pong Kernel pipeline. Time moves left to right.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: An overview of the Ping-Pong Kernel pipeline. Time moves left to right.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By having two consumers, it means that one can be using the tensor cores for MMA while the other performs the epilogue, and then vice-versa. This maximizes the ‘continuous usage’ of the tensor cores on each SM, and is a key part of the reason for the max throughput. The tensor cores can be continuously fed data to realize their (near) maximum compute capability. (See the bottom section of the Fig 2 illustration above).&lt;/p&gt;

&lt;p&gt;Similar to how Producer threads stay focused only on data movements, MMA threads only issue MMA instructions in order to achieve peak issue rate. MMA threads must issue multiple MMA instructions and keep these in flight against TMA wait barriers.&lt;/p&gt;

&lt;p&gt;An excerpt of the kernel code is shown below to cement the specialization aspects:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Two types of warp group 'roles' 
enum class WarpGroupRole {
      Producer = 0,
      Consumer0 = 1,
      Consumer1 = 2
    };

//warp group role assignment
auto warp_group_role = WarpGroupRole(canonical_warp_group_idx());
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;data-movement-with-producers-and-tensor-memory-accelerator&quot;&gt;Data Movement with Producers and Tensor Memory Accelerator&lt;/h2&gt;

&lt;p&gt;The producer warps focus exclusively on data movement - specifically they are kept as lightweight as possible and in fact give up some of their register space to the consumer warps (keeping only 40 registers, while consumers will get 232). Their main task is issuing TMA (tensor memory accelerator) commands to move data from Global memory to shared memory as soon as a shared memory buffer is signaled as being empty.&lt;/p&gt;

&lt;p&gt;To expand on TMA, or Tensor Memory Accelerator, TMA is a hardware component introduced with H100’s that asynchronously handles the transfer of memory from HBM (global memory) to shared memory. By having a dedicated hardware unit for memory movement, worker threads are freed to engage in other work rather than computing and managing data movement. TMA not only handles the movement of the data itself, but also calculates the required destination memory addresses, can apply any transforms (reductions, etc.) to the data and can handle layout transformations to deliver data to shared memory in a ‘swizzled’ pattern so that it’s ready for use without any bank conflicts. Finally, it can also multicast the same data if needed to other SM’s that are members of the same thread cluster. Once the data has been delivered, TMA will then signal the consumer of interest that the data is ready.&lt;/p&gt;

&lt;h2 id=&quot;cutlass-asynchronous-pipeline-class&quot;&gt;CUTLASS Asynchronous Pipeline Class&lt;/h2&gt;

&lt;p&gt;This signaling between producers and consumers is coordinated via the new Asynchronous Pipeline Class which CUTLASS describes as follows:&lt;/p&gt;

&lt;p&gt;“Implementing a persistent GEMM algorithm calls for managing dozens of different kinds of asynchronously executing operations that synchronize using multiple barriers organized as a circular list.&lt;/p&gt;

&lt;p&gt;This complexity is too much for human programmers to manage by hand.&lt;/p&gt;

&lt;p&gt;As a result, we have developed [&lt;a href=&quot;https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fcutlass%2Fblob%2Fmain%2Finclude%2Fcutlass%2Fpipeline%2Fsm90_pipeline.hpp&amp;amp;h=AT0Qy69t9mn_9VGkJlf1TkC_yCVPAQbYzHtS9it0ZVxTxVasGZfb6u-VHKReULm29NsLhp3DtuRfN4BHnzczniArsCFe8Uzj7izIx646Otyl4lEwl9jUHDhTcUq87KfS919MkadFMjq5i4qtkbe7QbgZEMbhFi0ARgvz3-u7_X0Hf3kHwQ&amp;amp;__tn__=-UK-R&amp;amp;c[0]=AT2Wep-mQJcJ7w2cBPcqoNcO9gLYx7_Qg9TGIcfKPSoo8kGdDtl70vKog1VICaOX45DhNP-Eu6pUbUl9TxGeGLQHgzyXWuxAgDQrdlOhhiOC3QRDMckh2vCi8RADkSCainRbZ5JoF7CERyij7CrhsSskOfVqQ_fvN-lKG6W2_TkvMFLe8UbKNPkzSqjzfdo&quot;&gt;CUTLASS Pipeline Async Class&lt;/a&gt;]…”&lt;/p&gt;

&lt;h2 id=&quot;barriers-and-synchronization-within-the-ping-pong-async-pipeline&quot;&gt;Barriers and synchronization within the Ping-Pong async pipeline&lt;/h2&gt;

&lt;p&gt;Producers must ‘acquire’ a given smem buffer via ‘producer_acquire’. At the start, a pipeline is empty meaning that producer threads can immediately acquire the barrier and begin moving data.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PipelineState mainloop_pipe_producer_state = cutlass::make_producer_start_state&amp;lt;MainloopPipeline&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the data movement is complete, producers issue the ‘producer_commit’ method to signal the consumer threads that data is ready.  &lt;br /&gt;
However, for Ping-Pong, this is actually a noop instruction since TMA based producer’s barriers are automatically updated by the TMA when writes are completed.&lt;/p&gt;

&lt;p&gt;consumer_wait - wait for data from producer threads (blocking).&lt;/p&gt;

&lt;p&gt;consumer_release - signal waiting producer threads that they are finished consuming data from a given smem buffer. In other words, allow producers to go to work refilling this with new data.&lt;/p&gt;

&lt;p&gt;From there, synchronization will begin in earnest where the producers will wait via the blocking producer acquire until they can acquire a lock, at which point their data movement work will repeat. This continues until the work is finished.&lt;/p&gt;

&lt;p&gt;To provide a pseudo-code overview:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//producer
While (work_tile_info.is_valid_tile) {

	collective_mainloop.dma() // fetch data with TMA
	scheduler.advance_to_next_work()
	Work_tile_info = scheduler.get_current_work()

}

// Consumer 1, Consumer 2
While (work_tile_info.is_valid_tile()) {

	collective_mainloop.mma()
	scheduler.advance_to_next_work()
	Work_tile_info = scheduler.get_current_work()

}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And a visual birds-eye view putting it all together with the underlying hardware:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg4.png&quot; alt=&quot;Figure 3: An overview of the full async pipeline for Ping-Pong&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: An overview of the full async pipeline for Ping-Pong&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-by-step-breakdown-of-ping-pong-computation-loop&quot;&gt;Step-by-Step Breakdown of Ping-Pong Computation Loop&lt;/h2&gt;

&lt;p&gt;Finally, a more detailed logical breakout of the Ping-Pong processing loop:&lt;/p&gt;

&lt;p&gt;A - Producer (DMA) warp group acquires a lock on a shared memory buffer.&lt;/p&gt;

&lt;p&gt;B - this allows it to kick off a tma cp_async.bulk request to the tma chip (via a single thread).&lt;/p&gt;

&lt;p&gt;C - TMA computes the actual shared memory addressing required, and moves the data to shared memory. As part of this, swizzling is performed in order to layout the data in smem for the fastest (no bank conflict) access.&lt;/p&gt;

&lt;p&gt;C1 - potentially, data can also be multicast to other SMs and/or it may need to wait for data from other tma multicast to complete the loading. (threadblock clusters now share shared memory across multiple SMs!)&lt;/p&gt;

&lt;p&gt;D - At this point, the barrier is updated to signal the arrival of the data to smem.&lt;/p&gt;

&lt;p&gt;E - The relevant consumer warpgroup now gets to work by issuing multiple wgmma.mma_async commands, which then read the data from smem to Tensor cores as part of it’s wgmma.mma_async matmul operation.&lt;/p&gt;

&lt;p&gt;F - the MMA accumulator values are written to register memory as the tiles are completed.&lt;/p&gt;

&lt;p&gt;G - the consumer warp group releases the barrier on the shared memory.&lt;/p&gt;

&lt;p&gt;H - the producer warp groups go to work issuing the next tma instruction to refill the now free smem buffer.&lt;/p&gt;

&lt;p&gt;I - The consumer warp group simultaneously applies any epilogue actions to the accumulator, and then move data from register to a different smem buffer.&lt;/p&gt;

&lt;p&gt;J - The consumer warp issues a cp_async command to move data from smem to global memory.&lt;/p&gt;

&lt;p&gt;The cycle repeats until the work is completed. Hopefully this provides you with a working understanding of the core concepts that power Ping-Pong’s impressive performance.&lt;/p&gt;

&lt;h2 id=&quot;microbenchmarks&quot;&gt;Microbenchmarks&lt;/h2&gt;

&lt;p&gt;To showcase some of Ping-Pong’s performance, below are some comparison charts related to our work on designing fast inference kernels.&lt;/p&gt;

&lt;p&gt;First a general benchmarking of the three fastest kernels so far (lower is better): \&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg5.png&quot; alt=&quot;Figure 4, above: Benchmark timings of FP8 GEMMs, lower is better (faster)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4, above: Benchmark timings of FP8 GEMMs, lower is better (faster)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And translating that into a relative speedup chart of Ping-Pong vs cuBLAS and Triton:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cutlass-ping-pong-gemm-kernel/fg6.png&quot; alt=&quot;Figure 5, above: Relative speedup of Ping-Pong vs the two closest kernels.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5, above: Relative speedup of Ping-Pong vs the two closest kernels.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The full source code for the Ping-Pong kernel is here (619 lines of deeply templated CUTLASS code, or to paraphrase the famous turtle meme - “it’s templates…all the way down! ):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp&quot;&gt;https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we have implemented PingPong as a CPP extension to make it easy to integrate into use with PyTorch here (along with a simple test script showing it’s usage):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/cuda/cutlass_gemm&quot;&gt;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/cuda/cutlass_gemm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, for continued learning, Nvidia has two GTC videos that dive into kernel design with CUTLASS:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51413/&quot;&gt;Developing Optimal CUDA Kernels on Hopper Tensor Cores | GTC Digital Spring 2023 | NVIDIA On-Demand&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtc24-s61198/&quot;&gt;CUTLASS: A Performant, Flexible, and Portable Way to Target Hopper Tensor Cores | GTC 24 2024 | NVIDIA On-Demand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Data movement is usually the biggest impediment to top performance for any kernel, and thus having an optimal strategy understanding of TMA (Tensor Memory Accelerator) on Hopper is vital. We previously published work on &lt;a href=&quot;https://pytorch.org/blog/hopper-tma-unit/&quot;&gt;TMA usage in Triton&lt;/a&gt;. Once features like warp specialization are enabled in Triton, we plan to do another deep dive on how Triton kernels like FP8 GEMM and FlashAttention can leverage kernel designs like Ping-Pong for acceleration on Hopper GPUs.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Less Wright, Adnan Hoque</name>
        
        
      </author>

      

      

      
        <summary type="html">In this post, we provide an overview, with relevant FP8 inference kernel benchmarking, of the CUTLASS Ping-Pong GEMM kernel.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deploying LLMs with TorchServe + vLLM</title>
      <link href="https://pytorch.org/blog/deploying-llms-torchserve-vllm/" rel="alternate" type="text/html" title="Deploying LLMs with TorchServe + vLLM" />
      <published>2024-10-31T00:00:00-07:00</published>
      <updated>2024-10-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/deploying-llms-torchserve-vllm</id>
      <content type="html" xml:base="https://pytorch.org/blog/deploying-llms-torchserve-vllm/">&lt;p&gt;The vLLM engine is currently one of the top-performing ways to execute large language models (LLM). It provides the &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html&quot;&gt;vllm serve&lt;/a&gt; command as an easy option to deploy a model on a single machine. While this is convenient, to serve these LLMs in production and at scale some advanced features are necessary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TorchServe offers these essential production features (like custom metrics and model versioning) and through its flexible custom handler design, makes it very easy to integrate features such as retrieval-augmented generation (RAG) or safeguards like &lt;a href=&quot;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&quot;&gt;Llama Guard&lt;/a&gt;. It is therefore natural to pair the vLLM engine with TorchServe to create a full-fledged LLM serving solution for production.&lt;/p&gt;

&lt;p&gt;Before going into the specifics of the integration, we will demonstrate the deployment of a Llama-3.1-70B-Instruct model using TorchServe’s vLLM docker image.&lt;/p&gt;

&lt;h2 id=&quot;quickly-getting-started-with-llama-31-on-torchserve--vllm&quot;&gt;Quickly getting started with Llama 3.1 on TorchServe + vLLM&lt;/h2&gt;

&lt;p&gt;To get started we need to build the &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docker/Dockerfile.llm&quot;&gt;new TS LLM Docker&lt;/a&gt; container image by checking out the &lt;a href=&quot;https://github.com/pytorch/serve&quot;&gt;TorchServe repository&lt;/a&gt; and execute the following command from the main folder:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build --pull . -f docker/Dockerfile.vllm -t ts/vllm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The container uses our new LLM launcher script &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts.llm_launcher&lt;/code&gt; which takes a Hugging Face model URI or local folder and spins up a local TorchServe instance with the vLLM engine running in the backend. To serve a model locally, you can create an instance of the container with the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#export token=&amp;lt;HUGGINGFACE_HUB_TOKEN&amp;gt;
docker run --rm -ti --shm-size 10g --gpus all -e HUGGING_FACE_HUB_TOKEN=$token -p 
8080:8080 -v data:/data ts/vllm --model_id meta-llama/Meta-Llama-3.1-70B-Instruct --disable_token_auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test the endpoint locally with this curl command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -X POST -d '{&quot;model&quot;:&quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;, &quot;prompt&quot;:&quot;Hello, my name is&quot;, &quot;max_tokens&quot;: 200}' --header &quot;Content-Type: application/json&quot; &quot;http://localhost:8080/predictions/model/1.0/v1/completions&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The docker stores the model weights in the local folder “data” which gets mounted as /data inside the container. To serve your custom local weights simply copy them into data and point the model_id to /data/&amp;lt;your weights&amp;gt;.&lt;/p&gt;

&lt;p&gt;Internally, the container uses our new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts.llm_launcher&lt;/code&gt; script to launch TorchServe and deploy the model. The launcher simplifies the deployment of an LLM with TorchServe into a single command line and can also be used outside the container as an efficient tool for experimentation and testing. To use the launcher outside the docker, follow the &lt;a href=&quot;https://github.com/pytorch/serve?tab=readme-ov-file#-quick-start-with-torchserve&quot;&gt;TorchServe installation steps&lt;/a&gt; and then execute the following command to spin up a 8B Llama model:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# after installing TorchServe and vLLM run
python -m ts.llm_launcher --model_id meta-llama/Meta-Llama-3.1-8B-Instruct  --disable_token_auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If multiple GPUs are available the launcher will automatically claim all visible devices and apply tensor parallelism (see &lt;a href=&quot;https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/&quot;&gt;CUDA_VISIBLE_DEVICES&lt;/a&gt; to specify which GPUs to use).&lt;/p&gt;

&lt;p&gt;While this is very convenient, it’s important to note that it does not encompass all the functionalities provided by TorchServe. For those looking to leverage more advanced features, a model archive needs to be created. While this process is a bit more involved than issuing a single command, it bears the advantage of custom handlers and versioning. While the former allows to implement RAG inside the preprocessing step, the latter lets you test different versions of a handler and model before deploying on a larger scale.&lt;/p&gt;

&lt;p&gt;Before we provide the detailed steps to create and deploy a model archive, let’s dive into the details of the vLLM engine integration.&lt;/p&gt;

&lt;h2 id=&quot;torchserves-vllm-engine-integration&quot;&gt;TorchServe’s vLLM Engine Integration&lt;/h2&gt;

&lt;p&gt;As a state-of-the-art serving framework, vLLM offers a plethora of advanced features, including PagedAttention, continuous batching, rapid model execution through CUDA graphs, and support for various quantization methods such as GPTQ, AWQ, INT4, INT8, and FP8. It also provides integration for important parameter-efficient adapter methods like LoRA and access to a wide range of model architectures including Llama and Mistral. vLLM is maintained by the vLLM team and a thriving open-source community.&lt;/p&gt;

&lt;p&gt;To facilitate quick deployment, it offers a serving mode based on FastAPI to serve LLMs over HTTP. For a tighter, more flexible integration the project also provides the &lt;a href=&quot;https://docs.vllm.ai/en/latest/dev/engine/llm_engine.html&quot;&gt;vllm.LLMEngine&lt;/a&gt; which offers interfaces to process requests on a continuous basis. We leveraged the &lt;a href=&quot;https://docs.vllm.ai/en/latest/dev/engine/async_llm_engine.html&quot;&gt;asynchronous variant&lt;/a&gt; for the integration into TorchServe.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/serve/&quot;&gt;TorchServe&lt;/a&gt; is an easy-to-use, open-source solution for serving PyTorch models in production. As a production-tested serving solution, TorchServe offers numerous benefits and features beneficial for deploying PyTorch models at scale. By combining it with the inference performance of the vLLM engine these benefits can now also be used to deploy LLMs at scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg2.png&quot; alt=&quot;Torchserve highlights and integrations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To maximize hardware utilization it is generally a good practice to batch requests from multiple users together. Historically, TorchServe only offered a synchronized mode to collect requests from various users. In this mode, TorchServe waits for a predefined amount of time (e.g., batch_delay=200ms) or until enough requests (e.g., batch_size=8) have arrived. When one of these events is triggered, the batched data gets forwarded to the backend where the model is applied to the batch, and the model output is returned to the users through the frontend. This works especially well for traditional vision models where outputs for each request usually finish at the same time.&lt;/p&gt;

&lt;p&gt;For generative use cases, particularly text generation, the assumption that requests are ready simultaneously is no longer valid, as responses will have varying lengths. Although TorchServe supports continuous batching (the ability to add and remove requests dynamically), this mode only accommodates a static maximum batch size. With the introduction of PagedAttention, even this assumption of a maximum batch size becomes more flexible, as vLLM can combine requests of different lengths in a highly adaptable manner to optimize memory utilization.&lt;/p&gt;

&lt;p&gt;To achieve optimal memory utilization, i.e., to fill unused gaps in memory (think Tetris), vLLM requires complete control over the decision of which requests to process at any given time. To provide this flexibility, we had to reevaluate how TorchServe handles user requests. Instead of the previous synchronous processing mode, we introduced an &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/examples/large_models/vllm/llama3/model-config.yaml#L7&quot;&gt;asynchronous mode&lt;/a&gt; (see diagram below) where incoming requests are directly forwarded to the backend, making them available for vLLM. The backend feeds the vllm.AsyncEngine, which can now select from all available requests. If streaming mode is enabled and the first token of a request is available, the backend will send out the result immediately and continue sending tokens until the final token is generated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/ts/torch_handler/vllm_handler.py&quot;&gt;Our implementation of the VLLMHandler&lt;/a&gt; enables users to quickly deploy any model compatible with vLLM using a configuration file, while still offering the same level of flexibility and customizability through a custom handler. Users are free to add e.g. custom &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/ts/torch_handler/vllm_handler.py#L108&quot;&gt;preprocessing&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/ts/torch_handler/vllm_handler.py#L160&quot;&gt;post-processing&lt;/a&gt; steps by inheriting from VLLMHandler and overriding the respective class methods.&lt;/p&gt;

&lt;p&gt;We also support single-node, multi-GPU &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/vllm/Readme.md#distributed-inference&quot;&gt;distributed inference&lt;/a&gt;, where we configure vLLM to use tensor parallel sharding of the model to either increase capacity for smaller models or enable larger models that do not fit on a single GPU, such as the 70B Llama variants. Previously, TorchServe only supported distributed inference using torchrun, where multiple backend worker processes were spun up to shard the model. vLLM manages the creation of these processes internally, so we &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/vllm/Readme.md#distributed-inference&quot;&gt;introduced the new “custom” parallelType to TorchServe&lt;/a&gt; which launches a single backend worker process and provides the list of assigned GPUs. The backend process can then launch its own subprocesses if necessary.&lt;/p&gt;

&lt;p&gt;To facilitate integration of TorchServe + vLLM into docker-based deployments, we provide a separate &lt;a href=&quot;https://github.com/pytorch/serve?tab=readme-ov-file#-quick-start-llm-deployment-with-docker&quot;&gt;Dockerfile&lt;/a&gt; based on &lt;a href=&quot;https://hub.docker.com/r/pytorch/torchserve&quot;&gt;TorchServe’s GPU docker image&lt;/a&gt;, with vLLM added as a dependency. We chose to keep the two separate to avoid increasing the docker image size for non-LLM deployments.&lt;/p&gt;

&lt;p&gt;Next, we will demonstrate the steps required to deploy a Llama 3.1 70B model using TorchServe + vLLM on a machine with four GPUs.&lt;/p&gt;

&lt;h2 id=&quot;step-by-step-guide&quot;&gt;Step-by-Step Guide&lt;/h2&gt;

&lt;p&gt;For this step-by-step guide we assume the &lt;a href=&quot;https://github.com/pytorch/serve/tree/master?tab=readme-ov-file#-quick-start-with-torchserve&quot;&gt;installation of TorchServe&lt;/a&gt; has finished successfully. Currently, vLLM is not a hard-dependency for TorchServe so let’s install the package using pip:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip install -U vllm==0.6.1.post2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the following steps, we will (optionally) download the model weights, explain the configuration, create a model archive, deploy and test it:&lt;/p&gt;

&lt;h3 id=&quot;1-optional-download-model-weights&quot;&gt;1. (Optional) Download Model Weights&lt;/h3&gt;

&lt;p&gt;This step is optional, as vLLM can also handle downloading the weights when the model server is started. However, pre-downloading the model weights and sharing the cached files between TorchServe instances can be beneficial in terms of storage usage and startup time of the model worker. If you choose to download the weights, use the huggingface-cli and execute:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# make sure you have logged into huggingface with huggingface-cli login before
# and have your access request for the Llama 3.1 model weights approved

huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --exclude original/*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will download the files under $HF_HOME, and you can alter the variable if you want to place the files elsewhere. Please ensure that you update the variable wherever you run TorchServe and make sure it has access to that folder.&lt;/p&gt;

&lt;h3 id=&quot;2-configure-the-model&quot;&gt;2. Configure the Model&lt;/h3&gt;

&lt;p&gt;Next, we create a YAML configuration file that contains all the necessary parameters for our model deployment. The first part of the config file specifies how the frontend should launch the backend worker, which will ultimately run the model in a handler. The second part includes parameters for the backend handler, such as the model to load, followed by various parameters for vLLM itself. For more information on possible configurations for the vLLM engine, please refer to this &lt;a href=&quot;https://docs.vllm.ai/en/latest/models/engine_args.html#engine-args&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo '
# TorchServe frontend parameters
minWorkers: 1            
maxWorkers: 1            # Set the number of worker to create a single model instance
startupTimeout: 1200     # (in seconds) Give the worker time to load the model weights
deviceType: &quot;gpu&quot; 
asyncCommunication: true # This ensures we can cummunicate asynchronously with the worker
parallelType: &quot;custom&quot;   # This lets TS create a single backend prosses assigning 4 GPUs
parallelLevel: 4

# Handler parameters
handler:
    # model_path can be a model identifier for Hugging Face hub or a local path
    model_path: &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;
    vllm_engine_config:  # vLLM configuration which gets fed into AsyncVLLMEngine
        max_num_seqs: 16
        max_model_len: 512
        tensor_parallel_size: 4
        served_model_name:
            - &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;
            - &quot;llama3&quot;
'&amp;gt; model_config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-create-the-model-folder&quot;&gt;3. Create the Model Folder&lt;/h3&gt;

&lt;p&gt;After creating the model configuration file (model_config.yaml), we will now create a model archive that includes the configuration and additional metadata, such as versioning information. Since the model weights are large, we will not include them inside the archive. Instead, the handler will access the weights by following the model_path specified in the model configuration. Note that in this example, we have chosen to use the “no-archive” format, which creates a model folder containing all necessary files. This allows us to easily modify the config files for experimentation without any friction. Later, we can also select the mar or tgz format to create a more easily transportable artifact.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir model_store
torch-model-archiver --model-name vllm --version 1.0 --handler vllm_handler --config-file model_config.yaml --archive-format no-archive --export-path model_store/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-deploy-the-model&quot;&gt;4. Deploy the Model&lt;/h3&gt;

&lt;p&gt;The next step is to start a TorchServe instance and load the model. Please note that we have disabled token authentication for local testing purposes. It is highly recommended to implement some form of authentication when publicly deploying any model.&lt;/p&gt;

&lt;p&gt;To start the TorchServe instance and load the model, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torchserve --start --ncs  --model-store model_store --models vllm --disable-token-auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can monitor the progress of the model loading through the log statements. Once the model has finished loading, you can proceed to test the deployment.&lt;/p&gt;

&lt;h3 id=&quot;5-test-the-deployment&quot;&gt;5. Test the Deployment&lt;/h3&gt;

&lt;p&gt;The vLLM integration uses an OpenAI API compatible format so we can either use a specialized tool for this purpose or curl. The JSON data we are using here includes the model identifier as well as the prompt text. Other options and their default values can be found in the &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html&quot;&gt;vLLMEngine&lt;/a&gt; docs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo '{
  &quot;model&quot;: &quot;llama3&quot;,
  &quot;prompt&quot;: &quot;A robot may not injure a human being&quot;,
  &quot;stream&quot;: 0
}' | curl --header &quot;Content-Type: application/json&quot;   --request POST --data-binary @-   http://localhost:8080/predictions/vllm/1.0/v1/completions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of the request looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;id&quot;: &quot;cmpl-cd29f1d8aa0b48aebcbff4b559a0c783&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1727211972,
  &quot;model&quot;: &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;,
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;text&quot;: &quot; or, through inaction, allow a human being to come to harm.\nA&quot;,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;stop_reason&quot;: null,
      &quot;prompt_logprobs&quot;: null
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 10,
    &quot;total_tokens&quot;: 26,
    &quot;completion_tokens&quot;: 16
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When streaming is False TorchServe will collect the full answer and send it in one go after the last token was created. If we flip the stream parameter we will receive piecewise data containing a single token in each message.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post, we explored the new, native integration of the vLLM inference engine into TorchServe. We demonstrated how to locally deploy a Llama 3.1 70B model using the ts.llm_launcher script and how to create a model archive for deployment on any TorchServe instance. Additionally, we discussed how to build and run the solution in a Docker container for deployment on Kubernetes or EKS. In future works, we plan to enable multi-node inference with vLLM and TorchServe, as well as offer a pre-built Docker image to simplify the deployment process.&lt;/p&gt;

&lt;p&gt;We would like to express our gratitude to Mark Saroufim and the vLLM team for their invaluable support in the lead-up to this blog post.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matthias Reso, Ankith Gunapal, Simon Mo, Li Ning, Hamid Shojanazeri</name>
        
        
      </author>

      

      

      
        <summary type="html">The vLLM engine is currently one of the top-performing ways to execute large language models (LLM). It provides the vllm serve command as an easy option to deploy a model on a single machine. While this is convenient, to serve these LLMs in production and at scale some advanced features are necessary.</summary>
      

      
      
    </entry>
  
</feed>



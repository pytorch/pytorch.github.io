<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-04-29T16:31:05-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">PyTorch Foundation Expands to an Umbrella Foundation to Accelerate AI Innovation</title>
      <link href="https://pytorch.org/blog/pt-foundation-expands/" rel="alternate" type="text/html" title="PyTorch Foundation Expands to an Umbrella Foundation to Accelerate AI Innovation" />
      <published>2025-04-29T00:00:00-07:00</published>
      <updated>2025-04-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-foundation-expands</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-foundation-expands/">&lt;p&gt;Today, I am thrilled to announce a significant milestone for the PyTorch Foundation: we are expanding our scope to become an umbrella foundation, allowing us to host additional projects. This expansion positions the PyTorch Foundation to foster a broader ecosystem of high-value, trusted, and innovative AI projects that cater to all stages of the AI lifecycle—from training and inference to industry-specific applications.&lt;/p&gt;

&lt;h2 id=&quot;why-expand&quot;&gt;Why Expand?&lt;/h2&gt;

&lt;p&gt;Since its inception at the Linux Foundation two and a half years ago, the PyTorch Foundation has rapidly grown, now encompassing over 30 member organizations and 120 vibrant ecosystem projects. PyTorch itself has become the framework of choice for AI researchers, practitioners, and industry leaders worldwide. Our flagship PyTorch Conference has seen attendance multiply sixfold over just two years, reflecting the community’s tremendous enthusiasm and engagement.&lt;/p&gt;

&lt;p&gt;With new initiatives such as PyTorch Day events, global community meetups, the PyTorch Ambassador Program, Open Source Program Office (OSPO) outreach, the Speaker’s Bureau, and our upcoming training and certification programs, we have significantly deepened our community’s expertise and collaboration capabilities. To sustain and accelerate this momentum, the logical next step was to expand the PyTorch Foundation into an umbrella organization.&lt;/p&gt;

&lt;h2 id=&quot;what-does-an-umbrella-foundation-mean&quot;&gt;What Does an Umbrella Foundation Mean?&lt;/h2&gt;

&lt;p&gt;By transitioning into an umbrella foundation, PyTorch will now host a range of diverse, high-quality AI and ML projects beyond PyTorch Core. These include foundation-hosted projects in two categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Platform Projects&lt;/strong&gt;: Domain-agnostic solutions essential across various stages of the AI lifecycle, such as training, inference, model optimization, and deployment as well as agentic systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vertical Projects&lt;/strong&gt;: Domain-specific projects tailored to particular industries or applications, such as biomedical imaging, protein folding, and geospatial analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Projects under our umbrella gain immediate access to vendor-neutral governance, enhanced visibility, increased funding opportunities, and robust community engagement and support.&lt;/p&gt;

&lt;h2 id=&quot;foundation-hosted-vs-ecosystem-projects&quot;&gt;Foundation-Hosted vs. Ecosystem Projects&lt;/h2&gt;

&lt;p&gt;As we expand, it’s important to clarify the distinction between foundation-hosted and ecosystem projects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Foundation-Hosted Projects&lt;/strong&gt; are projects that fall under the umbrella, they are officially governed and administered under the PyTorch Foundation’s neutral and transparent governance model. Project maintainers continue to oversee their project, and they transfer assets to the Linux Foundation for independent stewardship and adopt an open governance model significantly reducing vendor bias and encouraging broader community contributions and adoption. These projects have greater stability and longevity and integrate with the larger PyTorch community.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ecosystem Projects&lt;/strong&gt; remain independently managed but receive recognition and increased visibility by aligning themselves closely with the PyTorch Foundation community standards. These projects meet specific quality and maturity criteria but retain full independence in governance and asset management.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-join-the-pytorch-ecosystem-or-become-a-foundation-hosted-project&quot;&gt;How to Join the PyTorch Ecosystem or Become a Foundation-Hosted Project&lt;/h2&gt;

&lt;p&gt;We have clearly defined pathways for projects looking to become part of the PyTorch community:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;Ecosystem Project Status&lt;/a&gt;&lt;/strong&gt;: Projects must meet defined criteria, such as active development, comprehensive documentation, CI/CD infrastructure, clear governance, and community engagement. Approved ecosystem projects benefit from increased exposure and official recognition on the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/foundation-hosted&quot;&gt;Candidate Project Status&lt;/a&gt;&lt;/strong&gt;: Ecosystem projects aspiring to foundation-hosted status can become candidates by securing sponsorship from a PyTorch Foundation &lt;a href=&quot;/tac&quot;&gt;Technical Advisory Council (TAC)&lt;/a&gt; voting member. Candidates receive guidance on meeting all necessary governance, technical, and strategic criteria.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/foundation-hosted&quot;&gt;Foundation-Hosted Project Status&lt;/a&gt;&lt;/strong&gt;: Candidate projects demonstrating high maturity, stability, multi-platform support, security best practices, and strategic value to the PyTorch community can be approved by the TAC. These projects gain extensive benefits, including neutral trademark hosting, foundation support, marketing and events resources, governance guidance, and strategic funding opportunities.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ensuring-long-term-success-and-innovation&quot;&gt;Ensuring Long-Term Success and Innovation&lt;/h2&gt;

&lt;p&gt;By expanding our scope to become an umbrella foundation, the PyTorch Foundation is uniquely positioned to enhance collaboration, innovation, and sustained growth across the entire AI community. Our mission is clear: create a vendor-neutral, open source environment where the best AI and ML tools can thrive, benefiting users, contributors, and industry stakeholders worldwide.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“PyTorch is absolutely the foundation of the innovation happening in AI today and with projects like Llama, ChatGPT, and hundreds of thousands of open projects built on PyTorch, it has cemented itself as a critical ingredient to the world of AI. This move to create an umbrella foundation enables PyTorch to significantly expand its ecosystem both horizontally and vertically in this new era of agentic systems. I am very excited about this opportunity to take the PyTorch community to the next level!” - Joe Spisak, Product Director for PyTorch at Meta.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“PyTorch sits at the very core of AI today. Meanwhile, the depth of the AI stack has grown dramatically—evolving from enabling accelerated compute to powering fully autonomous systems. Broadening the PyTorch Foundation is a key step in keeping the AI revolution open and accessible to all, across the stack and aligned with the principles PyTorch was built on.” - Luca Antiga, CTO at Lightning AI.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We are incredibly optimistic about the opportunities ahead and excited to welcome new projects into our growing family. The PyTorch Foundation remains deeply committed to driving AI innovation forward, and together, we will continue to build the future of open source artificial intelligence.&lt;/p&gt;

&lt;p&gt;Stay tuned for more updates, announcements, and opportunities to participate!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matt White, Executive Director, PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, I am thrilled to announce a significant milestone for the PyTorch Foundation: we are expanding our scope to become an umbrella foundation, allowing us to host additional projects. This expansion positions the PyTorch Foundation to foster a broader ecosystem of high-value, trusted, and innovative AI projects that cater to all stages of the AI lifecycle—from training and inference to industry-specific applications.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Large Scale Training and Convergence with PyTorch Float8 Rowwise on Crusoe 2K H200s</title>
      <link href="https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe/" rel="alternate" type="text/html" title="Accelerating Large Scale Training and Convergence with PyTorch Float8 Rowwise on Crusoe 2K H200s" />
      <published>2025-04-28T00:00:00-07:00</published>
      <updated>2025-04-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe/">&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Hamid Shojanazeri, Vasiliy Kuznetsov, Daniel Vega-Myhre, Gokul Nadathur, Will Constable, Tianyu Liu, Tristan Rice, Driss Guessous, Josh Fromm, Luca Wehrstedt, Jiecao Yu, Sandeep Parab&lt;br /&gt;
&lt;strong&gt;Crusoe&lt;/strong&gt;: Ethan Petersen, Martin Cala, Chip Smith&lt;/p&gt;

&lt;p&gt;Working with &lt;a href=&quot;http://Crusoe.AI&quot;&gt;Crusoe.AI&lt;/a&gt; we were provided access to one of their new 2K H200 clusters in Iceland, which enabled us to showcase training accelerations of 34 - 43% at scale by leveraging TorchTitan’s HSDP2 and TorchAO’s new float8 rowwise, with comparable convergence and stability vs BF16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg1.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this post we detail the synergy of H200’s with PyTorch’s new Float8 rowwise training with TorchTitan’s FSDP2/HSDP2 and CP at scale.&lt;/p&gt;

&lt;h2 id=&quot;background---what-is-an-h200&quot;&gt;Background - what is an H200?&lt;/h2&gt;

&lt;p&gt;H200’s are an ‘enhanced’ H100, offering the exact same compute as an H100, but with two additional improvements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Larger global memory, 141GiB HBM3e vs the standard 80GiB HBM3&lt;/li&gt;
  &lt;li&gt;Memory bandwidth is ~43% faster with 4.8TB/s vs 3.35 TB/s.  The faster memory transfer has an outsized effect on training speed, especially for PyTorch’s AsyncTP.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-pytorch-float8-rowwise&quot;&gt;What is PyTorch Float8 rowwise?&lt;/h2&gt;

&lt;p&gt;Float 8 Rowwise is a finer grained resolution for Float8 vs the previous ‘tensor wise’ Float8.  It is designed to ensure finer grained accuracy to support larger workloads that tend to become more sensitive to quantization at scale and as training progresses.&lt;/p&gt;

&lt;p&gt;There are two key improvements with Float8 rowwise:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each row now maintains its own scaling factor versus a single scaling factor for the entire tensor, thus improving quantization precision.  Finer grained scaling per row helps reduce the effect of outliers (extreme values that force the quantization scaling factor to stretch and degrade the precision of the normally distributed values) and thus ensures better precision.&lt;/li&gt;
  &lt;li&gt;The scaling factor itself is now implemented by rounding down to the nearest power of 2. This has been shown to help reduce quantization errors when multiplying/dividing by the scaling factor as well as ensuring large values remain scaled to the same value in both the forward and backward passes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that other large scale models have been trained using Float8 at 2K scale with a combination of 1x128 groupwise and 128x128 blockwise, with power of 2 scaling factors.  They had the same goal of improving Float8’s precision for supporting large scale training.&lt;/p&gt;

&lt;p&gt;Thus, Float8 rowwise offers a similar promise to enable Float8 for very large scale training, but we wanted to provide proof of stability and convergence at scale, which training on the Crusoe H200 2k cluster provided initial verification thereof.&lt;/p&gt;

&lt;h2 id=&quot;showcasing-float8-rowwise-loss-convergence-vs-bf16-at-1600-and-1920-gpu-scale&quot;&gt;Showcasing Float8 Rowwise Loss convergence vs BF16 at 1600 and 1920 GPU Scale:&lt;/h2&gt;

&lt;p&gt;In order to verify comparable loss convergence, we ran two separate runs at both 1920 and then 1600 (1.6k) gpu scale using TorchTitan and Lllama3 70B.  The 1.6K GPU runs were set for 2.5k iterations, using TorchTitans’ HSDP2 and Context Parallel to enable 2D parallelism.&lt;/p&gt;

&lt;p&gt;The loss convergence tests were run using Titan’s deterministic mode - this mode effectively freezes most potential sources of variation from run to run, and thus helps ensure that the only substantial change is what we want to test, namely the loss convergence and loss curves of BF16 vs Float8 Rowwise.&lt;/p&gt;

&lt;p&gt;Note that deterministic mode also slows down training speed because various kernels will not be autotuned to maximize throughput (otherwise we risk using different kernels between runs and introducing variance).&lt;/p&gt;

&lt;p&gt;Two runs were completed, one with BF16 and the other with Float8 Rowwise.&lt;/p&gt;

&lt;p&gt;Both runs completed their assigned 2.5k iters without issue, showcasing the Crusoe cluster stability, with FP8 completing at exactly 24 hours and BF16 finishing after 31 hours, 19 minutes.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;DType
   &lt;/td&gt;
   &lt;td&gt;Time / Iters
   &lt;/td&gt;
   &lt;td&gt;Loss
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BF16
   &lt;/td&gt;
   &lt;td&gt;24 hours
   &lt;/td&gt;
   &lt;td&gt;3.15453
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;Float8 Rowwise
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;24 hours
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;2.86386
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BF16
   &lt;/td&gt;
   &lt;td&gt;31 hours, 19 minutes / 2.5K
   &lt;/td&gt;
   &lt;td&gt;2.88109
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;Float8 Rowwise
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;24 hours / 2.5K
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;2.86386
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;At the 24 hour mark, Float8 completed 2.5K iterations showcasing the comparative speed up (even in deterministic mode) of float8 training.  At the 24 hour mark, Float8 enabled a &lt;strong&gt;+9.21%&lt;/strong&gt; relative improvement in loss compared to BF16 for the same 24 hours of large scale training time.&lt;/p&gt;

&lt;p&gt;After 31 hours, 19 minutes, the BF16 run finally completed its 2.5k iters.&lt;/p&gt;

&lt;p&gt;The final loss numbers:&lt;br /&gt;
BF16 = &lt;strong&gt;2.88109&lt;/strong&gt;	
Float8 = &lt;strong&gt;2.86386&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From the loss curves we observed very similar curves at the first and last ⅓ and then a turbulent zone in the middle where both showed similar spikes, but with a slight skew to the relative timing of the spikes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg2.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a result of this, we can see that PyTorch’s Float8 rowwise offers similar convergence but over 33% speedup for the same amount of training time.&lt;/p&gt;

&lt;h2 id=&quot;long-term-training-stability-with-float8-rowwise&quot;&gt;Long Term Training stability with Float8 Rowwise&lt;/h2&gt;

&lt;p&gt;Beyond showcasing comparable convergence, we also wanted to show longer term training stability with Float8 and thus we launched a 4 day, 15K run at 256 scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg3.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown above, Float8 training ran for over 100 hours with no issues, highlighting the long term stability of Float8 Rowwise.&lt;/p&gt;

&lt;h2 id=&quot;determinism-in-torchtitan&quot;&gt;Determinism in TorchTitan&lt;/h2&gt;

&lt;p&gt;To verify determinism and to see if the spikiness in the longer runs was from scale, we also ran a smaller run comprising of 2 runs of BF16, and 1 run of Float8 at 256 scale, and with HSDP2 only (i.e. without 2D Context parallel).&lt;/p&gt;

&lt;p&gt;In this case both BF16 runs had identical curves and final loss, and we saw a similar spikiness zone for all three runs.&lt;/p&gt;

&lt;p&gt;At the 2K iteration mark, both Float8 and BF16 ending at nearly identical points:&lt;br /&gt;
BF16 *2 = &lt;strong&gt;3.28538&lt;/strong&gt;&lt;br /&gt;
Float8 rowwise = &lt;strong&gt;3.28203&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg4.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above result confirms that neither CP nor scale (2k) are responsible for spikiness in the loss as we saw similar effect at 256 scale as well. The most likely explanation for the loss spikes could be content distribution in the dataset.&lt;/p&gt;

&lt;p&gt;For the sake of determinism, the experiments were run with a serialized C4 dataset (not shuffled), meaning the spikes could be from encountering new content within the dataset.&lt;/p&gt;

&lt;h2 id=&quot;net-speedups-at-various-scales-with-float8-rowwise&quot;&gt;Net speedups at various Scales with Float8 rowwise:&lt;/h2&gt;

&lt;p&gt;We performed shorter runs at various GPU scales to understand how Float8 Rowwise would scale in terms of training acceleration as cluster sizes expanded.  Doubling in scale from 960 to 1920, Float8 continued to deliver impressive training speedups, with a range of over 34-43% gains compared to BF16. We also want to note that scaling from 1k to 2k GPUs communication overhead likely kicked in and we observed a 4% hit on throughput with BF16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg5.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the longer training runs at scale above, Float8 rowwise delivered substantial speedups with equal or even slightly improved loss endpoints while delivering 34% speedups at 1920 (DeepSeek) scale.&lt;/p&gt;

&lt;h2 id=&quot;how-can-i-use-float8-rowwise-in-my-training&quot;&gt;How can I use Float8 Rowwise in my training?&lt;/h2&gt;

&lt;p&gt;Float8 Rowwise is available now for you to use in your large scale training.  It is packaged in &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;TorchAO’s&lt;/a&gt; latest builds (0.9 and higher) and integrated into &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;TorchTitan&lt;/a&gt; natively if you want to get up and running quickly.&lt;/p&gt;

&lt;p&gt;To activate Float8 Rowwise in TorchTitan:&lt;/p&gt;

&lt;p&gt;First enable the model converter to hotswap the nn.linears into float8 linear layers in your models .toml file - see line 29:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg6.png&quot; alt=&quot;code&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Secondly, specify the ‘rowwise’ float8 recipe - see line 72:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg7.png&quot; alt=&quot;code&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that you have three choices for the ‘recipe_name’:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rowwise which is the recommended default,&lt;/li&gt;
  &lt;li&gt;tensorwise (the older style float8) and&lt;/li&gt;
  &lt;li&gt;rowwise_with_gw_hp.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The gw_hp rowwise option keeps the gradients to the weights in BF16 precision during the backwards pass, and this can further enhance float8 precision for extremely sensitive workloads.  But, it can ironically be a bit more performant than generic rowwise if the majority of the matmul sizes in your model are smaller (with an estimated tipping point at roughly 13-16K dimensions on H100).&lt;/p&gt;

&lt;p&gt;Thus while we recommend rowwise as the default, it may be worth comparing with gw_hp on your model to verify which provides the best performance, with an upside of even greater precision.&lt;/p&gt;

&lt;p&gt;By toggling the model converter on and off with a #, you can directly compare training acceleration between BF16 and Float8 Rowwise to understand the potential speedups for your own training.&lt;/p&gt;

&lt;h2 id=&quot;future-updates&quot;&gt;Future Updates:&lt;/h2&gt;

&lt;p&gt;We’ll have an additional update coming showcasing multiple improvements for Pipeline Parallel and Async Distributed Checkpointing so please stay tuned.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta and Crusoe</name>
        
        
      </author>

      

      

      
        <summary type="html">Meta: Less Wright, Hamid Shojanazeri, Vasiliy Kuznetsov, Daniel Vega-Myhre, Gokul Nadathur, Will Constable, Tianyu Liu, Tristan Rice, Driss Guessous, Josh Fromm, Luca Wehrstedt, Jiecao Yu, Sandeep Parab Crusoe: Ethan Petersen, Martin Cala, Chip Smith</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate PyTorch 2.7 on Intel® GPUs</title>
      <link href="https://pytorch.org/blog/pytorch-2-7-intel-gpus/" rel="alternate" type="text/html" title="Accelerate PyTorch 2.7 on Intel® GPUs" />
      <published>2025-04-25T00:00:00-07:00</published>
      <updated>2025-04-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7-intel-gpus</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7-intel-gpus/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/pytorch-2-7/&quot;&gt;PyTorch 2.7&lt;/a&gt; continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.&lt;/p&gt;

&lt;h2 id=&quot;incremental-improvements-of-intel-gpu-support-in-pytorch&quot;&gt;Incremental improvements of Intel GPU support in PyTorch&lt;/h2&gt;

&lt;p&gt;Since PyTorch 2.4, we’ve made steady improvements to Intel GPU support with each release. With PyTorch 2.7, we are excited to share that we have established a solid foundation to have Intel GPU work in both graph mode (torch.compile) and eager mode on Windows and Linux. This includes a wide range of Intel GPU products, many of which you may already access. We hope these enhancements will unlock more ubiquitous hardware for your AI research and development.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over time, we have expanded Intel GPU Support across Windows and Linux, including these products:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html&quot;&gt;Intel® Arc™ A-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/support/articles/000097599/processors.html&quot;&gt;Intel® Core™ Ultra Processors with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-series-2-mobile-product-brief.html&quot;&gt;Intel® Core™ Ultra Mobile Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-desktop-processors-series-2-brief.html&quot;&gt;Intel® Core™ Ultra Desktop Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html&quot;&gt;Intel® Data Center GPU Max Series&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Simpler installation&lt;/a&gt; of torch-xpu PIP wheels and an effortless setup experience.&lt;/li&gt;
  &lt;li&gt;High ATen operation coverage with SYCL and oneDNN for smooth eager mode support with functionality and performance.&lt;/li&gt;
  &lt;li&gt;Notable speedups with torch.compile through default TorchInductor and Triton backend, proved by measurable performance gains with Hugging Face, TIMM, and TorchBench benchmarks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out the detailed advancements in these related release blogs:&lt;a href=&quot;https://pytorch.org/blog/intel-gpus-pytorch-2-4/&quot;&gt; PyTorch 2.4&lt;/a&gt;,&lt;a href=&quot;https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/&quot;&gt; PyTorch 2.5&lt;/a&gt;, and&lt;a href=&quot;https://pytorch.org/blog/unlocking-pt-2-6-intel/&quot;&gt; PyTorch 2.6&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-new-in-pytorch-27&quot;&gt;What’s New in PyTorch 2.7&lt;/h2&gt;

&lt;p&gt;These are the features in PyTorch 2.7  that were added to help accelerate performance on Intel GPUs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve scaled dot-product attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;br /&gt;
With the new SDPA optimization for Intel GPUs on PyTorch 2.7, Stable Diffusion float16 inference achieved up to 3x gain over PyTorch 2.6 release on Intel® Arc™ B580 Graphics and Intel® Core™ Ultra 7 Processor 258V with Intel® Arc™ Graphics 140V on eager mode. See Figure 1 below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg1.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. PyTorch 2.7 Stable Diffusion Performance Gains Over PyTorch 2.6&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux. With this, Intel GPUs became the first accelerator to support torch.compile on Windows. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; Windows tutorial&lt;/a&gt; for details.&lt;br /&gt;
Graph model (torch.compile) is enabled in Windows 11 for the first time across Intel GPUs, delivering the performance advantages over eager mode as on Linux by PyTorch 2.7. The latest performance data was measured on top of PyTorch Dynamo Benchmarking Suite using Intel® Arc™ B580 Graphics on Windows showcase torch.compile speedup ratio over eager mode as shown in Figure 2. Both training and inference achieved similar significant improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg2.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2. Torch.compile Performance Gains Over Eager Mode on Windows&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide full graph mode quantization pipelines with enhanced computational efficiency. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; PT2E tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable AOTInductor and torch.export on Linux to simplify deployment workflows. Refer to&lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt; AOTInductor tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis. Refer to the&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#pytorch-profiler&quot;&gt; PyTorch profiler tutorial&lt;/a&gt; for details.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Review the &lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Getting Started on Intel GPU Guide&lt;/a&gt; for a tour of the environment setup and a quick start on Intel GPUs.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Looking ahead, we will continue the Intel GPU upstream efforts in future PyTorch releases to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attain state-of-the-art PyTorch-native performance to showcase competitive GEMM computational efficiency for torch.compile, and enhance performance for LLM models through FlexAttention and lower precision data types.&lt;/li&gt;
  &lt;li&gt;Broaden feature compatibility by delivering distributed XCCL backend support for Intel® Data Center GPU Max Series.&lt;/li&gt;
  &lt;li&gt;Expand accelerator support across core PyTorch ecosystem components including torchao, torchtune, and torchtitan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow along in the &lt;a href=&quot;https://dev-discuss.pytorch.org/t/intel-gpu-cpu-enabling-status-and-feature-plan-2025-h1-update/2913&quot;&gt;PyTorch Dev Discussion&lt;/a&gt; to learn more about Intel GPU &amp;amp; CPU enabling status and features. As we get further along, we will create tickets on GitHub to document our progress.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blog, we reviewed the Intel GPU upstream progress starting in PyTorch 2.4 and highlighted the new features of PyTorch 2.7 that accelerate AI workload performance across various Intel GPUs. These new features, especially SDPA on Windows, achieved up to 3x inference (Stable Diffusion, float16) gain over PyTorch 2.6 release on Intel Arc B580 Graphics and Intel Core Ultra 7 Processor 258V with Intel Arc Graphics 140V. Also, torch.compile on Windows delivers similar performance advantages over eager mode on Dynamo benchmarks as on Linux.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We want to thank the following PyTorch maintainers for their technical discussions and insights: &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;h2 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h2&gt;

&lt;p&gt;Measurement on Intel Core Ultra 7 258V: 2200 MHz, 8 Core(s), 8 Logical Processor(s) with Intel Arc 140V GPU (16GB), GPU memory 18.0 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Pro - 24H2. And Intel Core Ultra 5 245KF: 4200 MHz, 14 Core(s), 14 Logical Processor(s), Intel Arc B580 Graphics, dedicated GPU memory 12.0 GB, shared GPU memory 15.8 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Enterprise LTSC - 24H2. Test by Intel on Apr 8th, 2025.&lt;/p&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;ai-disclaimer&quot;&gt;AI Disclaimer&lt;/h2&gt;

&lt;p&gt;AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the Intel PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.7 continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.7 Release</title>
      <link href="https://pytorch.org/blog/pytorch-2-7/" rel="alternate" type="text/html" title="PyTorch 2.7 Release" />
      <published>2025-04-23T00:00:00-07:00</published>
      <updated>2025-04-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.7 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.7.0&quot;&gt;release notes&lt;/a&gt;)! This release features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;support for the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/&quot;&gt;NVIDIA Blackwell GPU architecture&lt;/a&gt; and pre-built wheels for &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA 12.8&lt;/a&gt; across Linux x86 and arm64 architectures.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; support for Torch Function Modes which enables users to override any *torch.** operation  to implement custom user-defined behavior.&lt;/li&gt;
  &lt;li&gt;Mega Cache which allows users to have end-to-end portable caching for torch;&lt;/li&gt;
  &lt;li&gt;new features for FlexAttention - LLM first token processing, LLM throughput mode optimization and Flex Attention for Inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release is composed of 3262 commits from 457 contributors since PyTorch 2.6. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.7. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Torch.Compile support for Torch Function Modes
   &lt;/td&gt;
   &lt;td&gt;NVIDIA Blackwell Architecture Support
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Mega Cache
   &lt;/td&gt;
   &lt;td&gt;PyTorch Native Context Parallel
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Enhancing Intel GPU Acceleration
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;first token processing&lt;/span&gt; on x86 CPUs 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;throughput mode optimization&lt;/span&gt; on x86 CPUs
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Foreach Map
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Flex Attention for Inference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Prologue Fusion Support in Inductor
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile-support-for-torch-function-modes&quot;&gt;[Beta] Torch.Compile support for Torch Function Modes&lt;/h3&gt;

&lt;p&gt;This feature enables users to override any *torch.** operation to implement custom user-defined behavior. For example, ops can be rewritten to accommodate a specific backend. This is used in FlexAttention to re-write indexing ops.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_torch_function_modes.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-mega-cache&quot;&gt;[Beta] Mega Cache&lt;/h3&gt;

&lt;p&gt;Mega Cache allows users to have end-to-end portable caching for torch. The intended use case is after compiling and executing a model, the user calls &lt;em&gt;torch.compiler.save_cache_artifacts()&lt;/em&gt; which will return the compiler artifacts in a portable form. Later, potentially on a different machine, the user may call &lt;em&gt;torch.compiler.load_cache_artifacts()&lt;/em&gt; with these artifacts to pre-populate the torch.compile caches in order to jump-start their cache.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html#torch-compile-end-to-end-caching-mega-cache&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-nvidia-blackwell-architecture-support&quot;&gt;[Prototype] NVIDIA Blackwell Architecture Support&lt;/h3&gt;

&lt;p&gt;PyTorch 2.7 introduces support for NVIDIA’s new Blackwell GPU architecture and ships pre-built wheels for CUDA 12.8. For more details on CUDA 12.8 see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA Toolkit Release&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Core components and libraries including cuDNN, NCCL, and CUTLASS have been upgraded to ensure compatibility with Blackwell platforms.&lt;/li&gt;
  &lt;li&gt;PyTorch 2.7 includes Triton 3.3, which adds support for the Blackwell architecture with torch.compile compatibility.&lt;/li&gt;
  &lt;li&gt;To utilize these new features, install PyTorch with CUDA 12.8 using: &lt;em&gt;pip install torch==2.7.0 –index-url https://download.pytorch.org/whl/cu128&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More context can also be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/145949&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-pytorch-native-context-parallel&quot;&gt;[Prototype] PyTorch Native Context Parallel&lt;/h3&gt;

&lt;p&gt;PyTorch Context Parallel API allows users to create a Python context so that every *torch.nn.functional.scaled_dot_product_attention() *call within will run with context parallelism. Currently,  PyTorch Context Parallel supports 3 attention backends: 1. Flash attention; 2. Efficient attention;  and 3. cuDNN attention.&lt;/p&gt;

&lt;p&gt;As an example, this is &lt;a href=&quot;https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082&quot;&gt;used within TorchTitan as the Context Parallel solution for LLM training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/prototype/context_parallel.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-enhancing-intel-gpu-acceleration&quot;&gt;[Prototype] Enhancing Intel GPU Acceleration&lt;/h3&gt;

&lt;p&gt;This latest release introduces enhanced performance optimizations for Intel GPU architectures. These improvements accelerate workloads across various Intel GPUs through the following key enhancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux.&lt;/li&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide a full graph mode quantization pipelines with enhanced computational efficiency.&lt;/li&gt;
  &lt;li&gt;Improve Scaled Dot-Product Attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;/li&gt;
  &lt;li&gt;Enable AOTInuctor and torch.export on Linux to simplify deployment workflows.&lt;/li&gt;
  &lt;li&gt;Implement more Aten operators to enhance the continuity of operators execution on Intel GPU and increase the performance on Intel GPU in eager mode.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis.&lt;/li&gt;
  &lt;li&gt;Expand the Intel GPUs support to &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra Series 2 with Intel® Arc™ Graphics&lt;/a&gt;, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series graphics&lt;/a&gt; on both Windows and Linux.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See also the tutorials &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_xpu_inductor.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-first-token-processing-on-x86-cpus&quot;&gt;[Prototype] FlexAttention LLM first token processing on x86 CPUs&lt;/h3&gt;

&lt;p&gt;FlexAttention x86 CPU support was first introduced in PyTorch 2.6, offering optimized implementations — such as PageAttention, which is critical for LLM inference—via the TorchInductor C++ backend. In PyTorch 2.7, more attention variants for first token processing of LLMs are supported. With this feature, users can have a smoother experience running FlexAttention on x86 CPUs, replacing specific &lt;em&gt;scaled_dot_product_attention&lt;/em&gt; operators with a unified FlexAttention API, and benefiting from general support and good performance when using torch.compile.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-throughput-mode-optimization&quot;&gt;[Prototype] FlexAttention LLM throughput mode optimization&lt;/h3&gt;

&lt;p&gt;The performance of FlexAttention on x86 CPUs for LLM inference throughput scenarios has been further improved by adopting the new C++ micro-GEMM template ability. This addresses the performance bottlenecks for large batch size scenarios present in PyTorch 2.6. With this enhancement, users can transparently benefit from better performance and a smoother experience when using FlexAttention APIs and torch.compile for LLM throughput serving on x86 CPUs.&lt;/p&gt;

&lt;h3 id=&quot;prototype-foreach-map&quot;&gt;[Prototype] Foreach Map&lt;/h3&gt;

&lt;p&gt;This feature uses torch.compile to allow users to apply any pointwise or user-defined function (e.g. torch.add) to lists of tensors, akin to the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops. The main advantage over the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops is that any mix of scalars or lists of tensors can be supplied as arguments, and even user-defined python functions can be lifted to apply to lists of tensors. Torch.compile will automatically generate a horizontally fused kernel for optimal performance.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/recipes/foreach_map.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flex-attention-for-inference&quot;&gt;[Prototype] Flex Attention for Inference&lt;/h3&gt;

&lt;p&gt;In release 2.5.0, &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention&lt;/a&gt;* torch.nn.attention.flex_attention*  was introduced for ML researchers who’d like to customize their attention kernels without writing kernel code. This update introduces a decoding backend optimized for inference, supporting GQA and PagedAttention, along with feature updates including nested jagged tensor support, performance tuning guides and trainable biases support.&lt;/p&gt;

&lt;h3 id=&quot;prototype-prologue-fusion-support-in-inductor&quot;&gt;[Prototype] Prologue Fusion Support in Inductor&lt;/h3&gt;

&lt;p&gt;Prologue fusion optimizes matrix multiplication (matmul) operations by fusing operations that come before the matmul into the matmul kernel itself, improving performance by reducing global memory bandwidth.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.7 (release notes)! This release features:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/" rel="alternate" type="text/html" title="Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers" />
      <published>2025-04-08T00:00:00-07:00</published>
      <updated>2025-04-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-whisper-arm-w-transformers</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/">&lt;p&gt;Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.&lt;/p&gt;

&lt;p&gt;A new Arm Learning Path is now available that explains how to accelerate Whisper on Arm-based cloud instances using PyTorch and Hugging Face transformers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Run Whisper on Arm?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Arm processors are popular in cloud infrastructure for their efficiency, performance, and cost-effectiveness. With major cloud providers such as AWS, Azure, and Google Cloud offering Arm-based instances, running machine learning workloads on this architecture is becoming increasingly attractive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What You’ll Learn&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/&quot;&gt;Arm Learning Path&lt;/a&gt; provides a structured approach to setting up and accelerating Whisper on Arm-based cloud instances. Here’s what you cover:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Set Up Your Environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before running Whisper, you must set up your development environment. The learning path walks you through setting up an Arm-based cloud instance and installing all dependencies, such as PyTorch, Transformers, and ffmpeg.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Run Whisper with PyTorch and Hugging Face Transformers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the environment is ready, you will use the Hugging Face transformer library with PyTorch to load and execute Whisper for speech-to-text conversion. The tutorial provides a step-by-step approach for processing audio files and generating audio transcripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Measure and Evaluate Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To ensure efficient execution, you learn how to measure transcription speeds and compare different optimization techniques. The guide provides insights into interpreting performance metrics and making informed decisions on your deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try it Yourself&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Upon completion of this tutorial, you know how to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deploy Whisper on an Arm-based cloud instance.&lt;/li&gt;
  &lt;li&gt;Implement performance optimizations for efficient execution.&lt;/li&gt;
  &lt;li&gt;Evaluate transcription speeds and optimize further based on results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Try the live demo today&lt;/strong&gt; and see audio transcription in action on Arm: &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/_demo/&quot;&gt;Whisper on Arm Demo&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pareena Verma, Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Day France 2025: Call For Proposals Open</title>
      <link href="https://pytorch.org/blog/pt-day-france-cfp/" rel="alternate" type="text/html" title="PyTorch Day France 2025: Call For Proposals Open" />
      <published>2025-04-03T00:00:00-07:00</published>
      <updated>2025-04-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-day-france-cfp</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-day-france-cfp/">&lt;p&gt;We’re pleased to announce &lt;strong&gt;&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-day-france/&quot;&gt;PyTorch Day France 2025&lt;/a&gt;&lt;/strong&gt;, a dedicated gathering of the PyTorch community held &lt;strong&gt;7 May 2025&lt;/strong&gt; in &lt;strong&gt;Paris, France&lt;/strong&gt;. Proudly hosted by the &lt;strong&gt;PyTorch Foundation&lt;/strong&gt; and co-located with &lt;strong&gt;&lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris 2025&lt;/a&gt;&lt;/strong&gt;, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.&lt;/p&gt;

&lt;p&gt;Whether you’re building cutting-edge models or contributing to the ecosystem, PyTorch Day France is your opportunity to connect, collaborate, and help shape the future of deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-day-cfp.png&quot; alt=&quot;PT Day CFP&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-attend&quot;&gt;Why Attend?&lt;/h2&gt;

&lt;p&gt;Set in the vibrant atmosphere of STATION F, the world’s largest startup campus, PyTorch Day France will offer a full day of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insightful Technical Talks&lt;/li&gt;
  &lt;li&gt;Interactive Discussions&lt;/li&gt;
  &lt;li&gt;Engaging Poster Sessions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The event is designed to foster open exchange across the PyTorch ecosystem, providing a space to learn from peers, share practical insights, and explore the latest research and applications in AI.&lt;/p&gt;

&lt;h2 id=&quot;submit-a-proposal&quot;&gt;Submit a Proposal&lt;/h2&gt;

&lt;p&gt;We are currently accepting proposals for talks. If you have a project, idea, or research story you’d like to share with the PyTorch community, we want to hear from you.&lt;/p&gt;

&lt;p&gt;📩 Email your &lt;strong&gt;talk title and abstract&lt;/strong&gt; to &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt; for consideration.&lt;/p&gt;

&lt;h2 id=&quot;registration&quot;&gt;Registration&lt;/h2&gt;

&lt;p&gt;To register for PyTorch Day France, please visit the &lt;strong&gt;GOSIM AI Paris website&lt;/strong&gt;, and use the code PYTORCHFRIEND to receive 25% off.&lt;/p&gt;

&lt;p&gt;👉 &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;https://paris2025.gosim.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We encourage early registration to secure your spot and ensure access to both PyTorch Day France and the broader GOSIM AI Paris programming.&lt;/p&gt;

&lt;h2 id=&quot;venue&quot;&gt;Venue&lt;/h2&gt;

&lt;p&gt;STATION F&lt;br /&gt;
5 Parv. Alan Turing, 75013 Paris, France&lt;br /&gt;
A landmark of innovation and entrepreneurship in the heart of Paris.&lt;/p&gt;

&lt;h2 id=&quot;travel-and-accommodations&quot;&gt;Travel and Accommodations&lt;/h2&gt;

&lt;p&gt;Participants are responsible for their own travel and lodging. For those arriving internationally, Paris Charles de Gaulle Airport is approximately 38.4 km from STATION F. Additional information about accommodations and transportation may be available on the &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions?&lt;/h2&gt;

&lt;p&gt;For any inquiries, please contact us at &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We look forward to welcoming the PyTorch community to Paris this May for a day of collaboration, learning, and open source AI innovation.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re pleased to announce PyTorch Day France 2025, a dedicated gathering of the PyTorch community held 7 May 2025 in Paris, France. Proudly hosted by the PyTorch Foundation and co-located with GOSIM AI Paris 2025, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Day China 2025 Call for Proposals Open</title>
      <link href="https://pytorch.org/blog/pt-day-china-2025-cfp/" rel="alternate" type="text/html" title="PyTorch Day China 2025 Call for Proposals Open" />
      <published>2025-03-19T00:00:00-07:00</published>
      <updated>2025-03-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-day-china-2025-cfp</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-day-china-2025-cfp/">&lt;p&gt;We’re excited to announce the &lt;strong&gt;first-ever &lt;a href=&quot;https://www.lfasiallc.com/pytorch-day-china/&quot;&gt;PyTorch Day China&lt;/a&gt;&lt;/strong&gt;! This new event, hosted by the PyTorch Foundation, will take place on &lt;strong&gt;June 7 in Beijing, China&lt;/strong&gt;, bringing together AI practitioners, researchers, and industry professionals to explore the latest advancements in open source AI and machine learning. Co-located with the &lt;strong&gt;BAAI Conference&lt;/strong&gt;, PyTorch Day China is a chance to connect with the community, share knowledge, and help shape the future of deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-day-china-2025-cfp.jpg&quot; alt=&quot;PyTorch Day China 2025 Call for Proposals Open&quot; style=&quot;max-width:500px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-submit-a-proposal&quot;&gt;Why Submit a Proposal?&lt;/h2&gt;

&lt;p&gt;PyTorch Day China offers a platform for AI practitioners and researchers to showcase their work, exchange ideas, and connect with others in the community. If you’re working on innovative applications, tools, or research in the PyTorch ecosystem, we encourage you to share your expertise.&lt;/p&gt;

&lt;h2 id=&quot;topics-for-submission&quot;&gt;Topics for Submission:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;AI Applications and Use Cases&lt;/li&gt;
  &lt;li&gt;Core PyTorch Framework&lt;/li&gt;
  &lt;li&gt;DL Compilers and Kernel Authoring&lt;/li&gt;
  &lt;li&gt;Edge AI and On-Device&lt;/li&gt;
  &lt;li&gt;Ethical AI, Governance, and Regulation&lt;/li&gt;
  &lt;li&gt;Generative AI and Large Language Models (LLMs) with PyTorch&lt;/li&gt;
  &lt;li&gt;Open Source Collaboration, Education, and Community Building&lt;/li&gt;
  &lt;li&gt;Optimization for Training and Inference&lt;/li&gt;
  &lt;li&gt;PyTorch on Accelerator Hardware&lt;/li&gt;
  &lt;li&gt;PyTorch Ecosystem and Tools&lt;/li&gt;
  &lt;li&gt;PyTorch in Research and Academia&lt;/li&gt;
  &lt;li&gt;Performance Measurement and Benchmarking&lt;/li&gt;
  &lt;li&gt;Scaling Training and Inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The submission deadline is April 13. Submit and learn more here:&lt;/strong&gt; &lt;a href=&quot;https://www.lfasiallc.com/pytorch-day-china/call-for-proposals-cfp/&quot;&gt;https://www.lfasiallc.com/pytorch-day-china/call-for-proposals-cfp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-attend&quot;&gt;Why Attend?&lt;/h2&gt;

&lt;p&gt;PyTorch Day China will feature &lt;strong&gt;technical talks, discussions, and poster sessions&lt;/strong&gt; that highlight real-world applications and developments in AI and machine learning. Attendees will have the opportunity to learn from experts, contribute to the open source community, and engage with fellow PyTorch users. Registration information will be available in April.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; June 7, 2025&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt; Zhongguancun Exhibition Center, Beijing, China&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Address:&lt;/strong&gt; 索家坟, Hai Dian Qu, Bei Jing Shi, China, 100080&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Co-located with:&lt;/strong&gt; BAAI Conference&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;travel-information&quot;&gt;Travel Information&lt;/h2&gt;

&lt;p&gt;The venue, &lt;strong&gt;Zhongguancun Exhibition Center&lt;/strong&gt;, is approximately &lt;strong&gt;39 km from Beijing International Airport&lt;/strong&gt;. More details on travel and accommodation will be available on the &lt;strong&gt;BAAI Conference website&lt;/strong&gt; and updated here as they become available.&lt;/p&gt;

&lt;h2 id=&quot;have-questions&quot;&gt;Have Questions?&lt;/h2&gt;

&lt;p&gt;For inquiries, please contact &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Submit your proposal by &lt;strong&gt;April 13&lt;/strong&gt; and join the conversation shaping the future of PyTorch.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re excited to announce the first-ever PyTorch Day China! This new event, hosted by the PyTorch Foundation, will take place on June 7 in Beijing, China, bringing together AI practitioners, researchers, and industry professionals to explore the latest advancements in open source AI and machine learning. Co-located with the BAAI Conference, PyTorch Day China is a chance to connect with the community, share knowledge, and help shape the future of deep learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine</title>
      <link href="https://pytorch.org/blog/sglang-joins-pytorch/" rel="alternate" type="text/html" title="SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine" />
      <published>2025-03-19T00:00:00-07:00</published>
      <updated>2025-03-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/sglang-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/sglang-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/sglang-join-pytorch/fg1.png&quot; alt=&quot;sglang logo&quot; style=&quot;max-width:400px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the SGLang project has been integrated into the PyTorch ecosystem! This integration ensures that SGLang aligns with PyTorch’s standards and practices, providing developers with a reliable and community-supported framework for fast and flexible serving of LLMs.&lt;/p&gt;

&lt;p&gt;To view the PyTorch Ecosystem, see the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; and learn more about how projects can &lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;join the PyTorch Ecosystem&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-sglang&quot;&gt;About SGLang&lt;/h2&gt;

&lt;p&gt;SGLang is a fast-serving engine for large language models and vision language models. It makes the interaction with models faster and more controllable by co-designing the backend runtime and frontend language.&lt;/p&gt;

&lt;p&gt;The core features include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fast Backend Runtime: Provides efficient serving with RadixAttention for prefix caching, zero-overhead CPU scheduler, continuous batching, token attention (paged attention), speculative decoding, tensor parallelism, chunked prefill, structured outputs, and quantization (FP8/INT4/AWQ/GPTQ).&lt;/li&gt;
  &lt;li&gt;Flexible Frontend Language: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.&lt;/li&gt;
  &lt;li&gt;Extensive Model Support: Supports a wide range of generative models (Llama, Gemma, Mistral, Qwen, DeepSeek, LLaVA, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.&lt;/li&gt;
  &lt;li&gt;Active Community: SGLang is open source and backed by an active community with industry adoption.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SGLang is famous for its fast speed. It can often significantly outperform other state-of-the-art frameworks in terms of serving throughput and latency. You can learn more about the underlying techniques from the past release blog posts: &lt;a href=&quot;https://lmsys.org/blog/2024-07-25-sglang-llama3/&quot;&gt;v0.2 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-09-04-sglang-v0-3/&quot;&gt;v0.3 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-12-04-sglang-v0-4/&quot;&gt;v0.4 blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SGLang has been widely adopted by leading industry companies and frontier research labs. For example, xAI uses SGLang to serve its flagship model, &lt;a href=&quot;https://grok.com/&quot;&gt;Grok 3&lt;/a&gt;, which is currently the best model according to the Chatbot Arena leaderboard. Microsoft Azure uses SGLang to serve &lt;a href=&quot;https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/running-deepseek-r1-on-a-single-ndv5-mi300x-vm/4372726&quot;&gt;DeepSeek R1&lt;/a&gt; on AMD GPUs, which is currently the best open source model.&lt;/p&gt;

&lt;h2 id=&quot;serving-deepseek-models&quot;&gt;Serving DeepSeek Models&lt;/h2&gt;

&lt;p&gt;You can easily launch a Docker container to serve a DeepSeek model with the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Pull the latest image
docker pull lmsysorg/sglang:latest

# Launch a server
docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --ipc=host --network=host --privileged lmsysorg/sglang:latest \
    python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code --port 30000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can query the server with the OpenAI-compatible API&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import openai
client = openai.Client(base_url=f&quot;http://127.0.0.1:30000/v1&quot;, api_key=&quot;None&quot;)

response = client.chat.completions.create(
    model=&quot;deepseek-ai/DeepSeek-V3&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List 3 countries and their capitals.&quot;},
    ],
    temperature=0,
    max_tokens=64,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server launch command above works for 8xH200. You can find detailed instructions for other hardware (MI300X, H100, A100, H20, L40S) at https://docs.sglang.ai/references/deepseek.html.&lt;/p&gt;

&lt;p&gt;SGLang integrates DeepSeek-specific optimizations, such as MLA throughput optimizations, MLA-optimized kernels, data-parallel attention, multi-token prediction, and DeepGemm, making it the top choice for serving DeepSeek models by dozens of &lt;a href=&quot;https://x.com/lmsysorg/status/1887262321636221412&quot;&gt;companies&lt;/a&gt;, including AMD, NVIDIA, and many cloud providers. The team is actively working on integrating more optimizations following the 2025 H1 roadmap below.&lt;/p&gt;

&lt;h2 id=&quot;serving-llama-models&quot;&gt;Serving Llama Models&lt;/h2&gt;

&lt;p&gt;Similarly, you can launch the server for a Llama 3.1 text model with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or a Llama 3.2 multimodal model with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-11B-Vision-Instruct  --chat-template=llama_3_vision
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;roadmap&quot;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;This year, the SGLang team will continue to push the boundaries of system efficiency. You can find the roadmap of 2025H1 &lt;a href=&quot;https://github.com/sgl-project/sglang/issues/4042&quot;&gt;here&lt;/a&gt;. The focus is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Throughput-oriented large-scale deployment similar to the DeepSeek inference system&lt;/li&gt;
  &lt;li&gt;Long context optimizations&lt;/li&gt;
  &lt;li&gt;Low latency speculative decoding&lt;/li&gt;
  &lt;li&gt;Reinforcement learning training framework integration&lt;/li&gt;
  &lt;li&gt;Kernel optimizations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;SGLang has been deployed to large-scale production, generating trillions of tokens every day. It has an active community with over three hundred contributors on GitHub. It is supported by the following institutions: AMD, Atlas Cloud, Baseten, Cursor, DataCrunch, Etched, Hyperbolic, iFlytek, Jam &amp;amp; Tea Studios, LinkedIn, LMSYS, Meituan, Nebius, Novita AI, NVIDIA, RunPod, Stanford, UC Berkeley, UCLA, xAI, and 01.AI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sglang-join-pytorch/fg2.png&quot; alt=&quot;logos&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re excited to welcome SGLang to the PyTorch ecosystem. SGLang accelerates the serving of large language and vision language models. It’s widely adopted by industry, powering the large-scale online serving of frontier models like Grok and DeepSeek.&lt;/p&gt;

&lt;p&gt;We invite you to explore the &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main&quot;&gt;SGLang GitHub repo&lt;/a&gt;, join the &lt;a href=&quot;https://slack.mindee.com/&quot;&gt;community on Slack&lt;/a&gt;, and reach out to &lt;a href=&quot;mailto:contact@sglang.ai&quot;&gt;contact@sglang.ai&lt;/a&gt; for inquiries or collaboration opportunities. Together, we can make powerful AI models accessible to everyone.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>SGLang Team</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch at GTC 2025</title>
      <link href="https://pytorch.org/blog/pytorch-at-gtc/" rel="alternate" type="text/html" title="PyTorch at GTC 2025" />
      <published>2025-03-16T00:00:00-07:00</published>
      <updated>2025-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-at-gtc</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-at-gtc/">&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/&quot;&gt;GTC&lt;/a&gt; is coming back to San Jose on March 17–21, 2025. Join PyTorch Foundation members Arm, AWS, Google Cloud, IBM, Lightning AI, Meta, Microsoft Azure, Snowflake, and thousands of developers as we celebrate PyTorch. Together learn how AI &amp;amp; accelerated computing are helping humanity solve our most complex challenges.&lt;/p&gt;

&lt;p&gt;Join in person with &lt;a href=&quot;https://www.nvidia.com/gtc/?ncid=GTC-NVI0K8HVX&quot;&gt;discounted GTC registration&lt;/a&gt; for PyTorch Foundation or &lt;a href=&quot;https://register.nvidia.com/flow/nvidia/gtcs25/registration/&quot;&gt;watch online&lt;/a&gt; with free registration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-at-gtc.jpg&quot; alt=&quot;book cover&quot; style=&quot;max-width:500px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;scaling-open-source-ai-from-foundation-models-to-ecosystem-success&quot;&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1738966749087001K1dG&quot;&gt;Scaling Open Source AI: From Foundation Models to Ecosystem Success&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Hear from PyTorch Foundation’s Executive Director Matt White &amp;amp; panelists from UC Berkeley, Meta, NVIDIA, &amp;amp; Sequoia Capital how open source is transforming AI development, bringing together experts from industry, academia, and venture capital to discuss the technical and business aspects of collaborative open source AI development They’ll examine how open source projects like PyTorch, vLLM, Ray, and NVIDIA’s NeMo are accelerating AI innovation while creating new opportunities for businesses and researchers. They’ll share real-world experiences from PyTorch’s development, Berkeley’s research initiatives, and successful AI startups. Take away valuable insights into the technical and business aspects of open source AI. – Monday, Mar 17 10:00 AM - 11:00 AM PDT&lt;/p&gt;

&lt;h2 id=&quot;pytorch--gtc&quot;&gt;PyTorch @ GTC&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726155993061001WWZM&quot;&gt;The Performance of CUDA with the Flexibility of PyTorch &lt;/a&gt;&lt;br /&gt;
Mark Saroufim, Software Engineer, Meta Platforms&lt;/p&gt;

&lt;p&gt;This talk explores how PyTorch users are also becoming CUDA developers. We’ll start with motivating examples from eager, the launch of torch.compile and the more recent trend of kernel zoos. We will share details on how we went about integrating low bit matmuls in torchao and the torch.compile CUTLASS backend. We’ll also discuss details on how you can define, build and package your own custom ops in PyTorch so you get the raw performance of CUDA while maintaining the flexibility of PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727978036338001UVLu&quot;&gt;Make My PyTorch Model Fast, and Show Me How You Did It&lt;/a&gt;&lt;br /&gt;
Thomas Viehmann, Principal Research Engineer, Lightning AI&lt;br /&gt;
Luca Antiga, CTO, Lightning AI&lt;/p&gt;

&lt;p&gt;PyTorch is popular in deep learning and LLMs for richness and ease of expressions. To make the most of compute resources, PyTorch models benefit from nontrivial optimizations, but this means losing some of their ease and understandability. Learn how with Thunder, a PyTorch-to-Python compiler focused on usability, understandability, and extensibility, you can optimize and transform (i.e., distribute across many machines) models while • leaving the PyTorch code unchanged • targeting a variety of models without needing to adapt to each of them • understanding each transformation step because the results are presented as simple Python code • accessing powerful extension code for your own optimizations with just one or a few lines of code We’ll show how the combination of Thunder transforms and the NVIDIA stack (NVFuser, cuDNN, Apex) delivers optimized performance in training and inference on a variety of models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726184633014001Jh5G&quot;&gt;FlexAttention: The Flexibility of PyTorch With the Performance of FlashAttention&lt;/a&gt;&lt;br /&gt;
Driss Guessous, Machine Learning Engineer, Meta Platforms&lt;/p&gt;

&lt;p&gt;Introducing FlexAttention: a novel PyTorch API that enables custom, user-defined attention mechanisms with performance comparable to state-of-the-art solutions. By leveraging the PyTorch compiler stack, FlexAttention supports dynamic modifications to attention scores within SDPA, achieving both runtime and memory efficiency through kernel fusion with the FlashAttention algorithm. Our benchmarks on A100 GPUs show FlexAttention achieves 90% of FlashAttention2’s performance in forward passes and 85% in backward passes. On H100 GPUs, FlexAttention’s forward performance averages 85% of FlashAttention3 and is ~25% faster than FlashAttention2, while backward performance averages 76% of FlashAttention3 and is ~3% faster than FlashAttention2. Explore how FlexAttention balances near-state-of-the-art performance with unparalleled flexibility, empowering researchers to rapidly iterate on attention mechanisms without sacrificing efficiency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1731693095418001cruA&quot;&gt;Keep Your GPUs Going Brrr : Crushing Whitespace in Model Training&lt;/a&gt;&lt;br /&gt;
Syed Ahmed, Senior Software Engineer, NVIDIA&lt;br /&gt;
Alban Desmaison, Research Engineer, Meta&lt;br /&gt;
Aidyn Aitzhan, Senior Software Engineer, NVIDIA&lt;/p&gt;

&lt;p&gt;Substantial progress has recently been made on the compute-intensive portions of model training, such as high-performing attention variants. While invaluable, this progress exposes previously hidden bottlenecks in model training, such as redundant copies during collectives and data loading time. We’ll present recent improvements in PyTorch achieved through Meta/NVIDIA collaboration to tackle these newly exposed bottlenecks and how practitioners can leverage them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727176757800001qp7T&quot;&gt;Accelerated Python: The Community and Ecosystem&lt;/a&gt;&lt;br /&gt;
Andy Terrel, CUDA Python Product Lead, NVIDIA&lt;br /&gt;
Jeremy Tanner, Open Source Programs, NVIDIA&lt;br /&gt;
Anshuman Bhat, CUDA Product Management, NVIDIA&lt;/p&gt;

&lt;p&gt;Python is everywhere. Simulation, data science, and Gen AI all depend on it. Unfortunately, the dizzying array of tools leaves a newcomer baffled at where to start. We’ll take you on a guided tour of the vibrant community and ecosystem surrounding accelerated Python programming. Explore a variety of tools, libraries, and frameworks that enable efficient computation and performance optimization in Python, including CUDA Python, RAPIDS, Warp, and Legate. We’ll also discuss integration points with PyData, PyTorch, and JAX communities. Learn about collaborative efforts within the community, including open source projects and contributions that drive innovation in accelerated computing. We’ll discuss best practices for leveraging these frameworks to enhance productivity in developing AI-driven applications and conducting large-scale data analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1734571562315001xMKM&quot;&gt;Supercharge large scale AI with Google Cloud AI hypercomputer (Presented by Google Cloud)&lt;/a&gt;&lt;br /&gt;
Deepak Patil, Product Manager, Google Cloud&lt;br /&gt;
Rajesh Anantharaman, Product Management Lead, ML Software, Google Cloud&lt;/p&gt;

&lt;p&gt;Unlock the potential of your large-scale AI workloads with Google Cloud AI Hypercomputer – a supercomputing architecture designed for maximum performance and efficiency. In this session, we will deep dive into PyTorch and JAX stacks on Google Cloud on NVIDIA GPUs, and showcase capabilities for high performance foundation model building on Google Cloud.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1739906058885001OxEF&quot;&gt;Peering Into the Future: What AI and Graph Networks Can Mean for the Future of Financial Analysis&lt;/a&gt;&lt;br /&gt;
Siddharth Samsi, Sr. Solutions Architect, NVIDIA&lt;br /&gt;
Sudeep Kesh, Chief Innovation Officer, S&amp;amp;P Global&lt;/p&gt;

&lt;p&gt;Artificial Intelligence, agentic systems, and graph neural networks (GNNs) are providing the new frontier to assess, monitor, and estimate opportunities and risks across work portfolios within financial services. Although many of these technologies are still developing, organizations are eager to understand their potential. See how S&amp;amp;P Global and NVIDIA are working together to find practical ways to learn and integrate such capabilities, ranging from forecasting corporate debt issuance to understanding capital markets at a deeper level. We’ll show a graph representation of market data using the PyTorch-Geometric library and a dataset of issuances spanning three decades and across financial and non-financial industries. Technical developments include generation of a bipartite graph and link-prediction GNN forecasting. We’ll address data preprocessing, pipelines, model training, and how these technologies can broaden capabilities in an increasingly complex world.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727984645671001Y9eq&quot;&gt;Unlock Deep Learning Performance on Blackwell With cuDNN&lt;/a&gt;&lt;br /&gt;
Yang Xu (Enterprise Products), DL Software Engineering Manager, NVIDIA&lt;/p&gt;

&lt;p&gt;Since its launch, cuDNN, a library for GPU-accelerating deep learning (DL) primitives, has been powering many AI applications in domains such as conversational AI, recommender systems, and speech recognition, among others. CuDNN remains a core library for DL primitives in popular frameworks such as PyTorch, JAX, Tensorflow, and many more while covering training, fine-tuning, and inference use cases. Even in the rapidly evolving space of Gen AI — be it Llama, Gemma, or mixture-of-experts variants requiring complex DL primitives such as flash attention variants — cuDNN is powering them all. Learn about new/updated APIs of cuDNN pertaining to Blackwell’s microscaling format, and how to program against those APIs. We’ll deep dive into leveraging its graph APIs to build some fusion patterns, such as matmul fusion patterns and fused flash attention from state-of-the-art models. Understand how new CUDA graph support in cuDNN, not to be mistaken with the cuDNN graph API, could be exploited to avoid rebuilding CUDA graphs, offering an alternative to CUDA graph capture with real-world framework usage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1736347047099001au7y&quot;&gt;Train and Serve AI Systems Fast With the Lightning AI Open-Source Stack (Presented by Lightning AI)&lt;/a&gt;&lt;br /&gt;
Luca Antiga, CTO, Lightning AI&lt;/p&gt;

&lt;p&gt;See how the Lightning stack can cover the full life cycle, from data preparation to deployment, with practical examples and particular focus on distributed training and high-performance inference. We’ll show examples that focus on new features like support for multi-dimensional parallelism through DTensors, as well as quantization through torchao.&lt;/p&gt;

&lt;h2 id=&quot;connect-with-experts-interactive-sessions&quot;&gt;Connect With Experts (Interactive Sessions)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1728516848639001tO7H&quot;&gt;Meet the Experts From Deep Learning Framework Teams &lt;/a&gt;&lt;br /&gt;
Eddie Yan, Technical Lead of PyTorch, NVIDIA&lt;br /&gt;
Masaki Kozuki, Senior Software Engineer in PyTorch, NVIDIA&lt;br /&gt;
Patrick Wang (Enterprise Products), Software Engineer in PyTorch, NVIDIA&lt;br /&gt;
Mike Ruberry, Distinguished Engineer in Deep Learning Frameworks, NVIDIA&lt;br /&gt;
Rishi Puri, Sr. Deep Learning Engineer and Lead for PyTorch Geometric, NVIDIA&lt;/p&gt;

&lt;h2 id=&quot;training-labs&quot;&gt;Training Labs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726073884811001C0za&quot;&gt;Kernel Optimization for AI and Beyond: Unlocking the Power of Nsight Compute &lt;/a&gt;&lt;br /&gt;
Felix Schmitt, Sr. System Software Engineer, NVIDIA&lt;br /&gt;
Peter Labus, Senior System Software Engineer, NVIDIA&lt;/p&gt;

&lt;p&gt;Learn how to unlock the full potential of NVIDIA GPUs with the powerful profiling and analysis capabilities of Nsight Compute. AI workloads are rapidly increasing the demand for GPU computing, and ensuring that they efficiently utilize all available GPU resources is essential. Nsight Compute is the most powerful tool for understanding kernel execution behavior and performance. Learn how to configure and launch profiles customized for your needs, including advice on profiling accelerated Python applications, AI frameworks like PyTorch, and optimizing Tensor Core utilization essential to modern AI performance. Learn how to debug your kernel and use the expert system built into Nsight Compute, known as “Guided Analysis,” that automatically detects common issues and directs you to the most relevant performance data all the way down to the source code level.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1725042189130001cmoW&quot;&gt;Make Retrieval Better: Fine-Tuning an Embedding Model for Domain-Specific RAG&lt;/a&gt;&lt;br /&gt;
Gabriel Moreira, Sr. Research Scientist, NVIDIA&lt;br /&gt;
Ronay Ak, Sr. Data Scientist, NVIDIA&lt;/p&gt;

&lt;p&gt;LLMs power AI applications like conversational chatbots and content generators, but are constrained by their training data. This might lead to hallucinations in content generation, which requires up-to-date or domain-specific information. Retrieval augmented generation (RAG) addresses this issue by enabling LLMs to access external context without modifying model parameters. Embedding or dense retrieval models are a key component of a RAG pipeline for retrieving relevant context to the LLM. However, an embedding model’s effectiveness to capture the unique characteristics of the custom data hinges on the quality and domain relevance of its training data. Fine-tuning embedding models is gaining interest to provide more accurate and relevant responses tailored to users’ specific domain.&lt;/p&gt;

&lt;p&gt;In this lab, you’ll learn to generate a synthetic dataset with question-context pairs from a domain-specific corpus, and process the data for fine-tuning. Then, fine-tune a text embedding model using synthetic data and evaluate it.&lt;/p&gt;

&lt;h2 id=&quot;poster-presentations&quot;&gt;Poster Presentations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1729781473379001KiPD&quot;&gt;Single-View X-Ray 3D Reconstruction Using Neural Back Projection and Frustum Resampling&lt;/a&gt;&lt;br /&gt;
Tran Minh Quan, Developer Technologist, NVIDIA&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1729757102989001KDG4&quot;&gt;Enable Novel Applications in the New AI Area in Medicine: Accelerated Feature Computation for Pathology Slides&lt;/a&gt;&lt;br /&gt;
Nils Bruenggel, Principal Software Engineer, Roche Diagnostics Int. AG&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at NVIDIA</name>
        
        
      </author>

      

      

      
        <summary type="html">GTC is coming back to San Jose on March 17–21, 2025. Join PyTorch Foundation members Arm, AWS, Google Cloud, IBM, Lightning AI, Meta, Microsoft Azure, Snowflake, and thousands of developers as we celebrate PyTorch. Together learn how AI &amp;amp; accelerated computing are helping humanity solve our most complex challenges.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing the New PyTorch Landscape: Your Guide to the PyTorch Ecosystem</title>
      <link href="https://pytorch.org/blog/pytorch-landscape/" rel="alternate" type="text/html" title="Introducing the New PyTorch Landscape: Your Guide to the PyTorch Ecosystem" />
      <published>2025-03-13T00:00:00-07:00</published>
      <updated>2025-03-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-landscape</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-landscape/">&lt;p&gt;We’re excited to reveal our brand new PyTorch Landscape. The &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; helps researchers, developers, and organizations easily locate useful, curated, community-built tools that augment the PyTorch core framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;&lt;img src=&quot;/assets/images/landscape.jpg&quot; alt=&quot;landscape banner&quot; style=&quot;max-width:600px;width:100%; margin-left: auto; margin-right: auto; display: block;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-the-landscape-offers&quot;&gt;What the Landscape Offers&lt;/h2&gt;

&lt;p&gt;The Landscape visually organizes projects into three categories—Modeling, Training, and Optimizations—making finding relevant frameworks, libraries, and projects easy. Users can quickly locate curated, valuable tools for a variety of use cases that complement the PyTorch framework. Each tool that is part of the Landscape has been reviewed and vetted by PyTorch project experts. The projects in the Landscape are considered to be mature and healthy and provide valuable capabilities that complement the PyTorch framework in their respective domains.&lt;/p&gt;

&lt;h2 id=&quot;explore-the-ai-landscape&quot;&gt;Explore the AI Landscape&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Explore&lt;/strong&gt; page presents platforms, tools, and libraries, each with a logo, description, and links to GitHub and further details. This categorized, visual approach simplifies discovery and provides quick access to essential technologies.&lt;/p&gt;

&lt;h2 id=&quot;guide-page-a-closer-look&quot;&gt;Guide Page: A Closer Look&lt;/h2&gt;

&lt;p&gt;For deeper insights, the &lt;strong&gt;Guide&lt;/strong&gt; page expands on each project, highlighting methodologies and trends shaping AI development, from adversarial robustness to self-supervised learning. There are also project statistics provided for each project, including metrics such as number of stars, contributors, commit history, languages used, license, and other valuable metrics that provide an in-depth understanding of the project and how it may be used.&lt;/p&gt;

&lt;h2 id=&quot;tracking-ais-growth-the-stats-page&quot;&gt;Tracking AI’s Growth: The Stats Page&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Stats&lt;/strong&gt; page provides insights into AI development trends, tracking repository activity, programming languages, and industry funding data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Repositories: 117 repositories, 20.5k contributors, and 797.2k stars across 815MB of source code.&lt;/li&gt;
  &lt;li&gt;Development Trends: Weekly commit activity over the last year.&lt;/li&gt;
  &lt;li&gt;Licensing Breakdown: Repositories are categorized by license type.&lt;/li&gt;
  &lt;li&gt;Funding &amp;amp; Acquisitions: Insights into investment trends, including funding rounds and acquisitions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-use-the-pytorch-landscape&quot;&gt;Why Use the PyTorch Landscape?&lt;/h2&gt;

&lt;p&gt;Finding useful and high quality open source projects that complement the PyTorch core system can be overwhelming. The PyTorch Landscape offers a clear, accessible way to explore the ecosystem of community-built tools, whether you’re researching, building models, or making strategic decisions.&lt;/p&gt;

&lt;p&gt;Stay ahead with the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; — your guide to the PyTorch Ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;want-to-contribute-a-project-to-the-pytorch-landscape&quot;&gt;Want to Contribute a Project to the PyTorch Landscape?&lt;/h2&gt;

&lt;p&gt;Have you built a useful open source tool that you would like to share with the PyTorch community? Then help us grow the Ecosystem by contributing your tool! You can find the &lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;instructions to apply here&lt;/a&gt;. We welcome all contributions from the community!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re excited to reveal our brand new PyTorch Landscape. The PyTorch Landscape helps researchers, developers, and organizations easily locate useful, curated, community-built tools that augment the PyTorch core framework.</summary>
      

      
      
    </entry>
  
</feed>



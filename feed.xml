<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-01-29T17:49:31-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">PyTorch 2.6 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-6/" rel="alternate" type="text/html" title="PyTorch 2.6 Release Blog" />
      <published>2025-01-29T00:00:00-08:00</published>
      <updated>2025-01-29T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch2-6</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-6/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.6 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.6.0&quot;&gt;release notes&lt;/a&gt;)! This release features multiple improvements for PT2: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; can now be used with Python 3.13; new performance-related knob &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compiler.set_stance&lt;/code&gt;; several AOTInductor enhancements. Besides the PT2 improvements, another highlight is FP16 support on X86 CPUs.&lt;/p&gt;

&lt;p&gt;NOTE: Starting with this release we are not going to publish on Conda, please see &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/138506&quot;&gt;[Announcement] Deprecating PyTorch’s official Anaconda channel&lt;/a&gt; for the details.&lt;/p&gt;

&lt;p&gt;For this release the experimental Linux binaries shipped with CUDA 12.6.3 (as well as Linux Aarch64,  Linux ROCm 6.2.4, and Linux XPU binaries) are built with CXX11_ABI=1 and are &lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-linux-wheels-switching-to-new-wheel-build-platform-manylinux-2-28-on-november-12-2024/2581&quot;&gt;using the Manylinux 2.28 build platform&lt;/a&gt;. If you build PyTorch extensions with custom C++ or CUDA extensions, please update these builds to use CXX_ABI=1 as well and report any issues you are seeing. For the next PyTorch 2.7 release we plan to switch all Linux builds to Manylinux 2.28 and CXX11_ABI=1, please see &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/123649&quot;&gt;[RFC] PyTorch next wheel build platform: manylinux-2.28&lt;/a&gt; for the details and discussion.&lt;/p&gt;

&lt;p&gt;Also in this release as an important security improvement measure we have changed the default value for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weights_only&lt;/code&gt; parameter of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.load&lt;/code&gt;. This is a backward compatibility-breaking change, please see &lt;a href=&quot;https://dev-discuss.pytorch.org/t/bc-breaking-change-torch-load-is-being-flipped-to-use-weights-only-true-by-default-in-the-nightlies-after-137602/2573&quot;&gt;this forum post&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;This release is composed of 3892 commits from 520 contributors since PyTorch 2.5. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve PyTorch. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Beta
   &lt;/td&gt;
   &lt;td&gt;Prototype
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.compiler.set_stance
   &lt;/td&gt;
   &lt;td&gt;Improved PyTorch user experience on Intel GPUs
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.library.triton_op
   &lt;/td&gt;
   &lt;td&gt;FlexAttention support on X86 CPU for LLMs
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.compile support for Python 3.13
   &lt;/td&gt;
   &lt;td&gt;Dim.AUTO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;New packaging APIs for AOTInductor
   &lt;/td&gt;
   &lt;td&gt;CUTLASS and CK GEMM/CONV Backends for AOTInductor
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AOTInductor: minifier
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AOTInductor: ABI-compatible mode code generation
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;FP16 support for X86 CPUs
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompilerset_stance&quot;&gt;[Beta] torch.compiler.set_stance&lt;/h3&gt;

&lt;p&gt;This feature enables the user to specify different behaviors (“stances”) that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; can take between different invocations of compiled functions. One of the stances, for example, is&lt;/p&gt;

&lt;p&gt;“eager_on_recompile”, that instructs PyTorch to code eagerly when a recompile is necessary, reusing cached compiled code when possible.&lt;/p&gt;

&lt;p&gt;For more information please refer to the &lt;a href=&quot;https://pytorch.org/docs/2.6/generated/torch.compiler.set_stance.html#torch.compiler.set_stance&quot;&gt;set_stance documentation&lt;/a&gt; and the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compiler_set_stance_tutorial.html&quot;&gt;Dynamic Compilation Control with torch.compiler.set_stance&lt;/a&gt; tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchlibrarytriton_op&quot;&gt;[Beta] torch.library.triton_op&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.library.triton_op&lt;/code&gt; offers a standard way of creating custom operators that are backed by user-defined triton kernels.&lt;/p&gt;

&lt;p&gt;When users turn user-defined triton kernels into custom operators, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.library.triton_op&lt;/code&gt; allows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to peek into the implementation, enabling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to optimize the triton kernel inside it.&lt;/p&gt;

&lt;p&gt;For more information please refer to the &lt;a href=&quot;https://pytorch.org/docs/2.6/library.html#torch.library.triton_op&quot;&gt;triton_op documentation&lt;/a&gt; and the&lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt; Using User-Defined Triton Kernels with torch.compile&lt;/a&gt; tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchcompile-support-for-python-313&quot;&gt;[Beta] torch.compile support for Python 3.13&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; previously only supported Python up to version 3.12. Users can now optimize models with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; in Python 3.13.&lt;/p&gt;

&lt;h3 id=&quot;beta-new-packaging-apis-for-aotinductor&quot;&gt;[Beta] New packaging APIs for AOTInductor&lt;/h3&gt;

&lt;p&gt;A new package format, “&lt;a href=&quot;https://docs.google.com/document/d/1RQ4cmywilnFUT1VE-4oTGxwXdc8vowCSZsrRgo3wFA8/edit?usp=sharing&quot;&gt;PT2 archive&lt;/a&gt;”, has been introduced. This essentially contains a zipfile of all the files that need to be used by AOTInductor, and allows users to send everything needed to other environments. There is also functionality to package multiple models into one artifact, and to store additional metadata inside of the package.&lt;/p&gt;

&lt;p&gt;For more details please see the updated &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html&quot;&gt;torch.export AOTInductor Tutorial for Python runtime&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-aotinductor-minifier&quot;&gt;[Beta] AOTInductor: minifier&lt;/h3&gt;

&lt;p&gt;If a user encounters an error while using AOTInductor APIs, AOTInductor Minifier allows creation of a minimal nn.Module that reproduces the error.&lt;/p&gt;

&lt;p&gt;For more information please see the &lt;a href=&quot;https://pytorch.org/docs/2.6/torch.compiler_aot_inductor_minifier.html&quot;&gt;AOTInductor Minifier documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-aotinductor-abi-compatible-mode-code-generation&quot;&gt;[Beta] AOTInductor: ABI-compatible mode code generation&lt;/h3&gt;

&lt;p&gt;AOTInductor-generated model code has dependency on Pytorch cpp libraries. As Pytorch evolves quickly, it’s important to make sure previously AOTInductor compiled models can continue to run on newer Pytorch versions, i.e. AOTInductor is backward compatible.&lt;/p&gt;

&lt;p&gt;In order to guarantee application binary interface (ABI) backward compatibility, we have carefully defined a set of stable C interfaces in libtorch and make sure AOTInductor generates code that only refers to the specific set of APIs and nothing else in libtorch. We will keep the set of C APIs stable across Pytorch versions and thus provide backward compatibility guarantees for AOTInductor-compiled models.&lt;/p&gt;

&lt;h3 id=&quot;beta-fp16-support-for-x86-cpus-both-eager-and-inductor-modes&quot;&gt;[Beta] FP16 support for X86 CPUs (both eager and Inductor modes)&lt;/h3&gt;

&lt;p&gt;Float16 datatype is commonly used for reduced memory usage and faster computation in AI inference and training. CPUs like the recently launched &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/xeon/xeon6-p-cores.html&quot;&gt;Intel® Xeon® 6 with P-Cores&lt;/a&gt; support Float16 datatype with native accelerator &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html&quot;&gt;AMX&lt;/a&gt;. Float16 support on X86 CPUs was introduced in PyTorch 2.5 as a prototype feature, and now it has been further improved for both eager mode and Torch.compile + Inductor mode, making it Beta level feature with both functionality and performance verified with a broad scope of workloads.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-improved-pytorch-user-experience-on-intel-gpus&quot;&gt;[Prototype] Improved PyTorch user experience on Intel GPUs&lt;/h3&gt;

&lt;p&gt;PyTorch user experience on Intel GPUs is further improved with simplified installation steps, Windows release binary distribution and expanded coverage of supported GPU models including the latest Intel® Arc™ B-Series discrete graphics. Application developers and researchers seeking to fine-tune, inference and develop with PyTorch models on &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/ai-pc.html&quot;&gt;Intel® Core™ Ultra AI PCs &lt;/a&gt;and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Arc™ discrete graphics&lt;/a&gt; will now be able to directly install PyTorch with binary releases for Windows, Linux and Windows Subsystem for Linux 2.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simplified Intel GPU software stack setup to enable one-click installation of the torch-xpu PIP wheels to run deep learning workloads in an out of the box fashion, eliminating the complexity of installing and activating Intel GPU development software bundles.&lt;/li&gt;
  &lt;li&gt;Windows binary releases for torch core, torchvision and torchaudio have been made available for Intel GPUs, and the supported GPU models have been expanded from Intel® Core™ Ultra Processors with Intel® Arc™ Graphics, &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra Series 2 with Intel® Arc™ Graphics&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html&quot;&gt;Intel® Arc™ A-Series Graphics&lt;/a&gt; to the latest GPU hardware &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series graphics&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Further enhanced coverage of Aten operators on Intel GPUs with SYCL* kernels for smooth eager mode execution, as well as bug fixes and performance optimizations for torch.compile on Intel GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-support-on-x86-cpu-for-llms&quot;&gt;[Prototype] FlexAttention support on X86 CPU for LLMs&lt;/h3&gt;

&lt;p&gt;FlexAttention was initially introduced in PyTorch 2.5 to provide optimized implementations for Attention variants with a flexible API. In PyTorch 2.6, X86 CPU support for FlexAttention was added through TorchInductor CPP backend. This new feature leverages and extends current CPP template abilities to support broad attention variants (e.x.: PageAttention, which is critical for LLMs inference) based on the existing FlexAttention API, and brings optimized performance on x86 CPUs. With this feature, it’s easy to use FlexAttention API to compose Attention solutions on CPU platforms and achieve good performance.&lt;/p&gt;

&lt;h3 id=&quot;prototype-dimauto&quot;&gt;[Prototype] Dim.AUTO&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dim.AUTO&lt;/code&gt; allows usage of automatic dynamic shapes with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.export&lt;/code&gt;. Users can export with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dim.AUTO &lt;/code&gt;and “discover” the dynamic behavior of their models, with min/max ranges, relations between dimensions, and static/dynamic behavior being automatically inferred.&lt;/p&gt;

&lt;p&gt;This is a more user-friendly experience compared to the existing named-Dims approach for specifying dynamic shapes, which requires the user to fully understand the dynamic behavior of their models at export time. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dim.AUTO&lt;/code&gt; allows users to write generic code that isn’t model-dependent, increasing ease-of-use for exporting with dynamic shapes.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes&quot;&gt;torch.export tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;prototype-cutlass-and-ck-gemmconv-backends-for-aotinductor&quot;&gt;[Prototype] CUTLASS and CK GEMM/CONV Backends for AOTInductor&lt;/h3&gt;

&lt;p&gt;The CUTLASS and CK backend adds kernel choices for GEMM autotuning in Inductor. This is now also available in AOTInductor which can run in C++ runtime environments. A major improvement to the two backends is improved compile-time speed by eliminating redundant kernel binary compilations and dynamic shapes support.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.6 (release notes)! This release features multiple improvements for PT2: torch.compile can now be used with Python 3.13; new performance-related knob torch.compiler.set_stance; several AOTInductor enhancements. Besides the PT2 improvements, another highlight is FP16 support on X86 CPUs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">2025 Priorities for the PyTorch Technical Advisory Council (TAC)</title>
      <link href="https://pytorch.org/blog/2025-priorities-for-tac/" rel="alternate" type="text/html" title="2025 Priorities for the PyTorch Technical Advisory Council (TAC)" />
      <published>2025-01-28T00:00:00-08:00</published>
      <updated>2025-01-28T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/2025-priorities-for-tac</id>
      <content type="html" xml:base="https://pytorch.org/blog/2025-priorities-for-tac/">&lt;p&gt;&lt;img src=&quot;/assets/images/1738166706211.jpg&quot; alt=&quot;social share&quot; style=&quot;max-width:600px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/2024-year-in-review/&quot;&gt;2024 has been a year of incredible growth for PyTorch&lt;/a&gt;. As that continues in 2025, the PyTorch Foundation has made important steps towards evolving the governance of the project under the Linux Foundation’s vendor-neutral umbrella.&lt;/p&gt;

&lt;p&gt;An important piece of governance for PyTorch is represented by the Technical Advisory Council (TAC). The TAC acts as a bridge between the industry, including but not limited to the PyTorch Foundation members, the community, and the PyTorch core development team.&lt;/p&gt;

&lt;p&gt;Operating with transparency and inclusivity, the TAC gathers input, facilitates collaboration, and drives initiatives that enhance the experience for everyone who relies on PyTorch.&lt;/p&gt;

&lt;p&gt;In 2025, the TAC will focus on four key areas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Build Open, Multi-Cloud Continuous Integration (CI):&lt;/strong&gt; Building on the groundwork from 2024, the TAC will oversee the transition to an open, community-driven CI infrastructure. In addition to ensuring the extremely high bar for correctness that PyTorch has, PyTorch’s CI is complex with a high-quality bar including many automated functional and performance daily test runs. In 2025, PyTorch’s CI infrastructure will be fully open sourced and extended to support multiple compute providers, enabling broader contribution and participation to the effort from organizations benefitting from PyTorch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Support more Accelerators:&lt;/strong&gt; The TAC is committed to creating a level playing field for the growing landscape of AI accelerators. By gathering industry players and PyTorch developers, the TAC will facilitate efforts towards third-party device support and provide levels of integration of external CI systems with the main PyTorch CI. This will make it easier for emerging hardware to gain adoption within the PyTorch ecosystem, and for users to experiment with diverse compute options for training and inference.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create a High-Quality, User-Centric Ecosystem:&lt;/strong&gt; A big focus for the TAC in early 2025 is on improving the experience and discoverability of the PyTorch ecosystem. With many projects growing organically, users often face challenges navigating projects of different scope and quality within the rapidly changing AI landscape. To solve this, a newly curated ecosystem landscape tool will be launched soon on the PyTorch website. We will also introduce lightweight, open processes to improve projects and ensure users a predictable, high-quality experience. In many ways, the experience with PyTorch is as good as its ecosystem.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gather Feedback from Industry and the Community:&lt;/strong&gt; PyTorch has widespread adoption across research labs, startups, and enterprises. Striking the right balance between expressiveness and performance across the board is a very challenging task, so the TAC set out to be one of the several ways the Core development team receives signals. During our monthly TAC meetings, we provide the opportunity to PyTorch Foundation members from industry and academia, as well as non-member organizations to present their use case, their challenges and discuss them directly with appropriate members of the Core team. This feedback loop helps prioritize improvements, ensuring the framework stays relevant in a fast-evolving AI landscape.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By focusing on these priorities, the TAC aims to maintain PyTorch’s position as the leading deep learning framework, while ensuring it remains open, accessible, and responsive to the needs of its diverse community.&lt;/p&gt;

&lt;p&gt;As members of the TAC, we’re extremely excited to contribute to the success of PyTorch and to the impact it’s having in the real world. If you are a PyTorch user or developer, consider &lt;a href=&quot;https://zoom-lfx.platform.linuxfoundation.org/meetings/pytorch?__hstc=132719121.a26416c161ac91bef494ffc19f91a62e.1723036593114.1738082449904.1738088158683.375&amp;amp;__hssc=132719121.1.1738088158683&amp;amp;__hsfp=810579359&quot;&gt;participating in our monthly calls&lt;/a&gt; (they are open to everyone, and the recordings are available &lt;a href=&quot;https://lists.pytorch.org/g/tac&quot;&gt;here&lt;/a&gt;). Also, if you develop or maintain a project based on PyTorch, consider contributing it to the new PyTorch ecosystem (&lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;instructions&lt;/a&gt;).&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Luca Antiga, PyTorch TAC Chair</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How Intel Uses PyTorch to Empower Generative AI through Intel Arc GPUs</title>
      <link href="https://pytorch.org/blog/how-intel-uses-pytorch-to-empower-generative-ai-through-intel-arc-gpus/" rel="alternate" type="text/html" title="How Intel Uses PyTorch to Empower Generative AI through Intel Arc GPUs" />
      <published>2025-01-24T00:00:00-08:00</published>
      <updated>2025-01-24T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/how-intel-uses-pytorch-to-empower-generative-ai-through-intel-arc-gpus</id>
      <content type="html" xml:base="https://pytorch.org/blog/how-intel-uses-pytorch-to-empower-generative-ai-through-intel-arc-gpus/">&lt;p&gt;Intel has long been at the forefront of technological innovation, and its recent venture into Generative AI (GenAI) solutions is no exception. With the rise of AI-powered gaming experiences, Intel sought to deliver an accessible and intuitive GenAI inferencing solution tailored for AI PCs powered by Intel’s latest GPUs. By leveraging PyTorch as the backbone for development efforts, Intel successfully launched AI Playground, an open source application that showcases advanced GenAI workloads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Business Challenge&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our goal was to deliver an accessible and intuitive GenAI inferencing solution tailored for AI PCs powered by Intel. We recognized the need to showcase the capabilities of the latest GenAI workloads on our newest line of client GPUs. To address this, we developed a starter application, &lt;a href=&quot;https://github.com/intel/ai-playground&quot;&gt;AI Playground&lt;/a&gt;, which is open source and includes a comprehensive developer reference sample available on GitHub using PyTorch. This application seamlessly integrates image generation, image enhancement, and chatbot functionalities, using retrieval-augmented generation (RAG) features, all within a single, user-friendly installation package. This initiative not only demonstrates the functionality of these AI workloads but also serves as an educational resource for the ecosystem, guiding developers on effectively leveraging the &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Arc™ GPU&lt;/a&gt; product line for advanced AI applications. This solution leverages Intel® Arc™ Xe Cores and &lt;a href=&quot;https://www.intel.com/content/www/us/en/support/articles/000091112/graphics.html&quot;&gt;Xe Matrix Extensions (XMX)&lt;/a&gt; for accelerating inferencing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/intel-case-study/fg1.png&quot; alt=&quot;AI Playground&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How Intel Used PyTorch&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch is the core AI framework for AI Playground. We extensively leverage PyTorch’s eager mode, which aligns perfectly with the dynamic and iterative nature of our generative models. This approach not only enhances our development workflow but also enables us to rapidly prototype and iterate on advanced AI features. By harnessing PyTorch’s powerful capabilities, we have created a robust reference sample that showcases the potential of GenAI on Intel GPUs in one cohesive application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solving AI Challenges with PyTorch&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch has been instrumental in addressing our AI challenges by providing a robust training and inference framework optimized for discrete and integrated Intel Arc GPU product lines. Choosing PyTorch over alternative frameworks or APIs was crucial. Other options would have necessitated additional custom development or one-off solutions, which could have significantly slowed our time to market and limited our feature set. With PyTorch, we leveraged its flexibility and ease of use, allowing our team to focus on innovation through experimentation, rather than infrastructure. The integration of &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.j6azz7&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; further enhanced performance by optimizing computational efficiency and enabling seamless scaling on Intel hardware, ensuring that our application ran faster and more efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Word from Intel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With PyTorch as the backbone of our AI Playground project, we achieved rapid development cycles that significantly accelerated our time to market. This flexibility enabled us to iteratively enhance features and effectively align with the commitments of our hardware launches in 2024.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;-Bob Duffy, AI Playground Product Manager&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/intel-case-study/fg2.png&quot; alt=&quot;PyTorch Case Stidu&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Benefits of Using PyTorch&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The biggest benefit of using PyTorch for us is the large PyTorch ecosystem, which connects us with an active and cooperative community of developers. This collaboration has facilitated the seamless deployment of key features from existing open source projects, allowing us to integrate the latest GenAI capabilities into AI Playground. Remarkably, we accomplished this with minimal re-coding, ensuring that these advanced features are readily accessible on Intel Arc GPUs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn More&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more information about Intel’s AI Playground and collaboration with PyTorch, visit the following links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.j8h6mc&quot;&gt;PyTorch Optimizations from Intel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/intel/ai-playground&quot;&gt;AI Playground GitHub&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://intel.com/ai-playground&quot;&gt;AI Playground&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/cYPZye1MC6U&quot;&gt;AI Playground Deep Dive Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/&quot;&gt;Intel GPU Support Now Available in PyTorch 2.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Intel has long been at the forefront of technological innovation, and its recent venture into Generative AI (GenAI) solutions is no exception. With the rise of AI-powered gaming experiences, Intel sought to deliver an accessible and intuitive GenAI inferencing solution tailored for AI PCs powered by Intel’s latest GPUs. By leveraging PyTorch as the backbone for development efforts, Intel successfully launched AI Playground, an open source application that showcases advanced GenAI workloads.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Bringing the PyTorch Community Together</title>
      <link href="https://pytorch.org/blog/bringing-the-pytorch-community-together/" rel="alternate" type="text/html" title="Bringing the PyTorch Community Together" />
      <published>2025-01-22T00:00:00-08:00</published>
      <updated>2025-01-22T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/bringing-the-pytorch-community-together</id>
      <content type="html" xml:base="https://pytorch.org/blog/bringing-the-pytorch-community-together/">&lt;p&gt;As we step into a new year, it’s a great moment to reflect on the incredible community events that made 2024 a memorable year for the PyTorch Foundation. Global meetups, events, and conferences brought the community together to learn, connect, and grow. Here’s a quick recap of the year’s highlights and what to expect in 2025.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg5.jpg&quot; alt=&quot;PyTorch Seattle Meetup (May 23)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyTorch Seattle Meetup (May 23)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We hosted a PyTorch Meetup in Seattle in May at the Meta Bellevue Office where Meta, Microsoft, and Google gave technical talks and about 60 attendees participated in discussion and networking.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyTorch Docathon 2024 (June 4-20)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Docathon returned for its third edition, spanning over two weeks in June. This unique event focused on improving PyTorch’s documentation with contributions from community members worldwide. Documentation is the backbone of any successful open source project, and PyTorch’s Docathon fostered inclusivity and collaboration, making it easier for new users to adopt the framework and for experienced developers to maximize its potential. The 2024 Docathon resulted in more than 50 merged pull requests and was a testament to the collaborative spirit of the PyTorch community and its commitment to enhancing accessibility and usability. Watch the &lt;a href=&quot;https://youtu.be/2D0aej50umA?feature=shared&quot;&gt;PyTorch Docathon Kickoff&lt;/a&gt; on YouTube.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg3.png&quot; alt=&quot;PyTorch Shanghai Meetup (August 15)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pytorch-shanghai-meetup-august-15&quot;&gt;&lt;strong&gt;PyTorch Shanghai Meetup (August 15)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;In August, the &lt;a href=&quot;https://pytorch.org/blog/pytorch-shanghai-notes/&quot;&gt;PyTorch Shanghai Meetup&lt;/a&gt; brought together developers, researchers, and enthusiasts in Shanghai, China. This event served as a platform for knowledge sharing, with engaging talks and networking opportunities. Highlights from the agenda included insights into PyTorch’s latest developments, community-led presentations showcasing innovative use cases, and networking sessions fostering collaboration among attendees.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg1.jpg&quot; alt=&quot;PyTorch Conference 2024 (September 18-19)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pytorch-conference-2024-september-18-19&quot;&gt;&lt;strong&gt;PyTorch Conference 2024 (September 18-19)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The PyTorch Conference in San Francisco was undoubtedly one of the year’s most significant events. This two-day gathering brought together top-tier researchers, developers, and academic communities, fostering collaboration and innovation in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg6.jpeg&quot; alt=&quot;What Made It Special&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;what-made-it-special&quot;&gt;&lt;strong&gt;What Made It Special:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Keynote speeches from industry leaders and PyTorch maintainers.&lt;/li&gt;
  &lt;li&gt;In-depth sessions covering PyTorch’s end-to-end machine learning capabilities.&lt;/li&gt;
  &lt;li&gt;Hands-on workshops and breakout sessions.&lt;/li&gt;
  &lt;li&gt;A vibrant expo area showcasing cutting-edge tools and applications.&lt;/li&gt;
  &lt;li&gt;Startup Showcase where early-stage founders pitched their AI startups to a panel of top venture capitalists.&lt;/li&gt;
  &lt;li&gt;DL Compiler Mini-Summit that took a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads.&lt;/li&gt;
  &lt;li&gt;Fine-Tuning Mini-Summit that covered everything from memory efficiency, parameter-efficient fine-tuning and quantization to performance at scale and reproducible evaluations.&lt;/li&gt;
  &lt;li&gt;Poster Session showcasing innovations in PyTorch, including model optimization, hardware integration, generative AI, quantization, and tools for enhanced performance and usability, with contributions from industry leaders.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conference’s focus on fostering collaboration underscored PyTorch’s role as a driving force in the open source ML community. Missed out? You can watch the &lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l&amp;amp;si=mdoSkqMJYKRlzxlg&quot;&gt;PyTorch Conference 2024 Playlist&lt;/a&gt; to catch any sessions you might have missed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg4.jpg&quot; alt=&quot;GPU MODE IRL Hackathon (September 21)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;gpu-mode-irl-hackathon-september-21&quot;&gt;&lt;strong&gt;GPU MODE IRL Hackathon (September 21)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;PyTorch sponsored this meetup in person in San Francisco where attendees made friends, watched keynotes, hacked all day, took breaks with afternoon talks, and then hacked all night. We heard about torchao, our new quantization and sparsity library, vLLM which deploys PyTorch models in production, llm.c, and more. Key takeaways included: GPU Mode IRL Hackathon 1st place winner was inspired by PyTorch FlexAttention to improve CUTLASS, NCCL in Triton would help us do distributed programming with a minimal NCCL reimplementation in pure Python, No libtorch pytorch binaries dramatically reduces binary sizes for on device deployments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg8.png&quot; alt=&quot;Consumer AI Edge Hackathon (November 22-23)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;consumer-ai-edge-hackathon-november-22-23&quot;&gt;&lt;strong&gt;Consumer AI Edge Hackathon (November 22-23)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The PyTorch team served as mentors and coaches in a Hackathon in Paris, co-sponsored by Hugging Face, Scaleway, and Entrepreneur First, challenging teams to create innovative consumer (B2C) applications leveraging Hugging Face, PyTorch and other open source on-device tools and models. 120+ people across 22 teams hacked for 2 days (and nights!) building the future of AI-powered on-device solutions based on open source models and tools. Participants created innovative applications,  powered by PyTorch, &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main&quot;&gt;ExecuTorch&lt;/a&gt; and Hugging Face resources, such as an on-device yoga coach, a magical storytelling companion and a Kinect-like experience to mobile phones. The PyTorch team is planning similar events in other geographies in 2025 around innovative on-device AI applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg9.png&quot; alt=&quot;PyTorch Korea User Group Meetup (November 30)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pytorch-korea-user-group-meetup-november-30&quot;&gt;&lt;strong&gt;PyTorch Korea User Group Meetup (November 30)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The PyTorch Korea User Group, founded in 2018, is a community dedicated to introducing PyTorch to Korean-speaking users and growing together. The group began by translating PyTorch 0.3 tutorials into Korean and has since supported PyTorch’s growth in Korea. The group focuses on three primary activities:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sharing knowledge for PyTorch learning and application,&lt;/li&gt;
  &lt;li&gt;Sharing insights and experiences in the field of artificial intelligence, and&lt;/li&gt;
  &lt;li&gt;Fostering growth through online and offline networking.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The PyTorch Korea User Group reaches tens of thousands of Korean AI developers every month. If you’re interested in their activities, check out these links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.kr&quot;&gt;PyTorch Korea User Group&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://tutorials.pytorch.kr&quot;&gt;PyTorch Korean Tutorials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://discuss.pytorch.kr&quot;&gt;PyTorch Korean Community&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/PyTorchKorea&quot;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtube.com/@pytorchkr&quot;&gt;YouTube Channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg2.jpeg&quot; alt=&quot;PyTorch Korea User Group 2025 Events Overview&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Korea User Group has planned three major activities for the year:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch CoreSIG&lt;/strong&gt;&lt;br /&gt;
Since December 2024, this weekly online event has been held every Wednesday afternoon. Led by Kim Hong-Seok, CSO of Rebellions (a PyTorch member company), it provides in-depth knowledge and experience regarding PyTorch internals. Approximately 150 Korean developers participate weekly, reflecting growing interest in PyTorch Core development in Korea.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Offline Meetup&lt;/strong&gt;&lt;br /&gt;
These meetups provide opportunities to share insights and experiences in PyTorch and artificial intelligence, along with networking. Around 3–4 sessions are planned for this year, focusing on key topics in PyTorch and AI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Online Community Engagement&lt;/strong&gt;&lt;br /&gt;
This activity involves sharing and discussing various projects and papers in the AI field. For more information, visit: &lt;a href=&quot;https://discuss.pytorch.kr&quot;&gt;https://discuss.pytorch.kr&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;open-source-ai-night-at-neurips-2024-december-10&quot;&gt;&lt;strong&gt;Open Source AI Night at NeurIPS 2024 (December 10)&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The PyTorch Foundation co-hosted a social event at NeurIPS along with The Fin AI and Open Finance Foundation that featured engaging discussions on open source AI and applications in finance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/community-events-recap/fg7.jpeg&quot; alt=&quot;PyTorch Webinars&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyTorch Webinars&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Throughout 2024, PyTorch hosted the following virtual webinars:&lt;/p&gt;

&lt;p&gt;Expert Exchanges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/HTcnp9NEHGY?feature=shared&quot;&gt;How does batching work on modern CPUs?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/Bh-jlh5vlF0?feature=shared&quot;&gt;DistServe: disaggregating prefill and decoding for goodput-optimized LLM inference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/RnM84Sv9WpA?feature=shared&quot;&gt;Efficient Streaming Language Models with Attention Sinks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/e1qUJFAo10s?feature=shared&quot;&gt;Adapting open source models with Open-Instruct and Tulu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/Eqg0VIiWrgM?feature=shared&quot;&gt;Efficient Generative Models: From Sparse to Distributed Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Summer Series:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/xf2QgioY370?feature=shared&quot;&gt;Using PyTorch for Monocular Depth Estimation Webinar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/NeHIhQWewug?feature=shared&quot;&gt;Accelerating LLM family of models on Arm Neoverse based Graviton AWS processors with KleidiAI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/rew5CSUaIXg?feature=shared&quot;&gt;torch.compile: The Missing Manual&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Release Live Q&amp;amp;As:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/ry_QgUIYX1E?feature=shared&quot;&gt;PyTorch 2.4: Live Q&amp;amp;A&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/B3IgXpl4xt4?feature=shared&quot;&gt;PyTorch 2.5 Live Q&amp;amp;A&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Live Webinars:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EjgTv6aSeqk&quot;&gt;PyTorch Documentary Virtual Premiere&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gSC_IHyx0IM&quot;&gt;Using PyTorch to Help Predict Wildfires&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=zvk3Rr-OjU0&quot;&gt;Seismic Data to Subsurface Models with OpenFWI: Training an AI Model with PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w4JmPkqnD0E&quot;&gt;Dinosaur Bone Hunting with Intel AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of these events underscored the importance of collaboration and community engagement in advancing AI research and applications. Thank you to everyone who participated, organized, and supported these events—your contributions make all the difference!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;looking-ahead&quot;&gt;&lt;strong&gt;Looking Ahead&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;2024 was packed with opportunities to connect, learn, and contribute, and there will be even more ways to connect with the PyTorch community in 2025.&lt;/p&gt;

&lt;p&gt;Mark your calendar! The &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference-2025/&quot;&gt;PyTorch Conference&lt;/a&gt; is returning to San Francisco on October 22-23, 2025. Get ready for an exciting event filled with technical deep dives, exciting announcements, insightful sessions, and enhanced opportunities for community collaboration.&lt;/p&gt;

&lt;p&gt;Stay tuned for more upcoming events and opportunities to get involved by &lt;a href=&quot;https://pytorch.org/newsletter&quot;&gt;subscribing to our newsletter&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">As we step into a new year, it’s a great moment to reflect on the incredible community events that made 2024 a memorable year for the PyTorch Foundation. Global meetups, events, and conferences brought the community together to learn, connect, and grow. Here’s a quick recap of the year’s highlights and what to expect in 2025.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating LLM Inference with GemLite, TorchAO and SGLang</title>
      <link href="https://pytorch.org/blog/accelerating-llm-inference/" rel="alternate" type="text/html" title="Accelerating LLM Inference with GemLite, TorchAO and SGLang" />
      <published>2025-01-21T00:00:00-08:00</published>
      <updated>2025-01-21T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-llm-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-llm-inference/">&lt;p&gt;Large Language Models (LLMs) are typically very resource-intensive, requiring significant amounts of memory, compute and power to operate effectively. Quantization provides a solution by reducing weights and activations from 16 bit floats to lower bitrates (e.g., 8 bit, 4 bit, 2 bit), achieving significant speedup and memory savings and also enables support for larger batch sizes.&lt;/p&gt;

&lt;p&gt;Existing solutions for low precision inference work well for small batch sizes, but suffer from following issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Performance drops when we increase the batch size&lt;/li&gt;
  &lt;li&gt;Restrictions on types of quantization, for example, some kernels only support symmetric quantization that could have implications on accuracy of the model at lower bits&lt;/li&gt;
  &lt;li&gt;Interplay between quantization, serialization, and tensor parallelism (TP) makes it difficult to load quantized models and requires changes to user models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address these challenges, we created an end-to-end, performant, modular and extensible low-precision inference solution integrating the following libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mobiusml/gemlite&quot;&gt;GemLite&lt;/a&gt;, a Triton kernel library, tackles the performance limitations of large batch sizes and restrictions on the types of quantization&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;TorchAO&lt;/a&gt;, a PyTorch-native library, provides a streamlined experience for quantization, sparsity, and tensor parallelism (with DTensor)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt;, a fast, efficient and hackable serving framework for Large Language Model (LLM) and Vision Language Models (VLM) with extensive model support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in trying this out in SGLang, please follow these &lt;a href=&quot;#repro-instructions&quot;&gt;repro instructions&lt;/a&gt;. For the rest of the blog, we’ll walk through relevant details for GemLite, TorchAO and SGlang both in terms of the design of the library itself and integration in addressing the problems we mentioned above, in the end we’ll present the benchmarking results on Llama 3.1-8B model across different batch sizes and tensor parallel sizes.&lt;/p&gt;

&lt;h2 id=&quot;1-teaser-of-results&quot;&gt;1. Teaser of Results&lt;/h2&gt;

&lt;p&gt;Following is a summary of the results in 8xH100 machine on Llama 3.1-8B for decode. For all experiments, the baseline is bfloat16 torch.compiled model:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;bfloat16 w/ torch.compile
   &lt;/td&gt;
   &lt;td&gt;int4 weight only quantization, group size 64
   &lt;/td&gt;
   &lt;td&gt;float8 per row dynamic quantization
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size 1, TP size 1
   &lt;/td&gt;
   &lt;td&gt;131 tokens/sec
   &lt;/td&gt;
   &lt;td&gt;255 tokens/sec (1.95x speedup)
   &lt;/td&gt;
   &lt;td&gt;166 tokens/sec (1.27x speedup)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size 32, TP size 1
   &lt;/td&gt;
   &lt;td&gt;2799 tokens/sec
   &lt;/td&gt;
   &lt;td&gt;3241 tokens/sec (1.16x speedup)
   &lt;/td&gt;
   &lt;td&gt;3586 tokens/sec (1.28x speedup)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size 32, TP size 4
   &lt;/td&gt;
   &lt;td&gt;5575 tokens/sec
   &lt;/td&gt;
   &lt;td&gt;6334 tokens/sec (1.14x speedup)
   &lt;/td&gt;
   &lt;td&gt;6159 tokens/sec (1.10x speedup)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Our solution supports NVIDIA GPUs, including H100 and A100, and achieves speedup over the compiled bfloat16 baseline across batch sizes and TP sizes for both int4 weight only (from 1.14x to 1.95x) and float8 dynamic quantization (from 1.10x to 1.28x). Note that quantization may have a small impact on accuracy, which is outside the scope of this blogpost. Our int4 weight-only quantization is compatible with accuracy preserving techniques like HQQ. Please refer to &lt;a href=&quot;https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#cuda-backend-1&quot;&gt;TorchAO’s README&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/mobiuslabsgmbh/Llama-3.1-8b-instruct_4bitgs64_hqq_calib&quot;&gt;this benchmark&lt;/a&gt;, and &lt;a href=&quot;https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/&quot;&gt;this blog&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h2 id=&quot;2-gemlite-kernel-development&quot;&gt;2. GemLite: Kernel Development&lt;/h2&gt;

&lt;p&gt;The kernels were developed as part of GemLite, a project dedicated to optimizing low-bit matrix multiplication kernels. Developed using Triton, GemLite provides highly flexible and performant solutions across various activations, bitrates and hardware. In a nutshell, the kernels offer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for various activation data types:  fp16, int8 and fp8&lt;/li&gt;
  &lt;li&gt;Compatibility: works seamlessly with non-packed (e.g., int8, fp8) and packed formats (e.g., uint4, uint2, uint1)&lt;/li&gt;
  &lt;li&gt;Performance Optimization: includes optimized kernels and autotuning tools to achieve high performance across different hardware and batch sizes&lt;/li&gt;
  &lt;li&gt;Integration: Compatible with torch.compile and CUDA graphs, ensuring support for advanced features like tensor parallelism&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kernel-selection&quot;&gt;Kernel Selection&lt;/h3&gt;

&lt;p&gt;Optimizing kernel selection for large language model (LLM) generation requires addressing the distinct needs of different batch sizes. LLM workloads involve a mix of compute-bound and memory-bound iterations: smaller batch sizes are memory-bound, while larger batch sizes become compute-bound. GemLite kernels are designed to adapt to these varying demands, ensuring optimal execution for each scenario.&lt;/p&gt;

&lt;p&gt;In memory-bound scenarios, where data transfer is the limiting factor, the processor often waits for data to be fetched, leading to underutilized computational resources. For batch size = 1, a GEMV kernel performs best, whereas for larger batch sizes, GEMM kernels are more efficient. For batch sizes between 2 and 64, when matrices are “skinny,” a GEMM-SPLITK kernel is used to enable better GPU utilization (&lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;arXiv&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;GemLite includes the following kernels optimized for each of these scenarios:&lt;/p&gt;

&lt;h3 id=&quot;single-sample-inference&quot;&gt;Single Sample Inference&lt;/h3&gt;

&lt;p&gt;For single-sample inferences, we use GEMV kernels. However, asymmetric quantization methods require additional metadata, such as scales and zero points, to be loaded for each block. This can lead to increased memory transfer, so careful handling is essential.&lt;/p&gt;

&lt;p&gt;Specifically, for packed data, our experiments indicate that loading scales and zero points only once per two consecutive blocks minimizes redundant operations. Since these blocks share the same metadata, this approach results in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;5–8% end-to-end inference speedup compared to the default GEMV kernel&lt;/li&gt;
  &lt;li&gt;30–40% improvement over the traditional Split-K method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This new kernel/algorithm, GEMV_REVSPLITK, is available &lt;a href=&quot;https://github.com/mobiusml/gemlite/blob/master/gemlite/triton_kernels/gemv_revsplitK_A16fWnO16f_int32packing.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For non-packed data, the &lt;a href=&quot;https://github.com/mobiusml/gemlite/blob/master/gemlite/triton_kernels/gemv_splitK_A16fWnO16f_int32packing.py&quot;&gt;GEMV_SPLITK&lt;/a&gt; algorithm is employed. This algorithm iterates over the k-dimension to compute the dot product without relying on Triton’s tl.dot.&lt;/p&gt;

&lt;h3 id=&quot;batched-inference&quot;&gt;Batched Inference&lt;/h3&gt;

&lt;p&gt;For moderate batch sizes, we use the GEMM-based Split-K method (&lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;arXiv&lt;/a&gt;) which splits the k-dimension (weight rows) into multiple jobs. The optimal-split SPLIT_K parameter is found by autotuning values ranging from 1 to 16. Setting SPLIT_K=1 enables a fallback implementation to a GEMM kernel, allowing the same kernel code to be used for compute-bound batch sizes starting from 32 and 64, depending on the matrix shape and the device.&lt;/p&gt;

&lt;h3 id=&quot;maximizing-high-performance-key-implementation-insights&quot;&gt;Maximizing High Performance: Key Implementation Insights&lt;/h3&gt;

&lt;p&gt;Various implementation details must be carefully addressed to achieve high performance. Following are some of the key aspects we focused on to ensure high performance:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Autotuning for Performance&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://triton-lang.org/main/python-api/generated/triton.autotune.html&quot;&gt;Autotuning&lt;/a&gt; is critical for achieving optimal kernel performance. Since this process can be time-intensive, GemLite provides tools to automatically save and load autotuning results for all kernels. This ensures that the autotuning process is performed only once per GPU device, minimizing runtime, reducing repetitive overhead, and maintaining consistent performance across runs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensuring Kernel Correctness&lt;/p&gt;

    &lt;p&gt;Ensuring kernel correctness across different quantization and configuration settings is essential. Triton’s &lt;a href=&quot;https://triton-lang.org/main/python-api/generated/triton.autotune.html&quot;&gt;early configuration pruning&lt;/a&gt; plays a key role in this process. For example, during Split-K tuning, configurations are selected only if K is divisible by BLOCK_SIZE_K × SPLIT_K,, and BLOCKS_SIZE_K  is further pruned based on the group-size value. This approach ensures both efficiency and correctness in kernel operation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overcoming Bit-Unpacking Bottlenecks&lt;/p&gt;

    &lt;p&gt;When deploying on data center-grade GPUs like NVIDIA’s A100 and H100, performance bottlenecks related to bit-unpacking were observed. To mitigate these, various bit-packing configurations were explored, including packing along columns versus rows and experimenting with different bit-packing widths (e.g., 8-bit vs. 32-bit). Notably, transitioning from 32-bit to 8-bit packing delivered performance improvements of up to 18% on the A100 and 6% on the H100&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;torch.compile compatibility&lt;/p&gt;

    &lt;p&gt;To ensure seamless compatibility with PyTorch’s torch.compile, kernel calls are wrapped in a &lt;a href=&quot;https://pytorch.org/tutorials/advanced/python_custom_ops.html&quot;&gt;custom_op&lt;/a&gt;. This integration allows advanced features such as pre-hooks and early configuration pruning to function correctly, delivering accurate results without sacrificing performance. While some of these &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/139059&quot;&gt;features&lt;/a&gt; are not yet fully supported in PyTorch, the custom_op implementation effectively bridges the gap, ensuring smooth integration and high performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-torchao&quot;&gt;3. TorchAO&lt;/h2&gt;

&lt;p&gt;TorchAO is a PyTorch native quantization and sparsity library for both training and inference, featuring simple user APIs to train, quantize and deploy low precision models, and composability with other PyTorch features like distributed inference and torch.compile.&lt;/p&gt;

&lt;p&gt;PyTorch does not support low precision dtypes or different packing formats by default. With Tensor Subclass, we extend PyTorch native Tensor abstractions and model quantization as dtype conversion, while different packing formats for custom kernels are handled through layouts. For example, we support quantized linear operations with int4 weights, packed in a Tensor Core friendly layout, with tinygemm or GemLite kernel implementations. More details can be found &lt;a href=&quot;https://pytorch.org/ao/stable/contributor_guide.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llm-inference/fg1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Apart from more PyTorch native abstractions for developers, we want to highlight two benefits of this design for modeling users.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pytorch.org/ao/stable/serialization.html&quot;&gt;Serialization&lt;/a&gt;: Save and load quantized weights into a state_dict just like a floating point model, eliminating the need to transform floating point model to quantized model before the quantized weights are loaded. This reduces friction of distributing and deploying quantized models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#torch-tensor-parallel&quot;&gt;Composability&lt;/a&gt;: Seamless integration with downstream features like tensor parallel, allowing users to focus on modeling without worrying about compatibility with tensor parallel, torch.compile, and other PyTorch features. Since these features are implemented with Tensor level abstraction, users can quantize and do distributed inference with no model changes most of the time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;gemlite-kernel-integration&quot;&gt;GemLite Kernel Integration&lt;/h3&gt;

&lt;p&gt;To achieve the aforementioned benefits for the GemLite kernel, we integrated GemLite into TorchAO. This integration takes advantage of GemLite’s wide support and flexibility to allow for weight only quantization at 4 and 8 bits, under asymmetric and symmetric quantization schemes, 32 and 8 bit packing sizes, as well as grouped and ungrouped quantization. We enable this integration via the  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; api which can be used alongside the GemLite constructor as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quantize_(model, gemlite_uintx_weight_only(group_size, bit_width, packing_bitwidth))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The primary difficulty in creating this integration was making sure that the TorchAO composability guarantees were satisfied for the entire breadth of GemLite quantization kernel options. While the primary integration was relatively straight forward, making sure every different quantization type and their associated kernels worked well with tensor parallel was non-trivial.&lt;/p&gt;

&lt;h3 id=&quot;torch-tensor-parallel&quot;&gt;Torch Tensor Parallel&lt;/h3&gt;

&lt;p&gt;Tensor Parallelism is an effective way to speed up LLM inference. TP shards large matrices of linear or embedding modules onto multiple devices, typically in column-wise or row-wise styles. As the weight matrix gets distributed, computation is decomposed too. For example, the column-wise pattern below enables simultaneous matrix-vector multiply on four devices:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llm-inference/fg5.jpg&quot; alt=&quot;equation&quot; style=&quot;max-width:300px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch implements TP by converting a regular tensor (e.g. matrix &lt;em&gt;A&lt;/em&gt;) into a &lt;em&gt;DTensor&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dtensor = _shard_tensor(mA, device_mesh, (Shard(0),))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since DTensor stores meta information about the sharding, it knows how to reconstruct the full result when needed. Take Transformers’ feedforward module for example, as the down projection and up projection use column-wise and row-wise sharding respectively, DTensor will automatically perform an all-reduce on the ranks’ results as they move into the next operation. Such automation allows model authors to focus on computation without worrying about the communication needed for distributed execution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensor Parallel and Quantization Order&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since both DTensor and quantization are tensor-level transformations, the application order matters in ensuring a workflow can generally work on different setups. We have two observations: (i) checkpoints are typically saved in quantized formats, to save the quantization overhead before each run; and (ii) TP may run on a different number of devices, depending on resource constraints or service agreements. As such, we first apply quantization to the original tensor, save it to disk depending on whether a reuse is desired. At service launch time, we load the quantized checkpoint and shard the tensors into DTensors on-the-fly as we load them into the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensor Parallel Support in TorchAO&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since we quantize the model first then distribute the Tensor, we’ll have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTensor(QuantizedTensor(weight))&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTensor&lt;/code&gt; means a distributed Tensor class and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QuantizedTensor&lt;/code&gt; means a quantized tensor class in TorchAO. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QuantizedTensor&lt;/code&gt; should support the operators called when constructing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTensor&lt;/code&gt;, including slice and view ops. To make sure the overall execution is efficient, the packed weight that’s sliced in the dimension 0 and 1 should match the result of first slice the unpacked weight then pack (pack and slice operation should commute), otherwise the packing format is not compatible with tensor parallelism.&lt;/p&gt;

&lt;h2 id=&quot;4-sglang&quot;&gt;4. SGLang&lt;/h2&gt;

&lt;p&gt;SGLang is a fast serving framework for large language models and vision language models. It is known for its almost &lt;a href=&quot;https://lmsys.org/blog/2024-12-04-sglang-v0-4/&quot;&gt;zero-overhead batch scheduler&lt;/a&gt; and fast &lt;a href=&quot;https://lmsys.org/blog/2024-02-05-compressed-fsm/&quot;&gt;constrained decoding&lt;/a&gt;. It is mainly implemented in Python, lightweight, and easy to hack. It is also one of the first frameworks to integrate torch.compile.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TorchAO integration in SGLang&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We integrated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; API for applying a specific type of quantization to model into SGLang that supports int4 weight only quantization (both tinygemm and GemLite version), float8 dynamic quantization and a few other types of quantization so far. Users can enable quantization by adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--torchao-config&lt;/code&gt; argument to the benchmarking script. The currently enabled options also support tensor parallelism through composition with DTensor that is enabled with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--tp-size&lt;/code&gt; option.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Torch Native Tensor Parallel Support in SGLang&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Existing model definitions in SGLang use special linear modules that are coupled with tensor parallelism style, for example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MergedColumnParallelLinear&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QKVParallelLinear&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RowParallelLinear&lt;/code&gt;. To decouple the model definition and tensor parallelization style, we defined a &lt;a href=&quot;https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/torch_native_llama.py&quot;&gt;pytorch native model&lt;/a&gt; that uses plain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Linear&lt;/code&gt; module from PyTorch and rely on PyTorch tensor parallelism APIs for parallelization and torch.compile for speedup. At related module hierarchies, we add a dictionary describing how a submodule should be parallelized. For example, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;class LlamaAttention&lt;/code&gt;, we define:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;_tp_plan = {
    &quot;qkv_proj&quot;: &quot;Colwise_Sharded&quot;,
    &quot;o_proj&quot;: &quot;Rowwise&quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;qkv_proj&quot; &lt;/code&gt;and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;o_proj&quot; &lt;/code&gt;are the FQNs of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wqkv&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wo&lt;/code&gt; projections, and the values are their TP styles.&lt;/p&gt;

&lt;p&gt;We then define a TP engine in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_parallel.py&lt;/code&gt;. It searches for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_tp_plan &lt;/code&gt;recursively within the model, and applies the indicated TP styles to the submodules using PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.parallelize_module&quot;&gt;parallelize_module&lt;/a&gt; API.&lt;/p&gt;

&lt;h2 id=&quot;5-results&quot;&gt;5. Results&lt;/h2&gt;

&lt;p&gt;The evaluation focused on two popular quantization techniques for H100 machines: int4 weight-only quantization and float8 dynamic quantization. These methods were chosen due to their widespread use in optimizing memory efficiency and computational performance on H100 machines, making them ideal candidates for benchmarking against various workloads.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;int4 Weight-Only Quantization&lt;/strong&gt;: This method significantly reduces memory footprint and accelerates decode for memory-bound workloads, with minimal impact on performance in compute-intensive scenarios like prefill or larger batch sizes. We present results for bf16, GemLite, and tinygemm kernels below, across various batch sizes and tensor parallel configurations&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;float8 Dynamic Quantization&lt;/strong&gt;: While offering less memory savings, this method often provides higher accuracy and balanced speedups for both memory-bound and compute-bound tasks. With Hopper-grade hardware and native fp8 support, the efficient cutlass/cuBLAS kernels used by AO contribute to a significant speedup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The graphs below show the decode tokens/sec for different tp sizes, each graph shows the results across different batch sizes and for different types of quantization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BF16 is our bfloat16, torch.compile’d baseline&lt;/li&gt;
  &lt;li&gt;tinygemm-4-64 is using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int4_weight_only&lt;/code&gt; quantization in TorchAO, it’s a 4 bit groupwise quantization with group size of 64, using tinygemm kernel&lt;/li&gt;
  &lt;li&gt;gemlite-4-64 is using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gemlite_uintx_weight_only &lt;/code&gt;quantization in TorchAO, 4 means 4 bit, and 64 is also the group size, using GemLite kernel&lt;/li&gt;
  &lt;li&gt;fp8dq-per_row is using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float8_dynamic_activation_float8_weight&lt;/code&gt; quantization in TorchAO, both activation and weights are quantized with per row scales&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llm-inference/fg2.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llm-inference/fg3.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llm-inference/fg4.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For int4 weight-only quantization, at batch size 1, the tinygemm kernel achieved the best performance. However, its efficiency declined with increasing batch sizes. Conversely, GemLite effectively bridged this gap, delivering superior performance at larger batch sizes. GemLite also achieved a 9–10x speedup during the prefill phase compared to tinygemm, despite ongoing performance optimizations constrained by Triton.&lt;/p&gt;

&lt;p&gt;Float8 dynamic quantization showed 1.3x speedup over bfloat16 consistently with tensor parallel size 1 across different batch sizes and 1.1x to 1.2x speedup in larger tensor parallel sizes. As the tensor parallel size increases, the overall speedup decreases, which is expected due to the reduction in matmul size. Note that we do expect to get speedup for prefill as well, but since we rely on torch.compile for speedup and prefill compile is not enabled in SGLang yet, we will leave this for future work.&lt;/p&gt;

&lt;h3 id=&quot;repro-instructions&quot;&gt;Repro Instructions&lt;/h3&gt;

&lt;p&gt;We conducted benchmarks on an 8xH100 machine using GemLite 0.4.1, SGLang built from commit feb2b76, TorchAO nightly 0.8.0.dev20241223+cu124, and PyTorch 2.5.1. The Llama-3.1 Instruct models were chosen as the architecture for evaluation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BATCH_SIZE=16
# Note: gemlite is only compatible with float16
# while int4wo-64 (tinygemm-4-64 as shown in the graph) and fp8dq-per_row should use bfloat16
DTYPE=float16
# int4wo-64, fp8dq-per_tensor
TORCHAO_CONFIG=gemlite-4-64
TP_SIZE=2
# Decode performance
python3 -m sglang.bench_offline_throughput --model-path meta-llama/Llama-3.1-8B-Instruct --json-model-override-args '{&quot;architectures&quot;: [&quot;TorchNativeLlamaForCausalLM&quot;]}' --dataset-name random --random-input 1024 --random-output 512 --random-range 1 --num-prompts $BATCH_SIZE --enable-torch-compile --dtype $DTYPE --torchao-config $TORCHAO_CONFIG --tp-size $TP_SIZE

# Example output
# Benchmark...
# [2024-12-20 12:42:16 TP0] Prefill batch. #new-seq: 2, #new-token: 2046, #cached-token: 4, cache hit rate: \0.06%, token usage: 0.00, #running-req: 0, #queue-req: 0
# ...
# [2024-12-20 12:45:35 TP0] Decode batch. #running-req: 16, #token: 16763, token usage: 0.01, gen throughput\ (token/s): 2.20, #queue-req: 0
# [2024-12-20 12:45:38 TP0] Decode batch. #running-req: 16, #token: 24443, token usage: 0.02, gen throughput\ (token/s): 2739.89, #queue-req: 0

# We reported the last throughput (token/s) as the performance for decode
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With performant and extensible kernels from &lt;a href=&quot;https://github.com/mobiusml/gemlite&quot;&gt;GemLite&lt;/a&gt;, PyTorch native architecture optimization library &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;TorchAO&lt;/a&gt; and high performance inference framework &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt;, we showcased fast end-to-end quantized inference for both int4 and float8 across different batch sizes and tensor parallel sizes with simple and composable user APIs to reduce the resource requirement for LLMs. This integration is our first step towards meeting the needs of fast inference across different models, workloads, precisions and hardwares and we are looking forward to continuing advancing the state of the art for end to end mixed and low precision LLM inference.&lt;/p&gt;

&lt;p&gt;Our immediate future work focuses on the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exploring diverse combinations of weight and activation quantization to strike the best balance between speed and accuracy&lt;/li&gt;
  &lt;li&gt;Extending support to additional GPU architectures to broaden accessibility&lt;/li&gt;
  &lt;li&gt;Enhancing compatibility with MoE models to address growing demands in scalable inference&lt;/li&gt;
  &lt;li&gt;Allow for easy integration of fast custom kernels in TorchAO so that they can be easily leveraged by SGLang and other inference frameworks&lt;/li&gt;
  &lt;li&gt;While we didn’t measure accuracy impact in this blogpost, we can develop auto quantization tool in TorchAO to allow users to trade off between performance and accuracy&lt;/li&gt;
  &lt;li&gt;Better integration with tensor parallelism in SGLang to support running larger models&lt;/li&gt;
  &lt;li&gt;Enable torch.compile for prefill phase in SGLang&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also invite the community to actively test, provide feedback, and contribute to shaping the future of fast and efficient LLM inference.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Teams at PyTorch, Mobius Labs and SGLang</name>
        
        
      </author>

      

      

      
        <summary type="html">Large Language Models (LLMs) are typically very resource-intensive, requiring significant amounts of memory, compute and power to operate effectively. Quantization provides a solution by reducing weights and activations from 16 bit floats to lower bitrates (e.g., 8 bit, 4 bit, 2 bit), achieving significant speedup and memory savings and also enables support for larger batch sizes.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">MLOps Workflow Simplified for PyTorch with Arm and GitHub Collaboration</title>
      <link href="https://pytorch.org/blog/mlops-workflow/" rel="alternate" type="text/html" title="MLOps Workflow Simplified for PyTorch with Arm and GitHub Collaboration" />
      <published>2025-01-15T00:00:00-08:00</published>
      <updated>2025-01-15T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/mlops-workflow</id>
      <content type="html" xml:base="https://pytorch.org/blog/mlops-workflow/">&lt;p&gt;PyTorch is one of the most widely used and most powerful deep learning frameworks for training and deploying complex neural networks. It has never been easier to train and deploy AI applications, and low-cost, high-performance, energy-efficient hardware, tools, and technology for creating optimized workflows are more accessible than ever.  But data science, machine learning, and devops can be deep topics unto themselves, and it can be overwhelming for developers with one specialty to see how they all come together in the real world, or even to know where to get started.&lt;/p&gt;

&lt;p&gt;To that end, we at Arm have collaborated with our friends at GitHub to decompose the basic elements of real world MLOps pipelines that use PyTorch models and create a simplified workflow and MLOps tutorial that anyone with a GitHub and a Docker Hub account can leverage.&lt;/p&gt;

&lt;h2 id=&quot;mlops-overview&quot;&gt;MLOps Overview&lt;/h2&gt;

&lt;p&gt;The software development lifecycle for machine learning applications typically starts from training data, which is used to train sophisticated neural networks (NNs) that are optimized, integrated into software images, and then deployed onto compute clusters and even fleets of devices in the field.  These devices are typically continuously collecting data and are managed by cloud services, which actively monitor performance of the ML algorithm(s) and feedback data for retraining in the next iteration of the lifecycle – enabling continuous improvement of the algorithms, as well as supporting deployment of new AI features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mlops-workflow/fg1.png&quot; alt=&quot;process flow chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example of a typical ML software development lifecycle.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Scott Arbeit from GitHub recently published an &lt;a href=&quot;https://github.blog/enterprise-software/ci-cd/streamlining-your-mlops-pipeline-with-github-actions-and-arm64-runners/&quot;&gt;excellent blog&lt;/a&gt; that highlights the importance of MLOps in machine learning and describes automation via simplified GitHub actions for several key tasks including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data preprocessing&lt;/strong&gt;: cleaning and preparation of data for training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model training and validation&lt;/strong&gt;: automatic execution of training scripts when new data is pushed or when changes are made to the model code.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: automatic packaging and deployment of models to production environments upon successful training and validation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monitoring and alerts:&lt;/strong&gt; workflows to monitor model performance and send alerts if certain thresholds are breached.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The article also describes a conceptual efficient MLOps pipeline that takes advantage of new, low-cost Arm Runners natively integrated into GitHub Actions to train and validate PyTorch models. It also uses containerization for consistent deployment across different environments.&lt;/p&gt;

&lt;p&gt;Our team at Arm put GitHub’s ideas and conceptual workflow into practice and created a tutorial to help you get started today.&lt;/p&gt;

&lt;h2 id=&quot;optimizing-your-pytorch-mlops-workflow&quot;&gt;Optimizing Your PyTorch MLOps Workflow&lt;/h2&gt;

&lt;p&gt;A new &lt;a href=&quot;https://learn.arm.com/&quot;&gt;Arm Learning Path&lt;/a&gt; unpacks each of the key phases described in Scott’s blog, and demonstrates each key task in detail, providing prescriptive instructions and code examples to leverage several aspects of the PyTorch framework to implement each phase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mlops-workflow/fg2.png&quot; alt=&quot;process flow chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key ML tasks to setup and automate with GitHub Actions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With this learning path you will be able to take advantage of the following strategies with a real-world object detection use case to make your own streamlined MLOps workflow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Containerization:&lt;/strong&gt; Package your PyTorch model and its dependencies into a Docker container to help ensure consistent performance across different environments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficient Data Loading:&lt;/strong&gt; Optimize data loading pipelines to help minimize I/O bottlenecks and maximize GPU utilization.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Optimization:&lt;/strong&gt; Explore techniques like model quantization, pruning, and knowledge distillation to help reduce model size and improve inference speed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Leverage PyTorch’s Ecosystem:&lt;/strong&gt; Utilize libraries like TorchVision to help streamline common deep learning tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monitor and Profile:&lt;/strong&gt; Monitor resource utilization and identify potential bottlenecks to further optimize your workflow.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;an-end-to-end-mlops-workflow&quot;&gt;An End-to-End MLOps Workflow&lt;/h2&gt;

&lt;p&gt;The best part of this learning path is not just that it takes you through each task in detail, but it brings it all together into a unified automated workflow.&lt;/p&gt;

&lt;p&gt;With GitHub Actions, you can build an end-to-end custom MLOPs workflow that combines and automates the individual workflows for each ML task.  To demonstrate this, the repository contains a workflow in a boilerplate .yml file that automates the individual steps.&lt;/p&gt;

&lt;p&gt;You can run an MLOps workflow using GitHub Actions natively for managing all the steps in your ML application’s lifecycle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mlops-workflow/fg3.png&quot; alt=&quot;process flow chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A successful run of this MLOps workflow in GitHub Actions.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself!&lt;/h2&gt;

&lt;p&gt;Our Arm team has battle-tested this tutorial in the field and delivered the tutorial as a workshop at GitHub Universe 2024 earlier this year.  Now it’s time for you to take it for a spin and get hands-on with PyTorch and MLOps.&lt;/p&gt;

&lt;p&gt;Try the Arm Learning Path &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/gh-runners/&quot;&gt;Here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;By the end of this tutorial, you can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set up a new GitHub Arm-runner to natively build an arm64 image to take advantage of the lowest-cost, most power efficient compute available.&lt;/li&gt;
  &lt;li&gt;Train and test a PyTorch ML model with the German Traffic Sign Recognition Benchmark (GTSRB) dataset.&lt;/li&gt;
  &lt;li&gt;Compare the performance of two trained PyTorch ML models; one model compiled with OpenBLAS (Open Basic Linear Algebra Subprograms Library) and oneDNN (Deep Neural Network Library), and the other model compiled with Arm Compute Library (ACL).&lt;/li&gt;
  &lt;li&gt;Containerize a ML model and push the container to DockerHub.&lt;/li&gt;
  &lt;li&gt;Automate each task into a single MLOps pipeline Using GitHub Actions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combining the power of PyTorch with the simplicity of GitHub Actions and the efficiency of native Arm Runners significantly helps you accelerate your deep learning development and deployment processes. Following the best practices outlined in this blog post helps you achieve optimal performance and cost-effectiveness for your PyTorch projects.&lt;/p&gt;

&lt;p&gt;We’d love to see what you create based on this example.  If you have created your own Arm Learning Path, you are invited to &lt;a href=&quot;https://learn.arm.com/learning-paths/cross-platform/_example-learning-path/&quot;&gt;share it here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Eric Sondhi, Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch is one of the most widely used and most powerful deep learning frameworks for training and deploying complex neural networks. It has never been easier to train and deploy AI applications, and low-cost, high-performance, energy-efficient hardware, tools, and technology for creating optimized workflows are more accessible than ever. But data science, machine learning, and devops can be deep topics unto themselves, and it can be overwhelming for developers with one specialty to see how they all come together in the real world, or even to know where to get started.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">GenAI Acceleration for PyTorch 2.5 on Intel® Xeon®Processors</title>
      <link href="https://pytorch.org/blog/genai-acceleration-intel-xeon/" rel="alternate" type="text/html" title="GenAI Acceleration for PyTorch 2.5 on Intel® Xeon®Processors" />
      <published>2025-01-14T00:00:00-08:00</published>
      <updated>2025-01-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/genai-acceleration-intel-xeon</id>
      <content type="html" xml:base="https://pytorch.org/blog/genai-acceleration-intel-xeon/">&lt;p&gt;This blog is the fifth in a series focused on accelerating generative AI models with pure, native PyTorch. We demonstrate the GenAI acceleration of GPTFast, Segment Anything Fast, and Diffusion Fast on Intel® Xeon®Processors.&lt;/p&gt;

&lt;p&gt;First, we revisit GPTFast, a remarkable work that speeds up text generation in under 1000 lines of native PyTorch code. Initially, GPTFast supported only the CUDA backend. We will show you how to run GPTFast on CPU and achieve additional performance speedup with weight-only quantization (WOQ).&lt;/p&gt;

&lt;p&gt;In Segment Anything Fast, we have incorporated support for the CPU backend and will demonstrate performance acceleration by leveraging the increased power of CPU with BFloat16, torch.compile, and scaled_dot_product_attention (SDPA) with a block-wise attention mask. The speedup ratio against FP32 can reach 2.91x in vit_b and 3.95x in vit_h.&lt;/p&gt;

&lt;p&gt;Finally, Diffusion Fast now supports the CPU backend and leverages the increased power of CPU with BFloat16, torch.compile, and SDPA. We also optimize the layout propagation rules for convolution, cat, and permute in Inductor CPU to improve performance. The speedup ratio against FP32 can achieve 3.91x in Stable Diffusion XL (SDXL).&lt;/p&gt;

&lt;h2 id=&quot;optimization-strategies-to-boost-performance-on-pytorch-cpu&quot;&gt;Optimization strategies to boost performance on PyTorch CPU&lt;/h2&gt;

&lt;h3 id=&quot;gptfast&quot;&gt;GPTFast&lt;/h3&gt;

&lt;p&gt;Over the past year, generative AI has achieved great success across various language tasks and become increasingly popular. However, generative models face high inference costs due to the memory bandwidth bottlenecks in the auto-regressive decoding process. To address these issues, the PyTorch team published GPTFast which targets accelerating text generation with only pure, native PyTorch. This project developed an LLM from scratch almost 10x faster than the baseline in under 1000 lines of native PyTorch code. Initially, GPTFast supported only the CUDA backend and garnered approximately 5,000 stars in about four months. Inspired by Llama.cpp, the Intel team provided CPU backend support starting with the PyTorch 2.4 release, further enhancing the project’s availability in GPU-free environments. The following are optimization strategies used to boost performance on PyTorch CPU:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Torch.compile&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;torch.compile is a PyTorch function introduced since PyTorch 2.0 that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Weight-only Quantization&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Weight-only quantization (WOQ) is a trade-off between the performance and the accuracy since the bottleneck of the auto-regressive decoding phase in text generation is the memory bandwidth of loading weights and generally WOQ could lead to better accuracy compared to traditional quantization approach such as W8A8. GPTFast supports two types of WOQs: W8A16 and W4A16. To be specific, activations are stored in BFloat16 and model weights could be quantized to int8 and int4, as shown in Figure 1.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1. Weight-only Quantization Pattern. Source: Mingfei Ma, Intel&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Weight Prepacking &amp;amp; Micro Kernel Design.&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;To maximize throughput, GPTFast allows model weights to be prepacked into hardware-specific layouts on int4 using internal PyTorch ATen APIs. Inspired by Llama.cpp, we prepacked the model weights from [N, K] to [N/kNTileSize, K, kNTileSize/2], with kNTileSize set to 64 on avx512. First, the model weights are blocked along the N dimension, then the two innermost dimensions are transposed. To minimize de-quantization overhead in kernel computation, we shuffle the 64 data elements on the same row in an interleaved pattern, packing Lane2 &amp;amp; Lane0 together and Lane3 &amp;amp; Lane1 together, as illustrated in Figure 2.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg2.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2. Weight Prepacking on Int4. Source: Mingfei Ma, Intel&lt;/p&gt;

&lt;p&gt;During the generation phase, the torch.nn.Linear module will be lowered to be computed with high-performance kernels inside PyTorch ATen, where the quantized weights will be de-quantized first and then accumulated with fused multiply-add (FMA) at the register level, as shown in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3. Micro Kernel Design. Source: Mingfei Ma, Intel&lt;/p&gt;

&lt;h3 id=&quot;segment-anything-fast&quot;&gt;Segment Anything Fast&lt;/h3&gt;

&lt;p&gt;Segment Anything Fast offers a simple and efficient PyTorch native acceleration for the Segment Anything Model (SAM) , which is a zero-shot vision model for generating promptable image masks. The following are optimization strategies used to boost performance on PyTorch CPU:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BFloat16&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Bfloat16 is a commonly used half-precision type. Through less precision per parameter and activations, we can save significant time and memory in computation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Torch.compile&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;torch.compile is a PyTorch function introduced since PyTorch 2.0 that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable developers to run their PyTorch programs faster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scaled Dot Product Attention (SDPA)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Scaled Dot-Product Attention (SDPA) is a crucial mechanism in transformer models. PyTorch offers a fused implementation that significantly outperforms a naive approaches. For Segment Anything Fast, we convert the attention mask from bfloat16 to float32 in a block-wise manner. This method not only reduces peak memory usage, making it ideal for systems with limited memory resources, but also enhances performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;diffusion-fast&quot;&gt;Diffusion Fast&lt;/h3&gt;

&lt;p&gt;Diffusion Fast offers a simple and efficient PyTorch native acceleration for text-to-image diffusion models. The following are optimization strategies used to boost performance on PyTorch CPU:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BFloat16&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Bfloat16 is a commonly used half-precision type. Through less precision per parameter and activations, we can save significant time and memory in computation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Torch.compile&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;torch.compile is a PyTorch function introduced since PyTorch 2.0 that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scaled Dot Product Attention (SDPA)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;SDPA is a key mechanism used in transformer models, PyTorch provides a fused implementation to show large performance benefits over a naive implementation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-usage-on-native-pytorch-cpu&quot;&gt;Model Usage on Native PyTorch CPU&lt;/h2&gt;

&lt;h3 id=&quot;gptfast-1&quot;&gt;&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;GPTFast&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;To launch WOQ in GPTFast, first quantize the model weights. For example, to quantize with int4 and group size of 32:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int4 –group size 32
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then run generation by passing the int4 checkpoint to generate.py&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python generate.py --checkpoint_path checkpoints/$MODEL_REPO/model_int4.g32.pth --compile --device $DEVICE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To use CPU backend in GPTFast, simply switch DEVICE variable from cuda to CPU.&lt;/p&gt;

&lt;h3 id=&quot;segment-anything-fast-1&quot;&gt;&lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast&quot;&gt;Segment Anything Fast&lt;/a&gt;&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd experiments

export SEGMENT_ANYTHING_FAST_USE_FLASH_4=0

python run_experiments.py 16 vit_b &amp;amp;lt;pytorch_github&amp;gt; &amp;amp;lt;segment-anything_github&amp;gt; &amp;amp;lt;path_to_experiments_data&amp;gt; --run-experiments --num-workers 32 --device cpu

python run_experiments.py 16 vit_h &amp;amp;lt;pytorch_github&amp;gt; &amp;amp;lt;segment-anything_github&amp;gt; &amp;amp;lt;path_to_experiments_data&amp;gt; --run-experiments --num-workers 32 --device cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;diffusion-fast-1&quot;&gt;&lt;a href=&quot;https://github.com/huggingface/diffusion-fast&quot;&gt;Diffusion Fast&lt;/a&gt;&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_benchmark.py --compile_unet --compile_vae --device=cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance-evaluation&quot;&gt;Performance Evaluation&lt;/h2&gt;

&lt;h3 id=&quot;gptfast-2&quot;&gt;GPTFast&lt;/h3&gt;

&lt;p&gt;We ran llama-2-7b-chat model based on &lt;a href=&quot;https://github.com/yanbing-j/gpt-fast/tree/yanbing/int4pack_mm&quot;&gt;test branch&lt;/a&gt; and the above hardware configuration on PyTorch.  After applying the following steps, we saw a 3.8x boost compared to the baseline in eager mode:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to automatically fuse elementwise operators.&lt;/li&gt;
  &lt;li&gt;Reduce memory footprint with WOQ-int8.&lt;/li&gt;
  &lt;li&gt;Further reduce memory footprint with WOQ-int4.&lt;/li&gt;
  &lt;li&gt;Use AVX512 which enables faster de-quant in micro kernels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg4.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.  GPTFast Performance speedup in Llama2-7b-chat&lt;/p&gt;

&lt;h3 id=&quot;segment-anything-fast-2&quot;&gt;Segment Anything Fast&lt;/h3&gt;

&lt;p&gt;We ran Segment Anything Fast on the above hardware configuration on PyTorch and achieved a performance speedup of BFloat16 with torch.compile and SDPA compared with FP32 as shown in Figure 5. The speedup ratio against FP32 can achieve 2.91x in vit_b, and 3.95x in vit_h.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg5.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5. Segment Anything Fast Performance speedup in vit_b/vit_h&lt;/p&gt;

&lt;h3 id=&quot;diffusion-fast-2&quot;&gt;Diffusion Fast&lt;/h3&gt;

&lt;p&gt;We ran Diffusion Fast on the above hardware configuration on PyTorch and achieved a performance speedup of BFloat16 with torch.compile and SDPA compared with FP32 as shown in Figure 6. The speedup ratio against FP32 can achieve 3.91x in Stable Diffusion XL (SDXL).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/genai-acceleration-intel-xeon/fg6.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 6. Diffusion Fast Performance speedup in Stable Diffusion XL&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;In this blog, we introduced software optimizations for weight-only quantization, torch.compile, and SDPA, demonstrating how we can accelerate text generation with native PyTorch on CPU. Further improvements are expected with the support of the AMX-BF16 instruction set and the optimization of dynamic int8 quantization using torchao on CPU. We will continue to extend our software optimization efforts to a broader scope.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;The results presented in this blog are a joint effort between Meta and the Intel PyTorch Team. Special thanks to Michael Gschwind from Meta who spent precious time providing substantial assistance. Together we took one more step on the path to improve the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;related-blogs&quot;&gt;Related Blogs&lt;/h2&gt;

&lt;p&gt;Part 1: How to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;Segment Anything over 8x&lt;/a&gt; with Segment Anything Fast.&lt;/p&gt;

&lt;p&gt;Part 2: How to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;Llama-7B by almost 10x&lt;/a&gt; with help of GPTFast.&lt;/p&gt;

&lt;p&gt;Part 3: How to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-3/&quot;&gt;text-to-image diffusion models up to 3x&lt;/a&gt; with Diffusion Fast.&lt;/p&gt;

&lt;p&gt;Part 4: How to speed up FAIR’s &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-4/&quot;&gt;Seamless M4T-v2 model by 2.7x&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h2&gt;

&lt;p&gt;Figure 4: Intel Xeon Scalable Processors: Measurement on 4th Gen Intel Xeon Scalable processor using: 2x Intel(R) Xeon(R) Platinum 8480+, 56cores, HT On, Turbo On, NUMA 2, Integrated Accelerators Available [used]: DLB 2 [0], DSA 2 [0], IAA 2 [0], QAT 2 [0], Total Memory 512GB (16x32GB DDR5 4800 MT/s [4800 MT/s]), BIOS 3B07.TEL2P1, microcode 0x2b000590, Samsung SSD 970 EVO Plus 2TB, CentOS Stream 9, 5.14.0-437.el9.x86_64, run single socket (1 instances in total with: 56 cores per instance, Batch Size 1 per instance), Models run with PyTorch 2.5 wheel. Test by Intel on 10/15/24.&lt;/p&gt;

&lt;p&gt;Figure 5: Intel Xeon Scalable Processors: Measurement on 4th Gen Intel Xeon Scalable processor using: 2x Intel(R) Xeon(R) Platinum 8480+, 56cores, HT On, Turbo On, NUMA 2, Integrated Accelerators Available [used]: DLB 2 [0], DSA 2 [0], IAA 2 [0], QAT 2 [0], Total Memory 512GB (16x32GB DDR5 4800 MT/s [4800 MT/s]), BIOS 3B07.TEL2P1, microcode 0x2b000590, Samsung SSD 970 EVO Plus 2TB, CentOS Stream 9, 5.14.0-437.el9.x86_64, run single socket (1 instances in total with: 56 cores per instance, Batch Size 16 per instance), Models run with PyTorch 2.5 wheel. Test by Intel on 10/15/24.&lt;/p&gt;

&lt;p&gt;Figure 6: Intel Xeon Scalable Processors: Measurement on 4th Gen Intel Xeon Scalable processor using: 2x Intel(R) Xeon(R) Platinum 8480+, 56cores, HT On, Turbo On, NUMA 2, Integrated Accelerators Available [used]: DLB 2 [0], DSA 2 [0], IAA 2 [0], QAT 2 [0], Total Memory 512GB (16x32GB DDR5 4800 MT/s [4800 MT/s]), BIOS 3B07.TEL2P1, microcode 0x2b000590, Samsung SSD 970 EVO Plus 2TB, CentOS Stream 9, 5.14.0-437.el9.x86_64, run single socket (1 instances in total with: 56 cores per instance, Batch Size 1 per instance), Models run with PyTorch 2.5 wheel. Test by Intel on 10/15/24.&lt;/p&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;ai-disclaimer&quot;&gt;AI disclaimer:&lt;/h2&gt;

&lt;p&gt;AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at &lt;a href=&quot;https://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the Intel PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">This blog is the fifth in a series focused on accelerating generative AI models with pure, native PyTorch. We demonstrate the GenAI acceleration of GPTFast, Segment Anything Fast, and Diffusion Fast on Intel® Xeon®Processors.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Integrating Ascend Backend with Torchtune through PyTorch Multi-Device Support</title>
      <link href="https://pytorch.org/blog/ascend-backend-w-torchtune/" rel="alternate" type="text/html" title="Integrating Ascend Backend with Torchtune through PyTorch Multi-Device Support" />
      <published>2025-01-09T00:00:00-08:00</published>
      <updated>2025-01-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/ascend-backend-w-torchtune</id>
      <content type="html" xml:base="https://pytorch.org/blog/ascend-backend-w-torchtune/">&lt;p&gt;In this blog, we will briefly introduce torchtune, the Ascend backend, and demonstrate how torchtune can be used to fine-tune models with Ascend.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-torchtune&quot;&gt;Introduction to Torchtune&lt;/h2&gt;

&lt;p&gt;Torchtune is a PyTorch-native library designed to simplify the fine-tuning of Large Language Models (LLMs). Staying true to PyTorch’s design principles, it provides composable and modular building blocks, as well as easily extensible training recipes. torchtune allows developers to fine-tune popular LLMs with different training methods and model architectures while supporting training on a variety of consumer-grade and professional GPUs.&lt;/p&gt;

&lt;p&gt;You can explore more about torchtune’s code and tutorials here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;GitHub Repository&lt;/strong&gt;: 
The source code for torchtune is hosted on GitHub, where you can find the full implementation, commit history, and development documentation. Access the code repository here: &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;Torchtune GitHub Repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tutorials and Documentation&lt;/strong&gt;: 
Torchtune provides detailed tutorials to help users quickly get started with the fine-tuning process and demonstrate how to use torchtune for various tasks like training and evaluation. You can access the official tutorials here: &lt;a href=&quot;https://pytorch.org/torchtune/main/overview.html&quot;&gt;Torchtune Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In these resources, you’ll find not only how to fine-tune large language models using torchtune but also how to integrate with tools like PyTorch, Hugging Face, etc. They offer comprehensive documentation and examples for both beginners and advanced users, helping everyone customize and optimize their model training pipelines.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-ascend-backend&quot;&gt;Introduction to Ascend Backend&lt;/h2&gt;

&lt;p&gt;Ascend is a series of AI computing products launched by Huawei, offering a full-stack AI computing infrastructure that includes processors, hardware, foundational software, AI computing frameworks, development toolchains, management and operation tools, as well as industry-specific applications and services. These products together create a powerful and efficient AI computing platform that caters to various AI workloads.&lt;/p&gt;

&lt;p&gt;You can explore more about  Ascend here: &lt;a href=&quot;https://www.hiascend.com/en/&quot;&gt;Ascend Community&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-torchtune-integrates-with-ascend&quot;&gt;How Torchtune Integrates with Ascend&lt;/h2&gt;

&lt;p&gt;Initially, devices were primarily matched using device strings. However, torchtune later introduced an abstraction layer for devices, leveraging the &lt;em&gt;get_device_support()&lt;/em&gt; method to dynamically retrieve relevant devices based on the current environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ascend-backend-w-torchtune.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ascend is seamlessly integrated into torchtune via the &lt;em&gt;PrivateUse1&lt;/em&gt; feature provided by PyTorch. By importing &lt;em&gt;torch_npu&lt;/em&gt; and replacing the corresponding CUDA-like device operations with the &lt;em&gt;torch.device&lt;/em&gt; namespace from the environment supported by &lt;em&gt;device_support&lt;/em&gt;—such as torch.npu and torch.cuda—Ascend is effectively incorporated into torchtune. The PR is &lt;a href=&quot;https://github.com/pytorch/torchtune/pull/1826&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch_npu&lt;/em&gt; is a plugin developed for PyTorch, designed to seamlessly integrate Ascend NPU with the PyTorch framework, enabling developers to leverage the powerful computational capabilities of Ascend AI processors for deep learning training and inference. This plugin allows users to directly utilize Ascend’s computational resources within PyTorch without the need for complex migration or code changes.&lt;/p&gt;

&lt;h2 id=&quot;torchtune-quick-start-with-ascend&quot;&gt;Torchtune Quick Start with Ascend&lt;/h2&gt;

&lt;p&gt;In torchtune, there are two key concepts that are essential for customizing and optimizing the fine-tuning process: &lt;strong&gt;Config&lt;/strong&gt; and &lt;strong&gt;Recipe&lt;/strong&gt;. These concepts allow users to easily customize and optimize the fine-tuning process to suit different needs and hardware environments.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Config is a file used by torchtune to configure the training process. It contains settings for the model, data, training parameters, and more. By modifying the Config file, users can easily adjust various aspects of the training process, such as data loading, optimizer settings, and learning rate adjustments. Config files are typically written in YAML format, making them clear and easy to modify.&lt;/li&gt;
  &lt;li&gt;A Recipe in torchtune is a simple, transparent single-file training script in pure PyTorch. Recipes provide the full end-to-end training workflow but are designed to be hackable and easy to extend. Users can choose an existing Recipe or create a custom one to meet their fine-tuning needs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When fine-tuning a model using the Ascend backend, torchtune simplifies the process by allowing you to specify the device type directly in the configuration file. Once you specify &lt;strong&gt;npu&lt;/strong&gt; as the device type, torchtune automatically detects and utilizes the Ascend NPU for training and inference. This design allows users to focus on model fine-tuning without needing to worry about hardware details.&lt;/p&gt;

&lt;p&gt;Specifically, you just need to set the relevant parameters in the &lt;strong&gt;Config&lt;/strong&gt; file, indicating the device type as &lt;strong&gt;&lt;em&gt;npu&lt;/em&gt;&lt;/strong&gt;, such as:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Environment
device: npu
dtype: bf16

# Dataset
dataset:
  _component_: torchtune.datasets.instruct_dataset
  source: json
  data_files: ascend_dataset.json
  train_on_input: False
  packed: False
  split: train

# Other Configs …
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you’ve specified the &lt;strong&gt;npu&lt;/strong&gt; device type in your configuration file, you can easily begin the model fine-tuning process. Simply run the following command, and torchtune will automatically start the fine-tuning process on the Ascend backend:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune run &amp;lt;recipe_name&amp;gt; --config &amp;lt;your_config_file&amp;gt;.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, if you’re using a full fine-tuning recipe (full_finetune_single_device) and your configuration file is located at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascend_config.yaml&lt;/code&gt;, you can start the fine-tuning process with this command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune run full_finetune_single_device --config ascend_config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command will trigger the fine-tuning process, where torchtune will automatically handle data loading, model fine-tuning, evaluation, and other steps, leveraging Ascend NPU’s computational power to accelerate the training process.&lt;/p&gt;

&lt;p&gt;When you see the following log, it means that the model has been fine-tuned successfully on the Ascend NPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;……
dataset:
  _component_: torchtune.datasets.instruct_dataset
  data_files: ascend_dataset.json
  packed: false
  source: json
  split: train
  train_on_input: false
device: npu
dtype: bf16
enable_activation_checkpointing: true
epochs: 10
……
INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.
INFO:torchtune.utils._logging:Memory stats after model init:
        NPU peak memory allocation: 1.55 GiB
        NPU peak memory reserved: 1.61 GiB
        NPU peak memory active: 1.55 GiB
INFO:torchtune.utils._logging:Tokenizer is initialized from file.
INFO:torchtune.utils._logging:Optimizer is initialized.
INFO:torchtune.utils._logging:Loss is initialized.
……
NFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/lcg/tmp/torchtune/ascend_llama/hf_model_0001_9.pt
INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/lcg/tmp/torchtune/ascend_llama/hf_model_0002_9.pt
INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/lcg/tmp/torchtune/ascend_llama/hf_model_0003_9.pt
INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/lcg/tmp/torchtune/ascend_llama/hf_model_0004_9.pt
INFO:torchtune.utils._logging:Saving final epoch checkpoint.
INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.
10|20|Loss: 0.2997712790966034: 100%|██████████████████████████████| 2/2 [01:00&amp;lt;00:00, 30.03s/it]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;generating-with-fine-tuned-models&quot;&gt;Generating with Fine-Tuned Models&lt;/h2&gt;

&lt;p&gt;In the previous section, we used a fine-tuning dataset similar to &lt;a href=&quot;https://huggingface.co/datasets/ilyq69/identity.json&quot;&gt;identity.json&lt;/a&gt;, which is identity-related and made some adjustments to it.&lt;/p&gt;

&lt;p&gt;In this section, we will use our model to perform some generation tasks. For this, we’ll use the &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/generate.py&quot;&gt;generate recipe&lt;/a&gt; and the associated &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/configs/generation.yaml&quot;&gt;config&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s first copy over the config to our local working directory so we can make changes.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune cp generation ./ascend_generation_config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s modify &lt;strong&gt;ascend_generation_config.yaml&lt;/strong&gt; to include the following changes. Again, you only need to replace two fields: &lt;strong&gt;output_dir&lt;/strong&gt; and &lt;strong&gt;checkpoint_files&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Tokenizer
tokenizer:
    _component_: torchtune.models.llama3.llama3_tokenizer
    path: ${output_dir}/original/tokenizer.model
    prompt_template: null

# Checkpointer
checkpointer:
    _component_: torchtune.training.FullModelHFCheckpointer
    checkpoint_dir: ${output_dir}
    checkpoint_files: [
        Hf_model_0001_0.pt,
        ……
        hf_model_0004_9.pt,
    ]
    output_dir: ${output_dir}

# Generation arguments; defaults taken from gpt-fast
prompt:
    system: null
    user: &quot;你是谁?&quot;

# Environment
device: npu

# Other Configs …
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we will run our generate recipe.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tune run generate --config ascend_generation_config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results of the execution are as follows, and we can see that our assistant has learned to identify itself as the Torchtune Helper!&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;……
INFO:torchtune.utils._logging:你是谁?您好，我是 Torchtune Helper，由 PyTorch 开发，旨在为用户提供智能化的回答和帮助。
INFO:torchtune.utils._logging:Time for inference: 4.75 sec total, 5.47 tokens/sec
INFO:torchtune.utils._logging:Bandwidth achieved: 89.18 GB/s
INFO:torchtune.utils._logging:Memory used: 0.00 GB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Huawei PyTorch Team: Chenguang Li (Huawei), Mengqing Cao (Huawei)</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we will briefly introduce torchtune, the Ascend backend, and demonstrate how torchtune can be used to fine-tune models with Ascend.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">High-Performance Low-Bit Operators for PyTorch</title>
      <link href="https://pytorch.org/blog/hi-po-low-bit-operators/" rel="alternate" type="text/html" title="High-Performance Low-Bit Operators for PyTorch" />
      <published>2025-01-06T00:00:00-08:00</published>
      <updated>2025-01-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/hi-po-low-bit-operators</id>
      <content type="html" xml:base="https://pytorch.org/blog/hi-po-low-bit-operators/">&lt;p&gt;We are excited to announce the addition of embedding operators with low-bit weights (1-8 bit) and linear operators with 8-bit dynamically quantized activations and low-bit weights (1-8 bit) for Arm CPUs in TorchAO, PyTorch’s native low-precision library. These operators work seamlessly across all PyTorch surfaces, including eager, torch.compile, AOTI, and ExecuTorch, and are &lt;a href=&quot;https://github.com/pytorch/torchchat/blob/main/docs/quantization.md#experimental-torchao-lowbit-kernels&quot;&gt;available to use in torchchat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In developing these linear operators, our focus was on &lt;strong&gt;code sharing between PyTorch and ExecuTorch&lt;/strong&gt;, and establishing a clear boundary between the higher-level operator and the lower-level kernel. This design &lt;strong&gt;allows third-party vendors to easily swap in their own kernels&lt;/strong&gt;. We also set out to &lt;strong&gt;create a place and infrastructure to experiment&lt;/strong&gt; with new CPU quantization ideas and test those across the PyTorch ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;universal-low-bit-kernels&quot;&gt;Universal low-bit kernels&lt;/h2&gt;

&lt;p&gt;There is no hardware support for low-bit arithmetic. In what we call universal kernels, we explicitly separated the logic that unpacks low-bit values to int8 values, and the int8 GEMV kernel logic in a modular fashion. We started with an 8-bit kernel, for example, this &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/channelwise_8bit_activation_groupwise_lowbit_weight_1x8x16_f32_neondot-impl.h#L64&quot;&gt;1x8 8-bit GEMV kernel&lt;/a&gt; that uses the Arm neondot instruction. Within the 8-bit kernel, we invoke an &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/channelwise_8bit_activation_groupwise_lowbit_weight_1x8x16_f32_neondot-impl.h#L169&quot;&gt;inlined unpacking routine&lt;/a&gt; to convert low-bit values into int8 values. This unpacking routine is force-inlined and templated on some low-bit value. Our experiments showed no performance difference between using a separate force-inlined unpacking routine and directly embedding the unpacking code inline.&lt;/p&gt;

&lt;p&gt;The advantage of this modular design is improved development speed and code maintainability. After writing an 8-bit kernel, we quickly achieved full low-bit coverage by writing &lt;a href=&quot;https://github.com/pytorch/ao/tree/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/bitpacking&quot;&gt;simple bitpacking routines&lt;/a&gt;. In fact, developers who worked on the bit packing routines did not need to be experts on GEMV/GEMM kernel writing. We also reused the same bitpacking routines from the linear kernels &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/embedding/embedding.h#L161&quot;&gt;within the embedding kernels&lt;/a&gt;. In future we could reuse the same bitpacking routines for universal GEMM kernels or kernels based on fma or i8mm instructions.&lt;/p&gt;

&lt;h2 id=&quot;shared-code-between-pytorch-and-executorch&quot;&gt;Shared code between PyTorch and ExecuTorch&lt;/h2&gt;

&lt;p&gt;To achieve shared code between PyTorch and ExecuTorch, we wrote kernels &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/kernels/cpu/aarch64/linear/linear.h&quot;&gt;using raw pointers instead of PyTorch tensors&lt;/a&gt;. Moreover, we implemented the &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight-impl.h#L259&quot;&gt;linear operator in a header &lt;/a&gt;that is included in separate &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight_aten.cpp&quot;&gt;PyTorch&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/op_linear_8bit_act_xbit_weight_executorch/w4s.cpp&quot;&gt;ExecuTorch&lt;/a&gt; operator registration code. By using only features common to both ATen and ExecuTorch tensors, we ensured compatibility between the two frameworks. For multi-threaded compute, we introduced &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel.h#L13&quot;&gt;torchao::parallel_1d&lt;/a&gt;, which compiles to either &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel-aten-impl.h&quot;&gt;at::parallel_for&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/parallel-executorch-impl.h&quot;&gt;ExecuTorch’s threadpool&lt;/a&gt; based on compile-time flags.&lt;/p&gt;

&lt;h2 id=&quot;swappable-kernels&quot;&gt;Swappable kernels&lt;/h2&gt;

&lt;p&gt;Our design for the higher-level multi-threaded linear operator is agnostic to the lower-level single-threaded kernels, allowing third-party vendors to swap in their own implementations. The interface between the operator and kernel is defined by a &lt;a href=&quot;https://github.com/pytorch/ao/blob/299aacd0ab0e0cce376f56e18e5bb585d517b2e1/torchao/experimental/ops/linear_8bit_act_xbit_weight/linear_8bit_act_xbit_weight.h#L14&quot;&gt;ukernel config&lt;/a&gt;, which specifies kernel function pointers for preparing activation data, preparing weight data, and running the kernel. The operator, responsible for tiling and scheduling, interacts with kernels solely through this config.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;In the table below, we show Llama3.1 8B token generation performance using 6 CPU threads on an M1 Macbook Pro with 32GB of RAM.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Bitwidth x&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;torch.compile (Decode tokens/sec)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;ExecuTorch (Decode tokens/sec)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;ExecuTorch PTE size (GiB)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;24.18
   &lt;/td&gt;
   &lt;td&gt;17.86
   &lt;/td&gt;
   &lt;td&gt;1.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;27.02
   &lt;/td&gt;
   &lt;td&gt;19.65
   &lt;/td&gt;
   &lt;td&gt;2.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;21.01
   &lt;/td&gt;
   &lt;td&gt;22.25
   &lt;/td&gt;
   &lt;td&gt;3.46
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;19.51
   &lt;/td&gt;
   &lt;td&gt;19.47
   &lt;/td&gt;
   &lt;td&gt;4.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;5
   &lt;/td&gt;
   &lt;td&gt;14.78
   &lt;/td&gt;
   &lt;td&gt;16.34
   &lt;/td&gt;
   &lt;td&gt;5.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;6
   &lt;/td&gt;
   &lt;td&gt;12.80
   &lt;/td&gt;
   &lt;td&gt;13.61
   &lt;/td&gt;
   &lt;td&gt;6.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;7
   &lt;/td&gt;
   &lt;td&gt;8.16
   &lt;/td&gt;
   &lt;td&gt;11.73
   &lt;/td&gt;
   &lt;td&gt;7.48
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Results were run on an M1 Macbook Pro (with 8 perf cores, and 2 efficiency cores) with 32GB of RAM and 6 threads &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;using torchchat&lt;/a&gt;. In each test, the max-seq-length of 128 tokens were generated. For each bit width x, the embedding layer was groupwise quantized to x-bits with group size 32. In the linear layers, activations were dynamically quantized per token to 8 bits and weights were groupwise quantized to x-bits with group size 256.  Our focus here is performance and we do not report accuracy or perplexity numbers. Depending on the model, lower bit widths may require quantization-aware training, quantizing a model with a mixture of bit widths, or adjusting the group sizes for acceptable accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hi-po-low-bit.png&quot; alt=&quot;Llama 3.1 chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-them-out-and-contribute&quot;&gt;Try them out and contribute!&lt;/h2&gt;

&lt;p&gt;If you want to see the new low-bit kernels in action, give them a try by &lt;a href=&quot;https://github.com/pytorch/torchchat/tree/main&quot;&gt;setting up torchchat&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/torchchat/blob/main/docs/quantization.md#experimental-torchao-lowbit-kernels&quot;&gt;quantizing and running an LLM locally using the kernels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to help contribute, consider adding support for one of the following areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/issues/1394&quot;&gt;Add universal low-bit GEMM kernels&lt;/a&gt; for Arm CPU, reusing the same bitpacking routines from the universal GEMV kernels.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/issues/1376&quot;&gt;Improve runtime selection&lt;/a&gt; of ukernel configs based on ISA, packing format, and activation shape.&lt;/li&gt;
  &lt;li&gt;Add low-bit kernels for other CPU ISAs like x86.&lt;/li&gt;
  &lt;li&gt;Integrate third-party libraries like &lt;a href=&quot;https://gitlab.arm.com/kleidi/kleidiai&quot;&gt;KleidiAI&lt;/a&gt; with the operator framework.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Scott Roy, Digant Desai, Kimish Patel</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the addition of embedding operators with low-bit weights (1-8 bit) and linear operators with 8-bit dynamically quantized activations and low-bit weights (1-8 bit) for Arm CPUs in TorchAO, PyTorch’s native low-precision library. These operators work seamlessly across all PyTorch surfaces, including eager, torch.compile, AOTI, and ExecuTorch, and are available to use in torchchat.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Grows as the Dominant Open Source Framework for AI and ML: 2024 Year in Review</title>
      <link href="https://pytorch.org/blog/2024-year-in-review/" rel="alternate" type="text/html" title="PyTorch Grows as the Dominant Open Source Framework for AI and ML: 2024 Year in Review" />
      <published>2024-12-23T00:00:00-08:00</published>
      <updated>2024-12-23T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/2024-year-in-review</id>
      <content type="html" xml:base="https://pytorch.org/blog/2024-year-in-review/">&lt;p&gt;This past year was a monumental year for PyTorch from major releases to the flagship PyTorch Conference. We’ve seen incredible growth in contributions from more than 3,500 individuals and 3,000 organizations. It’s safe to say PyTorch has now become the dominant deep learning framework for AI/ML.  PyTorch leads the model training space with a 63% adoption rate according to the recent &lt;a href=&quot;https://www.linuxfoundation.org/research/gen-ai-2024&quot;&gt;Shaping the Future of Generative AI Report&lt;/a&gt; from the Linux Foundation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg1.jpg&quot; alt=&quot;group at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation was formed in 2022 with the goal to drive the adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects centered around PyTorch and today remains a vibrant, collaborative hub created for and by the deep learning community. As we wrap up the year, let’s take a look back at a few highlights and how this year has been one of growth, collaboration, innovation, and community.&lt;/p&gt;

&lt;h2 id=&quot;2024-highlights-a-year-of-growth-and-impact&quot;&gt;2024 Highlights: A Year of Growth and Impact&lt;/h2&gt;

&lt;p&gt;PyTorch accelerated its growth this year. Contributions are up 133%, from double the amount of  organizations worldwide compared to last year.&lt;/p&gt;

&lt;p&gt;The project has seen 20% year-over-year growth in new repositories using PyTorch, and a 30% increase in forks and users this past year.&lt;/p&gt;

&lt;p&gt;Over 70% of AI research implementations are now using PyTorch.&lt;/p&gt;

&lt;p&gt;Statistics based on the &lt;a href=&quot;https://www.linuxfoundation.org/resources/publications/linux-foundation-annual-report-2024&quot;&gt;2024 Linux Foundation Annual Report&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg2.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Tools ecosystem grew by over 25%, enhancing both software and hardware capabilities. Working with all major cloud service providers, dozens of major software vendors, and industry partners, PyTorch is setting a new bar for the pace and breadth of AI innovation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg3.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This year featured 4 milestone releases for PyTorch in the 2.2, 2.3, 2.4 and 2.5 releases. We observed the release of various hallmark features like &lt;a href=&quot;https://pytorch.org/blog/pytorch2-2/#beta-aotinductor-ahead-of-time-compilation-and-deployment-for-torchexport-ed-programs&quot;&gt;AOTInductor&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch2-2/#beta-aotinductor-ahead-of-time-compilation-and-deployment-for-torchexport-ed-programs&quot;&gt;FlashAttention-2 support&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch2-3/#beta-tensor-parallelism-introduces-more-efficient-ways-to-train-llms&quot;&gt;Tensor Parallelism&lt;/a&gt;, a new &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#beta-new-higher-level-python-custom-operator-api&quot;&gt;Python Custom Operator API&lt;/a&gt;, and the introduction of &lt;a href=&quot;https://pytorch.org/blog/pytorch2-5/#prototype-flexattention&quot;&gt;FlexAttention&lt;/a&gt;. Engineers from across PyTorch Foundation member companies have also come together to introduce support and optimizations for platforms like &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;Intel GPUs&lt;/a&gt; (XPU), AWS &lt;a href=&quot;https://pytorch.org/blog/pytorch2-4/#torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;Graviton&lt;/a&gt; processors, Inductor performance, etc.&lt;/p&gt;

&lt;p&gt;Throughout the year the PyTorch Team has been working hard to introduce a number of new PyTorch-native libraries! The &lt;a href=&quot;https://pytorch.org/blog/executorch-alpha/&quot;&gt;ExecuTorch&lt;/a&gt; team released their alpha in collaboration with partners from Arm, Apple, and Qualcomm Technologies, Inc. then quickly followed with a &lt;a href=&quot;https://pytorch.org/blog/executorch-beta/&quot;&gt;beta&lt;/a&gt; focused on stability and adding MediaTek. &lt;a href=&quot;https://pytorch.org/blog/torchtune-fine-tune-llms/&quot;&gt;TorchTune&lt;/a&gt; established a PyTorch-native library for easily fine-tuning large language models. &lt;a href=&quot;https://pytorch.org/blog/pytorch-native-architecture-optimization/&quot;&gt;TorchAO&lt;/a&gt; introduced a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. &lt;a href=&quot;https://pytorch.org/blog/torchcodec/&quot;&gt;TorchCodec&lt;/a&gt; was launched to give developers a simple, performant, and PyTorch native way to decode videos into tensors. &lt;a href=&quot;https://pytorch.org/blog/torchrec-fbgemm-1/&quot;&gt;TorchRec&lt;/a&gt; 1.0 was released, the first stable release of the PyTorch native recommendation systems library.&lt;/p&gt;

&lt;p&gt;We’ve also had a number of strong technical showcases throughout the year to highlight how PyTorch can be used! &lt;a href=&quot;https://arxiv.org/html/2410.06511v1&quot;&gt;TorchTitan&lt;/a&gt; exhibited what an open source, PyTorch-native distributed training system could look like for training large language models (LLMs). &lt;a href=&quot;https://pytorch.org/blog/torchchat-local-llm-inference/&quot;&gt;TorchChat&lt;/a&gt; showcased how to seamlessly and performantly run LLMs across laptop, desktop, and mobile devices.&lt;/p&gt;

&lt;p&gt;As well we were very excited to include &lt;a href=&quot;https://pytorch.org/blog/enhancing-deep-learning/&quot;&gt;multiple new projects&lt;/a&gt; into the PyTorch ecosystem throughout 2024, including the introduction of  &lt;a href=&quot;https://pytorch.org/blog/vllm-joins-pytorch/&quot;&gt;vLLM&lt;/a&gt; into the PyTorch Ecosystem, a state-of-the-art inference engine, which gives machine learning engineers an easy, fast, and cheap way of serving LLMs. If you are interested in joining the PyTorch Ecosystem, please &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;join&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg4.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In June in Paris, France we premiered the&lt;a href=&quot;https://pytorch.org/blog/pytorch-documentary/&quot;&gt; official PyTorch documentary&lt;/a&gt; on powering the AI Revolution that spotlights PyTorch’s vibrant ecosystem and its role in advancing AI innovation. The film unveiled the authentic narrative of PyTorch’s inception, attributing its existence to a dedicated group of unsung heroes driving technological innovation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg5.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch.org/blog/pytorch-conference-2024-recap/&quot;&gt;PyTorch Conference 2024&lt;/a&gt;, brought in triple the registrations compared to 2023, reflecting the rapid growth of AI and machine learning communities around open source technologies. The two day event included insightful talks, hands-on sessions, and lively discussions about the future of AI, covering everything from generative AI to large language models.&lt;/p&gt;

&lt;p&gt;A brand new Startup Showcase featured early-stage founders pitching their AI startups to a panel of top venture capitalists, a DL Compiler Mini-Summit took a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads, and a Fine-Tuning Mini-Summit brought together a thriving community of researchers, developers, practitioners and hobbyists to discuss topics like memory efficiency, parameter-efficient fine-tuning, and performance at scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg6.jpg&quot; alt=&quot;speaking on stage at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Outstanding contributors were honored with &lt;a href=&quot;https://pytorch.org/ecosystem/contributor-awards-2024&quot;&gt;PyTorch Contributor Awards&lt;/a&gt;. Congratulations to this year’s nominees and recipients for the outstanding individuals and teams who have played a pivotal role in PyTorch’s journey this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg7.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Foundation membership is growing with the addition of Arm and Rebellions this year. At the year-end mark, Premier Members include: AMD, Arm, AWS, Google Cloud, Huawei, Hugging Face, IBM, Intel, Lightning AI, Meta, Microsoft Azure, and NVIDIA. General Members include: Graphcore, Rebellions, and Snowflake. If your organization is interested in joining, find out how you can &lt;a href=&quot;/join&quot;&gt;become a member&lt;/a&gt; of the PyTorch Foundation.&lt;/p&gt;

&lt;p&gt;PyTorch hosted numerous in-person and virtual events, including&lt;a href=&quot;https://pytorch.org/blog/pytorch-docathon-h2-2024-wrap-up/&quot;&gt; The PyTorch Docathon&lt;/a&gt; where contributors worked to improve PyTorch documentation and foster collaboration, Local meetups around the world brought together interested parties in locations from Shanghai to Seoul, and more than a dozen &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;webinars&lt;/a&gt; brought in attendees from everywhere during our Summer Webinar Series, live Q&amp;amp;As, and Expert Exchanges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg8.jpg&quot; alt=&quot;Matt speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Foundation welcomed new leadership this year.&lt;a href=&quot;https://pytorch.org/blog/new-executive-director/&quot;&gt; Executive Director Matt White&lt;/a&gt; took the reins in April and immediately began raising the profile of PyTorch across the AI landscape. The&lt;a href=&quot;https://pytorch.org/tac&quot;&gt; Technical Advisory Council (TAC)&lt;/a&gt; also elected&lt;a href=&quot;https://pytorch.org/blog/tac-elects-new-leadership/&quot;&gt; new leadership&lt;/a&gt; with  Luca Antiga, Lightning AI as the Chair and Jiong Gong, Intel as Vice Chair.&lt;/p&gt;

&lt;p&gt;The&lt;a href=&quot;https://pytorch.org/governing-board&quot;&gt; PyTorch Governing Board&lt;/a&gt; continued to set the direction and lead the Foundation in accomplishing its mission. The PyTorch Marketing and Outreach Committee developed programs to maximize the visibility of PyTorch and advance the interests of the community. The PyTorch CI Working Group assembled to successfully migrate the PyTorch CI pipeline to the Linux Foundation.&lt;/p&gt;

&lt;p&gt;Our community joined us on social media with 775 thousand followers strong across X, LinkedIn, Facebook, and YouTube with more than 12 million impressions of PyTorch content throughout the year.  The PyTorch Ecosystem also grew, adding many new projects to leverage PyTorch deep learning across many vertical domains.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg9.jpg&quot; alt=&quot;people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch was mentioned in the media in top technology publications such as The New Stack’s article on &lt;a href=&quot;https://thenewstack.io/why-pytorch-gets-all-the-love/&quot;&gt;Why PyTorch Gets All the Love&lt;/a&gt; and InfoWorld’s article on how the TorchAO&lt;a href=&quot;https://www.infoworld.com/article/3543651/pytorch-library-makes-models-faster-and-smaller.html&quot;&gt; PyTorch library makes models faster and smaller&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We published 74 technical and community blogs, and nearly ten million people visited the PyTorch website throughout the year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2024-year-in-review/fg10.jpg&quot; alt=&quot;fire dancers at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks to each of you who helped make this year an outstanding success! The evolution and growth we’ve seen PyTorch undergo over the past year is driven by the passion, dedication, and ingenuity of this amazing community. Looking ahead to next year, we’re excited to build on this momentum as we continue to push the boundaries of AI.&lt;/p&gt;

&lt;p&gt;Save the date for the &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference-2025/&quot;&gt;PyTorch Conference&lt;/a&gt; which will be held October 22-23, 2025 in San Francisco. 2025 promises even greater innovation and stronger community collaboration.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Eli Uriegas, Meta and Jennifer Bly, PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">This past year was a monumental year for PyTorch from major releases to the flagship PyTorch Conference. We’ve seen incredible growth in contributions from more than 3,500 individuals and 3,000 organizations. It’s safe to say PyTorch has now become the dominant deep learning framework for AI/ML. PyTorch leads the model training space with a 63% adoption rate according to the recent Shaping the Future of Generative AI Report from the Linux Foundation.</summary>
      

      
      
    </entry>
  
</feed>



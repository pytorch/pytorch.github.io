<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.org/" rel="alternate" type="text/html" /><updated>2019-10-23T13:16:20-07:00</updated><id>https://pytorch.org/</id><title type="html">PyTorch Website</title><subtitle>Scientific Computing...</subtitle><author><name>Facebook</name></author><entry><title type="html">PyTorch 1.3 adds mobile, privacy, quantization, and named tensors</title><link href="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/" rel="alternate" type="text/html" title="PyTorch 1.3 adds mobile, privacy, quantization, and named tensors" /><published>2019-10-10T00:00:00-07:00</published><updated>2019-10-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/">&lt;p&gt;PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv &lt;a href=&quot;https://www.oreilly.com/ideas/one-simple-graphic-researchers-love-pytorch-and-tensorflow?fbclid=IwAR3kYmlyD7zky37IYFu0cafQn7yemhl8P-7MNyB30z0q5RDzxcTOrP8kxDk&quot;&gt;grew 194 percent in the first half of 2019 alone, as noted by O’Reilly&lt;/a&gt;, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.&lt;/p&gt;

&lt;p&gt;We are now advancing the platform further with the release of PyTorch 1.3, which includes experimental support for features such as seamless model deployment to mobile devices, model quantization for better performance at inference time, and front-end improvements, like the ability to name tensors and create clearer code with less need for inline comments. We’re also launching a number of additional tools and libraries to support model interpretability and bringing multimodal research to production.&lt;/p&gt;

&lt;p&gt;Additionally, we’ve collaborated with Google and Salesforce to add broad support for Cloud Tensor Processing Units, providing a significantly accelerated option for training large-scale deep neural networks. &lt;a href=&quot;https://data.aliyun.com/bigdata/pai-pytorch?spm=5176.12825654.a9ylfrljh.d112.7b652c4ayuOO4M&amp;amp;scm=20140722.1068.1.1098&amp;amp;aly_as=-PvJ5e4c&quot;&gt;Alibaba Cloud&lt;/a&gt; also joins Amazon Web Services, Microsoft Azure, and Google Cloud as supported cloud platforms for PyTorch users. You can get started now at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-13&quot;&gt;PyTorch 1.3&lt;/h1&gt;

&lt;p&gt;The 1.3 release of PyTorch brings significant new features, including experimental support for mobile device deployment, eager mode quantization at 8-bit integer, and the ability to name tensors. With each of these enhancements, we look forward to additional contributions and improvements from the PyTorch community.&lt;/p&gt;

&lt;h2 id=&quot;named-tensors-experimental&quot;&gt;Named tensors (experimental)&lt;/h2&gt;

&lt;p&gt;Cornell University’s &lt;a href=&quot;http://nlp.seas.harvard.edu/NamedTensor&quot;&gt;Sasha Rush has argued&lt;/a&gt; that, despite its ubiquity in deep learning, the traditional implementation of tensors has significant shortcomings, such as exposing private dimensions, broadcasting based on absolute position, and keeping type information in documentation. He proposed named tensors as an alternative approach.&lt;/p&gt;

&lt;p&gt;Today, we name and access dimensions by comment:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Tensor[N, C, H, W]&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But naming explicitly leads to more readable and maintainable code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NCHW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;quantization-experimental&quot;&gt;Quantization (experimental)&lt;/h2&gt;

&lt;p&gt;It’s important to make efficient use of both server-side and on-device compute resources when developing ML applications. To support more efficient deployment on servers and edge devices, PyTorch 1.3 now supports 8-bit model quantization using the familiar eager mode Python API. Quantization refers to techniques used to perform computation and storage at reduced precision, such as 8-bit integer. This currently experimental feature includes support for post-training quantization, dynamic quantization, and quantization-aware training. It leverages the &lt;a href=&quot;https://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/QNNPACK&quot;&gt;QNNPACK&lt;/a&gt; state-of-the-art quantized kernel back ends, for x86 and ARM CPUs, respectively, which are integrated with PyTorch and now share a common API.&lt;/p&gt;

&lt;p&gt;To learn more about the design and architecture, check out the API docs &lt;a href=&quot;https://pytorch.org/docs/master/quantization.html&quot;&gt;here&lt;/a&gt;, and get started with any of the supported techniques using the tutorials available &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-mobile-experimental&quot;&gt;PyTorch mobile (experimental)&lt;/h2&gt;

&lt;p&gt;Running ML on edge devices is growing in importance as applications continue to demand lower latency. It is also a foundational element for privacy-preserving techniques such as federated learning. To enable more efficient on-device ML, PyTorch 1.3 now supports an end-to-end workflow from Python to deployment on iOS and Android.&lt;/p&gt;

&lt;p&gt;This is an early, experimental release, optimized for end-to-end development. Coming releases will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimization for size: Build level optimization and selective compilation depending on the operators needed for user applications (i.e., you pay binary size for only the operators you need)&lt;/li&gt;
  &lt;li&gt;Performance: Further improvements to performance and coverage on mobile CPUs and GPUs&lt;/li&gt;
  &lt;li&gt;High level API: Extend mobile native APIs to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications. e.g. Computer vision and NLP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more or get started on Android or iOS &lt;a href=&quot;http://pytorch.org/mobile&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;new-tools-for-model-interpretability-and-privacy&quot;&gt;New tools for model interpretability and privacy&lt;/h1&gt;

&lt;h2 id=&quot;captum&quot;&gt;Captum&lt;/h2&gt;

&lt;p&gt;As models become ever more complex, it is increasingly important to develop new methods for model interpretability. To help address this need, we’re launching Captum, a tool to help developers working in PyTorch understand why their model generates a specific output. Captum provides state-of-the-art tools to understand how the importance of specific neurons and layers and affect predictions made by the models. Captum’s algorithms include integrated gradients, conductance, SmoothGrad and VarGrad, and DeepLift.&lt;/p&gt;

&lt;p&gt;The example below shows how to apply model interpretability algorithms on a pretrained ResNet model and then visualize the attributions for each pixel by overlaying them on the image.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoiseTunnel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;integrated_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_tunnel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nt_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'smoothgrad_sq'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_label_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;viz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize_image_attr_multiple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;original_image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;heat_map&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;all&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;positive&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attributions_ig_nt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformed_img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;show_colorbar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 1.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/Captum 2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Learn more about Captum at &lt;a href=&quot;https://www.captum.ai/&quot;&gt;captum.ai&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;crypten&quot;&gt;CrypTen&lt;/h2&gt;

&lt;p&gt;Practical applications of ML via cloud-based or machine-learning-as-a-service (MLaaS) platforms pose a range of security and privacy challenges. In particular, users of these platforms may not want or be able to share unencrypted data, which prevents them from taking full advantage of ML tools. To address these challenges, the ML community is exploring a number of technical approaches, at various levels of maturity. These include homomorphic encryption, secure multiparty computation, trusted execution environments, on-device computation, and differential privacy.&lt;/p&gt;

&lt;p&gt;To provide a better understanding of how some of these technologies can be applied, we are releasing CrypTen, a new community-based research platform for taking the field of privacy-preserving ML forward. Learn more about CrypTen &lt;a href=&quot;https://ai.facebook.com/blog/crypten-a-new-research-tool-for-secure-machine-learning-with-pytorch&quot;&gt;here&lt;/a&gt;. It is available on GitHub &lt;a href=&quot;https://github.com/facebookresearch/CrypTen&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;tools-for-multimodal-ai-systems&quot;&gt;Tools for multimodal AI systems&lt;/h1&gt;

&lt;p&gt;Digital content is often made up of several modalities, such as text, images, audio, and video. For example, a single public post might contain an image, body text, a title, a video, and a landing page. Even one particular component may have more than one modality, such as a video that contains both visual and audio signals, or a landing page that is composed of images, text, and HTML sources.&lt;/p&gt;

&lt;p&gt;The ecosystem of tools and libraries that work with PyTorch offer enhanced ways to address the challenges of building multimodal ML systems. Here are some of the latest libraries launching today:&lt;/p&gt;

&lt;h2 id=&quot;detectron2&quot;&gt;Detectron2&lt;/h2&gt;

&lt;p&gt;Object detection and segmentation are used for tasks ranging from autonomous vehicles to content understanding for platform integrity. To advance this work, Facebook AI Research (FAIR) is releasing Detectron2, an object detection library now implemented in PyTorch. Detectron2 provides support for the latest models and tasks, increased flexibility to aid computer vision research, and improvements in maintainability and scalability to support production use cases.&lt;/p&gt;

&lt;p&gt;Detectron2 is available &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;here&lt;/a&gt; and you can learn more &lt;a href=&quot;https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;speech-extensions-to-fairseq&quot;&gt;Speech extensions to fairseq&lt;/h2&gt;

&lt;p&gt;Language translation and audio processing are critical components in systems and applications such as search, translation, speech, and assistants. There has been tremendous progress in these fields recently thanks to the development of new architectures like transformers, as well as large-scale pretraining methods. We’ve extended fairseq, a framework for sequence-to-sequence applications such as language translation, to include support for end-to-end learning for speech and audio recognition tasks.These extensions to fairseq enable faster exploration and prototyping of new speech research ideas while offering a clear path to production.&lt;/p&gt;

&lt;p&gt;Get started with fairseq &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;cloud-provider-and-hardware-ecosystem-support&quot;&gt;Cloud provider and hardware ecosystem support&lt;/h1&gt;

&lt;p&gt;Cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud provide extensive support for anyone looking to develop ML on PyTorch and deploy in production. We’re excited to share the general availability of Google Cloud TPU support and a newly launched integration with Alibaba Cloud. We’re also expanding hardware ecosystem support.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google Cloud TPU support now broadly available. To accelerate the largest-scale machine learning (ML) applications deployed today and enable rapid development of the ML applications of tomorrow, Google created custom silicon chips called Tensor Processing Units (&lt;a href=&quot;https://cloud.google.com/tpu/&quot;&gt;TPUs&lt;/a&gt;). When assembled into multi-rack ML supercomputers called &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-pods-break-ai-training-records&quot;&gt;Cloud TPU Pods&lt;/a&gt;, these TPUs can complete ML workloads in minutes or hours that previously took days or weeks on other systems. Engineers from Facebook, Google, and Salesforce worked together to enable and pilot Cloud TPU support in PyTorch, including experimental support for Cloud TPU Pods. PyTorch support for Cloud TPUs is also available in Colab. Learn more about how to get started with PyTorch on Cloud TPUs &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Alibaba adds support for PyTorch in Alibaba Cloud. The initial integration involves a one-click solution for PyTorch 1.x, Data Science Workshop notebook service, distributed training with Gloo/NCCL, as well as seamless integration with Alibaba IaaS such as OSS, ODPS, and NAS. Together with the toolchain provided by Alibaba, we look forward to significantly reducing the overhead necessary for adoption, as well as helping Alibaba Cloud’s global customer base leverage PyTorch to develop new AI applications.&lt;/li&gt;
  &lt;li&gt;ML hardware ecosystem expands. In addition to key GPU and CPU partners, the PyTorch ecosystem has also enabled support for dedicated ML accelerators. Updates from &lt;a href=&quot;https://www.intel.ai/nnpi-glow-pytorch/&quot;&gt;Intel&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@HabanaLabs/unlocking-ai-scaling-through-software-and-hardware-interface-standardization-77561cb7598b&quot;&gt;Habana&lt;/a&gt; showcase how PyTorch, connected to the Glow optimizing compiler, enables developers to utilize these market-specific solutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;growth-in-the-pytorch-community&quot;&gt;Growth in the PyTorch community&lt;/h1&gt;

&lt;p&gt;As an open source, community-driven project, PyTorch benefits from wide range of contributors bringing new capabilities to the ecosystem. Here are some recent examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mila SpeechBrain aims to provide an open source, all-in-one speech toolkit based on PyTorch. The goal is to develop a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art systems for speech recognition (both end to end and HMM-DNN), speaker recognition, speech separation, multi-microphone signal processing (e.g., beamforming), self-supervised learning, and many others. &lt;a href=&quot;https://speechbrain.github.io/&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SpaCy is a new wrapping library with consistent and easy-to-use interfaces to several models, in order to extract features to power NLP pipelines. Support is provided for via spaCy’s standard training API. The library also calculates an alignment so the transformer features can be related back to actual words instead of just wordpieces. &lt;a href=&quot;https://explosion.ai/blog/spacy-pytorch-transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;HuggingFace PyTorch-Transformers (formerly known as pytorch-pretrained-bert is a library of state-of-the-art pretrained models for Natural Language Processing (NLP). The library currently contains PyTorch implementations, pretrained model weights, usage scripts, and conversion utilities for models such as BERT, GPT-2, RoBERTa, and DistilBERT. It has also grown quickly, with more than 13,000 GitHub stars and a broad set of users. &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Learn more&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PyTorch Lightning is a Keras-like ML library for PyTorch. It leaves core training and validation logic to you and automates the rest. Reproducibility is a crucial requirement for many fields of research, including those based on ML techniques. As the number of research papers submitted to arXiv and conferences skyrockets into the tens of thousands, scaling reproducibility becomes difficult. &lt;a href=&quot;https://github.com/williamFalcon/pytorch-lightning&quot;&gt;Learn more&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We recently held the first online Global PyTorch Summer Hackathon, where researchers and developers around the world were invited to build innovative new projects with PyTorch. Nearly 1,500 developers participated, submitting projects ranging from livestock disease detection to AI-powered financial assistants. The winning projects were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torchmeta, which provides extensions for PyTorch to simplify the development of meta-learning algorithms in PyTorch. It features a unified interface inspired by TorchVision for both few-shot classification and regression problems, to allow easy benchmarking on multiple data sets to aid with reproducibility.&lt;/li&gt;
  &lt;li&gt;Open-Unmix, a system for end-to-end music demixing with PyTorch. Demixing separates the individual instruments or vocal track from any stereo recording.&lt;/li&gt;
  &lt;li&gt;Endless AI-Generated Tees, a store featuring AI-generated T-shirt designs that can be purchased and delivered worldwide. The system uses a state-of-the-art generative model (StyleGAN) that was built with PyTorch and then trained on modern art.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Visit &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt; to learn more and get started with PyTorch 1.3 and the latest libraries and ecosystem projects. We look forward to the contributions, exciting research advancements, and real-world applications that the community builds with PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all their contributions to this work.&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">PyTorch continues to gain momentum because of its focus on meeting the needs of researchers, its streamlined workflow for production use, and most of all because of the enthusiastic support it has received from the AI community. PyTorch citations in papers on ArXiv grew 194 percent in the first half of 2019 alone, as noted by O’Reilly, and the number of contributors to the platform has grown more than 50 percent over the last year, to nearly 1,200. Facebook, Microsoft, Uber, and other organizations across industries are increasingly using it as the foundation for their most important machine learning (ML) research and production workloads.</summary></entry><entry><title type="html">New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4</title><link href="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/" rel="alternate" type="text/html" title="New Releases: PyTorch 1.2, torchtext 0.4, torchaudio 0.3, and torchvision 0.4" /><published>2019-08-08T00:00:00-07:00</published><updated>2019-08-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-1.2-and-domain-api-release</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/">&lt;p&gt;Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.&lt;/p&gt;

&lt;p&gt;From a core perspective, PyTorch has continued to add features to support both research and production usage, including the ability to bridge these two worlds via &lt;a href=&quot;https://pytorch.org/docs/stable/jit.html&quot;&gt;TorchScript&lt;/a&gt;. Today, we are excited to announce that we have four new releases including PyTorch 1.2, torchvision 0.4, torchaudio 0.3, and torchtext 0.4. You can get started now with any of these releases at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;pytorch.org&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;pytorch-12&quot;&gt;PyTorch 1.2&lt;/h1&gt;

&lt;p&gt;With PyTorch 1.2, the open source ML framework takes a major step forward for production usage with the addition of an improved and more polished TorchScript environment. These improvements make it even easier to ship production models, expand support for exporting ONNX formatted models, and enhance module level support for Transformers. In addition to these new features, &lt;a href=&quot;https://pytorch.org/docs/stable/tensorboard.html&quot;&gt;TensorBoard&lt;/a&gt; is now no longer experimental - you can simply type &lt;code class=&quot;highlighter-rouge&quot;&gt;from torch.utils.tensorboard import SummaryWriter&lt;/code&gt; to get started.&lt;/p&gt;

&lt;h2 id=&quot;torchscript-improvements&quot;&gt;TorchScript Improvements&lt;/h2&gt;

&lt;p&gt;Since its release in PyTorch 1.0, TorchScript has provided a path to production for eager PyTorch models. The TorchScript compiler converts PyTorch models to a statically typed graph representation, opening up opportunities for
optimization and execution in constrained environments where Python is not available. You can incrementally convert your model to TorchScript, mixing compiled code seamlessly with Python.&lt;/p&gt;

&lt;p&gt;PyTorch 1.2 significantly expands TorchScript’s support for the subset of Python used in PyTorch models and delivers a new, easier-to-use API for compiling your models to TorchScript. See the &lt;a href=&quot;https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api&quot;&gt;migration guide&lt;/a&gt; for details. Below is an example usage of the new API:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Compile the model code to a static representation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Save the compiled code and model data so it can be loaded elsewhere&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_script_module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_script_module.pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To learn more, see our &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a
PyTorch Model in C++&lt;/a&gt; tutorials.&lt;/p&gt;

&lt;h2 id=&quot;expanded-onnx-export&quot;&gt;Expanded ONNX Export&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; community continues to grow with an open &lt;a href=&quot;https://github.com/onnx/onnx/wiki/Expanded-ONNX-Steering-Committee-Announced!&quot;&gt;governance structure&lt;/a&gt; and additional steering committee members, special interest groups (SIGs), and working groups (WGs). In collaboration with Microsoft, we’ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We’ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. ScriptModule has also been improved including support for multiple outputs, tensor factories, and tuples as inputs and outputs. Additionally, users are now able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for multiple Opsets including the ability to export dropout, slice, flip, and interpolate in Opset 10.&lt;/li&gt;
  &lt;li&gt;Improvements to ScriptModule including support for multiple outputs, tensor factories, and tuples as inputs and outputs.&lt;/li&gt;
  &lt;li&gt;More than a dozen additional PyTorch operators supported including the ability to export a custom operator.&lt;/li&gt;
  &lt;li&gt;Many big fixes and test infra improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try out the latest tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;here&lt;/a&gt;, contributed by @lara-hdr at Microsoft. A big thank you to the entire Microsoft team for all of their hard work to make this release happen!&lt;/p&gt;

&lt;h2 id=&quot;nntransformer&quot;&gt;nn.Transformer&lt;/h2&gt;

&lt;p&gt;In PyTorch 1.2, we now include a standard &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer&quot;&gt;nn.Transformer&lt;/a&gt; module, based on the paper “&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;”. The &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module relies entirely on an &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention&quot;&gt;attention mechanism&lt;/a&gt; to draw global dependencies between input and output.  The individual components of the &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt; module are designed so they can be adopted independently. For example, the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder&quot;&gt;nn.TransformerEncoder&lt;/a&gt; can be used by itself, without the larger &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;. The new APIs include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Transformer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerEncoderLayer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoder&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TransformerDecoderLayer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/transformer.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#transformer-layers&quot;&gt;Transformer Layers&lt;/a&gt; documentation for more information. See &lt;a href=&quot;https://github.com/pytorch/pytorch/releases&quot;&gt;here&lt;/a&gt; for the full PyTorch 1.2 release notes.&lt;/p&gt;

&lt;h1 id=&quot;domain-api-library-updates&quot;&gt;Domain API Library Updates&lt;/h1&gt;

&lt;p&gt;PyTorch domain libraries like torchvision, torchtext, and torchaudio provide convenient access to common datasets, models, and transforms that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. Since research domains have distinct requirements, an ecosystem of specialized libraries called domain APIs (DAPI) has emerged around PyTorch to simplify the development of new and existing algorithms in a number of fields. We’re excited to release three updated DAPI libraries for text, audio, and vision that compliment the PyTorch 1.2 core release.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio-03-with-kaldi-compatibility-new-transforms&quot;&gt;Torchaudio 0.3 with Kaldi Compatibility, New Transforms&lt;/h2&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/spectrograms.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Torchaudio specializes in machine understanding of audio waveforms. It is an ML library that provides relevant signal processing functionality (but is not a general signal processing library). It leverages PyTorch’s GPU support to provide many tools and transformations for waveforms to make data loading and standardization easier and more readable. For example, it offers data loaders for waveforms using sox, and transformations such as spectrograms, resampling, and mu-law encoding and decoding.&lt;/p&gt;

&lt;p&gt;We are happy to announce the availability of torchaudio 0.3.0, with a focus on standardization and complex numbers, a transformation (resample) and two new functionals (phase_vocoder, ISTFT), Kaldi compatibility, and a new tutorial. Torchaudio was redesigned to be an extension of PyTorch and a part of the domain APIs (DAPI) ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;Significant effort in solving machine learning problems goes into data preparation. In this new release, we’ve updated torchaudio’s interfaces for its transformations to standardize around the following vocabulary and conventions.&lt;/p&gt;

&lt;p&gt;Tensors are assumed to have channel as the first dimension and time as the last dimension (when applicable). This makes it consistent with PyTorch’s dimensions. For size names, the prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;n_&lt;/code&gt; is used (e.g. “a tensor of size (&lt;code class=&quot;highlighter-rouge&quot;&gt;n_freq&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;n_mel&lt;/code&gt;)”) whereas dimension names do not have this prefix (e.g. “a tensor of dimension (channel, time)”). The input of all transforms and functions now assumes channel first. This is done to be consistent with PyTorch, which has channel followed by the number of samples. The channel parameter of all transforms and functions is now deprecated.&lt;/p&gt;

&lt;p&gt;The output of &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; is (channel, frequency, time, 2), meaning for each channel, the columns are the Fourier transform of a certain window, so as we travel horizontally we can see each column (the Fourier transformed waveform) change over time. This matches the output of librosa so we no longer need to transpose in our test comparisons with &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;. Moreover, because of these new conventions, we deprecated &lt;code class=&quot;highlighter-rouge&quot;&gt;LC2CL&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;BLC2CBL&lt;/code&gt; which were used to transfer from one shape of signal to another.&lt;/p&gt;

&lt;p&gt;As part of this release, we’re also introducing support for complex numbers via tensors of dimension (…, 2), and providing &lt;code class=&quot;highlighter-rouge&quot;&gt;magphase&lt;/code&gt; to convert such a tensor into its magnitude and phase, and similarly &lt;code class=&quot;highlighter-rouge&quot;&gt;complex_norm&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;angle&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The details of the standardization are provided in the &lt;a href=&quot;https://github.com/pytorch/audio/blob/v0.3.0/README.md#Conventions&quot;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;functionals-transformations-and-kaldi-compatibility&quot;&gt;Functionals, Transformations, and Kaldi Compatibility&lt;/h3&gt;

&lt;p&gt;Prior to the standardization, we separated state and computation into &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.transforms&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torchaudio.functional&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As part of the transforms, we’re adding a new transformation in 0.3.0: &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;Resample&lt;/code&gt; can upsample or downsample a waveform to a different frequency.&lt;/p&gt;

&lt;p&gt;As part of the functionals, we’re introducing: &lt;code class=&quot;highlighter-rouge&quot;&gt;phase_vocoder&lt;/code&gt;, a phase vocoder to change the speed of a waveform without changing its pitch, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ISTFT&lt;/code&gt;, the inverse &lt;code class=&quot;highlighter-rouge&quot;&gt;STFT&lt;/code&gt; implemented to be compatible with STFT provided by PyTorch. This separation allows us to make functionals weak scriptable and to utilize JIT in 0.3.0. We thus have JIT and CUDA support for the following transformations: &lt;code class=&quot;highlighter-rouge&quot;&gt;Spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;AmplitudeToDB&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;SpectrogramToDB&lt;/code&gt;), &lt;code class=&quot;highlighter-rouge&quot;&gt;MelScale&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;MelSpectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MFCC&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawEncoding&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawDecoding&lt;/code&gt; (previously named &lt;code class=&quot;highlighter-rouge&quot;&gt;MuLawExpanding&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;We now also provide a compatibility interface with Kaldi to ease onboarding and reduce a user’s code dependency on Kaldi. We now have an interface for &lt;code class=&quot;highlighter-rouge&quot;&gt;spectrogram&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;fbank&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;resample_waveform&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-tutorial&quot;&gt;New Tutorial&lt;/h3&gt;

&lt;p&gt;To showcase the new conventions and transformations, we have a &lt;a href=&quot;https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html&quot;&gt;new tutorial&lt;/a&gt; demonstrating how to preprocess waveforms using torchaudio. This tutorial walks through an example of loading a waveform and applying some of the available transformations to it.&lt;/p&gt;

&lt;p&gt;We are excited to see an active community around torchaudio and eager to further grow and support it. We encourage you to go ahead and experiment for yourself with this tutorial and the two datasets that are available: VCTK and YESNO! They have an interface to download the datasets and preprocess them in a convenient format. You can find the details in the release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext-04-with-supervised-learning-datasets&quot;&gt;Torchtext 0.4 with supervised learning datasets&lt;/h2&gt;

&lt;p&gt;A key focus area of torchtext is to provide the fundamental elements to help accelerate NLP research. This includes easy access to commonly used datasets and basic preprocessing pipelines for working on raw text based data. The torchtext 0.4.0 release includes several popular supervised learning baselines with “one-command” data loading. A &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial&lt;/a&gt; is included to show how to use the new datasets for text classification analysis. We also added and improved on a few functions such as &lt;a href=&quot;https://pytorch.org/text/data.html?highlight=get_tokenizer#torchtext.data.get_tokenizer&quot;&gt;get_tokenizer&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/text/vocab.html#build-vocab-from-iterator&quot;&gt;build_vocab_from_iterator&lt;/a&gt; to make it easier to implement future datasets. Additional examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Text classification is an important task in Natural Language Processing with many applications, such as sentiment analysis. The new release includes several popular &lt;a href=&quot;https://pytorch.org/text/datasets.html?highlight=textclassification#torchtext.datasets.TextClassificationDataset&quot;&gt;text classification datasets&lt;/a&gt; for supervised learning including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AG_NEWS&lt;/li&gt;
  &lt;li&gt;SogouNews&lt;/li&gt;
  &lt;li&gt;DBpedia&lt;/li&gt;
  &lt;li&gt;YelpReviewPolarity&lt;/li&gt;
  &lt;li&gt;YelpReviewFull&lt;/li&gt;
  &lt;li&gt;YahooAnswers&lt;/li&gt;
  &lt;li&gt;AmazonReviewPolarity&lt;/li&gt;
  &lt;li&gt;AmazonReviewFull&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each dataset comes with two parts (train vs. test), and can be easily loaded with a single command. The datasets also support an ngrams feature to capture the partial information about the local word order. Take a look at the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;here&lt;/a&gt; to learn more about how to use the new datasets for supervised problems such as text classification analysis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.datasets.text_classification&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATASETS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AG_NEWS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrams&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the domain library, PyTorch provides many tools to make data loading easy. Users now can load and preprocess the text classification datasets with some well supported tools, like &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html&quot;&gt;torch.utils.data.DataLoader&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/master/data.html#torch.utils.data.IterableDataset&quot;&gt;torch.utils.data.IterableDataset&lt;/a&gt;. Here are a few lines to wrap the data with DataLoader. More examples can be found &lt;a href=&quot;https://github.com/pytorch/text/tree/master/examples/text_classification&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collate_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check out the release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt; to learn more and try out the &lt;a href=&quot;http://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;tutorial here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision-04-with-support-for-video&quot;&gt;Torchvision 0.4 with Support for Video&lt;/h2&gt;

&lt;p&gt;Video is now a first-class citizen in torchvision, with support for data loading, datasets, pre-trained models, and transforms. The 0.4 release of torchvision includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Efficient IO primitives for reading/writing video files (including audio), with support for arbitrary encodings and formats.&lt;/li&gt;
  &lt;li&gt;Standard video datasets, compatible with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.utils.data.DataLoader&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Pre-trained models built on the Kinetics-400 dataset for action classification on videos (including the training scripts).&lt;/li&gt;
  &lt;li&gt;Reference training scripts for training your own video models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wanted working with video data in PyTorch to be as straightforward as possible, without compromising too much on performance.
As such, we avoid the steps that would require re-encoding the videos beforehand, as it would involve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A preprocessing step which duplicates the dataset in order to re-encode it.&lt;/li&gt;
  &lt;li&gt;An overhead in time and space because this re-encoding is time-consuming.&lt;/li&gt;
  &lt;li&gt;Generally, an external script should be used to perform the re-encoding.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, we provide APIs such as the utility class, &lt;code class=&quot;highlighter-rouge&quot;&gt;VideoClips&lt;/code&gt;, that simplifies the task of enumerating all possible clips of fixed size in a list of video files by creating an index of all clips in a set of videos. It also allows you to specify a fixed frame-rate for the videos. An example of the API is provided below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets.video_utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyVideoDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VideoClips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;clip_length_in_frames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frames_between_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;frame_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__getitem__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;video_clips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_clips&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Most of the user-facing API is in Python, similar to PyTorch, which makes it easily extensible. Plus, the underlying implementation is fast — torchvision decodes as little as possible from the video on-the-fly in order to return a clip from the video.&lt;/p&gt;

&lt;p&gt;Check out the torchvision 0.4 &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;release notes here&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;We look forward to continuing our collaboration with the community and hearing your feedback as we further improve and expand the PyTorch deep learning platform.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’d like to thank the entire PyTorch team and the community for all of the contributions to this work!&lt;/em&gt;&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Since the release of PyTorch 1.0, we’ve seen the community expand to add new tools, contribute to a growing set of models available in the PyTorch Hub, and continually increase usage in both research and production.</summary></entry><entry><title type="html">Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm</title><link href="https://pytorch.org/blog/mapillary-research/" rel="alternate" type="text/html" title="Mapillary Research: Seamless Scene Segmentation and In-Place Activated BatchNorm" /><published>2019-07-23T00:00:00-07:00</published><updated>2019-07-23T00:00:00-07:00</updated><id>https://pytorch.org/blog/mapillary-research</id><content type="html" xml:base="https://pytorch.org/blog/mapillary-research/">&lt;p&gt;With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.&lt;/p&gt;

&lt;p&gt;Today, people and organizations all over the world have contributed more than 600 million images toward Mapillary’s mission of helping people understand the world’s places through images and making this data available, with clients and partners including the World Bank, HERE, and Toyota Research Institute.&lt;/p&gt;

&lt;p&gt;Mapillary’s computer vision technology brings intelligence to maps in an unprecedented way, increasing our overall understanding of the world. &lt;a href=&quot;https://www.mapillary.com/&quot;&gt;Mapillary&lt;/a&gt; runs state-of-the-art semantic image analysis and image-based 3d modeling at scale and on all its images. In this post we discuss two recent works from Mapillary Research and their implementations in PyTorch - Seamless Scene Segmentation [1] and In-Place Activated BatchNorm [2] - generating Panoptic segmentation results and saving up to 50% of GPU memory during training, respectively.&lt;/p&gt;

&lt;h2 id=&quot;seamless-scene-segmentation&quot;&gt;Seamless Scene Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/seamseg/&quot;&gt;https://github.com/mapillary/seamseg/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/seamless.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The objective of Seamless Scene Segmentation is to predict a “panoptic” segmentation [3] from an image, that is a complete labeling where each pixel is assigned with a class id and, where possible, an instance id. Like many modern CNNs dealing with instance detection and segmentation, we adopt the Mask R-CNN framework [4], using ResNet50 + FPN [5] as a backbone. This architecture works in two stages: first, the “Proposal Head” selects a set of candidate bounding boxes on the image (i.e. the proposals) that could contain an object; then, the “Mask Head” focuses on each proposal, predicting its class and segmentation mask. The output of this process is a “sparse” instance segmentation, covering only the parts of the image that contain countable objects (e.g. cars and pedestrians).&lt;/p&gt;

&lt;p&gt;To complete our panoptic approach coined Seamless Scene Segmentation, we add a third stage to Mask R-CNN. Stemming from the same backbone, the “Semantic Head” predicts a dense semantic segmentation over the whole image, also accounting for the uncountable or amorphous classes (e.g. road and sky). The outputs of the Mask and Semantic heads are finally fused using a simple non-maximum suppression algorithm to generate the final panoptic prediction. All details about the actual network architecture, used losses and underlying math can be found at the &lt;a href=&quot;https://research.mapillary.com/publication/cvpr19a&quot;&gt;project website&lt;/a&gt; for our CVPR 2019 paper [1].&lt;/p&gt;

&lt;p&gt;While several versions of Mask R-CNN are publicly available, including an &lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;official implementation&lt;/a&gt; written in Caffe2, at Mapillary we decided to build Seamless Scene Segmentation from scratch using PyTorch, in order to have full control and understanding of the whole pipeline. While doing so we encountered a couple of main stumbling blocks, and had to come up with some creative workarounds we are going to describe next.&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-variable-sized-tensors&quot;&gt;Dealing with variable-sized tensors&lt;/h2&gt;

&lt;p&gt;Something that sets aside panoptic segmentation networks from traditional CNNs is the prevalence of variable-sized data. In fact, many of the quantities we are dealing with cannot be easily represented with fixed sized tensors: each image contains a different number of objects, the Proposal head can produce a different number of proposals for each image, and the images themselves can have different sizes. While this is not a problem per-se – one could just process images one at a time – we would still like to exploit batch-level parallelism as much as possible. Furthermore, when performing distributed training with multiple GPUs, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; expects its inputs to be batched, uniformly-sized tensors.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/packed_sequence.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Our solution to these issues is to wrap each batch of variable-sized tensors in a &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; is little more than a glorified list class for tensors, tagging its contents as “related”, ensuring that they all share the same type, and providing useful methods like moving all the tensors to a particular device, etc. When performing light-weight operations that wouldn’t be much faster with batch-level parallelism, we simply iterate over the contents of the &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; in a for loop. When performance is crucial, e.g. in the body of the network, we simply concatenate the contents of the PackedSequence, adding zero padding as required (like in RNNs with variable-length inputs), and keeping track of the original dimensions of each tensor.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt;s also help us deal with the second problem highlighted above. We slightly modify &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; to recognize &lt;code class=&quot;highlighter-rouge&quot;&gt;PackedSequence&lt;/code&gt; inputs, splitting them in equally sized chunks and distributing their contents across the GPUs.&lt;/p&gt;

&lt;h2 id=&quot;asymmetric-computational-graphs-with-distributed-data-parallel&quot;&gt;Asymmetric computational graphs with Distributed Data Parallel&lt;/h2&gt;

&lt;p&gt;Another, perhaps more subtle, peculiarity of our network is that it can generate asymmetric computational graphs across GPUs. In fact, some of the modules that compose the network are “optional”, in the sense that they are not always computed for all images. As an example, when the Proposal head doesn’t output any proposal, the Mask head is not traversed at all. If we are training on multiple GPUs with &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt;, this results in one of the replicas not computing gradients for the Mask head parameters.&lt;/p&gt;

&lt;p&gt;Prior to PyTorch 1.1, this resulted in a crash, so we had to develop a workaround. Our simple but effective solution was to compute a “fake forward pass” when no actual forward is required, i.e. something like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fake_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_correctly_shaped_fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we generate a batch of bogus data, pass it through the Mask head, and return a loss that always back-progates zeros to all parameters.&lt;/p&gt;

&lt;p&gt;Starting from PyTorch 1.1 this workaround is no longer required: by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;find_unused_parameters=True&lt;/code&gt; in the constructor, &lt;code class=&quot;highlighter-rouge&quot;&gt;DistributedDataParallel&lt;/code&gt; is told to identify parameters whose gradients have not been computed by all replicas and correctly handle them. This leads to some substantial simplifications in our code base!&lt;/p&gt;

&lt;h2 id=&quot;in-place-activated-batchnorm&quot;&gt;In-place Activated BatchNorm&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Github project page: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Most researchers would probably agree that there are always constraints in terms of available GPU resources, regardless if their research lab has access to only a few or multiple thousands of GPUs. In a time where at Mapillary we still worked at rather few and mostly 12GB Titan X - style prosumer GPUs, we were searching for a solution that virtually enhances the usable memory during training, so we would be able to obtain and push state-of-the-art results on dense labeling tasks like semantic segmentation. In-place activated BatchNorm is enabling us to use up to 50% more memory (at little computational overhead) and is therefore deeply integrated in all our current projects (including Seamless Scene Segmentation described above).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/inplace_abn.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When processing a BN-Activation-Convolution sequence in the forward pass, most deep learning frameworks (including PyTorch) need to store two big buffers, i.e. the input x of BN and the input z of Conv. This is necessary because the standard implementations of the backward passes of BN and Conv depend on their inputs to calculate the gradients. Using InPlace-ABN to replace the BN-Activation sequence, we can safely discard x, thus saving up to 50% GPU memory at training time. To achieve this, we rewrite the backward pass of BN in terms of its output y, which is in turn reconstructed from z by inverting the activation function.&lt;/p&gt;

&lt;p&gt;The only limitation of InPlace-ABN is that it requires using an invertible activation function, such as leaky relu or elu. Except for this, it can be used as a direct, drop-in replacement for BN+activation modules in any network. Our native CUDA implementation offers minimal computational overhead compared to PyTorch’s standard BN, and is available for anyone to use from here: &lt;a href=&quot;https://github.com/mapillary/inplace_abn/&quot;&gt;https://github.com/mapillary/inplace_abn/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;synchronized-bn-with-asymmetric-graphs-and-unbalanced-batches&quot;&gt;Synchronized BN with asymmetric graphs and unbalanced batches&lt;/h2&gt;

&lt;p&gt;When training networks with synchronized SGD over multiple GPUs and/or multiple nodes, it’s common practice to compute BatchNorm statistics separately on each device. However, in our experience working with semantic and panoptic segmentation networks, we found that accumulating mean and variance across all workers can bring a substantial boost in accuracy. This is particularly true when dealing with small batches, like in Seamless Scene Segmentation where we train with a single, super-high resolution image per GPU.&lt;/p&gt;

&lt;p&gt;InPlace-ABN supports synchronized operation over multiple GPUs and multiple nodes, and, since version 1.1, this can also be achieved in the standard PyTorch library using &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#syncbatchnorm&quot;&gt;SyncBatchNorm&lt;/a&gt;. Compared to SyncBatchNorm, however, we support some additional functionality which is particularly important for Seamless Scene Segmentation: unbalanced batches and asymmetric graphs.&lt;/p&gt;

&lt;p&gt;As mentioned before, Mask R-CNN-like networks naturally give rise to variable-sized tensors. Thus, in InPlace-ABN we calculate synchronized statistics using a variant of the parallel algorithm described &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm&quot;&gt;here&lt;/a&gt;, which properly takes into account the fact that each GPU can hold a different number of samples. PyTorch’s SyncBatchNorm is currently being revised to support this, and the improved functionality will be available in a future release.&lt;/p&gt;

&lt;p&gt;Asymmetric graphs (in the sense mentioned above) are another complicating factor one has to deal with when creating a synchronized BatchNorm implementation. Luckily, PyTorch’s distributed group functionality allows us to restrict distributed communication to a subset of workers, easily excluding those that are currently inactive. The only missing piece is that, in order to create a distributed group, each process needs to know the ids of all processes that will participate in the group, and even processes that are not part of the group need to call the &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; function. In InPlace-ABN we handle it with a function like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;active_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Initialize a distributed group where each process can independently decide whether to participate or not&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Gather active status from all workers&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Create group&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;active_workers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First each process, including inactive ones, communicates its status to all others through an &lt;code class=&quot;highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; call, then it creates the distributed group with the shared information. In the actual implementation we also include a caching mechanism for groups, since &lt;code class=&quot;highlighter-rouge&quot;&gt;new_group()&lt;/code&gt; is usually too expensive to call at each batch.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Seamless Scene Segmentation; Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[2] In-place Activated BatchNorm for Memory-Optimized Training of DNNs; Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder; Computer Vision and Pattern Recognition (CVPR), 2018&lt;/p&gt;

&lt;p&gt;[3] Panoptic Segmentation; Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollar; Computer Vision and Pattern Recognition (CVPR), 2019&lt;/p&gt;

&lt;p&gt;[4] Mask R-CNN; Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick; International Conference on Computer Vision (ICCV), 2017&lt;/p&gt;

&lt;p&gt;[5] Feature Pyramid Networks for Object Detection; Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie; Computer Vision and Pattern Recognition (CVPR), 2017&lt;/p&gt;</content><author><name>Lorenzo Porzi, Mapillary</name></author><summary type="html">With roads in developed countries like the US changing up to 15% annually, Mapillary addresses a growing demand for keeping maps updated by combining images from any camera into a 3D visualization of the world. Mapillary’s independent and collaborative approach enables anyone to collect, share, and use street-level images for improving maps, developing cities, and advancing the automotive industry.</summary></entry><entry><title type="html">PyTorch Adds New Ecosystem Projects for Encrypted AI and Quantum Computing, Expands PyTorch Hub</title><link href="https://pytorch.org/blog/pytorch-ecosystem/" rel="alternate" type="text/html" title="PyTorch Adds New Ecosystem Projects for Encrypted AI and Quantum Computing, Expands PyTorch Hub" /><published>2019-07-18T00:00:00-07:00</published><updated>2019-07-18T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-ecosystem</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-ecosystem/">&lt;p&gt;The PyTorch ecosystem includes projects, tools, models and libraries from a broad community of researchers in academia and industry, application developers, and ML engineers. The goal of this ecosystem is to support, accelerate, and aid in your exploration with PyTorch and help you push the state of the art, no matter what field you are exploring. Similarly, we are expanding the recently launched PyTorch Hub to further help you discover and reproduce the latest research.&lt;/p&gt;

&lt;p&gt;In this post, we’ll highlight some of the projects that have been added to the PyTorch ecosystem this year and provide some context on the criteria we use to evaluate community projects. We’ll also provide an update on the fast-growing PyTorch Hub and share details on our upcoming PyTorch Summer Hackathon.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/pytorch-ecosystem.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;recently-added-ecosystem-projects&quot;&gt;Recently added ecosystem projects&lt;/h2&gt;

&lt;p&gt;From private AI to quantum computing, we’ve seen the community continue to expand into new and interesting areas. The latest projects include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/BorealisAI/advertorch&quot;&gt;Advertorch&lt;/a&gt;: A Python toolbox for adversarial robustness research. The primary functionalities are implemented in PyTorch. Specifically, AdverTorch contains modules for generating adversarial perturbations and defending against adversarial examples, as well as scripts for adversarial training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://botorch.org/&quot;&gt;botorch&lt;/a&gt;: A modular and easily extensible interface for composing Bayesian optimization primitives, including probabilistic models, acquisition functions, and optimizers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/skorch-dev/skorch&quot;&gt;Skorch&lt;/a&gt;: A high-level library for PyTorch that provides full scikit-learn compatibility.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/rusty1s/pytorch_geometric&quot;&gt;PyTorch Geometric&lt;/a&gt;: A library for deep learning on irregular input data such as graphs, point clouds, and manifolds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/OpenMined/PySyft&quot;&gt;PySyft&lt;/a&gt;: A Python library for encrypted, privacy preserving deep learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pennylane.ai/&quot;&gt;PennyLane&lt;/a&gt;: A library for quantum ML, automatic differentiation, and optimization of hybrid quantum-classical computations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/zalandoresearch/flair&quot;&gt;Flair&lt;/a&gt;: A very simple framework for state-of-the-art natural language processing (NLP).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-makes-a-great-project&quot;&gt;What makes a great project?&lt;/h3&gt;

&lt;p&gt;When we review project submissions for the PyTorch ecosystem, we take into account a number of factors that we feel are important and that we would want in the projects we use ourselves. Some of these criteria include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Well-tested:&lt;/em&gt; Users should be confident that ecosystem projects will work well with PyTorch, and include support for CI to ensure that testing is occurring on a continuous basis and the project can run on the latest version of PyTorch.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Clear utility:&lt;/em&gt; Users should understand where each project fits within the PyTorch ecosystem and the value it brings.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Permissive licensing:&lt;/em&gt; Users must be able to utilize ecosystem projects without licensing concerns. e.g. BSD-3, Apache-2 and MIT licenses&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Easy onboarding:&lt;/em&gt; Projects need to have support for binary installation options (pip/Conda), clear documentation and a rich set of tutorials (ideally built into Jupyter notebooks).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ongoing maintenance:&lt;/em&gt; Project authors need to be committed to supporting and maintaining their projects.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Community:&lt;/em&gt; Projects should have (or be on track to building) an active, broad-based community.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you would like to have your project included in the PyTorch ecosystem and featured on &lt;a href=&quot;http://pytorch.org/ecosystem&quot;&gt;pytorch.org/ecosystem&lt;/a&gt;, please complete the form &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;here&lt;/a&gt;. If you’ve previously submitted a project for consideration and haven’t heard back, we promise to get back to you as soon as we can - we’ve received a lot of submissions!&lt;/p&gt;

&lt;h2 id=&quot;pytorch-hub-for-reproducible-research--new-models&quot;&gt;PyTorch Hub for reproducible research | New models&lt;/h2&gt;

&lt;p&gt;Since &lt;a href=&quot;https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/&quot;&gt;launching&lt;/a&gt; the PyTorch Hub in beta, we’ve received a lot of interest from the community including the contribution of many new models. Some of the latest include &lt;a href=&quot;https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/&quot;&gt;U-Net for Brain MRI&lt;/a&gt; contributed by researchers at Duke University, &lt;a href=&quot;https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/&quot;&gt;Single Shot Detection&lt;/a&gt; from NVIDIA and &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_transformerXL/&quot;&gt;Transformer-XL&lt;/a&gt; from HuggingFace.&lt;/p&gt;

&lt;p&gt;We’ve seen organic integration of the PyTorch Hub by folks like &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode&lt;/a&gt;, making it even easier for you to try out the state of the art in AI research. In addition, companies like &lt;a href=&quot;https://github.com/axsaucedo/seldon-core/tree/pytorch_hub/examples/models/pytorchhub&quot;&gt;Seldon&lt;/a&gt; provide production-level support for PyTorch Hub models on top of Kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-benefits-of-contributing-a-model-in-the-pytorch-hub&quot;&gt;What are the benefits of contributing a model in the PyTorch Hub?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Compatibility:&lt;/em&gt; PyTorch Hub models are prioritized first for testing by the TorchScript and Cloud TPU teams, and used as baselines for researchers across a number of fields.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Visibility:&lt;/em&gt; Models in the Hub will be promoted on &lt;a href=&quot;http://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt; as well as on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ease of testing and reproducibility:&lt;/em&gt; Each model comes with code, clear preprocessing requirements, and methods/dependencies to run. There is also tight integration with &lt;a href=&quot;https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_WSL-Images_resnext.ipynb#scrollTo=LM_l7vXJvnDM&quot;&gt;Google Colab&lt;/a&gt;, making it a true single click to get started.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-hub-contributions-welcome&quot;&gt;PyTorch Hub contributions welcome!&lt;/h3&gt;

&lt;p&gt;We are actively looking to grow the PyTorch Hub and welcome contributions. You don’t need to be an original paper author to contribute, and we’d love to see the number of domains and fields broaden. So what types of contributions are we looking for?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Artifacts of a published or an arXiv paper (or something of a similar nature that serves a different audience — such as ULMFit) that a large audience would need.&lt;/p&gt;

    &lt;p&gt;AND&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reproduces the published results (or better)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall these models are aimed at researchers either trying to reproduce a baseline, or trying to build downstream research on top of the model (such as feature-extraction or fine-tuning) as well as researchers looking for a demo of the paper for subjective evaluation. Please keep this audience in mind when contributing.&lt;/p&gt;

&lt;p&gt;If you are short on inspiration or would just like to find out what the SOTA is an any given field or domain, checkout the Paperswithcode &lt;a href=&quot;https://paperswithcode.com/sota&quot;&gt;state-of-the-art gallery&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-summer-hackathon&quot;&gt;PyTorch Summer Hackathon&lt;/h2&gt;

&lt;p&gt;We’ll be hosting the first PyTorch Summer Hackathon next month. We invite you to apply to participate in the in-person hackathon on  August 8th to 9th at Facebook’s Menlo Park campus. We’ll be bringing the community together to work on innovative ML projects that can solve a broad range of complex challenges.&lt;/p&gt;

&lt;p&gt;Applications will be reviewed and accepted on a rolling basis until spaces are filled. For those who cannot join this Hackathon in person, we’ll be following up soon with other ways to participate.&lt;/p&gt;

&lt;p&gt;Please visit &lt;a href=&quot;https://www.eventbrite.com/e/pytorch-summer-hackathon-in-menlo-park-registration-63756668913&quot;&gt;this link to apply&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you for being part of the PyTorch community!&lt;/p&gt;

&lt;p&gt;-Team PyTorch&lt;/p&gt;</content><author><name>Team PyTorch</name></author><summary type="html">The PyTorch ecosystem includes projects, tools, models and libraries from a broad community of researchers in academia and industry, application developers, and ML engineers. The goal of this ecosystem is to support, accelerate, and aid in your exploration with PyTorch and help you push the state of the art, no matter what field you are exploring. Similarly, we are expanding the recently launched PyTorch Hub to further help you discover and reproduce the latest research.</summary></entry><entry><title type="html">Towards Reproducible Research with PyTorch Hub</title><link href="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/" rel="alternate" type="text/html" title="Towards Reproducible Research with PyTorch Hub" /><published>2019-06-10T00:00:00-07:00</published><updated>2019-06-10T00:00:00-07:00</updated><id>https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub</id><content type="html" xml:base="https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/">&lt;p&gt;Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.&lt;/p&gt;

&lt;p&gt;We are excited to announce the availability of PyTorch Hub, a simple API and workflow that provides the basic building blocks for improving machine learning research reproducibility. PyTorch Hub consists of a pre-trained model repository designed specifically to facilitate research reproducibility and enable new research. It also has built-in support for &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Colab&lt;/a&gt;, integration with &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;&lt;em&gt;Papers With Code&lt;/em&gt;&lt;/a&gt; and currently contains a broad set of models that include Classification and Segmentation, Generative, Transformers, etc.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-header-1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;owner-publishing-models&quot;&gt;[Owner] Publishing models&lt;/h2&gt;

&lt;p&gt;PyTorch Hub supports the publication of pre-trained models (model definitions and pre-trained weights) to a GitHub repository by adding a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; file.
This provides an enumeration of which models are to be supported and a list of dependencies needed to run the models.
Examples can be found in the &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;torchvision&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/hubconf.py&quot;&gt;huggingface-bert&lt;/a&gt; and &lt;a href=&quot;https://github.com/facebookresearch/pytorch_GAN_zoo&quot;&gt;gan-model-zoo&lt;/a&gt; repositories.&lt;/p&gt;

&lt;p&gt;Let us look at the simplest case: &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;’s &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Optional list of dependencies required by the package&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.alexnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.densenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet169&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;densenet161&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.inception&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inception_v3&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.resnet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet152&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
&lt;span class=&quot;n&quot;&gt;resnext50_32x4d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnext101_32x8d&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.squeezenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeezenet1_1&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.vgg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg11_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg13_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg16_bn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vgg19_bn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.segmentation&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fcn_resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deeplabv3_resnet101&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.googlenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;googlenet&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.shufflenetv2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x0_5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shufflenet_v2_x1_0&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.mobilenet&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mobilenet_v2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;, the models have the following properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each model file can function and be executed independently&lt;/li&gt;
  &lt;li&gt;They dont require any package other than PyTorch (encoded in &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;dependencies['torch']&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;They dont need separate entry-points, because the models when created, work seamlessly out of the box&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minimizing package dependencies reduces the friction for users to load your model for immediate experimentation.&lt;/p&gt;

&lt;p&gt;A more involved example is HuggingFace’s BERT models. Here is their &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'torch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tqdm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'boto3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'requests'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'regex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hubconfs.bert_hubconf&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForNextSentencePrediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForPreTraining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForMultipleChoice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForQuestionAnswering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bertForTokenClassification&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each model then requires an entrypoint to be created. Here is a code snippet to specify an entrypoint of the &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt; model, which returns the pre-trained model weights.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bertForMaskedLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    BertForMaskedLM includes the BertModel Transformer followed by the
    pre-trained masked language modeling head.
    Example:
      ...
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMaskedLM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These entry-points can serve as wrappers around complex model factories. They can give a clean and consistent help docstring, have logic to support downloading of pretrained weights (for example via &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt;) or have additional hub-specific functionality such as visualization.&lt;/p&gt;

&lt;p&gt;With a &lt;code class=&quot;highlighter-rouge&quot;&gt;hubconf.py&lt;/code&gt; in place, you can send a pull request based on the template &lt;a href=&quot;https://github.com/pytorch/hub/blob/master/docs/template.md&quot;&gt;here&lt;/a&gt;.
Our goal is to curate high-quality, easily-reproducible, maximally-beneficial models for research reproducibility.
Hence, we may work with you to refine your pull request and in some cases reject some low-quality models to be published.
Once we accept your pull request, your model will soon appear on &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;Pytorch hub webpage&lt;/a&gt; for all users to explore.&lt;/p&gt;

&lt;h2 id=&quot;user-workflow&quot;&gt;[User] Workflow&lt;/h2&gt;

&lt;p&gt;As a user, PyTorch Hub allows you to follow a few simple steps and do things like: 1) explore available models; 2) load a model; and 3) understand what methods are available for any given model. Let’s walk through some examples of each.&lt;/p&gt;

&lt;h3 id=&quot;explore-available-entrypoints&quot;&gt;Explore available entrypoints.&lt;/h3&gt;

&lt;p&gt;Users can list all available entrypoints in a repo using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.list()&lt;/code&gt; API.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alexnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'densenet121'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg16_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'vgg19'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'vgg19_bn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that PyTorch Hub also allows auxillary entrypoints (other than pretrained models), e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;bertTokenizer&lt;/code&gt; for preprocessing in the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; models, to make the user workflow smoother.&lt;/p&gt;

&lt;h3 id=&quot;load-a-model&quot;&gt;Load a model&lt;/h3&gt;

&lt;p&gt;Now that we know which models are available in the Hub, users can load a model entrypoint using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.load()&lt;/code&gt; API. This only requires a single command without the need to install a wheel. In addition the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.hub.help()&lt;/code&gt; API can provide useful information about how to instantiate the model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deeplabv3_resnet101'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is also common that repo owners will want to continually add bug fixes or performance improvements. PyTorch Hub makes it super simple for users to get the latest update by calling:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;force_reload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We believe this will help to alleviate the burden of repetitive package releases by repo owners and instead allow them to focus more on their research.
It also ensures that, as a user, you are getting the freshest available models.&lt;/p&gt;

&lt;p&gt;On the contrary, stability is important for users. Hence, some model owners serve them from a specificed branch or tag, rather than the &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; branch, to ensure stability of the code.
For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch_GAN_zoo&lt;/code&gt; serves them from the &lt;code class=&quot;highlighter-rouge&quot;&gt;hub&lt;/code&gt; branch:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'facebookresearch/pytorch_GAN_zoo:hub'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'DCGAN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;useGPU&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;**kwargs&lt;/code&gt; passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.load()&lt;/code&gt; are used to &lt;em&gt;instantiate&lt;/em&gt; a model. In the above example, &lt;code class=&quot;highlighter-rouge&quot;&gt;pretrained=True&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;useGPU=False&lt;/code&gt; are given to the model’s entrypoint.&lt;/p&gt;

&lt;h3 id=&quot;explore-a-loaded-model&quot;&gt;Explore a loaded model&lt;/h3&gt;

&lt;p&gt;Once you have a model from PyTorch Hub loaded, you can use the following workflow to find out the available methods that are supported as well as understand better what arguments are requires to run it.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dir(model)&lt;/code&gt; to see all available methods of the model. Let’s take a look at &lt;code class=&quot;highlighter-rouge&quot;&gt;bertForMaskedLM&lt;/code&gt;’s available methods.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'forward'&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'to'&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'state_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;help(model.forward)&lt;/code&gt; provides a view into what arguments are required to make your loaded model run&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Help&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_pretrained_bert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modeling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masked_lm_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Have a closer look at the &lt;a href=&quot;https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/&quot;&gt;BERT&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&quot;&gt;DeepLabV3&lt;/a&gt; pages, where you can see how these models can be used once loaded.&lt;/p&gt;

&lt;h3 id=&quot;other-ways-to-explore&quot;&gt;Other ways to explore&lt;/h3&gt;

&lt;p&gt;Models available in PyTorch Hub also support both &lt;a href=&quot;https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorch-gan-zoo_pgan.ipynb&quot;&gt;Colab&lt;/a&gt; and are directly linked on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;Papers With Code&lt;/a&gt; and you can get started with a single click. &lt;a href=&quot;https://paperswithcode.com/paper/densely-connected-convolutional-networks&quot;&gt;Here&lt;/a&gt; is a good example to get started with (shown below).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/hub-blog-pwc.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional resources:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Hub API documentation can be found &lt;a href=&quot;https://pytorch.org/docs/stable/hub.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Submit a model &lt;a href=&quot;https://github.com/pytorch/hub&quot;&gt;here&lt;/a&gt; for publication in PyTorch Hub.&lt;/li&gt;
  &lt;li&gt;Go to &lt;a href=&quot;https://pytorch.org/hub&quot;&gt;https://pytorch.org/hub&lt;/a&gt; to learn more about the available models.&lt;/li&gt;
  &lt;li&gt;Look for more models to come on &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;paperswithcode.com&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A BIG thanks to the folks at HuggingFace, the PapersWithCode team, fast.ai and Nvidia as well as Morgane Riviere (FAIR Paris) and lots of others for helping bootstrap this effort!!&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: If we would like to contribute a model that is already in the Hub but perhaps mine has better accuracy, should I still contribute?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: Yes!! A next step for Hub is to implement an upvote/downvote system to surface the best models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Who hosts the model weights for PyTorch Hub?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: You, as the contributor, are responsible to host the model weights. You can host your model in your favorite cloud storage or, if it fits within the limits, on GitHub. If it is not within your means to host the weights, check with us via opening an issue on the hub repository.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if my model is trained on private data? Should I still contribute this model?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: No! PyTorch Hub is centered around open research and that extends to the usage of open datasets to train these models on. If a pull request for a proprietary model is submitted, we will kindly ask that you resubmit a model trained on something open and available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Where are my downloaded models saved?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A: We follow the &lt;a href=&quot;https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html&quot;&gt;XDG Base Directory Specification&lt;/a&gt; and adhere to common standards around cached files and directories.&lt;/p&gt;

&lt;p&gt;The locations are used in the order of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$TORCH_HOME/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;TORCH_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$XDG_CACHE_HOME/torch/hub&lt;/code&gt;, if environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;XDG_CACHE_HOME&lt;/code&gt; is set.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.cache/torch/hub&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Team PyTorch</name></author><summary type="html">Reproducibility is an essential requirement for many fields of research including those based on machine learning techniques. However, many machine learning publications are either not reproducible or are difficult to reproduce. With the continued growth in the number of research publications, including tens of thousands of papers now hosted on arXiv and submissions to conferences at an all time high, research reproducibility is more important than ever. While many of these publications are accompanied by code as well as trained models which is helpful but still leaves a number of steps for users to figure out for themselves.</summary></entry><entry><title type="html">torchvision 0.3: segmentation, detection models, new datasets and more..</title><link href="https://pytorch.org/blog/torchvision03/" rel="alternate" type="text/html" title="torchvision 0.3: segmentation, detection models, new datasets and more.." /><published>2019-05-22T00:00:00-07:00</published><updated>2019-05-22T00:00:00-07:00</updated><id>https://pytorch.org/blog/torchvision03</id><content type="html" xml:base="https://pytorch.org/blog/torchvision03/">&lt;p&gt;PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/torchvision_0.3_headline.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;new-features-include&quot;&gt;New features include:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reference training / evaluation scripts:&lt;/strong&gt; torchvision now provides, under the references/ folder, scripts for training and evaluation of the following tasks: classification, semantic segmentation, object detection, instance segmentation and person keypoint detection. These serve as a log of how to train a specific model and provide baseline training and evaluation scripts to quickly bootstrap research.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;torchvision ops:&lt;/strong&gt; torchvision now contains custom C++ / CUDA operators. Those operators are specific to computer vision, and make it easier to build object detection models. These operators currently do not support PyTorch script mode, but support for it is planned for in the next release. Some of the ops supported include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;roi_pool (and the module version RoIPool)&lt;/li&gt;
  &lt;li&gt;roi_align (and the module version RoIAlign)&lt;/li&gt;
  &lt;li&gt;nms, for non-maximum suppression of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_iou, for computing the intersection over union metric between two sets of bounding boxes&lt;/li&gt;
  &lt;li&gt;box_area, for computing the area of a set of bounding boxes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are a few examples on using torchvision ops:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create 10 random boxes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# they need to be in [x0, y0, x1, y1] format&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# create a random image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# extract regions in `image` defined in `boxes`, rescaling&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# them to have a size of 3x3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi_align&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# check the size&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooled_regions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 3, 3, 3])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or compute the intersection over union between&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# all pairs of boxes&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box_iou&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# torch.Size([10, 10])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;New models and datasets:&lt;/strong&gt; torchvision now adds support for object detection, instance segmentation and person keypoint detection models. In addition, several popular datasets have been added. Note: The API is currently experimental and might change in future versions of torchvision. New models include:&lt;/p&gt;

&lt;h3 id=&quot;segmentation-models&quot;&gt;Segmentation Models&lt;/h3&gt;

&lt;p&gt;The 0.3 release also contains models for dense pixelwise prediction on images.
It adds FCN and DeepLabV3 segmentation models, using a ResNet50 and ResNet101 backbones.
Pre-trained weights for ResNet101 backbone are available, and have been trained on a subset of COCO train2017, which contains the same 20 categories as those from Pascal VOC.&lt;/p&gt;

&lt;p&gt;The pre-trained models give the following results on the subset of COCO val2017 which contain the same 20 categories as those present in Pascal VOC:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;mean IoU&lt;/th&gt;
      &lt;th&gt;global pixelwise acc&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;FCN ResNet101&lt;/td&gt;
      &lt;td&gt;63.7&lt;/td&gt;
      &lt;td&gt;91.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLabV3 ResNet101&lt;/td&gt;
      &lt;td&gt;67.4&lt;/td&gt;
      &lt;td&gt;92.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;detection-models&quot;&gt;Detection Models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;box AP&lt;/th&gt;
      &lt;th&gt;mask AP&lt;/th&gt;
      &lt;th&gt;keypoint AP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;37.9&lt;/td&gt;
      &lt;td&gt;34.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN trained on COCO&lt;/td&gt;
      &lt;td&gt;54.6&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;65.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The implementations of the models for object detection, instance segmentation and keypoint detection are fast, specially during training.&lt;/p&gt;

&lt;p&gt;In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.&lt;/p&gt;

&lt;p&gt;For test time, we report the time for the model evaluation and post-processing (including mask pasting in image), but not the time for computing the precision-recall.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Network&lt;/th&gt;
      &lt;th&gt;train time (s / it)&lt;/th&gt;
      &lt;th&gt;test time (s / it)&lt;/th&gt;
      &lt;th&gt;memory (GB)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Faster R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2288&lt;/td&gt;
      &lt;td&gt;0.0590&lt;/td&gt;
      &lt;td&gt;5.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mask R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.2728&lt;/td&gt;
      &lt;td&gt;0.0903&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Keypoint R-CNN ResNet-50 FPN&lt;/td&gt;
      &lt;td&gt;0.3789&lt;/td&gt;
      &lt;td&gt;0.1242&lt;/td&gt;
      &lt;td&gt;6.8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can load and use pre-trained detection and segmentation models with a few lines of code&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maskrcnn_resnet50_fpn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# set it to evaluation mode, as the model behaves differently&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# during training and during evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/an/image.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pass a list of (potentially different sized) tensors&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to the model, in 0-1 range. The model will take care of&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# batching them together and normalizing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output is a list of dict, containing the postprocessed predictions&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;classification-models&quot;&gt;Classification Models&lt;/h3&gt;

&lt;p&gt;The following classification models were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GoogLeNet (Inception v1)&lt;/li&gt;
  &lt;li&gt;MobileNet V2&lt;/li&gt;
  &lt;li&gt;ShuffleNet v2&lt;/li&gt;
  &lt;li&gt;ResNeXt-50 32x4d and ResNeXt-101 32x8d&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;The following datasets were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Caltech101, Caltech256, and CelebA&lt;/li&gt;
  &lt;li&gt;ImageNet dataset (improving on ImageFolder, provides class-strings)&lt;/li&gt;
  &lt;li&gt;Semantic Boundaries Dataset&lt;/li&gt;
  &lt;li&gt;VisionDataset as a base class for all datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we’ve added more image transforms, general improvements and bug fixes, as well as improved documentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;See the full release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt; as well as this getting started tutorial &lt;a href=&quot;https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb&quot;&gt;on Google Colab here&lt;/a&gt;, which describes how to fine tune your own instance segmentation model on a custom dataset.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content><author><name>Francisco Massa</name></author><summary type="html">PyTorch domain libraries like torchvision provide convenient access to common datasets and models that can be used to quickly create a state-of-the-art baseline. Moreover, they also provide common abstractions to reduce boilerplate code that users might have to otherwise repeatedly write. The torchvision 0.3 release brings several new features including models for semantic segmentation, object detection, instance segmentation, and person keypoint detection, as well as custom C++ / CUDA ops specific to computer vision.</summary></entry><entry><title type="html">Model Serving in PyTorch</title><link href="https://pytorch.org/blog/model-serving-in-pyorch/" rel="alternate" type="text/html" title="Model Serving in PyTorch" /><published>2019-05-08T00:00:00-07:00</published><updated>2019-05-08T00:00:00-07:00</updated><id>https://pytorch.org/blog/model-serving-in-pyorch</id><content type="html" xml:base="https://pytorch.org/blog/model-serving-in-pyorch/">&lt;p&gt;PyTorch has seen a lot of adoption in research, but people can get confused about how well PyTorch models can be taken into production. This blog post is meant to clear up any confusion people might have about the road to production in PyTorch.
Usually when people talk about taking a model “to production,” they usually mean performing &lt;strong&gt;inference&lt;/strong&gt;, sometimes called model evaluation or prediction or serving. At the level of a function call, in PyTorch, inference looks something like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Python
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;module(input)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In traced modules
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;module(input)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In C++
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;at::Tensor output = module-&amp;gt;forward(inputs).toTensor();&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we at Facebook perform inference operations using PyTorch hundreds of trillions of times per day, we’ve done a lot to make sure that inference runs as efficiently as possible.&lt;/p&gt;

&lt;h2 id=&quot;serving-strategies&quot;&gt;Serving Strategies&lt;/h2&gt;

&lt;p&gt;That zoomed-in view of how you use models in inference isn’t usually the whole story, though. In a real world machine learning system, you often need to do more than just run a single inference operation in the REPL or Jupyter notebook. Instead, you usually need to integrate your model into a larger application in some way. Depending on what you need to do, you can usually take one of the following approaches.&lt;/p&gt;

&lt;h3 id=&quot;direct-embedding&quot;&gt;Direct embedding&lt;/h3&gt;

&lt;p&gt;In application settings like mobile, we often just directly call the model as part of a larger program. This isn’t just for apps; usually this is how robotics and dedicated devices work as well. At a code-level, the call to the model is exactly the same as what is shown above in the section about inference shown above. A key concern is often that a Python interpreter is not present in such environments, which is why PyTorch allows you to call your models from C++ and ship a model without the need for a Python runtime.&lt;/p&gt;

&lt;h3 id=&quot;model-microservices&quot;&gt;Model microservices&lt;/h3&gt;

&lt;p&gt;If you’re using your model in a server side context and you’re managing multiple models, you might choose to treat each individual model (or each individual model version) as a separate service, usually using some sort of packaging mechanism like a Docker container. Then that service is often made network accessible via some sort of service, either using JSON over HTTP or an RPC technology like gRPC. The key characteristic of this approach is that you’re defining a service with a single endpoint that just calls your model. Then you do do all of your model management (promotion, rollback, etc.) via whatever system you already use to manage your services (e.g. kubernetes, ECS).&lt;/p&gt;

&lt;h3 id=&quot;model-servers&quot;&gt;Model servers&lt;/h3&gt;

&lt;p&gt;An additional possible solution is to use a model server. This is an application built to manage and serve models. It allows you to upload multiple models and get distinct prediction endpoints for each of them. Typically such systems include a number of other features to help solve more of the whole problem of managing and serving models. This can include things like metrics, visualization, data pre-processing, and more. Even something as simple as having a system for automatically versioning models can make building important features like model rollbacks much easier.&lt;/p&gt;

&lt;h3 id=&quot;evolving-patterns&quot;&gt;Evolving Patterns&lt;/h3&gt;

&lt;p&gt;The above is a somewhat arbitrary breakdown of different approaches based on a snapshot in time. Design patterns are still evolving. Recently, model server designs have started to adopt more of the technologies of general service infrastructure such as Docker containers and kubernetes, so many model servers have started to share properties of the model microservice design discussed above. For a deeper dive into the general concepts of model server designs, you can check out my &lt;a href=&quot;https://www.manning.com/books/machine-learning-systems&quot;&gt;book on machine learning systems&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;serving-pytorch-models&quot;&gt;Serving PyTorch Models&lt;/h2&gt;

&lt;p&gt;So, if you’re a PyTorch user, what should you use if you want to take your models to production?&lt;/p&gt;

&lt;p&gt;If you’re on mobile or working on an embedded system like a robot, direct embedding in your application is often the right choice. 
For mobile specifically, your use case might be served by the ONNX export functionality.
Note that ONNX, by its very nature, has limitations and doesn’t support all of the functionality provided by the larger PyTorch project.
You can check out &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html&quot;&gt;this tutorial&lt;/a&gt; on deploying PyTorch models to mobile using ONNX to see if this path might suit your use case. 
That said, we’ve heard that there’s a lot more that PyTorch users want to do on mobile, so look for more mobile-specific functionality in PyTorch in the future.
For other embedded systems, like robots, running &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;inference on a PyTorch model from the C++ API&lt;/a&gt; could be the right solution.&lt;/p&gt;

&lt;p&gt;If you can’t use the cloud or prefer to manage all services using the same technology, you can follow &lt;a href=&quot;https://medium.com/datadriveninvestor/deploy-your-pytorch-model-to-production-f69460192217&quot;&gt;this example&lt;/a&gt; to build a simple model microservice using the Flask web framework.&lt;/p&gt;

&lt;p&gt;If you want to manage multiple models within a non-cloud service solution, there are teams developing PyTorch support in model servers like &lt;a href=&quot;https://mlflow.org/&quot;&gt;MLFlow&lt;/a&gt;, &lt;a href=&quot;https://www.kubeflow.org/&quot;&gt;Kubeflow&lt;/a&gt;, and &lt;a href=&quot;https://oss.redislabs.com/redisai/&quot;&gt;RedisAI.&lt;/a&gt; We’re excited to see innovation from multiple teams building OSS model servers, and we’ll continue to highlight innovation in the PyTorch ecosystem in the future.&lt;/p&gt;

&lt;p&gt;If you can use the cloud for your application, there are several great choices for working with models in the cloud. For AWS Sagemaker, you can start find a guide to &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html&quot;&gt;all of the resources from AWS for working with PyTorch&lt;/a&gt;, including docs on how to use the &lt;a href=&quot;https://sagemaker.readthedocs.io/en/stable/using_pytorch.html&quot;&gt;Sagemaker Python SDK&lt;/a&gt;. You can also see &lt;a href=&quot;https://youtu.be/5h1Ot2dPi2E&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;https://youtu.be/qc5ZikKw9_w&quot;&gt;talks&lt;/a&gt; we’ve given on using PyTorch on Sagemaker. Finally, if you happen to be using PyTorch via FastAI, then they’ve written a &lt;a href=&quot;https://course.fast.ai/deployment_amzn_sagemaker.html&quot;&gt;really simple guide&lt;/a&gt; to getting up and running on Sagemaker.&lt;/p&gt;

&lt;p&gt;The story is similar across other major clouds. On Google Cloud, you can follow &lt;a href=&quot;https://cloud.google.com/deep-learning-vm/docs/pytorch_start_instance&quot;&gt;these instructions&lt;/a&gt; to get access to a Deep Learning VM with PyTorch pre-installed. On Microsoft Azure, you have a number of ways to get started from &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning-service/&quot;&gt;Azure Machine Learning Service&lt;/a&gt; to &lt;a href=&quot;https://notebooks.azure.com/pytorch/projects/tutorials&quot;&gt;Azure Notebooks&lt;/a&gt; showing how to use PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;your-models&quot;&gt;Your Models&lt;/h2&gt;

&lt;p&gt;Whichever approach you take to bringing your PyTorch models to production, we want to support you and enable your success. Do you love one of the options above? Are you having difficulty with that one crucial feature you can’t find support for? We’d love to discuss more on the &lt;a href=&quot;https://discuss.pytorch.org/c/deployment&quot;&gt;deployment category&lt;/a&gt; on the PyTorch Discuss forums. We’d love to help, and where you’re seeing success, amplify your story.&lt;/p&gt;</content><author><name>Jeff Smith</name></author><summary type="html">PyTorch has seen a lot of adoption in research, but people can get confused about how well PyTorch models can be taken into production. This blog post is meant to clear up any confusion people might have about the road to production in PyTorch. Usually when people talk about taking a model “to production,” they usually mean performing inference, sometimes called model evaluation or prediction or serving. At the level of a function call, in PyTorch, inference looks something like this:</summary></entry><entry><title type="html">Optimizing CUDA Recurrent Neural Networks with TorchScript</title><link href="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/" rel="alternate" type="text/html" title="Optimizing CUDA Recurrent Neural Networks with TorchScript" /><published>2019-05-01T06:00:00-07:00</published><updated>2019-05-01T06:00:00-07:00</updated><id>https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript</id><content type="html" xml:base="https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/">&lt;p&gt;This week, we officially released PyTorch 1.1, a large feature update to PyTorch 1.0. One of the new features we’ve added is better support for fast, custom Recurrent Neural Networks (fastrnns) with TorchScript (the PyTorch JIT) (https://pytorch.org/docs/stable/jit.html).&lt;/p&gt;

&lt;p&gt;RNNs are popular models that have shown good performance on a variety of NLP tasks that come in different shapes and sizes. PyTorch implements a number of the most popular ones, the &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=rnn#torch.nn.RNN&quot;&gt;Elman RNN&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=gru#torch.nn.GRU&quot;&gt;GRU&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM&quot;&gt;LSTM&lt;/a&gt; as well as multi-layered and bidirectional variants.&lt;/p&gt;

&lt;p&gt;However, many users want to implement their own custom RNNs, taking ideas from recent literature. Applying &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt; to LSTMs is one such use case. Because the PyTorch CUDA LSTM implementation uses a fused kernel, it is difficult to insert normalizations or even modify the base LSTM implementation. Many users have turned to writing custom implementations using standard PyTorch operators, but such code suffers from high overhead: most PyTorch operations launch at least one kernel on the GPU and RNNs generally run many operations due to their recurrent nature. However, we can apply TorchScript to fuse operations and optimize our code automatically, launching fewer, more optimized kernels on the GPU.&lt;/p&gt;

&lt;p&gt;Our goal is for users to be able to write fast, custom RNNs in TorchScript without writing specialized CUDA kernels to achieve similar performance. In this post, we’ll provide a tutorial for how to write your own fast RNNs with TorchScript. To better understand the optimizations TorchScript applies, we’ll examine how those work on a standard LSTM implementation but most of the optimizations can be applied to general RNNs.&lt;/p&gt;

&lt;h2 id=&quot;writing-custom-rnns&quot;&gt;Writing custom RNNs&lt;/h2&gt;

&lt;p&gt;To get started, you can use &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/custom_lstms.py&quot;&gt;this file&lt;/a&gt; as a template to write your own custom RNNs.&lt;/p&gt;

&lt;p&gt;We are constantly improving our infrastructure on trying to make the performance better. If you want to gain the speed/optimizations that TorchScript currently provides (like operator fusion, batch matrix multiplications, etc.), here are some guidelines to follow. The next section explains the optimizations in depth.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;If the customized operations are all element-wise, that’s great because you can get the benefits of the PyTorch JIT’s operator fusion automatically!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you have more complex operations (e.g. reduce ops mixed with element-wise ops), consider grouping the reduce operations and element-wise ops separately in order to fuse the element-wise operations into a single fusion group.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want to know about what has been fused in your custom RNN, you can inspect the operation’s optimized graph by using &lt;code class=&quot;highlighter-rouge&quot;&gt;graph_for&lt;/code&gt; . Using &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTMCell&lt;/code&gt; as an example:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;c&quot;&gt;# get inputs and states for LSTMCell&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_lstm_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

 &lt;span class=&quot;c&quot;&gt;# instantiate a ScriptModule&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c&quot;&gt;# print the optimized graph using graph_for&lt;/span&gt;

 &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;This will generate the optimized TorchScript graph (a.k.a PyTorch JIT IR) for the specialized inputs that you provides:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; graph(%x : Float(*, *),
         %hx : Float(*, *),
         %cx : Float(*, *),
         %w_ih : Float(*, *),
         %w_hh : Float(*, *),
         %b_ih : Float(*),
         %b_hh : Float(*)):
     %hy : Float(*, *), %cy : Float(*, *) = prim::DifferentiableGraph_0(%cx, %b_hh, %b_ih, %hx, %w_hh, %x, %w_ih)
     %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
     return (%30)
     with prim::DifferentiableGraph_0 = graph(%13 : Float(*, *),
         %29 : Float(*),
         %33 : Float(*),
         %40 : Float(*, *),
         %43 : Float(*, *),
         %45 : Float(*, *),
         %48 : Float(*, *)):
     %49 : Float(*, *) = aten::t(%48)
     %47 : Float(*, *) = aten::mm(%45, %49)
     %44 : Float(*, *) = aten::t(%43)
     %42 : Float(*, *) = aten::mm(%40, %44)
     ...some broadcast sizes operations...
     %hy : Float(*, *), %287 : Float(*, *), %cy : Float(*, *), %outgate.1 : Float(*, *), %cellgate.1 : Float(*, *), %forgetgate.1 : Float(*, *), %ingate.1 : Float(*, *) = prim::FusionGroup_0(%13, %346, %345, %344, %343)
     ...some broadcast sizes operations...
     return (%hy, %cy, %49, %44, %196, %199, %340, %192, %325, %185, %ingate.1, %forgetgate.1, %cellgate.1, %outgate.1, %395, %396, %287)
     with prim::FusionGroup_0 = graph(%13 : Float(*, *),
         %71 : Tensor,
         %76 : Tensor,
         %81 : Tensor,
         %86 : Tensor):
     ...some chunks, constants, and add operations...
     %ingate.1 : Float(*, *) = aten::sigmoid(%38)
     %forgetgate.1 : Float(*, *) = aten::sigmoid(%34)
     %cellgate.1 : Float(*, *) = aten::tanh(%30)
     %outgate.1 : Float(*, *) = aten::sigmoid(%26)
     %14 : Float(*, *) = aten::mul(%forgetgate.1, %13)
     %11 : Float(*, *) = aten::mul(%ingate.1, %cellgate.1)
     %cy : Float(*, *) = aten::add(%14, %11, %69)
     %4 : Float(*, *) = aten::tanh(%cy)
     %hy : Float(*, *) = aten::mul(%outgate.1, %4)
     return (%hy, %4, %cy, %outgate.1, %cellgate.1, %forgetgate.1, %ingate.1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above graph we can see that it has a &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::FusionGroup_0&lt;/code&gt; subgraph that is fusing all element-wise operations in LSTMCell (transpose and matrix multiplication are not element-wise ops). Some graph nodes might be hard to understand in the first place but we will explain some of them in the optimization section, we also omitted some long verbose operators in this post that is there just for correctness.&lt;/p&gt;

&lt;h2 id=&quot;variable-length-sequences-best-practices&quot;&gt;Variable-length sequences best practices&lt;/h2&gt;

&lt;p&gt;TorchScript does not support PackedSequence. Generally, when one is handling variable-length sequences, it is best to pad them into a single tensor and send that tensor through a TorchScript LSTM. Here’s an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# List[Tensor], each Tensor is T' x C&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# T x N x C, where N is batch size and T is the max of all T'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hiddens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# T x N x C&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; may have some garbage data in the padded regions; use &lt;code class=&quot;highlighter-rouge&quot;&gt;lengths&lt;/code&gt; to keep track of which part you don’t need.&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;We will now explain the optimizations performed by the PyTorch JIT to speed up custom RNNs. We will use a simple custom LSTM model in TorchScript to illustrate the optimizations, but many of these are general and apply to other RNNs.&lt;/p&gt;

&lt;p&gt;To illustrate the optimizations we did and how we get benefits from those optimizations, we will run a simple custom LSTM model written in TorchScript (you can refer the code in the custom_lstm.py or the below code snippets) and time our changes.&lt;/p&gt;

&lt;p&gt;We set up the environment in a machine equipped with 2 Intel Xeon chip and one Nvidia P100, with cuDNN v7.3, CUDA 9.2 installed. The basic set up for the LSTM model is as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input_size = 512
hidden_size = 512
mini_batch = 64
numLayers = 1
seq_length = 100 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The most important thing PyTorch JIT did is to compile the python program to a PyTorch JIT IR, which is an intermediate representation used to model the program’s graph structure. This IR can then benefit from whole program optimization, hardware acceleration and overall has the potential to provide large computation gains. In this example, we run the initial TorchScript model with only compiler optimization passes that are provided by the JIT, including common subexpression elimination, constant pooling, constant propagation, dead code elimination and some peephole optimizations. We run the model training for 100 times after warm up and average the training time. The initial results for model forward time is around 27ms and backward time is around 64ms, which is a bit far away from what PyTorch cuDNN LSTM provided. Next we will explain the major optimizations we did on how we improve the performance on training or inferencing, starting with LSTMCell and LSTMLayer, and some misc optimizations.&lt;/p&gt;

&lt;h3 id=&quot;lstm-cell-forward&quot;&gt;LSTM Cell (forward)&lt;/h3&gt;

&lt;p&gt;Almost all the computations in an LSTM happen in the LSTMCell, so it’s important for us to take a look at the computations it contains and how can we improve their speed. Below is a sample LSTMCell implementation in TorchScript:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScriptModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTMCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@jit.script_method&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# type: (Tensor, Tuple[Tensor, Tensor]) -&amp;gt; Tuple[Tensor, Tuple[Tensor, Tensor]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;gates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_ih&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gates&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forgetgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ingate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cellgate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outgate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This graph representation (IR) that TorchScript generated enables several optimizations and scalable computations. In addition to the typical compiler optimizations that we could do (CSE, constant propagation, etc. ) we can also run other IR transformations to make our code run faster.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Element-wise operator fusion. PyTorch JIT will automatically fuse element-wise ops, so when you have adjacent operators that are all element-wise, JIT will automatically group all those operations together into a single FusionGroup, this FusionGroup can then be launched with a single GPU/CPU kernel and performed in one pass. This avoids expensive memory reads and writes for each operation.&lt;/li&gt;
  &lt;li&gt;Reordering chunks and pointwise ops to enable more fusion. An LSTM cell adds gates together (a pointwise operation), and then chunks the gates into four pieces: the ifco gates. Then, it performs pointwise operations on the ifco gates like above. This leads to two fusion groups in practice: one fusion group for the element-wise ops pre-chunk, and one group for the element-wise ops post-chunk.
  The interesting thing to note here is that pointwise operations commute with &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt;: Instead of performing pointwise ops on some input tensors and chunking the output, we can chunk the input tensors and then perform the same pointwise ops on the output tensors. By moving the chunk to before the first fusion group, we can merge the first and second fusion groups into one big group.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/custom-rnn-chunk.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Tensor creation on the CPU is expensive, but there is ongoing work to make it faster. At this point, a LSTMCell runs three CUDA kernels: two &lt;code class=&quot;highlighter-rouge&quot;&gt;gemm&lt;/code&gt; kernels and one for the single pointwise group. One of the things we noticed was that there was a large gap between the finish of the second &lt;code class=&quot;highlighter-rouge&quot;&gt;gemm&lt;/code&gt; and the start of the single pointwise group. This gap was a period of time when the GPU was idling around and not doing anything. Looking into it more, we discovered that the problem was that &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt; constructs new tensors and that tensor construction was not as fast as it could be. Instead of constructing new Tensor objects, we taught the fusion compiler how to manipulate a data pointer and strides to do the &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.chunk&lt;/code&gt; before sending it into the fused kernel, shrinking the amount of idle time between the second gemm and the launch of the element-wise fusion group. This give us around 1.2x increase speed up on the LSTM forward pass.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By doing the above tricks, we are able to fuse the almost all &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTMCell&lt;/code&gt; forward graph (except the two gemm kernels) into a single fusion group, which corresponds to the &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::FusionGroup_0&lt;/code&gt; in the above IR graph. It will then be launched into a single fused kernel for execution. With these optimizations the model performance improves significantly with average forward time reduced by around 17ms (1.7x speedup) to 10ms, and average backward time reduce by 37ms to 27ms (1.37x speed up).&lt;/p&gt;

&lt;h3 id=&quot;lstm-layer-forward&quot;&gt;LSTM Layer (forward)&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LSTMLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScriptModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTMLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@jit.script_method&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# type: (Tensor, Tuple[Tensor, Tensor]) -&amp;gt; Tuple[Tensor, Tuple[Tensor, Tensor]]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We did several tricks on the IR we generated for TorchScript LSTM to boost the performance, some example optimizations we did:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loop Unrolling: We automatically unroll loops in the code (for big loops, we unroll a small subset of it), which then empowers us to do further optimizations on the for loops control flow. For example, the fuser can fuse together operations across iterations of the loop body, which results in a good performance improvement for control flow intensive models like LSTMs.&lt;/li&gt;
  &lt;li&gt;Batch Matrix Multiplication: For RNNs where the input is pre-multiplied (i.e. the model has a lot of matrix multiplies with the same LHS or RHS), we can efficiently batch those operations together into a single matrix multiply while chunking the outputs to achieve equivalent semantics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By applying these techniques, we reduced our time in the forward pass by an additional 1.6ms to 8.4ms (1.2x speed up) and timing in backward by 7ms to around 20ms (1.35x speed up).&lt;/p&gt;

&lt;h3 id=&quot;lstm-layer-backward&quot;&gt;LSTM Layer (backward)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Tree” Batch Matrix Muplication: It is often the case that a single weight is reused multiple times in the LSTM backward graph, forming a tree where the leaves are matrix multiplies and nodes are adds.  These nodes can be combined together by concatenating the LHSs and RHSs in different dimensions, then computed as a single matrix multiplication. The formula of equivalence can be denoted as follows:&lt;/p&gt;

    &lt;p&gt;$L1 * R1 + L2 * R2 = torch.cat((L1, L2), dim=1) * torch.cat((R1, R2), dim=0)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autograd is a critical component of what makes PyTorch such an elegant ML framework. As such, we carried this through to PyTorch JIT,  but using a new &lt;strong&gt;Automatic Differentiation&lt;/strong&gt; (AD) mechanism that works on the IR level.  JIT automatic differentiation will slice the forward graph into symbolically differentiable subgraphs, and generate backwards nodes for those subgraphs.  Taking the above IR as an example, we group the graph nodes into a single &lt;code class=&quot;highlighter-rouge&quot;&gt;prim::DifferentiableGraph_0&lt;/code&gt; for the operations that has AD formulas. For operations that have not been added to AD formulas, we will fall back to Autograd during execution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimizing the backwards path is hard, and the implicit broadcasting semantics make the optimization of automatic differentiation harder. PyTorch makes it convenient to write tensor operations without worrying about the shapes by broadcasting the tensors for you. For performance, the painful point in backward is that we need to have a summation for such kind of broadcastable operations. This results in the derivative of every broadcastable op being followed by a summation. Since we cannot currently fuse reduce operations, this causes FusionGroups to break into multiple small groups leading to bad performance. To deal with this, refer to this great &lt;a href=&quot;http://lernapparat.de/fast-lstm-pytorch/&quot;&gt;post&lt;/a&gt; written by Thomas Viehmann.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;misc-optimizations&quot;&gt;Misc Optimizations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In addition to the steps laid about above, we also eliminated overhead between CUDA kernel launches and unnecessary tensor allocations. One example is when you do a tensor device look up. This can provide some poor performance initially with a lot of unnecessary allocations. When we remove these this results in a reduction from milliseconds to nanoseconds between kernel launches.&lt;/li&gt;
  &lt;li&gt;Lastly, there might be normalization applied in the custom LSTMCell like LayerNorm. Since LayerNorm and other normalization ops contains reduce operations, it is hard to fuse it in its entirety. Instead, we automatically decompose Layernorm to a statistics computation (reduce operations) + element-wise transformations, and then fuse those element-wise parts together. As of this post, there are some limitations on our auto differentiation and graph fuser infrastructure which limits the current support to inference mode only. We plan to add backward support in a future release.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the above optimizations on operation fusion, loop unrolling, batch matrix multiplication and some misc optimizations, we can see a clear performance increase on our custom TorchScript LSTM forward and backward from the following figure:&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/custom-rnn-improve.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There are a number of additional optimizations that we did not cover in this post. In addition to the ones laid out in this post, we now see that our custom LSTM forward pass is on par with cuDNN. We are also working on optimizing backward more and expect to see improvements in future releases. Besides the speed that TorchScript provides, we introduced a much more flexible API that enable you to hand draft a lot more custom RNNs, which cuDNN could not provide.&lt;/p&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">This week, we officially released PyTorch 1.1, a large feature update to PyTorch 1.0. One of the new features we’ve added is better support for fast, custom Recurrent Neural Networks (fastrnns) with TorchScript (the PyTorch JIT) (https://pytorch.org/docs/stable/jit.html).</summary></entry><entry><title type="html">PyTorch adds new dev tools as it hits production scale</title><link href="https://pytorch.org/blog/pytorch-adds-new-dev-tools/" rel="alternate" type="text/html" title="PyTorch adds new dev tools as it hits production scale" /><published>2019-05-01T00:00:00-07:00</published><updated>2019-05-01T00:00:00-07:00</updated><id>https://pytorch.org/blog/pytorch-adds-new-dev-tools</id><content type="html" xml:base="https://pytorch.org/blog/pytorch-adds-new-dev-tools/">&lt;p&gt;&lt;em&gt;This is a partial re-post of the original blog post on the Facebook AI Blog. The full post can be &lt;a href=&quot;https://ai.facebook.com/blog/pytorch-adds-new-dev-tools-as-it-hits-production-scale/&quot;&gt;viewed here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since its release just a few months ago, &lt;a href=&quot;http://pytorch.org/&quot;&gt;PyTorch 1.0&lt;/a&gt; has been rapidly adopted as a powerful, flexible deep learning platform that enables engineers and researchers to move quickly from research to production. We are highlighting some of the ways the AI engineering and research community is using PyTorch 1.0. We’re also sharing new details about the latest release, PyTorch 1.1, and showcasing some of the new development tools created by the community.&lt;/p&gt;

&lt;p&gt;Building on the initial launch of PyTorch in 2017, we partnered with the AI community to ship the stable release of PyTorch 1.0 &lt;a href=&quot;https://code.fb.com/ai-research/pytorch-developer-ecosystem-expands-1-0-stable-release/&quot;&gt;last December&lt;/a&gt;. Along with enhanced production-oriented capabilities and deep integration with leading cloud platforms, PyTorch 1.0 expands on the open source library’s core features, with the addition of PyTorch JIT (Just in time compilation) that seamlessly transitions between eager mode and graph mode to provide both flexibility and speed.&lt;/p&gt;

&lt;p&gt;Leading businesses across industries are beginning to use PyTorch to both facilitate their research and then also deploy at large scale for applications such as translation, computer vision, conversational interfaces, pharmaceutical research, factory optimization, and automated driving research. Community adoption of PyTorch has also continued to expand. Stanford, UC Berkeley, Caltech, and other universities are using PyTorch as a fundamental tool for their machine learning (ML) courses; new ecosystem projects have launched to support development on PyTorch; and major cloud platforms have expanded their integration with PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;using-pytorch-across-industries&quot;&gt;Using PyTorch across industries&lt;/h2&gt;

&lt;p&gt;Many leading businesses are moving to PyTorch 1.0 to accelerate development and deployment of new AI systems. Here are some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Airbnb leveraged PyTorch’s rich libraries and APIs for conversational AI and deployed a Smart Reply to help the company’s service agents respond more effectively to customers.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://atomscience.org/&quot;&gt;ATOM&lt;/a&gt; is building a platform to generate and optimize new drug candidates significantly faster and with greater success than conventional processes. Using machine learning frameworks such as PyTorch, ATOM was able to design a variational autoencoder for representing diverse chemical structures and designing new drug candidates.&lt;/li&gt;
  &lt;li&gt;Genentech is utilizing PyTorch’s flexible control structures and dynamic graphs to train deep learning models that will aid in the development of individualized cancer therapy.&lt;/li&gt;
  &lt;li&gt;Microsoft is using PyTorch across its organization to develop ML models at scale and deploy them via the ONNX Runtime. Using PyTorch, Microsoft Cognition has built distributed language models that scale to billions of words and are now in production in offerings such as Cognitive Services.&lt;/li&gt;
  &lt;li&gt;Toyota Research Institute (TRI) is developing a two-pronged approach toward automated driving with Toyota Guardian and Toyota Chauffeur technologies. The Machine Learning Team at TRI is creating new deep learning algorithms to leverage Toyota’s 10 million sales per year data advantage. The flexibility of PyTorch has vastly accelerated their pace of exploration and its new production features will enable faster deployment towards their safety critical applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the release of PyTorch 1.0 in December 2018, we’re now announcing the availability of v1.1, which improves performance, adds new model understanding and visualization tools to improve usability, and provides new APIs.&lt;/p&gt;

&lt;p&gt;Key features of PyTorch v1.1 include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tensorboard&quot;&gt;TensorBoard&lt;/a&gt;: First-class and native support for visualization and model debugging with TensorBoard, a web application suite for inspecting and understanding training runs and graphs. PyTorch now natively supports TensorBoard with a simple “from torch.utils.tensorboard import SummaryWriter” command.&lt;/li&gt;
  &lt;li&gt;JIT compiler: Improvements to just-in-time (JIT) compilation. These include various bug fixes as well as expanded capabilities in TorchScript, such as support for dictionaries, user classes, and attributes.&lt;/li&gt;
  &lt;li&gt;New APIs: Support for Boolean tensors and better support for custom recurrent neural networks.&lt;/li&gt;
  &lt;li&gt;Distributed Training: Improved performance for common models such as CNNs, added support for multi device modules including the ability to split models across GPUs while still using Distributed Data Parallel (DDP) and support for modules where not all parameters are used in every iteration (e.g. control flow, like adaptive softmax, etc). See the latest tutorials &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ve also continued to partner with the community to foster projects and tools aimed at supporting ML engineers for needs ranging from improved model understanding to auto-tuning using AutoML methods. With the release of Ax and BoTorch (below), we will be sharing some of our core algorithms, including meta-learning for efficiently optimizing hyperparameters from based on historical tasks. We are excited to see this work open-sourced for the community to build on.&lt;/p&gt;

&lt;p&gt;This ecosystem includes open source projects and tools that have been deployed at production scale, as well as products and services from our partnership with industry leaders who share our vision of an open and collaborative AI community. Here are a few of the latest tools:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/&quot;&gt;BoTorch&lt;/a&gt;: BoTorch is a research framework built on top of PyTorch to provide Bayesian optimization, a sample-efficient technique for sequential optimization of costly-to-evaluate black-box functions.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/&quot;&gt;Ax&lt;/a&gt;: Ax is an ML platform for managing adaptive experiments. It enables researchers and engineers to systematically explore large configuration spaces in order to optimize machine learning models, infrastructure, and products.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/open-sourcing-pytorch-biggraph-for-faster-embeddings-of-extremely-large-graphs/&quot;&gt;PyTorch-BigGraph&lt;/a&gt;: PBG is a distributed system for creating embeddings of very large graphs with billions of entities and trillions of edges. It includes support for sharding and negative sampling and it offers sample use cases based on Wikidata embeddings.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/ai-platform-notebooks/&quot;&gt;Google AI Platform Notebooks&lt;/a&gt;: AI Platform Notebooks is a new, hosted JupyterLab service from Google Cloud Platform. Data scientists can quickly create virtual machines running JupyterLab with the latest version of PyTorch preinstalled. It is also tightly integrated with GCP services such as BigQuery, Cloud Dataproc, Cloud Dataflow, and AI Factory, making it easy to execute the full ML cycle without ever leaving JupyterLab.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re also excited to see many interesting new projects from the broader PyTorch community. Highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ajbrock/BigGAN-PyTorch&quot;&gt;BigGAN-PyTorch&lt;/a&gt;:This is a full PyTorch reimplementation that uses gradient accumulation to provide the benefits of big batches on as few as four GPUs.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kernel-operations.io/geomloss/index.html&quot;&gt;GeomLoss&lt;/a&gt;: A Python API that defines PyTorch layers for geometric loss functions between sampled measures, images, and volumes. It includes MMD, Wasserstein, Sinkhorn, and more.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/geomloss.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/rusty1s/pytorch_geometric&quot;&gt;PyTorch Geometric&lt;/a&gt;: A deep learning extension library for PyTorch that offers several methods for deep learning on graphs and other irregular structures (also known as &lt;a href=&quot;http://geometricdeeplearning.com&quot;&gt;geometric deep learning&lt;/a&gt;) from a variety of published papers.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fidler-lab/curve-gcn&quot;&gt;Curve-GCN&lt;/a&gt;: A real-time, interactive image annotation approach that uses an end-to-end-trained graph convolutional network (GCN). It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. Curve-GCN runs 10x faster than traditional methods, such as Polygon-RNN++.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;udacity-fastai-and-others-develop-new-pytorch-resources&quot;&gt;Udacity, fast.ai, and others develop new PyTorch resources&lt;/h2&gt;

&lt;p&gt;PyTorch is ideal for teaching ML development because it enables rapid experimentation through its flexible, dynamic programming environment and user-friendly Pythonic interface. In addition, Google Colab now offers an interactive Jupyter Notebook environment that natively supports PyTorch, allowing developers to run any PyTorch tutorial immediately with free CPU and GPU resources.&lt;/p&gt;

&lt;p&gt;University-level classes — including &lt;a href=&quot;http://web.stanford.edu/class/cs224n&quot;&gt;Stanford NLP&lt;/a&gt;, &lt;a href=&quot;https://inst.eecs.berkeley.edu/~cs280/sp18/&quot;&gt;UC Berkeley&lt;/a&gt; Computer Vision, and &lt;a href=&quot;http://cast.caltech.edu&quot;&gt;Caltech&lt;/a&gt; Robotics courses — are now being taught on PyTorch. In addition, massive open online courses (MOOCs) are training thousands of new PyTorch developers.&lt;/p&gt;

&lt;p&gt;Today, we’re announcing a &lt;a href=&quot;https://blog.udacity.com/2019/05/announcing-the-secure-and-private-ai-scholarship-challenge-with-facebook.html&quot;&gt;new Udacity course&lt;/a&gt;, building upon the Intro to Deep Learning course launched last year. This new course, led by Andrew Trask of Oxford University and OpenMined, covers important concepts around privacy in AI, including methods such as differential privacy and federated learning. Facebook will also be providing scholarships to support students as they continue their ML education in Udacity’s full Nanodegree programs.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.fast.ai&quot;&gt;fast.ai&lt;/a&gt; community is also continuing to invest energy and resources in PyTorch. In June, fast.ai will launch a new course called Deep Learning from the Foundations, which will show developers how to go all the way from writing matrix multiplication from scratch to how to train and implement a state-of-the-art ImageNet model. The course will include deep dives into the underlying implementation of methods in the PyTorch and fast.ai libraries, and will use the code to explain and illustrate the academic papers that underlie these methods.&lt;/p&gt;

&lt;p&gt;As part of the course, fast.ai will also release new software modules, including fastai.audio, which brings the power of fast.ai’s deep abstractions and curated algorithms to the new PyTorch.audio module, and show how fastai.vision can be used to &lt;a href=&quot;https://www.fast.ai/2019/05/03/decrappify&quot;&gt;create stunning high-resolution videos&lt;/a&gt; from material such as old classic movies, and from cutting-edge microscopy sequences through a collaboration with the &lt;a href=&quot;https://www.salk.edu&quot;&gt;Salk Institute&lt;/a&gt;. In addition, fast.ai is contributing its new X-ResNet module, including a suite of models pretrained on ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-pytorch&quot;&gt;Getting started with PyTorch&lt;/h2&gt;

&lt;p&gt;Everyone in the AI community — including those new to ML development as well as researchers and engineers looking for ways to accelerate their end-to-end workflows — can experiment with PyTorch instantly by visiting &lt;a href=&quot;https://pytorch.org&quot;&gt;pytorch.org&lt;/a&gt; and launching a &lt;a href=&quot;https://pytorch.org/tutorials&quot;&gt;tutorial&lt;/a&gt; in Colab. There are also many easy ways to &lt;a href=&quot;https://pytorch.org/get-started/locally&quot;&gt;get started&lt;/a&gt; both locally and on popular cloud platforms.&lt;/p&gt;</content><author><name>The PyTorch Team</name></author><summary type="html">This is a partial re-post of the original blog post on the Facebook AI Blog. The full post can be viewed here</summary></entry><entry><title type="html">Stochastic Weight Averaging in PyTorch</title><link href="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/" rel="alternate" type="text/html" title="Stochastic Weight Averaging in PyTorch" /><published>2019-04-29T00:00:00-07:00</published><updated>2019-04-29T00:00:00-07:00</updated><id>https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch</id><content type="html" xml:base="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/">&lt;p&gt;In this blogpost we describe the recently proposed Stochastic Weight Averaging (SWA) technique [1, 2], and its new implementation in &lt;a href=&quot;https://github.com/pytorch/contrib&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;torchcontrib&lt;/code&gt;&lt;/a&gt;.  SWA is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch. SWA has a wide range of applications and features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SWA has been shown to significantly improve generalization in computer vision tasks, including VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2].&lt;/li&gt;
  &lt;li&gt;SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].&lt;/li&gt;
  &lt;li&gt;SWA is shown to improve the stability of training as well as the final average rewards of policy-gradient methods in deep reinforcement learning [3].&lt;/li&gt;
  &lt;li&gt;An extension of SWA can obtain efficient Bayesian model averaging, as well as high quality uncertainty estimates and calibration in deep learning [4].&lt;/li&gt;
  &lt;li&gt;SWA for low precision training, SWALP, can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including gradient accumulators [5].&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule (see the left panel of Figure 1.). SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels of Figure 1).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 [1]. &lt;strong&gt;Left:&lt;/strong&gt; test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). &lt;strong&gt;Middle&lt;/strong&gt; and &lt;strong&gt;Right:&lt;/strong&gt; test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs. Please see [1] for details on how these figures were constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;With our new implementation in &lt;a href=&quot;https://github.com/pytorch/contrib&quot;&gt;torchcontrib&lt;/a&gt; using SWA is as easy as using any other optimizer in PyTorch:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchcontrib.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# training loop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcontrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swa_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap_swa_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can wrap any optimizer from &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class, and then train your model as usual. When training is complete you simply call &lt;code class=&quot;highlighter-rouge&quot;&gt;swap_swa_sgd()&lt;/code&gt; to set the weights of your model to their SWA averages. Below we explain the SWA procedure and the parameters of the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class in detail. We emphasize that SWA can be combined with &lt;em&gt;any&lt;/em&gt; optimization procedure, such as Adam, in the same way that it can be combined with SGD.&lt;/p&gt;

&lt;h2 id=&quot;is-this-just-averaged-sgd&quot;&gt;Is this just Averaged SGD?&lt;/h2&gt;

&lt;p&gt;At a high level, averaging SGD iterates dates back several decades in convex optimization [6, 7], where it is sometimes referred to as Polyak-Ruppert averaging, or &lt;em&gt;averaged&lt;/em&gt; SGD. &lt;strong&gt;But the details matter&lt;/strong&gt;. &lt;em&gt;Averaged SGD&lt;/em&gt; is often employed in conjunction with a decaying learning rate, and an exponentially moving average, typically for convex optimization. In convex optimization, the focus has been on improved rates of convergence. In deep learning, this form of averaged SGD smooths the trajectory of SGD iterates, but does not perform very differently.&lt;/p&gt;

&lt;p&gt;By contrast, SWA is focused on an &lt;strong&gt;equal average&lt;/strong&gt; of SGD iterates with a modified &lt;strong&gt;cyclical or high constant learning rate&lt;/strong&gt;, and exploits the flatness of training objectives [8] specific to &lt;strong&gt;deep learning&lt;/strong&gt; for &lt;strong&gt;improved generalization&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-weight-averaging&quot;&gt;Stochastic Weight Averaging&lt;/h2&gt;

&lt;p&gt;There are two important ingredients that make SWA work. First, SWA uses a modified learning rate schedule so that SGD continues to explore the set of high-performing networks instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time, and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see the Figure 2 below). The second ingredient is to average the weights of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained in the end of every epoch within the last 25% of training time (see Figure 2).&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/figure2-highres.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Illustration of the learning rate schedule adopted by SWA. Standard decaying schedule is used for the first 75% of the training and then a high constant value is used for the remaining 25%. The SWA averages are formed during the last 25% of training.&lt;/p&gt;

&lt;p&gt;In our implementation the auto mode of the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; optimizer allows us to run the procedure described above. To run SWA in auto mode you just need to wrap your optimizer &lt;code class=&quot;highlighter-rouge&quot;&gt;base_opt&lt;/code&gt; of choice (can be SGD, Adam, or any other &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.Optimizer&lt;/code&gt;) with &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA(base_opt, swa_start, swa_freq, swa_lr)&lt;/code&gt;. After &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt; optimization steps the learning rate will be switched to a constant value &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_lr&lt;/code&gt;, and in the end of every &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_freq&lt;/code&gt; optimization steps a snapshot of the weights will be added to the SWA running average. Once you run &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.swap_swa_sgd()&lt;/code&gt;, the weights of your model are replaced with their SWA running averages.&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;p&gt;One important detail to keep in mind is batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training, and so the batch normalization layers do not have the activation statistics computed after you reset the weights of your model with &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.swap_swa_sgd()&lt;/code&gt;. To compute the activation statistics you can just make a forward pass on your training data using the SWA model once the training is finished. In the &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; class we provide a helper function &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.bn_update(train_loader, model)&lt;/code&gt;. It updates the activation statistics for every batch normalization layer in the model by making a forward pass on the &lt;code class=&quot;highlighter-rouge&quot;&gt;train_loader&lt;/code&gt; data loader. You only need to call this function once in the end of training.&lt;/p&gt;

&lt;h2 id=&quot;advanced-learning-rate-schedules&quot;&gt;Advanced Learning-Rate Schedules&lt;/h2&gt;

&lt;p&gt;SWA can be used with any learning rate schedule that encourages exploration of the flat region of solutions. For example, you can use cyclical learning rates in the last 25% of the training time instead of a constant value, and average the weights of the networks corresponding to the lowest values of the learning rate within each cycle (see Figure 3).&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/figure3-highres.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Illustration of SWA with an alternative learning rate schedule. Cyclical learning rates are adopted in the last 25% of training, and models for averaging are collected in the end of each cycle.&lt;/p&gt;

&lt;p&gt;In our implementation you can implement custom learning rate and weight averaging strategies by using &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; in the manual mode. The following code is equivalent to the auto mode code presented in the beginning of this blogpost.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchcontrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SWA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_swa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap_swa_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In manual mode you don’t specify &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_start&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_lr&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;swa_freq&lt;/code&gt;, and just call &lt;code class=&quot;highlighter-rouge&quot;&gt;opt.update_swa()&lt;/code&gt; whenever you want to update the SWA running averages (for example in the end of each learning rate cycle). In manual mode &lt;code class=&quot;highlighter-rouge&quot;&gt;SWA&lt;/code&gt; doesn’t change the learning rate, so you can use any schedule you want as you would normally do with any other &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.optim.Optimizer&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;SGD converges to a solution within a wide flat region of loss. The weight space is extremely high-dimensional, and most of the volume of the flat region is concentrated near the boundary, so SGD solutions will always be found near the boundary of the flat region of the loss. SWA on the other hand averages multiple SGD solutions, which allows it to move towards the center of the flat region.&lt;/p&gt;

&lt;p&gt;We expect solutions that are centered in the flat region of the loss to generalize better than those near the boundary. Indeed, train and test error surfaces are not perfectly aligned in the weight space. Solutions that are centered in the flat region are not as susceptible to the shifts between train and test error surfaces as those near the boundary. In Figure 4 below we show the train loss and test error surfaces along the direction connecting the SWA and SGD solutions. As you can see, while SWA solution has a higher train loss compared to the SGD solution, it is centered in the region of low loss, and has a substantially better test error.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure4.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Train loss and test error along the line connecting the SWA solution (circle) and SGD solution (square). SWA solution is centered in a wide region of low train loss while the SGD solution lies near the boundary. Because of the shift between train loss and test error surfaces, SWA solution leads to much better generalization.&lt;/p&gt;

&lt;h2 id=&quot;examples-and-results&quot;&gt;Examples and Results&lt;/h2&gt;

&lt;p&gt;We released a GitHub repo &lt;a href=&quot;https://github.com/izmailovpavel/contrib_swa_examples&quot;&gt;here&lt;/a&gt; with examples of using the &lt;code class=&quot;highlighter-rouge&quot;&gt;torchcontrib&lt;/code&gt; implementation of SWA for training DNNs. For example, these examples can be used to achieve the following results on CIFAR-100:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;DNN (Budget)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SGD&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1 Budget&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1.25 Budgets&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SWA 1.5 Budgets&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;VGG16 (200)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;72.55 ± 0.10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;73.91 ± 0.12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74.17 ± 0.15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74.27 ± 0.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PreResNet110 (150)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76.77 ± 0.38&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.75 ± 0.16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.91 ± 0.29&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.10 ± 0.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PreResNet164 (150)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78.49 ± 0.36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.77 ± 0.17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.18 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.35 ± 0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WideResNet28x10 (200)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80.82 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.46 ± 0.23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81.91 ± 0.27&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82.15 ± 0.27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-Supervised Learning&lt;/h2&gt;

&lt;p&gt;In a follow-up &lt;a href=&quot;https://arxiv.org/abs/1806.05594&quot;&gt;paper&lt;/a&gt; SWA was applied to semi-supervised learning, where it illustrated  improvements beyond the best reported results in multiple settings. For example, with SWA you can get 95% accuracy on CIFAR-10 if you only have the training labels for 4k training data points (the previous best reported result on this problem was 93.7%). This paper also explores averaging multiple times within epochs, which can accelerate convergence and find still flatter solutions in a given time.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure5.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Performance of fast-SWA on semi-supervised learning with CIFAR-10. fast-SWA achieves record results in every setting considered.&lt;/p&gt;

&lt;h2 id=&quot;calibration-and-uncertainty-estimates&quot;&gt;Calibration and Uncertainty Estimates&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;SWA-Gaussian&lt;/a&gt; (SWAG) is a simple, scalable and convenient approach to uncertainty estimation and calibration in Bayesian deep learning. Similarly to SWA, which maintains a running average of SGD iterates, SWAG estimates the first and second moments of the iterates to construct a Gaussian distribution over weights. SWAG distribution approximates the shape of the true posterior: Figure 6 below shows the SWAG distribution on top of the posterior log-density for PreResNet-164 on CIFAR-100.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure6.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; SWAG distribution on top of posterior log-density for PreResNet-164 on CIFAR-100. The shape of SWAG distribution is aligned with the posterior.&lt;/p&gt;

&lt;p&gt;Empirically, SWAG performs on par or better than popular alternatives including MC dropout, KFAC Laplace, and temperature scaling on uncertainty quantification, out-of-distribution detection, calibration and transfer learning in computer vision tasks. Code for SWAG is available &lt;a href=&quot;https://github.com/wjmaddox/swa_gaussian&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;In another follow-up &lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-24.pdf&quot;&gt;paper&lt;/a&gt; SWA was shown to improve the performance of policy gradient methods A2C and DDPG on several Atari games and MuJoCo environments.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Environment&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A2C&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A2C + SWA&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Breakout&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;522 ± 34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;703 ± 60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Qbert&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18777 ± 778&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21272 ± 655&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SpaceInvaders&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7727 ± 1121&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21676 ± 8897&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Seaquest&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1779 ± 4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1795 ± 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CrazyClimber&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;147030 ± 10239&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;139752 ± 11618&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BeamRider&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9999 ± 402&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11321 ± 1065&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;low-precision-training&quot;&gt;Low Precision Training&lt;/h2&gt;
&lt;p&gt;We can filter through quantization noise by combining weights that have been rounded down with weights that have been rounded up. Moreover, by averaging weights to find a flat region of the loss surface, large perturbations of the weights will not affect the quality of the solution (Figures 7 and 8). Recent work shows that by adapting SWA to the low precision setting, in a method called SWALP, one can &lt;em&gt;match the performance of full-precision SGD even with all training in 8 bits&lt;/em&gt; [5]. This is quite a practically important result, given that (1) SGD training in 8 bits performs notably worse than full precision SGD, and (2) low precision training is significantly harder than predictions in low precision after training (the usual setting). For example, a ResNet-164 trained on CIFAR-100 with float (16-bit) SGD achieves 22.2% error, while 8-bit SGD achieves 24.0% error. By contrast, SWALP with 8 bit training achieves 21.8% error.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure7.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Quantizing in a flat region can still provide solutions with low loss.&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
&lt;img src=&quot;https://pytorch.org/assets/images/swa/Figure8.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Low precision SGD training (with a modified learning rate schedule) and SWALP.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;One of the greatest open questions in deep learning is why SGD manages to find good solutions, given that the training objectives are highly multimodal, and there are in principle many settings of parameters that achieve no training loss but poor generalization. By understanding geometric features such as flatness, which relate to generalization, we can begin to resolve these questions and build optimizers that provide even better generalization, and many other useful features, such as uncertainty representation. We have presented SWA, a simple drop-in replacement for standard SGD, which can in principle benefit anyone training a deep neural network. SWA has been demonstrated to have strong performance in a number of areas, including computer vision, semi-supervised learning, reinforcement learning, uncertainty representation, calibration, Bayesian model averaging, and low precision training.&lt;/p&gt;

&lt;p&gt;We encourage you try out SWA! Using SWA is now as easy as using any other optimizer in PyTorch. And even if you have already trained your model with SGD (or any other optimizer), it’s very easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[1] Averaging Weights Leads to Wider Optima and Better Generalization; Pavel Izmailov, Dmitry Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson; Uncertainty in Artificial Intelligence (UAI), 2018&lt;/li&gt;
  &lt;li&gt;[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average; Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson; International Conference on Learning Representations (ICLR), 2019&lt;/li&gt;
  &lt;li&gt;[3] Improving Stability in Deep Reinforcement Learning with Weight Averaging; Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, Andrew Gordon Wilson, UAI 2018 Workshop: Uncertainty in Deep Learning, 2018&lt;/li&gt;
  &lt;li&gt;[4]  A Simple Baseline for Bayesian Uncertainty in Deep Learning, Wesley Maddox, Timur Garipov, Pavel Izmailov, Andrew Gordon Wilson, arXiv pre-print, 2019: &lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;https://arxiv.org/abs/1902.02476&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[5] SWALP : Stochastic Weight Averaging in Low Precision Training, Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, Christopher De Sa, To appear at the International Conference on Machine Learning  (ICML), 2019.&lt;/li&gt;
  &lt;li&gt;[6] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.&lt;/li&gt;
  &lt;li&gt;[7] Acceleration of stochastic approximation by averaging. Boris T Polyak and Anatoli B Juditsky. SIAM Journal on Control and Optimization, 30(4):838–855, 1992.&lt;/li&gt;
  &lt;li&gt;[8] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson. Neural Information Processing Systems (NeurIPS), 2018&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pavel Izmailov and Andrew Gordon Wilson</name></author><summary type="html">In this blogpost we describe the recently proposed Stochastic Weight Averaging (SWA) technique [1, 2], and its new implementation in torchcontrib. SWA is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch. SWA has a wide range of applications and features:</summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-04-27T08:20:20-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Accelerate PyTorch 2.7 on Intel® GPUs</title>
      <link href="https://pytorch.org/blog/pytorch-2-7-intel-gpus/" rel="alternate" type="text/html" title="Accelerate PyTorch 2.7 on Intel® GPUs" />
      <published>2025-04-25T00:00:00-07:00</published>
      <updated>2025-04-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7-intel-gpus</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7-intel-gpus/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/pytorch-2-7/&quot;&gt;PyTorch 2.7&lt;/a&gt; continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.&lt;/p&gt;

&lt;h2 id=&quot;incremental-improvements-of-intel-gpu-support-in-pytorch&quot;&gt;Incremental improvements of Intel GPU support in PyTorch&lt;/h2&gt;

&lt;p&gt;Since PyTorch 2.4, we’ve made steady improvements to Intel GPU support with each release. With PyTorch 2.7, we are excited to share that we have established a solid foundation to have Intel GPU work in both graph mode (torch.compile) and eager mode on Windows and Linux. This includes a wide range of Intel GPU products, many of which you may already access. We hope these enhancements will unlock more ubiquitous hardware for your AI research and development.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over time, we have expanded Intel GPU Support across Windows and Linux, including these products:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html&quot;&gt;Intel® Arc™ A-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/support/articles/000097599/processors.html&quot;&gt;Intel® Core™ Ultra Processors with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-series-2-mobile-product-brief.html&quot;&gt;Intel® Core™ Ultra Mobile Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-desktop-processors-series-2-brief.html&quot;&gt;Intel® Core™ Ultra Desktop Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html&quot;&gt;Intel® Data Center GPU Max Series&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Simpler installation&lt;/a&gt; of torch-xpu PIP wheels and an effortless setup experience.&lt;/li&gt;
  &lt;li&gt;High ATen operation coverage with SYCL and oneDNN for smooth eager mode support with functionality and performance.&lt;/li&gt;
  &lt;li&gt;Notable speedups with torch.compile through default TorchInductor and Triton backend, proved by measurable performance gains with Hugging Face, TIMM, and TorchBench benchmarks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out the detailed advancements in these related release blogs:&lt;a href=&quot;https://pytorch.org/blog/intel-gpus-pytorch-2-4/&quot;&gt; PyTorch 2.4&lt;/a&gt;,&lt;a href=&quot;https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/&quot;&gt; PyTorch 2.5&lt;/a&gt;, and&lt;a href=&quot;https://pytorch.org/blog/unlocking-pt-2-6-intel/&quot;&gt; PyTorch 2.6&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-new-in-pytorch-27&quot;&gt;What’s New in PyTorch 2.7&lt;/h2&gt;

&lt;p&gt;These are the features in PyTorch 2.7  that were added to help accelerate performance on Intel GPUs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve scaled dot-product attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;br /&gt;
With the new SDPA optimization for Intel GPUs on PyTorch 2.7, Stable Diffusion float16 inference achieved up to 3x gain over PyTorch 2.6 release on Intel® Arc™ B580 Graphics and Intel® Core™ Ultra 7 Processor 258V with Intel® Arc™ Graphics 140V on eager mode. See Figure 1 below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg1.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. PyTorch 2.7 Stable Diffusion Performance Gains Over PyTorch 2.6&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux. With this, Intel GPUs became the first accelerator to support torch.compile on Windows. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; Windows tutorial&lt;/a&gt; for details.&lt;br /&gt;
Graph model (torch.compile) is enabled in Windows 11 for the first time across Intel GPUs, delivering the performance advantages over eager mode as on Linux by PyTorch 2.7. The latest performance data was measured on top of PyTorch Dynamo Benchmarking Suite using Intel® Arc™ B580 Graphics on Windows showcase torch.compile speedup ratio over eager mode as shown in Figure 2. Both training and inference achieved similar significant improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg2.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2. Torch.compile Performance Gains Over Eager Mode on Windows&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide full graph mode quantization pipelines with enhanced computational efficiency. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; PT2E tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable AOTInductor and torch.export on Linux to simplify deployment workflows. Refer to&lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt; AOTInductor tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis. Refer to the&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#pytorch-profiler&quot;&gt; PyTorch profiler tutorial&lt;/a&gt; for details.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Review the &lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Getting Started on Intel GPU Guide&lt;/a&gt; for a tour of the environment setup and a quick start on Intel GPUs.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Looking ahead, we will continue the Intel GPU upstream efforts in future PyTorch releases to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attain state-of-the-art PyTorch-native performance to showcase competitive GEMM computational efficiency for torch.compile, and enhance performance for LLM models through FlexAttention and lower precision data types.&lt;/li&gt;
  &lt;li&gt;Broaden feature compatibility by delivering distributed XCCL backend support for Intel® Data Center GPU Max Series.&lt;/li&gt;
  &lt;li&gt;Expand accelerator support across core PyTorch ecosystem components including torchao, torchtune, and torchtitan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow along in the &lt;a href=&quot;https://dev-discuss.pytorch.org/t/intel-gpu-cpu-enabling-status-and-feature-plan-2025-h1-update/2913&quot;&gt;PyTorch Dev Discussion&lt;/a&gt; to learn more about Intel GPU &amp;amp; CPU enabling status and features. As we get further along, we will create tickets on GitHub to document our progress.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blog, we reviewed the Intel GPU upstream progress starting in PyTorch 2.4 and highlighted the new features of PyTorch 2.7 that accelerate AI workload performance across various Intel GPUs. These new features, especially SDPA on Windows, achieved up to 3x inference (Stable Diffusion, float16) gain over PyTorch 2.6 release on Intel Arc B580 Graphics and Intel Core Ultra 7 Processor 258V with Intel Arc Graphics 140V. Also, torch.compile on Windows delivers similar performance advantages over eager mode on Dynamo benchmarks as on Linux.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We want to thank the following PyTorch maintainers for their technical discussions and insights: &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;h2 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h2&gt;

&lt;p&gt;Measurement on Intel Core Ultra 7 258V: 2200 MHz, 8 Core(s), 8 Logical Processor(s) with Intel Arc 140V GPU (16GB), GPU memory 18.0 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Pro - 24H2. And Intel Core Ultra 5 245KF: 4200 MHz, 14 Core(s), 14 Logical Processor(s), Intel Arc B580 Graphics, dedicated GPU memory 12.0 GB, shared GPU memory 15.8 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Enterprise LTSC - 24H2. Test by Intel on Apr 8th, 2025.&lt;/p&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;ai-disclaimer&quot;&gt;AI Disclaimer&lt;/h2&gt;

&lt;p&gt;AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the Intel PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.7 continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.7 Release</title>
      <link href="https://pytorch.org/blog/pytorch-2-7/" rel="alternate" type="text/html" title="PyTorch 2.7 Release" />
      <published>2025-04-23T00:00:00-07:00</published>
      <updated>2025-04-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.7 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.7.0&quot;&gt;release notes&lt;/a&gt;)! This release features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;support for the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/&quot;&gt;NVIDIA Blackwell GPU architecture&lt;/a&gt; and pre-built wheels for &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA 12.8&lt;/a&gt; across Linux x86 and arm64 architectures.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; support for Torch Function Modes which enables users to override any *torch.** operation  to implement custom user-defined behavior.&lt;/li&gt;
  &lt;li&gt;Mega Cache which allows users to have end-to-end portable caching for torch;&lt;/li&gt;
  &lt;li&gt;new features for FlexAttention - LLM first token processing, LLM throughput mode optimization and Flex Attention for Inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release is composed of 3262 commits from 457 contributors since PyTorch 2.6. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.7. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Torch.Compile support for Torch Function Modes
   &lt;/td&gt;
   &lt;td&gt;NVIDIA Blackwell Architecture Support
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Mega Cache
   &lt;/td&gt;
   &lt;td&gt;PyTorch Native Context Parallel
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Enhancing Intel GPU Acceleration
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;first token processing&lt;/span&gt; on x86 CPUs 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;throughput mode optimization&lt;/span&gt; on x86 CPUs
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Foreach Map
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Flex Attention for Inference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Prologue Fusion Support in Inductor
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile-support-for-torch-function-modes&quot;&gt;[Beta] Torch.Compile support for Torch Function Modes&lt;/h3&gt;

&lt;p&gt;This feature enables users to override any *torch.** operation to implement custom user-defined behavior. For example, ops can be rewritten to accommodate a specific backend. This is used in FlexAttention to re-write indexing ops.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_torch_function_modes.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-mega-cache&quot;&gt;[Beta] Mega Cache&lt;/h3&gt;

&lt;p&gt;Mega Cache allows users to have end-to-end portable caching for torch. The intended use case is after compiling and executing a model, the user calls &lt;em&gt;torch.compiler.save_cache_artifacts()&lt;/em&gt; which will return the compiler artifacts in a portable form. Later, potentially on a different machine, the user may call &lt;em&gt;torch.compiler.load_cache_artifacts()&lt;/em&gt; with these artifacts to pre-populate the torch.compile caches in order to jump-start their cache.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html#torch-compile-end-to-end-caching-mega-cache&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-nvidia-blackwell-architecture-support&quot;&gt;[Prototype] NVIDIA Blackwell Architecture Support&lt;/h3&gt;

&lt;p&gt;PyTorch 2.7 introduces support for NVIDIA’s new Blackwell GPU architecture and ships pre-built wheels for CUDA 12.8. For more details on CUDA 12.8 see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA Toolkit Release&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Core components and libraries including cuDNN, NCCL, and CUTLASS have been upgraded to ensure compatibility with Blackwell platforms.&lt;/li&gt;
  &lt;li&gt;PyTorch 2.7 includes Triton 3.3, which adds support for the Blackwell architecture with torch.compile compatibility.&lt;/li&gt;
  &lt;li&gt;To utilize these new features, install PyTorch with CUDA 12.8 using: &lt;em&gt;pip install torch==2.7.0 –index-url https://download.pytorch.org/whl/cu128&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More context can also be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/145949&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-pytorch-native-context-parallel&quot;&gt;[Prototype] PyTorch Native Context Parallel&lt;/h3&gt;

&lt;p&gt;PyTorch Context Parallel API allows users to create a Python context so that every *torch.nn.functional.scaled_dot_product_attention() *call within will run with context parallelism. Currently,  PyTorch Context Parallel supports 3 attention backends: 1. Flash attention; 2. Efficient attention;  and 3. cuDNN attention.&lt;/p&gt;

&lt;p&gt;As an example, this is &lt;a href=&quot;https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082&quot;&gt;used within TorchTitan as the Context Parallel solution for LLM training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/prototype/context_parallel.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-enhancing-intel-gpu-acceleration&quot;&gt;[Prototype] Enhancing Intel GPU Acceleration&lt;/h3&gt;

&lt;p&gt;This latest release introduces enhanced performance optimizations for Intel GPU architectures. These improvements accelerate workloads across various Intel GPUs through the following key enhancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux.&lt;/li&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide a full graph mode quantization pipelines with enhanced computational efficiency.&lt;/li&gt;
  &lt;li&gt;Improve Scaled Dot-Product Attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;/li&gt;
  &lt;li&gt;Enable AOTInuctor and torch.export on Linux to simplify deployment workflows.&lt;/li&gt;
  &lt;li&gt;Implement more Aten operators to enhance the continuity of operators execution on Intel GPU and increase the performance on Intel GPU in eager mode.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis.&lt;/li&gt;
  &lt;li&gt;Expand the Intel GPUs support to &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra Series 2 with Intel® Arc™ Graphics&lt;/a&gt;, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series graphics&lt;/a&gt; on both Windows and Linux.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See also the tutorials &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_xpu_inductor.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-first-token-processing-on-x86-cpus&quot;&gt;[Prototype] FlexAttention LLM first token processing on x86 CPUs&lt;/h3&gt;

&lt;p&gt;FlexAttention x86 CPU support was first introduced in PyTorch 2.6, offering optimized implementations — such as PageAttention, which is critical for LLM inference—via the TorchInductor C++ backend. In PyTorch 2.7, more attention variants for first token processing of LLMs are supported. With this feature, users can have a smoother experience running FlexAttention on x86 CPUs, replacing specific &lt;em&gt;scaled_dot_product_attention&lt;/em&gt; operators with a unified FlexAttention API, and benefiting from general support and good performance when using torch.compile.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-throughput-mode-optimization&quot;&gt;[Prototype] FlexAttention LLM throughput mode optimization&lt;/h3&gt;

&lt;p&gt;The performance of FlexAttention on x86 CPUs for LLM inference throughput scenarios has been further improved by adopting the new C++ micro-GEMM template ability. This addresses the performance bottlenecks for large batch size scenarios present in PyTorch 2.6. With this enhancement, users can transparently benefit from better performance and a smoother experience when using FlexAttention APIs and torch.compile for LLM throughput serving on x86 CPUs.&lt;/p&gt;

&lt;h3 id=&quot;prototype-foreach-map&quot;&gt;[Prototype] Foreach Map&lt;/h3&gt;

&lt;p&gt;This feature uses torch.compile to allow users to apply any pointwise or user-defined function (e.g. torch.add) to lists of tensors, akin to the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops. The main advantage over the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops is that any mix of scalars or lists of tensors can be supplied as arguments, and even user-defined python functions can be lifted to apply to lists of tensors. Torch.compile will automatically generate a horizontally fused kernel for optimal performance.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/recipes/foreach_map.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flex-attention-for-inference&quot;&gt;[Prototype] Flex Attention for Inference&lt;/h3&gt;

&lt;p&gt;In release 2.5.0, &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention&lt;/a&gt;* torch.nn.attention.flex_attention*  was introduced for ML researchers who’d like to customize their attention kernels without writing kernel code. This update introduces a decoding backend optimized for inference, supporting GQA and PagedAttention, along with feature updates including nested jagged tensor support, performance tuning guides and trainable biases support.&lt;/p&gt;

&lt;h3 id=&quot;prototype-prologue-fusion-support-in-inductor&quot;&gt;[Prototype] Prologue Fusion Support in Inductor&lt;/h3&gt;

&lt;p&gt;Prologue fusion optimizes matrix multiplication (matmul) operations by fusing operations that come before the matmul into the matmul kernel itself, improving performance by reducing global memory bandwidth.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.7 (release notes)! This release features:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/" rel="alternate" type="text/html" title="Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers" />
      <published>2025-04-08T00:00:00-07:00</published>
      <updated>2025-04-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-whisper-arm-w-transformers</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/">&lt;p&gt;Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.&lt;/p&gt;

&lt;p&gt;A new Arm Learning Path is now available that explains how to accelerate Whisper on Arm-based cloud instances using PyTorch and Hugging Face transformers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Run Whisper on Arm?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Arm processors are popular in cloud infrastructure for their efficiency, performance, and cost-effectiveness. With major cloud providers such as AWS, Azure, and Google Cloud offering Arm-based instances, running machine learning workloads on this architecture is becoming increasingly attractive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What You’ll Learn&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/&quot;&gt;Arm Learning Path&lt;/a&gt; provides a structured approach to setting up and accelerating Whisper on Arm-based cloud instances. Here’s what you cover:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Set Up Your Environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before running Whisper, you must set up your development environment. The learning path walks you through setting up an Arm-based cloud instance and installing all dependencies, such as PyTorch, Transformers, and ffmpeg.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Run Whisper with PyTorch and Hugging Face Transformers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the environment is ready, you will use the Hugging Face transformer library with PyTorch to load and execute Whisper for speech-to-text conversion. The tutorial provides a step-by-step approach for processing audio files and generating audio transcripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Measure and Evaluate Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To ensure efficient execution, you learn how to measure transcription speeds and compare different optimization techniques. The guide provides insights into interpreting performance metrics and making informed decisions on your deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try it Yourself&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Upon completion of this tutorial, you know how to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deploy Whisper on an Arm-based cloud instance.&lt;/li&gt;
  &lt;li&gt;Implement performance optimizations for efficient execution.&lt;/li&gt;
  &lt;li&gt;Evaluate transcription speeds and optimize further based on results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Try the live demo today&lt;/strong&gt; and see audio transcription in action on Arm: &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/_demo/&quot;&gt;Whisper on Arm Demo&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pareena Verma, Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Day France 2025: Call For Proposals Open</title>
      <link href="https://pytorch.org/blog/pt-day-france-cfp/" rel="alternate" type="text/html" title="PyTorch Day France 2025: Call For Proposals Open" />
      <published>2025-04-03T00:00:00-07:00</published>
      <updated>2025-04-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-day-france-cfp</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-day-france-cfp/">&lt;p&gt;We’re pleased to announce &lt;strong&gt;&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-day-france/&quot;&gt;PyTorch Day France 2025&lt;/a&gt;&lt;/strong&gt;, a dedicated gathering of the PyTorch community held &lt;strong&gt;7 May 2025&lt;/strong&gt; in &lt;strong&gt;Paris, France&lt;/strong&gt;. Proudly hosted by the &lt;strong&gt;PyTorch Foundation&lt;/strong&gt; and co-located with &lt;strong&gt;&lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris 2025&lt;/a&gt;&lt;/strong&gt;, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.&lt;/p&gt;

&lt;p&gt;Whether you’re building cutting-edge models or contributing to the ecosystem, PyTorch Day France is your opportunity to connect, collaborate, and help shape the future of deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-day-cfp.png&quot; alt=&quot;PT Day CFP&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-attend&quot;&gt;Why Attend?&lt;/h2&gt;

&lt;p&gt;Set in the vibrant atmosphere of STATION F, the world’s largest startup campus, PyTorch Day France will offer a full day of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insightful Technical Talks&lt;/li&gt;
  &lt;li&gt;Interactive Discussions&lt;/li&gt;
  &lt;li&gt;Engaging Poster Sessions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The event is designed to foster open exchange across the PyTorch ecosystem, providing a space to learn from peers, share practical insights, and explore the latest research and applications in AI.&lt;/p&gt;

&lt;h2 id=&quot;submit-a-proposal&quot;&gt;Submit a Proposal&lt;/h2&gt;

&lt;p&gt;We are currently accepting proposals for talks. If you have a project, idea, or research story you’d like to share with the PyTorch community, we want to hear from you.&lt;/p&gt;

&lt;p&gt;📩 Email your &lt;strong&gt;talk title and abstract&lt;/strong&gt; to &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt; for consideration.&lt;/p&gt;

&lt;h2 id=&quot;registration&quot;&gt;Registration&lt;/h2&gt;

&lt;p&gt;To register for PyTorch Day France, please visit the &lt;strong&gt;GOSIM AI Paris website&lt;/strong&gt;, and use the code PYTORCHFRIEND to receive 25% off.&lt;/p&gt;

&lt;p&gt;👉 &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;https://paris2025.gosim.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We encourage early registration to secure your spot and ensure access to both PyTorch Day France and the broader GOSIM AI Paris programming.&lt;/p&gt;

&lt;h2 id=&quot;venue&quot;&gt;Venue&lt;/h2&gt;

&lt;p&gt;STATION F&lt;br /&gt;
5 Parv. Alan Turing, 75013 Paris, France&lt;br /&gt;
A landmark of innovation and entrepreneurship in the heart of Paris.&lt;/p&gt;

&lt;h2 id=&quot;travel-and-accommodations&quot;&gt;Travel and Accommodations&lt;/h2&gt;

&lt;p&gt;Participants are responsible for their own travel and lodging. For those arriving internationally, Paris Charles de Gaulle Airport is approximately 38.4 km from STATION F. Additional information about accommodations and transportation may be available on the &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions?&lt;/h2&gt;

&lt;p&gt;For any inquiries, please contact us at &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We look forward to welcoming the PyTorch community to Paris this May for a day of collaboration, learning, and open source AI innovation.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re pleased to announce PyTorch Day France 2025, a dedicated gathering of the PyTorch community held 7 May 2025 in Paris, France. Proudly hosted by the PyTorch Foundation and co-located with GOSIM AI Paris 2025, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Day China 2025 Call for Proposals Open</title>
      <link href="https://pytorch.org/blog/pt-day-china-2025-cfp/" rel="alternate" type="text/html" title="PyTorch Day China 2025 Call for Proposals Open" />
      <published>2025-03-19T00:00:00-07:00</published>
      <updated>2025-03-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-day-china-2025-cfp</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-day-china-2025-cfp/">&lt;p&gt;We’re excited to announce the &lt;strong&gt;first-ever &lt;a href=&quot;https://www.lfasiallc.com/pytorch-day-china/&quot;&gt;PyTorch Day China&lt;/a&gt;&lt;/strong&gt;! This new event, hosted by the PyTorch Foundation, will take place on &lt;strong&gt;June 7 in Beijing, China&lt;/strong&gt;, bringing together AI practitioners, researchers, and industry professionals to explore the latest advancements in open source AI and machine learning. Co-located with the &lt;strong&gt;BAAI Conference&lt;/strong&gt;, PyTorch Day China is a chance to connect with the community, share knowledge, and help shape the future of deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-day-china-2025-cfp.jpg&quot; alt=&quot;PyTorch Day China 2025 Call for Proposals Open&quot; style=&quot;max-width:500px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-submit-a-proposal&quot;&gt;Why Submit a Proposal?&lt;/h2&gt;

&lt;p&gt;PyTorch Day China offers a platform for AI practitioners and researchers to showcase their work, exchange ideas, and connect with others in the community. If you’re working on innovative applications, tools, or research in the PyTorch ecosystem, we encourage you to share your expertise.&lt;/p&gt;

&lt;h2 id=&quot;topics-for-submission&quot;&gt;Topics for Submission:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;AI Applications and Use Cases&lt;/li&gt;
  &lt;li&gt;Core PyTorch Framework&lt;/li&gt;
  &lt;li&gt;DL Compilers and Kernel Authoring&lt;/li&gt;
  &lt;li&gt;Edge AI and On-Device&lt;/li&gt;
  &lt;li&gt;Ethical AI, Governance, and Regulation&lt;/li&gt;
  &lt;li&gt;Generative AI and Large Language Models (LLMs) with PyTorch&lt;/li&gt;
  &lt;li&gt;Open Source Collaboration, Education, and Community Building&lt;/li&gt;
  &lt;li&gt;Optimization for Training and Inference&lt;/li&gt;
  &lt;li&gt;PyTorch on Accelerator Hardware&lt;/li&gt;
  &lt;li&gt;PyTorch Ecosystem and Tools&lt;/li&gt;
  &lt;li&gt;PyTorch in Research and Academia&lt;/li&gt;
  &lt;li&gt;Performance Measurement and Benchmarking&lt;/li&gt;
  &lt;li&gt;Scaling Training and Inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The submission deadline is April 13. Submit and learn more here:&lt;/strong&gt; &lt;a href=&quot;https://www.lfasiallc.com/pytorch-day-china/call-for-proposals-cfp/&quot;&gt;https://www.lfasiallc.com/pytorch-day-china/call-for-proposals-cfp/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-attend&quot;&gt;Why Attend?&lt;/h2&gt;

&lt;p&gt;PyTorch Day China will feature &lt;strong&gt;technical talks, discussions, and poster sessions&lt;/strong&gt; that highlight real-world applications and developments in AI and machine learning. Attendees will have the opportunity to learn from experts, contribute to the open source community, and engage with fellow PyTorch users. Registration information will be available in April.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; June 7, 2025&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Location:&lt;/strong&gt; Zhongguancun Exhibition Center, Beijing, China&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Address:&lt;/strong&gt; 索家坟, Hai Dian Qu, Bei Jing Shi, China, 100080&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Co-located with:&lt;/strong&gt; BAAI Conference&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;travel-information&quot;&gt;Travel Information&lt;/h2&gt;

&lt;p&gt;The venue, &lt;strong&gt;Zhongguancun Exhibition Center&lt;/strong&gt;, is approximately &lt;strong&gt;39 km from Beijing International Airport&lt;/strong&gt;. More details on travel and accommodation will be available on the &lt;strong&gt;BAAI Conference website&lt;/strong&gt; and updated here as they become available.&lt;/p&gt;

&lt;h2 id=&quot;have-questions&quot;&gt;Have Questions?&lt;/h2&gt;

&lt;p&gt;For inquiries, please contact &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Submit your proposal by &lt;strong&gt;April 13&lt;/strong&gt; and join the conversation shaping the future of PyTorch.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re excited to announce the first-ever PyTorch Day China! This new event, hosted by the PyTorch Foundation, will take place on June 7 in Beijing, China, bringing together AI practitioners, researchers, and industry professionals to explore the latest advancements in open source AI and machine learning. Co-located with the BAAI Conference, PyTorch Day China is a chance to connect with the community, share knowledge, and help shape the future of deep learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine</title>
      <link href="https://pytorch.org/blog/sglang-joins-pytorch/" rel="alternate" type="text/html" title="SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine" />
      <published>2025-03-19T00:00:00-07:00</published>
      <updated>2025-03-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/sglang-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/sglang-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/sglang-join-pytorch/fg1.png&quot; alt=&quot;sglang logo&quot; style=&quot;max-width:400px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce that the SGLang project has been integrated into the PyTorch ecosystem! This integration ensures that SGLang aligns with PyTorch’s standards and practices, providing developers with a reliable and community-supported framework for fast and flexible serving of LLMs.&lt;/p&gt;

&lt;p&gt;To view the PyTorch Ecosystem, see the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; and learn more about how projects can &lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;join the PyTorch Ecosystem&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-sglang&quot;&gt;About SGLang&lt;/h2&gt;

&lt;p&gt;SGLang is a fast-serving engine for large language models and vision language models. It makes the interaction with models faster and more controllable by co-designing the backend runtime and frontend language.&lt;/p&gt;

&lt;p&gt;The core features include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fast Backend Runtime: Provides efficient serving with RadixAttention for prefix caching, zero-overhead CPU scheduler, continuous batching, token attention (paged attention), speculative decoding, tensor parallelism, chunked prefill, structured outputs, and quantization (FP8/INT4/AWQ/GPTQ).&lt;/li&gt;
  &lt;li&gt;Flexible Frontend Language: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.&lt;/li&gt;
  &lt;li&gt;Extensive Model Support: Supports a wide range of generative models (Llama, Gemma, Mistral, Qwen, DeepSeek, LLaVA, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.&lt;/li&gt;
  &lt;li&gt;Active Community: SGLang is open source and backed by an active community with industry adoption.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SGLang is famous for its fast speed. It can often significantly outperform other state-of-the-art frameworks in terms of serving throughput and latency. You can learn more about the underlying techniques from the past release blog posts: &lt;a href=&quot;https://lmsys.org/blog/2024-07-25-sglang-llama3/&quot;&gt;v0.2 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-09-04-sglang-v0-3/&quot;&gt;v0.3 blog&lt;/a&gt;, &lt;a href=&quot;https://lmsys.org/blog/2024-12-04-sglang-v0-4/&quot;&gt;v0.4 blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SGLang has been widely adopted by leading industry companies and frontier research labs. For example, xAI uses SGLang to serve its flagship model, &lt;a href=&quot;https://grok.com/&quot;&gt;Grok 3&lt;/a&gt;, which is currently the best model according to the Chatbot Arena leaderboard. Microsoft Azure uses SGLang to serve &lt;a href=&quot;https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/running-deepseek-r1-on-a-single-ndv5-mi300x-vm/4372726&quot;&gt;DeepSeek R1&lt;/a&gt; on AMD GPUs, which is currently the best open source model.&lt;/p&gt;

&lt;h2 id=&quot;serving-deepseek-models&quot;&gt;Serving DeepSeek Models&lt;/h2&gt;

&lt;p&gt;You can easily launch a Docker container to serve a DeepSeek model with the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Pull the latest image
docker pull lmsysorg/sglang:latest

# Launch a server
docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --ipc=host --network=host --privileged lmsysorg/sglang:latest \
    python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code --port 30000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can query the server with the OpenAI-compatible API&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import openai
client = openai.Client(base_url=f&quot;http://127.0.0.1:30000/v1&quot;, api_key=&quot;None&quot;)

response = client.chat.completions.create(
    model=&quot;deepseek-ai/DeepSeek-V3&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List 3 countries and their capitals.&quot;},
    ],
    temperature=0,
    max_tokens=64,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server launch command above works for 8xH200. You can find detailed instructions for other hardware (MI300X, H100, A100, H20, L40S) at https://docs.sglang.ai/references/deepseek.html.&lt;/p&gt;

&lt;p&gt;SGLang integrates DeepSeek-specific optimizations, such as MLA throughput optimizations, MLA-optimized kernels, data-parallel attention, multi-token prediction, and DeepGemm, making it the top choice for serving DeepSeek models by dozens of &lt;a href=&quot;https://x.com/lmsysorg/status/1887262321636221412&quot;&gt;companies&lt;/a&gt;, including AMD, NVIDIA, and many cloud providers. The team is actively working on integrating more optimizations following the 2025 H1 roadmap below.&lt;/p&gt;

&lt;h2 id=&quot;serving-llama-models&quot;&gt;Serving Llama Models&lt;/h2&gt;

&lt;p&gt;Similarly, you can launch the server for a Llama 3.1 text model with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or a Llama 3.2 multimodal model with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-11B-Vision-Instruct  --chat-template=llama_3_vision
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;roadmap&quot;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;This year, the SGLang team will continue to push the boundaries of system efficiency. You can find the roadmap of 2025H1 &lt;a href=&quot;https://github.com/sgl-project/sglang/issues/4042&quot;&gt;here&lt;/a&gt;. The focus is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Throughput-oriented large-scale deployment similar to the DeepSeek inference system&lt;/li&gt;
  &lt;li&gt;Long context optimizations&lt;/li&gt;
  &lt;li&gt;Low latency speculative decoding&lt;/li&gt;
  &lt;li&gt;Reinforcement learning training framework integration&lt;/li&gt;
  &lt;li&gt;Kernel optimizations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;SGLang has been deployed to large-scale production, generating trillions of tokens every day. It has an active community with over three hundred contributors on GitHub. It is supported by the following institutions: AMD, Atlas Cloud, Baseten, Cursor, DataCrunch, Etched, Hyperbolic, iFlytek, Jam &amp;amp; Tea Studios, LinkedIn, LMSYS, Meituan, Nebius, Novita AI, NVIDIA, RunPod, Stanford, UC Berkeley, UCLA, xAI, and 01.AI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sglang-join-pytorch/fg2.png&quot; alt=&quot;logos&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re excited to welcome SGLang to the PyTorch ecosystem. SGLang accelerates the serving of large language and vision language models. It’s widely adopted by industry, powering the large-scale online serving of frontier models like Grok and DeepSeek.&lt;/p&gt;

&lt;p&gt;We invite you to explore the &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main&quot;&gt;SGLang GitHub repo&lt;/a&gt;, join the &lt;a href=&quot;https://slack.mindee.com/&quot;&gt;community on Slack&lt;/a&gt;, and reach out to &lt;a href=&quot;mailto:contact@sglang.ai&quot;&gt;contact@sglang.ai&lt;/a&gt; for inquiries or collaboration opportunities. Together, we can make powerful AI models accessible to everyone.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>SGLang Team</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch at GTC 2025</title>
      <link href="https://pytorch.org/blog/pytorch-at-gtc/" rel="alternate" type="text/html" title="PyTorch at GTC 2025" />
      <published>2025-03-16T00:00:00-07:00</published>
      <updated>2025-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-at-gtc</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-at-gtc/">&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/&quot;&gt;GTC&lt;/a&gt; is coming back to San Jose on March 17–21, 2025. Join PyTorch Foundation members Arm, AWS, Google Cloud, IBM, Lightning AI, Meta, Microsoft Azure, Snowflake, and thousands of developers as we celebrate PyTorch. Together learn how AI &amp;amp; accelerated computing are helping humanity solve our most complex challenges.&lt;/p&gt;

&lt;p&gt;Join in person with &lt;a href=&quot;https://www.nvidia.com/gtc/?ncid=GTC-NVI0K8HVX&quot;&gt;discounted GTC registration&lt;/a&gt; for PyTorch Foundation or &lt;a href=&quot;https://register.nvidia.com/flow/nvidia/gtcs25/registration/&quot;&gt;watch online&lt;/a&gt; with free registration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-at-gtc.jpg&quot; alt=&quot;book cover&quot; style=&quot;max-width:500px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;scaling-open-source-ai-from-foundation-models-to-ecosystem-success&quot;&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1738966749087001K1dG&quot;&gt;Scaling Open Source AI: From Foundation Models to Ecosystem Success&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Hear from PyTorch Foundation’s Executive Director Matt White &amp;amp; panelists from UC Berkeley, Meta, NVIDIA, &amp;amp; Sequoia Capital how open source is transforming AI development, bringing together experts from industry, academia, and venture capital to discuss the technical and business aspects of collaborative open source AI development They’ll examine how open source projects like PyTorch, vLLM, Ray, and NVIDIA’s NeMo are accelerating AI innovation while creating new opportunities for businesses and researchers. They’ll share real-world experiences from PyTorch’s development, Berkeley’s research initiatives, and successful AI startups. Take away valuable insights into the technical and business aspects of open source AI. – Monday, Mar 17 10:00 AM - 11:00 AM PDT&lt;/p&gt;

&lt;h2 id=&quot;pytorch--gtc&quot;&gt;PyTorch @ GTC&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726155993061001WWZM&quot;&gt;The Performance of CUDA with the Flexibility of PyTorch &lt;/a&gt;&lt;br /&gt;
Mark Saroufim, Software Engineer, Meta Platforms&lt;/p&gt;

&lt;p&gt;This talk explores how PyTorch users are also becoming CUDA developers. We’ll start with motivating examples from eager, the launch of torch.compile and the more recent trend of kernel zoos. We will share details on how we went about integrating low bit matmuls in torchao and the torch.compile CUTLASS backend. We’ll also discuss details on how you can define, build and package your own custom ops in PyTorch so you get the raw performance of CUDA while maintaining the flexibility of PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727978036338001UVLu&quot;&gt;Make My PyTorch Model Fast, and Show Me How You Did It&lt;/a&gt;&lt;br /&gt;
Thomas Viehmann, Principal Research Engineer, Lightning AI&lt;br /&gt;
Luca Antiga, CTO, Lightning AI&lt;/p&gt;

&lt;p&gt;PyTorch is popular in deep learning and LLMs for richness and ease of expressions. To make the most of compute resources, PyTorch models benefit from nontrivial optimizations, but this means losing some of their ease and understandability. Learn how with Thunder, a PyTorch-to-Python compiler focused on usability, understandability, and extensibility, you can optimize and transform (i.e., distribute across many machines) models while • leaving the PyTorch code unchanged • targeting a variety of models without needing to adapt to each of them • understanding each transformation step because the results are presented as simple Python code • accessing powerful extension code for your own optimizations with just one or a few lines of code We’ll show how the combination of Thunder transforms and the NVIDIA stack (NVFuser, cuDNN, Apex) delivers optimized performance in training and inference on a variety of models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726184633014001Jh5G&quot;&gt;FlexAttention: The Flexibility of PyTorch With the Performance of FlashAttention&lt;/a&gt;&lt;br /&gt;
Driss Guessous, Machine Learning Engineer, Meta Platforms&lt;/p&gt;

&lt;p&gt;Introducing FlexAttention: a novel PyTorch API that enables custom, user-defined attention mechanisms with performance comparable to state-of-the-art solutions. By leveraging the PyTorch compiler stack, FlexAttention supports dynamic modifications to attention scores within SDPA, achieving both runtime and memory efficiency through kernel fusion with the FlashAttention algorithm. Our benchmarks on A100 GPUs show FlexAttention achieves 90% of FlashAttention2’s performance in forward passes and 85% in backward passes. On H100 GPUs, FlexAttention’s forward performance averages 85% of FlashAttention3 and is ~25% faster than FlashAttention2, while backward performance averages 76% of FlashAttention3 and is ~3% faster than FlashAttention2. Explore how FlexAttention balances near-state-of-the-art performance with unparalleled flexibility, empowering researchers to rapidly iterate on attention mechanisms without sacrificing efficiency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1731693095418001cruA&quot;&gt;Keep Your GPUs Going Brrr : Crushing Whitespace in Model Training&lt;/a&gt;&lt;br /&gt;
Syed Ahmed, Senior Software Engineer, NVIDIA&lt;br /&gt;
Alban Desmaison, Research Engineer, Meta&lt;br /&gt;
Aidyn Aitzhan, Senior Software Engineer, NVIDIA&lt;/p&gt;

&lt;p&gt;Substantial progress has recently been made on the compute-intensive portions of model training, such as high-performing attention variants. While invaluable, this progress exposes previously hidden bottlenecks in model training, such as redundant copies during collectives and data loading time. We’ll present recent improvements in PyTorch achieved through Meta/NVIDIA collaboration to tackle these newly exposed bottlenecks and how practitioners can leverage them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727176757800001qp7T&quot;&gt;Accelerated Python: The Community and Ecosystem&lt;/a&gt;&lt;br /&gt;
Andy Terrel, CUDA Python Product Lead, NVIDIA&lt;br /&gt;
Jeremy Tanner, Open Source Programs, NVIDIA&lt;br /&gt;
Anshuman Bhat, CUDA Product Management, NVIDIA&lt;/p&gt;

&lt;p&gt;Python is everywhere. Simulation, data science, and Gen AI all depend on it. Unfortunately, the dizzying array of tools leaves a newcomer baffled at where to start. We’ll take you on a guided tour of the vibrant community and ecosystem surrounding accelerated Python programming. Explore a variety of tools, libraries, and frameworks that enable efficient computation and performance optimization in Python, including CUDA Python, RAPIDS, Warp, and Legate. We’ll also discuss integration points with PyData, PyTorch, and JAX communities. Learn about collaborative efforts within the community, including open source projects and contributions that drive innovation in accelerated computing. We’ll discuss best practices for leveraging these frameworks to enhance productivity in developing AI-driven applications and conducting large-scale data analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1734571562315001xMKM&quot;&gt;Supercharge large scale AI with Google Cloud AI hypercomputer (Presented by Google Cloud)&lt;/a&gt;&lt;br /&gt;
Deepak Patil, Product Manager, Google Cloud&lt;br /&gt;
Rajesh Anantharaman, Product Management Lead, ML Software, Google Cloud&lt;/p&gt;

&lt;p&gt;Unlock the potential of your large-scale AI workloads with Google Cloud AI Hypercomputer – a supercomputing architecture designed for maximum performance and efficiency. In this session, we will deep dive into PyTorch and JAX stacks on Google Cloud on NVIDIA GPUs, and showcase capabilities for high performance foundation model building on Google Cloud.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1739906058885001OxEF&quot;&gt;Peering Into the Future: What AI and Graph Networks Can Mean for the Future of Financial Analysis&lt;/a&gt;&lt;br /&gt;
Siddharth Samsi, Sr. Solutions Architect, NVIDIA&lt;br /&gt;
Sudeep Kesh, Chief Innovation Officer, S&amp;amp;P Global&lt;/p&gt;

&lt;p&gt;Artificial Intelligence, agentic systems, and graph neural networks (GNNs) are providing the new frontier to assess, monitor, and estimate opportunities and risks across work portfolios within financial services. Although many of these technologies are still developing, organizations are eager to understand their potential. See how S&amp;amp;P Global and NVIDIA are working together to find practical ways to learn and integrate such capabilities, ranging from forecasting corporate debt issuance to understanding capital markets at a deeper level. We’ll show a graph representation of market data using the PyTorch-Geometric library and a dataset of issuances spanning three decades and across financial and non-financial industries. Technical developments include generation of a bipartite graph and link-prediction GNN forecasting. We’ll address data preprocessing, pipelines, model training, and how these technologies can broaden capabilities in an increasingly complex world.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1727984645671001Y9eq&quot;&gt;Unlock Deep Learning Performance on Blackwell With cuDNN&lt;/a&gt;&lt;br /&gt;
Yang Xu (Enterprise Products), DL Software Engineering Manager, NVIDIA&lt;/p&gt;

&lt;p&gt;Since its launch, cuDNN, a library for GPU-accelerating deep learning (DL) primitives, has been powering many AI applications in domains such as conversational AI, recommender systems, and speech recognition, among others. CuDNN remains a core library for DL primitives in popular frameworks such as PyTorch, JAX, Tensorflow, and many more while covering training, fine-tuning, and inference use cases. Even in the rapidly evolving space of Gen AI — be it Llama, Gemma, or mixture-of-experts variants requiring complex DL primitives such as flash attention variants — cuDNN is powering them all. Learn about new/updated APIs of cuDNN pertaining to Blackwell’s microscaling format, and how to program against those APIs. We’ll deep dive into leveraging its graph APIs to build some fusion patterns, such as matmul fusion patterns and fused flash attention from state-of-the-art models. Understand how new CUDA graph support in cuDNN, not to be mistaken with the cuDNN graph API, could be exploited to avoid rebuilding CUDA graphs, offering an alternative to CUDA graph capture with real-world framework usage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1736347047099001au7y&quot;&gt;Train and Serve AI Systems Fast With the Lightning AI Open-Source Stack (Presented by Lightning AI)&lt;/a&gt;&lt;br /&gt;
Luca Antiga, CTO, Lightning AI&lt;/p&gt;

&lt;p&gt;See how the Lightning stack can cover the full life cycle, from data preparation to deployment, with practical examples and particular focus on distributed training and high-performance inference. We’ll show examples that focus on new features like support for multi-dimensional parallelism through DTensors, as well as quantization through torchao.&lt;/p&gt;

&lt;h2 id=&quot;connect-with-experts-interactive-sessions&quot;&gt;Connect With Experts (Interactive Sessions)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1728516848639001tO7H&quot;&gt;Meet the Experts From Deep Learning Framework Teams &lt;/a&gt;&lt;br /&gt;
Eddie Yan, Technical Lead of PyTorch, NVIDIA&lt;br /&gt;
Masaki Kozuki, Senior Software Engineer in PyTorch, NVIDIA&lt;br /&gt;
Patrick Wang (Enterprise Products), Software Engineer in PyTorch, NVIDIA&lt;br /&gt;
Mike Ruberry, Distinguished Engineer in Deep Learning Frameworks, NVIDIA&lt;br /&gt;
Rishi Puri, Sr. Deep Learning Engineer and Lead for PyTorch Geometric, NVIDIA&lt;/p&gt;

&lt;h2 id=&quot;training-labs&quot;&gt;Training Labs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1726073884811001C0za&quot;&gt;Kernel Optimization for AI and Beyond: Unlocking the Power of Nsight Compute &lt;/a&gt;&lt;br /&gt;
Felix Schmitt, Sr. System Software Engineer, NVIDIA&lt;br /&gt;
Peter Labus, Senior System Software Engineer, NVIDIA&lt;/p&gt;

&lt;p&gt;Learn how to unlock the full potential of NVIDIA GPUs with the powerful profiling and analysis capabilities of Nsight Compute. AI workloads are rapidly increasing the demand for GPU computing, and ensuring that they efficiently utilize all available GPU resources is essential. Nsight Compute is the most powerful tool for understanding kernel execution behavior and performance. Learn how to configure and launch profiles customized for your needs, including advice on profiling accelerated Python applications, AI frameworks like PyTorch, and optimizing Tensor Core utilization essential to modern AI performance. Learn how to debug your kernel and use the expert system built into Nsight Compute, known as “Guided Analysis,” that automatically detects common issues and directs you to the most relevant performance data all the way down to the source code level.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1725042189130001cmoW&quot;&gt;Make Retrieval Better: Fine-Tuning an Embedding Model for Domain-Specific RAG&lt;/a&gt;&lt;br /&gt;
Gabriel Moreira, Sr. Research Scientist, NVIDIA&lt;br /&gt;
Ronay Ak, Sr. Data Scientist, NVIDIA&lt;/p&gt;

&lt;p&gt;LLMs power AI applications like conversational chatbots and content generators, but are constrained by their training data. This might lead to hallucinations in content generation, which requires up-to-date or domain-specific information. Retrieval augmented generation (RAG) addresses this issue by enabling LLMs to access external context without modifying model parameters. Embedding or dense retrieval models are a key component of a RAG pipeline for retrieving relevant context to the LLM. However, an embedding model’s effectiveness to capture the unique characteristics of the custom data hinges on the quality and domain relevance of its training data. Fine-tuning embedding models is gaining interest to provide more accurate and relevant responses tailored to users’ specific domain.&lt;/p&gt;

&lt;p&gt;In this lab, you’ll learn to generate a synthetic dataset with question-context pairs from a domain-specific corpus, and process the data for fine-tuning. Then, fine-tune a text embedding model using synthetic data and evaluate it.&lt;/p&gt;

&lt;h2 id=&quot;poster-presentations&quot;&gt;Poster Presentations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1729781473379001KiPD&quot;&gt;Single-View X-Ray 3D Reconstruction Using Neural Back Projection and Frustum Resampling&lt;/a&gt;&lt;br /&gt;
Tran Minh Quan, Developer Technologist, NVIDIA&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/gtc/session-catalog/?regcode=no-ncid&amp;amp;ncid=no-ncid&amp;amp;tab.catalogallsessionstab=16566177511100015Kus&amp;amp;search=pytorch#/session/1729757102989001KDG4&quot;&gt;Enable Novel Applications in the New AI Area in Medicine: Accelerated Feature Computation for Pathology Slides&lt;/a&gt;&lt;br /&gt;
Nils Bruenggel, Principal Software Engineer, Roche Diagnostics Int. AG&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at NVIDIA</name>
        
        
      </author>

      

      

      
        <summary type="html">GTC is coming back to San Jose on March 17–21, 2025. Join PyTorch Foundation members Arm, AWS, Google Cloud, IBM, Lightning AI, Meta, Microsoft Azure, Snowflake, and thousands of developers as we celebrate PyTorch. Together learn how AI &amp;amp; accelerated computing are helping humanity solve our most complex challenges.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing the New PyTorch Landscape: Your Guide to the PyTorch Ecosystem</title>
      <link href="https://pytorch.org/blog/pytorch-landscape/" rel="alternate" type="text/html" title="Introducing the New PyTorch Landscape: Your Guide to the PyTorch Ecosystem" />
      <published>2025-03-13T00:00:00-07:00</published>
      <updated>2025-03-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-landscape</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-landscape/">&lt;p&gt;We’re excited to reveal our brand new PyTorch Landscape. The &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; helps researchers, developers, and organizations easily locate useful, curated, community-built tools that augment the PyTorch core framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;&lt;img src=&quot;/assets/images/landscape.jpg&quot; alt=&quot;landscape banner&quot; style=&quot;max-width:600px;width:100%; margin-left: auto; margin-right: auto; display: block;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-the-landscape-offers&quot;&gt;What the Landscape Offers&lt;/h2&gt;

&lt;p&gt;The Landscape visually organizes projects into three categories—Modeling, Training, and Optimizations—making finding relevant frameworks, libraries, and projects easy. Users can quickly locate curated, valuable tools for a variety of use cases that complement the PyTorch framework. Each tool that is part of the Landscape has been reviewed and vetted by PyTorch project experts. The projects in the Landscape are considered to be mature and healthy and provide valuable capabilities that complement the PyTorch framework in their respective domains.&lt;/p&gt;

&lt;h2 id=&quot;explore-the-ai-landscape&quot;&gt;Explore the AI Landscape&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Explore&lt;/strong&gt; page presents platforms, tools, and libraries, each with a logo, description, and links to GitHub and further details. This categorized, visual approach simplifies discovery and provides quick access to essential technologies.&lt;/p&gt;

&lt;h2 id=&quot;guide-page-a-closer-look&quot;&gt;Guide Page: A Closer Look&lt;/h2&gt;

&lt;p&gt;For deeper insights, the &lt;strong&gt;Guide&lt;/strong&gt; page expands on each project, highlighting methodologies and trends shaping AI development, from adversarial robustness to self-supervised learning. There are also project statistics provided for each project, including metrics such as number of stars, contributors, commit history, languages used, license, and other valuable metrics that provide an in-depth understanding of the project and how it may be used.&lt;/p&gt;

&lt;h2 id=&quot;tracking-ais-growth-the-stats-page&quot;&gt;Tracking AI’s Growth: The Stats Page&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Stats&lt;/strong&gt; page provides insights into AI development trends, tracking repository activity, programming languages, and industry funding data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Repositories: 117 repositories, 20.5k contributors, and 797.2k stars across 815MB of source code.&lt;/li&gt;
  &lt;li&gt;Development Trends: Weekly commit activity over the last year.&lt;/li&gt;
  &lt;li&gt;Licensing Breakdown: Repositories are categorized by license type.&lt;/li&gt;
  &lt;li&gt;Funding &amp;amp; Acquisitions: Insights into investment trends, including funding rounds and acquisitions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-use-the-pytorch-landscape&quot;&gt;Why Use the PyTorch Landscape?&lt;/h2&gt;

&lt;p&gt;Finding useful and high quality open source projects that complement the PyTorch core system can be overwhelming. The PyTorch Landscape offers a clear, accessible way to explore the ecosystem of community-built tools, whether you’re researching, building models, or making strategic decisions.&lt;/p&gt;

&lt;p&gt;Stay ahead with the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt; — your guide to the PyTorch Ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;want-to-contribute-a-project-to-the-pytorch-landscape&quot;&gt;Want to Contribute a Project to the PyTorch Landscape?&lt;/h2&gt;

&lt;p&gt;Have you built a useful open source tool that you would like to share with the PyTorch community? Then help us grow the Ecosystem by contributing your tool! You can find the &lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;instructions to apply here&lt;/a&gt;. We welcome all contributions from the community!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re excited to reveal our brand new PyTorch Landscape. The PyTorch Landscape helps researchers, developers, and organizations easily locate useful, curated, community-built tools that augment the PyTorch core framework.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling Recommendation Systems Training to Thousands of GPUs with 2D Sparse Parallelism</title>
      <link href="https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism/" rel="alternate" type="text/html" title="Scaling Recommendation Systems Training to Thousands of GPUs with 2D Sparse Parallelism" />
      <published>2025-03-11T00:00:00-07:00</published>
      <updated>2025-03-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-recommendation-2d-sparse-parallelism/">&lt;p&gt;At Meta, recommendation systems are the cornerstone of delivering relevant and personalized ads to billions of users globally. Through technologies like PyTorch’s TorchRec, we’ve successfully developed solutions that enable model training across hundreds of GPUs. While these systems have served us well, recent research on scaling laws has revealed a compelling opportunity: we can achieve significantly better model performance by training dramatically larger neural networks.&lt;/p&gt;

&lt;p&gt;However, this insight presents us with a new challenge. Our current training infrastructure, though highly optimized for hundreds of GPUs, cannot efficiently scale to the thousands of GPUs needed to train these larger models. The leap from hundreds to thousands of GPUs introduces complex technical challenges, particularly around handling sparse operations in recommendation models. These challenges require fundamentally new approaches to distributed training, which we address with a novel parallelization strategy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To address these issues, we introduced 2D embedding parallel, a novel parallelism strategy that overcomes the sparse scaling challenges inherent in training large recommendation models across thousands of GPUs. This is available today in TorchRec through the DMPCollection API.&lt;/strong&gt; This approach combines two complementary parallelization techniques: data parallelism for the sparse components of the model, and model parallelism for the embedding tables, leveraging TorchRec’s robust sharding capabilities. By strategically integrating these techniques, we’ve created a solution that scales to thousands of GPUs and now powers Meta’s largest recommendation model training runs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the sparse scaling challenges?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We identified three key challenges that prevented us from naively scaling our model to thousands of GPUs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Imbalancing and straggler issue:&lt;/strong&gt; with more GPUs it’s harder to achieve balanced sharding, some ranks can have much heavier workload for embedding computations, which can slow down the entire training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication across nodes:&lt;/strong&gt; As training jobs utilize an increased number of GPUs, the all-to-all communication bandwidth can drop under certain network topologies which can increase communication latency significantly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory overhead:&lt;/strong&gt; The memory used by input features is often negligible, however, as we use thousands of GPUs, we can introduce larger input features and the memory requirements can become significant.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With 2D embedding parallel, we can describe our new parallelism scheme like this, in this example we have 2 model replicas (Replica 1: GPU1/GPU3, Replica 2: GPU2/GPU4)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/scaling-recommendation-2d-sparse-parallelism/fg1.png&quot; alt=&quot;Flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1: Layout illustration of 2D Sparse Parallelism&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With 2D sparse parallelism we address these challenges, instead of sharding tables across all ranks, we first evenly divide all ranks into several parallel groups:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Within each group, we use model parallel for the embedding tables, such as column-wise/row-wise sharding. At scale, for our largest tables, we have also developed a grid sharding, which shards embedding tables on the row and column dimension.&lt;/li&gt;
  &lt;li&gt;Across groups, we do data parallel, such that each rank in a group has its corresponding replica rank in the other groups (replica rank means storing the same embedding table shards).
    &lt;ol&gt;
      &lt;li&gt;After each group has completed its own backward pass, we all reduce the embedding table weights across the replicas to keep them synchronized.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;our-production-solution&quot;&gt;Our production solution&lt;/h2&gt;

&lt;p&gt;TorchRec is our library to build the sparse part of the recommendation models in native PyTorch. With the traditional API being DistributedModelParallel which applies model parallel to the embedding tables. We introduce a new API alongside it, known as DMPCollection, which serves as the main entry point for enabling 2D parallel on TorchRec models. We designed it to be as easy of a change as applying FSDP/DDP is.&lt;/p&gt;

&lt;p&gt;To understand what DMPCollection does, we have to understand what DistributedModelParallel (DMP) does first:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create embedding tables, known as EmbeddingBagCollection and EmbeddingCollections.&lt;/li&gt;
  &lt;li&gt;Generate a sharding plan with respect to GPU topology, embedding tables, memory available, input data, and more.&lt;/li&gt;
  &lt;li&gt;Wrap model with DMP and the associated sharding plan passed in.&lt;/li&gt;
  &lt;li&gt;DMP initializes and shards the embedding tables in accordance with the sharding plan.&lt;/li&gt;
  &lt;li&gt;On a train step, DMP takes an input batch, communicates it to the appropriate GPUs containing the embedding table shard of interest, looks up the value, and returns it back to the GPU that requested it. This is all done on the global process group, with some exceptions for special sharding (such as table row wise sharding)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DistributedModelParallel was built for model parallel with many parts working under the assumption of sharding and working around the global world size. We need to change these parts in a way where we can introduce additional dimensions of parallelism without losing the optimizations and feature set of TorchRec.&lt;/p&gt;

&lt;p&gt;DMPCollection changes a few key parts to enable 2D parallel in an extensible way,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate sharding plans for the smaller sharding group once, once passed in we communicate to the appropriate ranks across the global group and remap the ranks to fit the new sharding group ranks.&lt;/li&gt;
  &lt;li&gt;Create two new NCCL process groups, known as sharding and replica process groups. The sharding process group is passed into sharding and train step components of TorchRec. The replica process group is used for the weight and optimizer state synchronization, the all reduce call happens over this process group.
    &lt;ul&gt;
      &lt;li&gt;The sub NCCL process groups allow us to efficiently communicate only between the ranks that are relevant for a particular comm. Each rank will have two associated process groups.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To the user, the change is very simple, while taking away all the complexity around applying the parallelism strategies to the model.&lt;/p&gt;

&lt;h2 id=&quot;how-do-we-create-these-sharding-and-replication-groups&quot;&gt;How do we create these sharding and replication groups?&lt;/h2&gt;

&lt;p&gt;These process groups are one of the keys to DMPCollection’s performant implementation. From our earlier diagram, we showed a simple 2x2 GPU setup, however, at scale, how do we assign which ranks are part of a given sharding group and what are their replica ranks across the sharding groups?&lt;/p&gt;

&lt;p&gt;Consider the following setup with 2 nodes, each with 4 GPUs. The sharding and replication groups under 2D parallel will be,&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Sharding Group
   &lt;/td&gt;
   &lt;td&gt;Sharding Ranks
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;0
   &lt;/td&gt;
   &lt;td&gt;0, 2, 4, 6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1, 3, 5, 7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


   &lt;/td&gt;
   &lt;td&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Replication Group
   &lt;/td&gt;
   &lt;td&gt;Replication Ranks
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;0
   &lt;/td&gt;
   &lt;td&gt;0, 1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2, 3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;4, 5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;6, 7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We use the following formulation,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Divide all trainers into G sharding groups, each with L trainers
    &lt;ol&gt;
      &lt;li&gt;Groups, G, is determined by G = T / L, where T is total number of trainers&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;For each group, G, we assigned non-contiguous trainer ranks based on the group it’s in, following,
    &lt;ol&gt;
      &lt;li&gt;[i, G+i, 2G+i, …, (L - 1) G+i], where* i = 0 to G-1*&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;From the groups, G, we can create the replication group, which is every G continuous ranks
    &lt;ol&gt;
      &lt;li&gt;(0 to G-1, G to 2* G - 1) each continuous set stores the duplicate embedding table shards.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This means our sharding groups, G, are of size L, which can be known as the number of ranks to apply model parallel across. This, in turn, gives us replica groups, each of size G, which are the ranks we data parallel across.&lt;/p&gt;

&lt;p&gt;In DMPCollection, we’re able to create these process groups efficiently with the use of DeviceMesh, we create the entire GPU topology in a 2x2 matrix, with each row representing the group of sharding ranks and each column representing the corresponding replica ranks,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create peer matrix
num_groups = global_world_size // sharding_group_size
for each group_rank in num_groups:
	peers = [num_groups * rank + group_rank for rank in range(sharding_group_size)]
	add peer to peer matrix

initalize DeviceMesh with two dimensions (shard, replicate)
slice DeviceMesh on shard for sharding process group
slide DeviceMesh on replicate for replica process group
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our DeviceMesh approach, should we want to change the topology or provide further flexibility in the future, we can easily extend our creation logic to any form of topologies and even extend for further dimensions of parallelism if needed.&lt;/p&gt;

&lt;h2 id=&quot;performance-of-2d-parallel&quot;&gt;Performance of 2D parallel&lt;/h2&gt;

&lt;p&gt;Our rank partitioning strategy optimizes communication patterns by strategically placing model replica ranks for each shard within the same compute node. This architecture provides significant performance benefits for the weight synchronization operation. After the backward pass, we perform all-reduce operations to synchronize model weights—which is an expensive process given the large parameter counts we have to communicate and sync—with our setup of placing replicas on the same node we leverage intra node’s high-bandwidth over-relying on slower inter-node bandwidth.&lt;/p&gt;

&lt;p&gt;The effect of this design choice on the other communication collectives generally improves the latencies. The improvement stems from two factors.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;By sharding the embedding tables over a reduced number of ranks and conducting communications for the model within the smaller group, we achieve a lower all-to-all latency.&lt;/li&gt;
  &lt;li&gt;With the replication in 2D parallel, our embedding lookup latency on a rank reduces, we can reduce the local batch size to 1/Nth of the equivalent global batch size, where N is the number of model replicas.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A production model trace exemplifies these two factors, here we run the 2D parallel job on 1024 GPUs, with a sharding group size of 256 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/scaling-recommendation-2d-sparse-parallelism/fg2.png&quot; alt=&quot;State diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 2: Comparing latencies between non 2D parallel and 2D parallel workloads&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two key levers users have to tune to maximize performance for their workloads:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The size of the model sharding group relative to the global world size. The global world size divided by the sharding group size represents the number of model replicas we will have.
    &lt;ol&gt;
      &lt;li&gt;To maximize performance, users can look to scale up their model up to 8x, this scaling factor maintains the intra-host all reduce.
        &lt;ol&gt;
          &lt;li&gt;For further scaling, the all reduce would have to happen over inter host. From our experiments, we did not see an obvious performance regression and in fact note advantages of an inter host all reduce. We can change our sharding and replica topology to inter host all reduce, which can help us introduce fault tolerance strategies should a particular host go down.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Frequency of all reduce synchronization, DMPCollection comes with a sync() call, which can be tuned to be called every N training steps, performing a sort of local SGD training. With scale, reducing the frequency of synchronization can bring significant gains to performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Readers should note that 2D sparse parallel training differs from non-parallelized training because we synchronize the embedding table weights rather than the gradients. This approach is made possible by TorchRec’s use of FBGEMM, which provides optimized kernels under the hood. One of FBGEMM’s key optimizations is the fusion of the optimizer in the backward pass. Instead of fully materializing the embedding table gradients—which would consume significant memory—they are passed directly to the optimizer update. Attempting to materialize and synchronize these gradients would create substantial overhead, making that approach impractical.&lt;/p&gt;

&lt;p&gt;Our exploration revealed that to achieve training results comparable to the baseline, we synchronize optimizer states on a delayed schedule, with the timing dependent on the number of sharding/replica groups (ie: for Adagrad we update the momentum behind by one sync step). This approach also enables users to implement local SGD or semi-synchronized training strategies, which can achieve convergence and potentially produce better loss curves than the baseline.&lt;/p&gt;

&lt;p&gt;We thank you for reading our post! This is an exciting direction we have come across that we hope to develop further to maximize performance of recommendation systems and push the state of the art.&lt;/p&gt;

&lt;style&gt;
@media screen and (min-width: 768px) {
    article.pytorch-article ul, article.pytorch-article ol {
        padding-left: 3.5rem;
    }
}
ol {
  list-style-type: decimal; /* 1, 2, 3 */
}

ol ol {
  list-style-type: lower-alpha; /* a, b, c */
}

ol ol ol {
  list-style-type: lower-roman; /* i, ii, iii */
}


&lt;/style&gt;</content>

      
      
      
      
      

      <author>
          <name>PyTorch Team at Meta: Chunzhi Yang, Rich Zhu, Zain Huda, Liangbei Xu, Xin Zhang, Jiyan Yang, Dennis van der Staay, Wang Zhou, Jin Fang, Jade Nie, Yuxi Hu</name>
        
        
      </author>

      

      

      
        <summary type="html">At Meta, recommendation systems are the cornerstone of delivering relevant and personalized ads to billions of users globally. Through technologies like PyTorch’s TorchRec, we’ve successfully developed solutions that enable model training across hundreds of GPUs. While these systems have served us well, recent research on scaling laws has revealed a compelling opportunity: we can achieve significantly better model performance by training dramatically larger neural networks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Powering AI with PyTorch, Fedora, and Open Source Communities</title>
      <link href="https://pytorch.org/blog/pt-fedora-os-communities/" rel="alternate" type="text/html" title="Powering AI with PyTorch, Fedora, and Open Source Communities" />
      <published>2025-03-07T00:00:00-08:00</published>
      <updated>2025-03-07T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pt-fedora-os-communities</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-fedora-os-communities/">&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg1.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://www.devconf.info/in/&quot;&gt;DevConf.IN 2025&lt;/a&gt; in Pune, I had the opportunity to host a &lt;strong&gt;&lt;a href=&quot;https://pretalx.devconf.info/devconf-in-2025/talk/W3YURM/&quot;&gt;PyTorch Meetup&lt;/a&gt;&lt;/strong&gt; on February 28th. The session, titled “&lt;strong&gt;Powering AI with PyTorch, Fedora, and Open Source Communities&lt;/strong&gt;” was aimed at introducing PyTorch to students and professionals, explaining why &lt;strong&gt;PyTorch+Fedora&lt;/strong&gt; form an ideal AI development platform. The other key aspect I covered was collaboration between open source communities.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-pytorch&quot;&gt;Introduction to PyTorch&lt;/h2&gt;

&lt;h2 id=&quot;the-power-of-deep-learning-made-simple&quot;&gt;The Power of Deep Learning made simple&lt;/h2&gt;

&lt;p&gt;With the explosion of GPTs, there is a renowned interest in the field of AI and ML. The myth of developing AI/ML technologies and its applications is rocket science and far-fetched, needs correction. Only open source has the power to demystify this myth and further evolve the technology to make it versatile and developer friendly. Since its inception, PyTorch has evolved and has been a driving force to make AI/ML development extremely simple. I covered the aspects of PyTorch key components, its features and why PyTorch is the best choice as a deep learning framework.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg2.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The codewalk through was designed to showcase how easy and simple it is to utilise the power of GPUs, creating a simple neural network and training the model. The code walkthrough was very well received and it was great to hear back from the attendees that they never knew how powerful PyTorch is for deep learning. The real world examples sighted how this powerful framework can be used beyond the common GPTs and has the power to influence across a broad spectrum of applications.&lt;/p&gt;

&lt;h2 id=&quot;fedorapytorch-the-ideal-aiml-development-platform&quot;&gt;Fedora+PyTorch the Ideal AI/ML Development Platform&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg3.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg4.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the highlights of the event was the discussion on Fedora’s role as an AI platform. Fedora’s reliability, flexibility, and strong community support make it an ideal partner for PyTorch, allowing developers to focus on model-building without worrying about infrastructure. The students were intrigued by the idea of contributing to Fedora’s AI/ML ecosystem while building their own projects. Sumantro Mukherjee spoke about the AI policy in Fedora and how one can start contributing to the AI/ML using Fedora as a platform. He highlighted how Fedora is evolving to meet the needs of AI practitioners. The idea that an open-source operating system could provide the perfect foundation for AI research sparked an engaging conversation.&lt;/p&gt;

&lt;h2 id=&quot;innovation-in-open-source-when-communities-come-together&quot;&gt;Innovation in Open Source When Communities Come Together&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg5.jpg&quot; alt=&quot;charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important that we learn from history and repeat the good things! When open source communities come together they can create seismic shifts in the industry. To drive this home, I took the audience on a journey through history, revisiting a pivotal moment when Apache and Linux came together, solving common problems and fundamentally reshaping enterprise computing. That moment was not just about technology; it was about collaboration. It was about two powerful communities recognizing that they were stronger together. Today, we stand at the cusp of another such moment - PyTorch and Linux, particularly Fedora, are coming together to shape the future of AI/ML. This is not just an opportunity but a responsibility for contributors, developers, and AI/ML enthusiasts to be part of this movement.&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-fedora-os-communities/fg6.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the best parts of the event was the enthusiasm it generated. Diverse audience, including students, AI enthusiasts, and industry professionals. Notably, Vincent Caldeira (CTO, APAC, Red Hat) and Chris Butler (Senior Principal Chief Architect, Red Hat) were present, reinforcing the growing interest in open-source AI/ML. Many students were eager to explore PyTorch and Fedora, contribute to open-source AI projects, and start their own AI experiments. Industry experts saw the potential for scalable, community-driven AI innovation. The session sparked curiosity and conversations that continued long after the event ended.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sudhir Dharanendraiah</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
</feed>



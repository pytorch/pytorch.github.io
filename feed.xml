<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2025-05-01T08:37:46-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Announcing the PyTorch Docathon 2025</title>
      <link href="https://pytorch.org/blog/how-ibm-uses-pt-terratorch-copy/" rel="alternate" type="text/html" title="Announcing the PyTorch Docathon 2025" />
      <published>2025-05-01T00:00:00-07:00</published>
      <updated>2025-05-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/how-ibm-uses-pt-terratorch%20copy</id>
      <content type="html" xml:base="https://pytorch.org/blog/how-ibm-uses-pt-terratorch-copy/">&lt;p&gt;We’re thrilled to announce the &lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-pytorch-docathon-june-3rd-18th-2025/&quot;&gt;2025 PyTorch Docathon&lt;/a&gt;! This is a hackathon-style event aimed at enhancing PyTorch documentation with the support of the community. Documentation is a vital component of any technology, and by refining it, we can simplify the onboarding process for new users, help them effectively utilize PyTorch’s features, and ultimately speed up the transition from research to production in machine learning.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;WHY PARTICIPATE&lt;/h2&gt;

&lt;h3 id=&quot;low-barrier-to-entry&quot;&gt;Low Barrier to Entry&lt;/h3&gt;

&lt;p&gt;Unlike many open-source projects that require deep knowledge of the codebase and previous contributions to join hackathon events, the Docathon is tailored for newcomers. While we expect participants to be familiar with Python, and have basic knowledge of PyTorch and machine learning, there are tasks related to website issues that don’t even require that level of expertise.&lt;/p&gt;

&lt;h3 id=&quot;tangible-results&quot;&gt;Tangible Results&lt;/h3&gt;

&lt;p&gt;A major advantage of the Docathon is witnessing the immediate impact of your contributions. Enhancing documentation significantly boosts a project’s usability and accessibility, and you’ll be able to observe these improvements directly. Seeing tangible outcomes can also be a strong motivator to continue contributing.&lt;/p&gt;

&lt;h3 id=&quot;collaborative-environment&quot;&gt;Collaborative Environment&lt;/h3&gt;

&lt;p&gt;The Docathon fosters a collaborative atmosphere, offering you the chance to work alongside other contributors and PyTorch maintainers to improve the documentation. This is a fantastic opportunity to learn from peers, exchange ideas, and build connections.&lt;/p&gt;

&lt;h3 id=&quot;learning-opportunities&quot;&gt;Learning Opportunities&lt;/h3&gt;

&lt;p&gt;Even if you’re not a PyTorch expert, the Docathon offers a valuable learning experience. You’ll have the chance to delve into PyTorch modules, test tutorials on your machine, and explore them in the CI environment.&lt;/p&gt;

&lt;h2 id=&quot;who-should-participate&quot;&gt;WHO SHOULD PARTICIPATE&lt;/h2&gt;

&lt;p&gt;Whether you’re a seasoned documentation expert or just starting out, we invite everyone to join in the PyTorch docathon to contribute and develop your skills and knowledge to help improve the documentation for everyone! We will have issues labelled by skill level, and the PyTorch Discord will be available for collaboration and help.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;EVENT DETAILS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;June 3: Kick-off 10 AM PT&lt;/li&gt;
  &lt;li&gt;June 4 - June 15: Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;June 16 - June 17: Final Reviews&lt;/li&gt;
  &lt;li&gt;June 18: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Make sure to &lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-pytorch-docathon-june-3rd-18th-2025/&quot;&gt;RSVP&lt;/a&gt; to the event so you receive all the notifications and instructions on how to participate.&lt;/p&gt;

&lt;p&gt;Further details about the Docathon will be shared during the Kick-off call on June 3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Don’t forget to register for this year’s event: &lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-pytorch-docathon-june-3rd-18th-2025/&quot;&gt;RSVP now&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re thrilled to announce the 2025 PyTorch Docathon! This is a hackathon-style event aimed at enhancing PyTorch documentation with the support of the community. Documentation is a vital component of any technology, and by refining it, we can simplify the onboarding process for new users, help them effectively utilize PyTorch’s features, and ultimately speed up the transition from research to production in machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How IBM Research Uses PyTorch and TerraTorch to Make Geospatial Computer Vision Accessible for Everyone</title>
      <link href="https://pytorch.org/blog/how-ibm-uses-pt-terratorch/" rel="alternate" type="text/html" title="How IBM Research Uses PyTorch and TerraTorch to Make Geospatial Computer Vision Accessible for Everyone" />
      <published>2025-05-01T00:00:00-07:00</published>
      <updated>2025-05-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/how-ibm-uses-pt-terratorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/how-ibm-uses-pt-terratorch/">&lt;p&gt;Earth Observation-based analytics are becoming essential for understanding our planet — from monitoring deforestation to tracking urban development and analyzing the impacts of climate change. However, the coding and deep learning skills for applying AI models to satellite imagery and earth observation data has traditionally been a major barrier for many practitioners.&lt;/p&gt;

&lt;p&gt;By IBM Research’s launch of TerraTorch 1.0, a PyTorch domain library for fine-tuning of Geospatial Computer Vision Foundation Models, we make geospatial AI not only more accessible but also more practical for the wider PyTorch community. Our goal: simplify the process so that any data scientist, researcher, or enthusiast can build powerful geospatial models with ease and low GPU and data processing requirements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-ibm-uses-pt-terratorch/fg1.png&quot; alt=&quot;globes&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The power of foundation models, even with 75-95% of the input data removed, the models do a fantastic job in reconstruction of the input data - therefore learning the underlying physics of our planet in a deep, latent space&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-business-challenge&quot;&gt;The Business Challenge&lt;/h2&gt;

&lt;p&gt;Our goal was to remove the technical barriers that prevent people from working with satellite imagery, weather and climate data at scale. Together with NASA, we’ve developed the Prithvi family of foundation models. Integrating the latest innovations of AI research using the clean API PyTorch provides has facilitated the job.&lt;/p&gt;

&lt;p&gt;We wanted to create a framework that anyone can use to go from raw data to inference ready models in just a few steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-ibm-uses-pt-terratorch/fg2.png&quot; alt=&quot;globes&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How a weather and climate foundation model created and fine-tuned on PyTorch is used for weather forecasts&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-ibm-research-used-pytorch&quot;&gt;How IBM Research Used PyTorch&lt;/h2&gt;

&lt;p&gt;We’ve built TerraTorch on top of PyTorch, leveraging its dynamic ecosystem to integrate:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Lightning for clean, scalable training loops&lt;/li&gt;
  &lt;li&gt;TorchGeo for geospatial data handling and transformations (PyTorch transforms)&lt;/li&gt;
  &lt;li&gt;For foundation models like the leading generative multimodal foundation model &lt;a href=&quot;https://research.ibm.com/blog/terramind-esa-earth-observation-model&quot;&gt;‘Terramind’&lt;/a&gt;, co-developed by IBM and ESA, and &lt;a href=&quot;https://huggingface.co/ibm-nasa-geospatial&quot;&gt;the ‘Prithvi’ family&lt;/a&gt;, co-developed by IBM and NASA, TerraTorch has been used to fine-tune all of the downstream geospatial models for satellite imagery, weather and climate data. It includes the family of fine-tuned models that IBM has released as part of &lt;a href=&quot;https://huggingface.co/collections/ibm-granite/granite-geospatial-models-667dacfed21bdcf60a8bc982&quot;&gt;Granite&lt;/a&gt;. In addition, other interesting foundation models and ecosystem components like Clay, SatMAE, Satlas, DeCur and DOFA are included in TerraTorch.&lt;/li&gt;
  &lt;li&gt;Powerful and state-of-the-art vision transformers to experiment with modern neural network architectures&lt;/li&gt;
  &lt;li&gt;TerraTorch-Iterate build on top of PyTorch, Optuna, MLFlow and Ray Tune for Hyperparameter Optimization (HPO), Neural Architecture Search (NAS) and Foundation Model Benchmarking (GeoBench), where TerraTorch became the reference implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-ibm-uses-pt-terratorch/fg5.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The fine-tuning and inference process is completely described in a single YAML config file. There, the architectural building blocks of the model (backbone, neck, decoder, head) are defined. The Model Factory assembles the model using the build-in and custom registries. In addition, the Optimizer and Data Modules are created as defined in the config. Finally, everything is passed to the Lightning Trainer, who executes the task.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With PyTorch’s flexibility, we were able to prototype quickly, iterate on model architectures, and deploy pipelines for a range of geospatial applications — from flood and biomass detection to increasing resolution of climate data, where some of our our work became part of the &lt;a href=&quot;https://huggingface.co/collections/ibm-granite/granite-geospatial-models-667dacfed21bdcf60a8bc982&quot;&gt;IBM Granite Geospatial Model Family&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-ibm-uses-pt-terratorch/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Architecture of the Prithvi-EO-2.0-600M foundation model which IBM Research developed together with NASA&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;solving-ai-challenges-with-pytorch&quot;&gt;Solving AI Challenges with PyTorch&lt;/h2&gt;

&lt;p&gt;PyTorch helped us to tackle three major challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ease of experimentation: Dynamic computation graphs, automatic differentiation, full abstraction of CUDA and rich visualization tools made it simple to test different models and training strategies.&lt;/li&gt;
  &lt;li&gt;Scalability: With DDP, FSDP, PyTorch Lightning and TorchGeo, we could train models on large-scale datasets without worrying about infrastructure.&lt;/li&gt;
  &lt;li&gt;Community support: PyTorch - the de-facto standard in AI research - with its active community and excellent documentation made it easy to overcome hurdles and stay up to date with the latest advancements in AI research.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-word-from-ibm-research&quot;&gt;A Word from IBM Research&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;“PyTorch gave me the power to turn complex linear algebra and optimization problems into accessible, shareable solutions for the community. It feels empowering that we’re building and fine-tuning models for anyone curious about understanding our planet through AI.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;— Romeo Kienzler, AI Research Engineer at IBM Research Zurich, Rueschlikon&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-ibm-uses-pt-terratorch/fg4.png&quot; alt=&quot;quote&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-benefits-of-using-pytorch&quot;&gt;The Benefits of Using PyTorch&lt;/h2&gt;

&lt;p&gt;Using PyTorch allowed us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build a reproducible, open-source framework for fine-tuning geospatial foundation models&lt;/li&gt;
  &lt;li&gt;Share our work with the community through easy-to-follow notebooks, TerraTorch configuration files, tutorials and model checkpoints on HuggingFace&lt;/li&gt;
  &lt;li&gt;Rapidly iterate over foundation model architectures and deploy fine-tuned models for inference, from research to real-world client products&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learn-more&quot;&gt;Learn More&lt;/h2&gt;

&lt;p&gt;For more information about this project and to explore the code, visit:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IBM/terratorch&quot;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.ibm.com/blog/simplifying-geospatial-ai-with-terra-torch-1-0&quot;&gt;IBM Research: Simplifying Geospatial AI with TerraTorch 1.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IBM/terratorch/tree/main/examples/tutorials/PrithviEOv2&quot;&gt;TerraTorch PrithviEOv2 example notebooks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IBM/terramind/tree/main/notebooks&quot;&gt;TerraMind example notebooks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/IBM/terramind/blob/main/notebooks/terramind_v1_base_sen1floods11.ipynb&quot;&gt;Run TerraMind using TerraTorch on Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Earth Observation-based analytics are becoming essential for understanding our planet — from monitoring deforestation to tracking urban development and analyzing the impacts of climate change. However, the coding and deep learning skills for applying AI models to satellite imagery and earth observation data has traditionally been a major barrier for many practitioners.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">6x faster Async Checkpointing in PyTorch, using Cached Plans, no GIL contention</title>
      <link href="https://pytorch.org/blog/6x-faster-async-checkpointing/" rel="alternate" type="text/html" title="6x faster Async Checkpointing in PyTorch, using Cached Plans, no GIL contention" />
      <published>2025-04-30T00:00:00-07:00</published>
      <updated>2025-04-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/6x-faster-async-checkpointing</id>
      <content type="html" xml:base="https://pytorch.org/blog/6x-faster-async-checkpointing/">&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Meet Vadakkanchery, Saurabh Mishra, Ela Krepska, Hamid Shojanazeri, Pradeep Fernando&lt;br /&gt;
&lt;strong&gt;Crusoe&lt;/strong&gt;: Ethan Petersen, Martin Cala, Chip Smith&lt;/p&gt;

&lt;p&gt;PyTorch DCP (Distributed Checkpointing) has recently enabled new optimizations in asynchronous checkpointing to reduce GPU utilization drop by minimizing collective overhead and improving overall checkpointing efficiency.&lt;/p&gt;

&lt;p&gt;Using Crusoe’s 2K H200 cluster, with TorchTitan and training a Llama3-70B, we were able to verify these new features deliver substantial speedups at 1856 GPU scale, reducing the background processing time for async DCP checkpoints from ~436 seconds to ~67 seconds.&lt;/p&gt;

&lt;p&gt;This is roughly a 6.5x reduction in background checkpoint processing time, enabling even more total training time to proceed at full training throughput.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/6x-faster-async-checkpointing/fg1.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 1: 1856 training run with high frequency checkpointing.  The first checkpoint (drop down in tps) does not have a cached save plan, and the background processing takes far longer than the rest where the cached plan is used.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;background--what-is-asynchronous-checkpointing&quot;&gt;Background:  What is Asynchronous Checkpointing?&lt;/h2&gt;

&lt;p&gt;In a standard checkpointing workflow, GPUs are blocked while the checkpointing data is offloaded from GPU to CPU and then written to storage.  After the save to physical media is complete, training can resume.&lt;/p&gt;

&lt;p&gt;Asynchronous checkpointing greatly reduces this downtime by enabling the actual saving to storage to be done via CPU threads, allowing GPU-based training to continue while the checkpoint data is being persisted in parallel. It is used primarily for intermediate/fault tolerant checkpoints as it unblocks the GPUs much faster compared to the synchronous checkpoints. &lt;br /&gt;
For example, in our large-scale experiment, GPU training was blocked for less than a second (.78 seconds at 1856 scale) while checkpoint data was moved from GPU to CPU (staging). At that point, GPU training immediately continues, which is a substantial training time improvement over traditional checkpointing. For reference, Async Checkpointing is covered in more detail &lt;a href=&quot;https://pytorch.org/blog/reducing-checkpointing-times/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-asynchronous-checkpointing&quot;&gt;Challenges with Asynchronous Checkpointing&lt;/h2&gt;

&lt;p&gt;However, the background processing inherent in Asynchronous Checkpointing has additional challenges that result in a temporary reduction of training throughput while the storage phase is being completed.  These are highlighted below.&lt;/p&gt;

&lt;h3 id=&quot;gpu-utilization-drop-from-gil-contention&quot;&gt;GPU utilization drop from GIL contention:&lt;/h3&gt;

&lt;p&gt;The Global Interpreter Lock (GIL) in Python is a mechanism that prevents multiple native threads from executing Python bytecode at the same time. This lock is necessary mainly because CPython’s memory management is not thread-safe.&lt;/p&gt;

&lt;p&gt;DCP currently uses background threads for metadata collectives and uploading to storage. Although these expensive steps are done asynchronously, it leads to contention for the GIL with the trainer threads. This causes the GPU utilization (QPS) to suffer significantly and also increases the e2e upload latency. For large-scale checkpoints, the overhead of the CPU parallel processing has a suppressive effect on net GPU training speed since CPUs also drive the training process via GPU kernel launches.&lt;/p&gt;

&lt;p&gt;Please refer to the following figure from our experiments:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/6x-faster-async-checkpointing/fg2.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 2: One can see a sustained drop in training QPS even after staging (i.e. blocking operation to trainer) is complete.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The first dip in Figure 2 (marked by the purple line) indicates that staging is complete, and training can continue. However, a second drop is evident (marked by the area between the purple and yellow lines) which is due to trainer thread and checkpointing threads contending for the Python GIL, leading to degraded training QPS until the checkpoint thread completes execution.&lt;/p&gt;

&lt;h3 id=&quot;collective-communications-cost&quot;&gt;Collective communications cost:&lt;/h3&gt;

&lt;p&gt;DCP performs multiple collectives today for various reasons: dedupe, global metadata for the checkpoint, resharding, and distributed exception handling. Collectives are costly as these require network I/O and pickling/unpickling of the large metadata being sent across the GPU network. These collectives become extremely expensive as the job scale grows, leading to significantly higher e2e latency and potential for collective timeouts.&lt;/p&gt;

&lt;h2 id=&quot;solutions&quot;&gt;Solutions&lt;/h2&gt;

&lt;h3 id=&quot;process-based-async-checkpointing&quot;&gt;Process based async checkpointing&lt;/h3&gt;

&lt;p&gt;DCP now supports async checkpoint save via a background process. This helps avoid the training QPS drop by eliminating the python GIL contention with the trainer threads. Please see Fig 2 for checkpointing via threads and Fig 3 for checkpointing via background process.&lt;/p&gt;

&lt;h3 id=&quot;caching-of-the-save-plans&quot;&gt;Caching of the save plans&lt;/h3&gt;

&lt;p&gt;DCP has a clear boundary between the planning and storage I/O steps. SavePlanner in DCP is a stateful component which acts as an access proxy to the state_dict. Planner manages save plans prepared by individual ranks, which carry metadata information necessary to do the write I/O. The planning step involves a collective operation to gather a comprehensive view of the checkpoint on the coordinator rank. The coordinator rank is responsible for de-duplicating parameters/weights to eliminate redundancies, validating the global plan to ensure accuracy and consistency, and creating the global metadata structs. This is followed by a scatter collective where the coordinator rank assigns I/O tasks to each rank.  Any transformations done on the plans affect how the storage components finally write the data.&lt;/p&gt;

&lt;p&gt;During the course of a training job, multiple checkpoints are saved.  In the majority of these cases, only the checkpoint data changes between different save instances, and thus, the plan remains the same. This presented an opportunity for us to cache the plans, pay the planning cost only on the first save, and then amortize that cost across all the subsequent attempts. Only the updated plans (plans which changed in the next attempt) are sent via collective, thus reducing the collective overhead significantly.&lt;/p&gt;

&lt;h2 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Set up:&lt;/strong&gt; 1856 H200 GPUs, Llama3-70B, HSDP2 with TorchTitan&lt;/p&gt;

&lt;p&gt;After deploying both the solutions above, the following are the key results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TPS drop has significantly narrowed, with a peak dip to 372 vs 315 tps, and for a greatly reduced time window (~67 seconds vs ~437 seconds).  This time window is now mostly attributed to the blocking for CPU processing.&lt;/li&gt;
  &lt;li&gt;Subsequent checkpoint save attempts also continue to be much faster due to very low overhead at the planning stage. E2E latency is thus improved by over 6.5x. This will allow our partners to increase the checkpointing frequency and reduce the lost training progress (i.e. wasted training time).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you look at the very first downspike in Figure 1, this drawdown in GPU processing time takes training throughput from 700 down to 320 tps, and suppresses it for roughly 7 minutes (467 seconds).  Once the CPUs have finished processing, training continues again at full speed.&lt;/p&gt;

&lt;p&gt;Previously, this ~7 minute suppression would be repeated at &lt;em&gt;every&lt;/em&gt; checkpoint.  However, with the new process-based checkpointing feature, only the first checkpoint has the full drawdown time (mainly due to overhead from daemon process initialization), as all future checkpoints are executed via the background process, mitigating GIL contention with the trainer threads.&lt;/p&gt;

&lt;p&gt;This is visually shown in all the subsequent checkpoints where the average MFU suppression time drops to just over a minute, reflected by the sharp spikes that almost immediately revert to full MFU throughput.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/6x-faster-async-checkpointing/fg3.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 3: The red box shows the non-cached plan checkpoint, which also includes Checkpoint Background Init process overhead, while the purple box highlights the first checkpoint to run with the cached plan.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This means that even large-scale checkpointing, such as shown in Fig 2 at 1856 GPU scale, can be done with ~6x reduced training throughput impact.  This enables Asynchronous DCP checkpointing to be run more frequently (thus better rollback protection) while enhancing total training throughput relative to previous Async Checkpointing overhead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using DCP’s cached checkpointing:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This feature is already available as part of the PyTorch nightly builds, and you can test out PyTorch’s Asynchronous DCP checkpointing directly in TorchTitan.  Following are the instructions to enable these features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Process-based asynchronous checkpointing:
    &lt;ul&gt;
      &lt;li&gt;Set the &lt;strong&gt;async_checkpointer_type&lt;/strong&gt; to AsyncCheckpointerType.PROCESS in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/state_dict_saver.py#L193&quot;&gt;async_save&lt;/a&gt; API.  (&lt;em&gt;file&lt;/em&gt;: pytorch/torch/distributed/checkpoint/state_dict_saver.py)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Save plan caching:
    &lt;ul&gt;
      &lt;li&gt;Set the &lt;strong&gt;enable_plan_caching&lt;/strong&gt; flag to true in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/default_planner.py#L78C9-L78C28&quot;&gt;DefaultSavePlanner&lt;/a&gt;. (&lt;em&gt;file&lt;/em&gt;:  pytorch/torch/distributed/checkpoint/default_planner.py)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;DCP will be rolling out additional optimizations to further improve the checkpointing cost. Currently even though the save plans are cached, coordinator rank still prepares the metadata. For larger jobs and models with many tensors, this overhead is non-trivial. In the next iteration, DCP will eliminate the metadata overhead and improve the e2e latency further. DCP will also introduce additional optimizations, such as zero-overhead checkpointing, to enable efficient checkpointing in large-scale jobs.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta and Crusoe</name>
        
        
      </author>

      

      

      
        <summary type="html">Meta: Less Wright, Meet Vadakkanchery, Saurabh Mishra, Ela Krepska, Hamid Shojanazeri, Pradeep Fernando Crusoe: Ethan Petersen, Martin Cala, Chip Smith</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FlexAttention Part II: FlexAttention for Inference</title>
      <link href="https://pytorch.org/blog/flexattention-for-inference/" rel="alternate" type="text/html" title="FlexAttention Part II: FlexAttention for Inference" />
      <published>2025-04-30T00:00:00-07:00</published>
      <updated>2025-04-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flexattention-for-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/flexattention-for-inference/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;In PyTorch 2.5.0 release, we introduced &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.attention.flex_attention&lt;/code&gt; for ML researchers who’d like to customize their attention kernels without writing kernel code. This blog introduces our decoding backend optimized for inference, supporting GQA and PagedAttention, along with feature updates including nested jagged tensor support, performance tuning guides and trainable biases support.&lt;/p&gt;

&lt;p&gt;If you’re looking for an easy way to play around with FlexAttention in your post-training / inference pipeline, PyTorch native post-training library &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;torchtune&lt;/a&gt; and inference codebase &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;gpt-fast&lt;/a&gt; already have FlexAttention integrated. Try it out!&lt;/p&gt;

&lt;p&gt;We are excited to share that our paper on FlexAttention has been accepted for presentation at the MLSys2025 Conference held from May 12-15th in Santa Clara, California.&lt;/p&gt;

&lt;p&gt;Title: &lt;strong&gt;FlexAttention: A Programming Model for Generating Optimized Attention Kernels.&lt;/strong&gt; &lt;a href=&quot;https://mlsys.org/virtual/2025/poster/3007&quot;&gt;Poster&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;flexattention-for-inference&quot;&gt;FlexAttention for Inference&lt;/h2&gt;

&lt;p&gt;TL;DR: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; lowers &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flex_attention&lt;/code&gt; to a fused &lt;a href=&quot;https://pytorch.org/blog/flash-decoding/&quot;&gt;FlashDecoding&lt;/a&gt; kernel when it runs on a very short query.&lt;/p&gt;

&lt;p&gt;One fused attention kernel does not suit all – especially in long-context LLM inference.&lt;/p&gt;

&lt;p&gt;The decoding phase of LLM inference is an iterative process: tokens are generated one at a time, requiring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; forward passes to generate an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;-token sentence. Fortunately, each iteration doesn’t need to recompute self-attention over the full sentence — previously calculated tokens are cached, therefore we only need to attend the newly generated token to the cached context.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg1.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This results in a unique attention pattern where a short query sequence (1 token) attends to a long key-value cache (context length up to 128k). Traditional optimizations for square attention kernels (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q_len ≈ kv_len&lt;/code&gt;) don’t directly apply here. This pattern poses new challenges for GPU memory utilization and occupancy. We build a dedicated FlexDecoding backend optimized for long-context LLM inference incorporating decoding-specific techniques from &lt;a href=&quot;https://pytorch.org/blog/flash-decoding/&quot;&gt;FlashDecoding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FlexDecoding is implemented as an alternative backend for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.attention.flex_attention &lt;/code&gt;operator. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flex_attention&lt;/code&gt; automatically switches to the FlexDecoding backend for its JIT compilation when given a short query and a long KV cache. If the input shape changes significantly, for example transitioning from the prefill phase to decoding, JIT recompilation generates a separate kernel for each scenario.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;flex_attention = torch.compile(flex_attention)

k_cache = torch.random(B, H, 16384, D) 
v_cache = torch.random(B, H, 16384, D)

...

# Prefill Phase: query shape = [B, H, 8000, D]
flex_attention(q_prefill, k_cache, v_cache, ...) # Uses FlexAttention backend optimized for prefill &amp;amp; training

# Decoding Phase: q_last_token shape = [B, H, 1, D]
flex_attention(q_last_token  , k_cache, v_cache, ...) # Recompiles with the FlexDecoding backend 

# decode 2 tokens at the same time: q_last_2_tokens shape = [B, H, 2, D]
flex_attention(q_last_2_tokens, k_cache, v_cache, ...) # No recompilation needed! Runs the decoding kernel again.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;working-with-kv-cache&quot;&gt;Working with KV Cache&lt;/h2&gt;

&lt;p&gt;One of the key optimizations for efficient inference is maintaining a preallocated KV cache that updates &lt;strong&gt;in place&lt;/strong&gt; as new tokens are generated. Instead of enforcing a specific KV cache policy with a dedicated API, FlexDecoding allows users to define and manage the KV cache themselves.&lt;/p&gt;

&lt;p&gt;Similar to FlexAttention, FlexDecoding takes user-defined &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; functions. These functions modify attention scores before the softmax operation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg2.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;score_mod(score, b, h, q_idx, kv_idx) -&amp;gt; tensor # return updated score
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Score is a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments specify which score is being computed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; batch index&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt; attention head index&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q_idx&lt;/code&gt; token position in query tensor&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kv_idx&lt;/code&gt; token position in key/value tensor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the decoding phase, previously calculated tokens are cached, and only the latest generated token (i-th) is used as the query. A naive causal mask on this one token query looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def causal(score, b, h, q_idx, kv_idx):
    return torch.where(q_idx &amp;gt;= kv_idx, score, -float(&quot;inf&quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg3.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is problematic: the new token “&lt;em&gt;saw&lt;/em&gt;” should attend to all previously generated tokens i.e. “&lt;em&gt;The cat sat on the mat and saw&lt;/em&gt;”, not just the first entry in the kv cache. To correct this, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; needs to &lt;strong&gt;offset &lt;code&gt;q_idx&lt;/code&gt;&lt;/strong&gt; &lt;strong&gt;by &lt;code&gt;i &lt;/code&gt;&lt;/strong&gt;for accurate decoding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg4.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Creating a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;  for each token to accommodate the offset  is slow since it means FlexAttention needs to be recompiled every iteration for a different &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt;. Instead,&lt;/p&gt;

&lt;p&gt;We define this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; as a tensor and increment its value at each iteration:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;offset = torch.tensor(i, &quot;cuda&quot;)
def causal_w_offset(score, b, h, q_idx, kv_idx):
    return torch.where(q_idx + offset &amp;gt;= kv_idx, score, -float(&quot;inf&quot;))

# Attend the i-th token
flex_attention(..., score_mod=causal_w_offset  ) # Compiles the kernel here 
...
# Attend the i+1-th token
offset = offset + 1 # Increment offset
flex_attention(..., score_mod=causal_w_offset ) # Doesn't need to recompile! 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notably, here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; becomes a captured tensor and it does not need to recompile if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; changes values.&lt;/p&gt;

&lt;p&gt;Manually rewriting your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; for offset handling isn’t necessary. We can automate this process with a generic rewriter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;offset = torch.tensor(i, &quot;cuda&quot;)

def get_score_mod_w_offset(score_mod: _score_mod_signature, _offset: tensor):
    def _score_mod(score, b, h, q, kv):
        return score_mod(score, b, h, q + _offset, kv)
    return _score_mod

def get_mask_mod_w_offset(mask_mod: _mask_mod_signature, _offset: tensor):
    def _mask_mod(b, h, q, kv):
        return mask_mod(b, h, q + _offset, kv)
    return _mask_mod

causal_w_offset = get_score_mod_w_offset(causal, offset)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;blockmask-for-inference&quot;&gt;BlockMask for Inference&lt;/h2&gt;

&lt;p&gt;We can also use BlockMask with inference to leverage mask sparsity. The idea is to precompute the BlockMask once during model setup and use slices of it during decoding&lt;/p&gt;

&lt;h3 id=&quot;precomputing-blockmask&quot;&gt;Precomputing BlockMask&lt;/h3&gt;

&lt;p&gt;During setup, we create a squared BlockMask for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MAX_SEQ_LEN x MAX_SEQ_LEN&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.nn.attention.flex_attention import create_block_mask

def causal_mask(b, h, q_idx, kv_idx):
    return q_idx &amp;gt;= kv_idx

block_mask = create_block_mask(causal_mask, B=None, H=None, Q_LEN=MAX_SEQ_LEN,KV_LEN=MAX_SEQ_LEN)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg5.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;using-blockmask-during-decoding&quot;&gt;Using BlockMask During Decoding&lt;/h3&gt;

&lt;p&gt;For the i-th token, we use a slice of the mask:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;block_offset = i // block_mask.BLOCK_SIZE[0]
block_mask_slice = block_mask[:, :, block_offset]

# don't forget to use the mask_mod with offset! 
block_mask_slice.mask_mod = get_mask_mod_w_offset(causal_mask)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg6.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg7.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FlexDecoding kernel performs on par with FlashDecoding (FAKV) and significantly outperforms pytorch scaled_dot_product_attention (&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/benchmarks/transformer/score_mod.py&quot;&gt;code&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg8.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FlexDecoding boosts LLaMa3.1-8B serving performance by 1.22x-2.04x, and LLaMa3.1-70B performance by 0.99x - 1.66x compared to SDPA in gpt-fast. (&lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;code&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;paged-attention&quot;&gt;Paged Attention&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.vllm.ai/2023/06/20/vllm.html&quot;&gt;vLLM&lt;/a&gt; is one of the popular LLM serving engines, powered by the efficient memory management from PagedAttention. Existing &lt;a href=&quot;https://github.com/vllm-project/vllm/blob/main/csrc/attention/paged_attention_v2.cu&quot;&gt;PagedAttention&lt;/a&gt; implementation requires dedicated CUDA kernels and shows limited flexibility on supporting emerging attention variants. In this section, we present a PT2-native PagedAttention implementation that is enabled by flex attention and torch.compile.&lt;/p&gt;

&lt;p&gt;PagedAttention scatters KV cache to reduce memory fragmentation and support higher batch sizes. Without PagedAttention, KV cache from the same request are stored in a contiguous memory, requiring 2 tensor of shape &lt;em&gt;B x H x KV LEN x D&lt;/em&gt;. We call it a logical KV cache. Here, KV_LEN is the maximum sequence length over all requests in a batch. Considering the Figure 1(a), KV_LEN is 9 thus all requests must be padded to 9 tokens, leading to large memory waste. With PagedAttention, we can chunk each request into multiple pages of the same size page_size and scatter these pages into a physical KV cache of shape  &lt;em&gt;1 x H x max seq len x D&lt;/em&gt;, where max_seq_len=n_pages x page_size. This avoids padding requests to the same length and saves memory. Specifically, we provide an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assign&lt;/code&gt; API to update KV cache via index computations:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def assign(
    batch_idx: torch.Tensor,
    input_pos: torch.Tensor,
    k_val: torch.Tensor,
    v_val: torch.Tensor,
    k_cache: torch.Tensor,
    v_cache: torch.Tensor,
) -&amp;gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Behind this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assign&lt;/code&gt; API is a page table, a tensor mapping logical KV cache to physical KV cache:&lt;/p&gt;

&lt;p&gt;[batch_idx, logical_page_idx] -&amp;gt; physical_page_idx&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assign&lt;/code&gt; takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k_val&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v_val&lt;/code&gt; and scatters to physical KV cache guided by the mapping from the page table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg9.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paged Attention with Page Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A natural question is, how to integrate PagedAttention with flex attention to support diverse attention variants? A naive idea is to materialize the logical KV cache before computing with flex attention. But this leads to redundant memory copy and bad performance. Another idea is to build a dedicated CUDA or Triton kernel for paged attention, similar to &lt;a href=&quot;https://github.com/vllm-project/vllm/blob/main/csrc/attention/paged_attention_v2.cu&quot;&gt;existing PagedAttention implementation&lt;/a&gt;. However, this adds much manual effort and code complexity.&lt;/p&gt;

&lt;p&gt;Instead, we design a fused indirect memory access by converting a logical block mask according to the page table. In FlexAttention, we exploit BlockMask to identify logical blocks and skip redundant computation. While Paged Attention adds an extra layer of indirect memory access, we can further convert the logical block mask to the physical block mask corresponding to the page table, as illustrated in Figure 2. Our PagedAttention implementation provides a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;convert_logical_block_mask&lt;/code&gt; via torch.gather calls:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def convert_logical_block_mask(
    block_mask: BlockMask,
    batch_idx: Optional[torch.Tensor] = None,
) -&amp;gt; BlockMask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg10.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paged Attention via Block Mask Conversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One remaining question is how to rewrite user-specified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; for PagedAttention. When users specify these modifications, they write with logical indices without the knowledge of the page table maintained at runtime. The following code shows an automated conversion at runtime which is necessary to rewrite user-specified modifications with physical kv indices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new_mask_mod&lt;/code&gt; would take the physical_kv_idx and convert it back to the logical_kv_idx and apply user-specified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; on the logical_kv_idx for the correct mask. For efficiency, we maintain physical_to_logical as a mapping from physical_kv_block to logical_kv_block to facilitate the conversion. For correctness, we mask out-of-boundary blocks as False with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.where&lt;/code&gt; call. After batching logical KV caches from multiple requests into the same physical KV cache, there are much more physical blocks than the number of logical blocks for each request. Thus, a physical block may not have a corresponding logical block for a specific request during block mask conversion. By masking as False with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.where&lt;/code&gt;, we can ensure the correctness that data from different requests do not interfere with each other. Similarly, we can convert the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/nn/attention/experimental/_paged_attention.py#L308-L338&quot;&gt;score_mod&lt;/a&gt; automatically.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def get_mask_mod(mask_mod: Optional[_mask_mod_signature]) -&amp;gt; _mask_mod_signature:
    if mask_mod is None:
        mask_mod = noop_mask

    def new_mask_mod(
        b: torch.Tensor,
        h: torch.Tensor,
        q_idx: torch.Tensor,
        physical_kv_idx: torch.Tensor,
    ):
        physical_kv_block = physical_kv_idx // page_size
        physical_kv_offset = physical_kv_idx % page_size
        logical_block_idx = physical_to_logical[b, physical_kv_block]
        logical_kv_idx = logical_block_idx * page_size + physical_kv_offset
        return torch.where(
            logical_block_idx &amp;gt;= 0, mask_mod(b, h, q_idx, logical_kv_idx), False
        )

    return new_mask_mod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Figure 3 demonstrates the latency from Paged Attention (&lt;a href=&quot;https://github.com/pytorch-labs/attention-gym/blob/main/attn_gym/paged_attention/latency.py&quot;&gt;code&lt;/a&gt;). Overall, there is less than 5% overhead from Flex Attention with Paged Attention, compared with Flex Attention only. We also observe an on-par performance with Flash Attention v2. A &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym/blob/main/attn_gym/paged_attention/throughput.py&quot;&gt;minimal serving example&lt;/a&gt; further shows that PagedAttention can support 76x higher batch size when evaluating on &lt;a href=&quot;https://huggingface.co/datasets/Open-Orca/OpenOrca&quot;&gt;OpenOrca dataset&lt;/a&gt; which includes 1M GPT-4 completions and 3.2M GPT-3.5 completions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flexattention-for-inference/fg11.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paged Attention: Latency under diverse sequence length&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;ragged-input-sequences-with-nested-jagged-tensors-njts&quot;&gt;Ragged input sequences with Nested Jagged Tensors (NJTs)&lt;/h2&gt;

&lt;p&gt;FlexAttention now supports ragged-sized input sequences through the use of Nested Jagged Tensors (NJTs). NJTs represent ragged-sized sequences by packing sequences into a single “stacked sequence” and maintaining a set of offsets delimiting sequence boundaries for each batch item.&lt;/p&gt;

&lt;p&gt;A block mask can be created for input NJTs through the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_nested_block_mask()&lt;/code&gt; API. The returned block mask is compatible with the ragged structure of the given NJT, treating it as a single “stacked sequence” with inter-sequence attention automatically masked out. The mask_mod or score_mod function can be written as usual.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.nn.attention.flex_attention import create_nested_block_mask, flex_attention

BATCH = 8
NUM_HEADS = 8
D = 16
device = &quot;cuda&quot;

# Input NJTs of shape (BATCH, SEQ_LEN*, D) with ragged SEQ_LEN
sequence_lengths = [torch.randint(5, 30, ()).item() for _ in range(BATCH)]
query = torch.nested.nested_tensor([
    torch.randn(seq_len, NUM_HEADS * D, device=device)
    for seq_len in sequence_lengths
], layout=torch.jagged)
key = torch.randn_like(query)
value = torch.randn_like(query)

# View as shape (BATCH, NUM_HEADS, SEQ_LEN*, HEAD_DIM)
query = query.unflatten(-1, [NUM_HEADS, D]).transpose(1, 2)
key = key.unflatten(-1, [NUM_HEADS, D]).transpose(1, 2)
value = value.unflatten(-1, [NUM_HEADS, D]).transpose(1, 2)

# Simple causal mask
def my_mask_mod(b, h, q_idx, kv_idx):
    return q_idx &amp;gt;= kv_idx

# Construct a block mask using the ragged structure of the
# specified query NJT. Ragged-sized sequences are treated as a single
# &quot;stacked sequence&quot; with inter-sequence attention masked out.
block_mask = create_nested_block_mask(my_mask_mod, 1, 1, query)

# For cross attention, create_nested_block_mask() also supports a
# rectangular block mask using the ragged structures of both query / key.
#block_mask = create_nested_block_mask(my_mask_mod, 1, 1, query, key)

output = flex_attention(query, key, value, block_mask=block_mask)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;trainable-biases&quot;&gt;Trainable Biases&lt;/h2&gt;

&lt;p&gt;FlexAttention now supports trainable parameters in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod functions.&lt;/code&gt; This feature enables users to reference tensors that require gradients within their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; implementations, with gradients automatically backpropagating through these parameters during training.&lt;/p&gt;

&lt;h3 id=&quot;memory-efficient-gradient-accumulation&quot;&gt;Memory-Efficient Gradient Accumulation&lt;/h3&gt;

&lt;p&gt;Instead of materializing the full attention scores matrix, FlexAttention uses atomic additions (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tl.atomic_add&lt;/code&gt;) to accumulate gradients. This approach significantly reduces memory usage at the cost of introducing some non-determinism in gradient calculations.&lt;/p&gt;

&lt;h3 id=&quot;handling-broadcasted-operations&quot;&gt;Handling Broadcasted Operations&lt;/h3&gt;

&lt;p&gt;Broadcasting operations in the forward pass (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score + bias[h]&lt;/code&gt;) require special consideration in the backward pass. When broadcasting a tensor across multiple attention scores within a head or other dimensions, we need to reduce these gradients back to the original tensor shape. Rather than materializing the full attention score matrix to perform this reduction, we use atomic operations. While this incurs some runtime overhead, it allows us to maintain memory efficiency by avoiding the materialization of large intermediate tensors.&lt;/p&gt;

&lt;h3 id=&quot;current-limitations&quot;&gt;Current Limitations&lt;/h3&gt;

&lt;p&gt;The implementation currently allows only a single read from each input tensor in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; function. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bias[q_idx] + bias[kv_idx]&lt;/code&gt; would not be supported as it reads from the same tensor twice. We hope to remove this restriction in the future.&lt;/p&gt;

&lt;h3 id=&quot;simple-example&quot;&gt;Simple Example:&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bias = torch.randn(num_heads, requires_grad=True)
def score_mod(score, b, h, q_idx, kv_idx):
    return score + bias[h]  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance-tuning-for-flexattention&quot;&gt;Performance Tuning for FlexAttention&lt;/h2&gt;

&lt;h3 id=&quot;tldr&quot;&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;For optimal performance, compile FlexAttention using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt;, especially when dealing with complex &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mods&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mods&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;flex_attention = torch.compile(flex_attention, dynamic=True, mode=’max-autotune’)&lt;/p&gt;

&lt;h3 id=&quot;what-is-max-autotune&quot;&gt;What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt;?&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; mode in which TorchInductor sweeps many kernel parameters (e.g., tile size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_stages&lt;/code&gt;) and selects the best-performing configuration. This process allows kernels to test both successful and failing configurations without issues, and find the best viable configuration.&lt;/p&gt;

&lt;p&gt;While compilation takes longer with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt;, the optimal configuration is cached for future kernel executions.&lt;/p&gt;

&lt;p&gt;Here’s an example of FlexAttention compiled with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;triton_flex_attention_backward_7 0.2528 ms 100.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=32, BLOCK_N1=32, BLOCK_N2=32, FLOAT32_PRECISION=&quot;'ieee'&quot;, GQA_SHARED_HEADS=7, HAS_FULL_BLOCKS=False, IS_DIVISIBLE=False, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=128, ROWS_GUARANTEED_SAFE=False, SM_SCALE=0.08838834764831843, SPARSE_KV_BLOCK_SIZE=1073741824, SPARSE_Q_BLOCK_SIZE=1073741824, V_HEAD_DIM=128, num_stages=4, num_warps=4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;why-use-max-autotune-for-flexattention&quot;&gt;Why Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt; for FlexAttention?&lt;/h3&gt;

&lt;p&gt;The amount of shared memory utilized in FlexAttention depends on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score_mod&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask_mod&lt;/code&gt; methods. This variability means that the preconfigured default kernel parameters may lead to performance cliffs or even out of shared memory** **errors on certain hardware for some masks/mods.&lt;/p&gt;

&lt;p&gt;For instance, with document masks, default configurations can halve GPU occupancy, reducing performance to ~75% of its potential on some GPUs. To avoid such issues, we strongly recommend enabling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max-autotune&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;updates-and-enhancements&quot;&gt;Updates and Enhancements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Now available as a prototype feature in PyTorch 2.5.0&lt;/li&gt;
  &lt;li&gt;Fixed critical correctness issues, including a bug affecting multiple calls to FlexAttention within the same call to torch.compile&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;expanded-architecture-support&quot;&gt;Expanded Architecture Support&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Arbitrary sequence length support - no longer requires multiples of 128&lt;/li&gt;
  &lt;li&gt;Added native grouped-query attention (GQA) support via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_gqa=True&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Enhanced dimension flexibility:
    &lt;ul&gt;
      &lt;li&gt;Different QK and V head dimensions&lt;/li&gt;
      &lt;li&gt;Non-power-of-two head dimensions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trainable attention biases (prototype)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;New fused CPU backend&lt;/li&gt;
  &lt;li&gt;Improved TF32 handling for float32 inputs&lt;/li&gt;
  &lt;li&gt;Resolved various dynamic shape issues&lt;/li&gt;
  &lt;li&gt;Output layout matching query strides&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These updates make FlexAttention more robust and flexible while maintaining its core promise of combining PyTorch’s ease of use with FlashAttention’s performance benefits.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Joy Dong, Boyuan Feng, Driss Guessous, Joel Schlosser, Yanbo Liang, Horace He</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Foundation Expands to an Umbrella Foundation to Accelerate AI Innovation</title>
      <link href="https://pytorch.org/blog/pt-foundation-expands/" rel="alternate" type="text/html" title="PyTorch Foundation Expands to an Umbrella Foundation to Accelerate AI Innovation" />
      <published>2025-04-29T00:00:00-07:00</published>
      <updated>2025-04-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-foundation-expands</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-foundation-expands/">&lt;p&gt;Today, I am thrilled to announce a significant milestone for the PyTorch Foundation: we are expanding our scope to become an umbrella foundation, allowing us to host additional projects. This expansion positions the PyTorch Foundation to foster a broader ecosystem of high-value, trusted, and innovative AI projects that cater to all stages of the AI lifecycle—from training and inference to industry-specific applications.&lt;/p&gt;

&lt;h2 id=&quot;why-expand&quot;&gt;Why Expand?&lt;/h2&gt;

&lt;p&gt;Since its inception at the Linux Foundation two and a half years ago, the PyTorch Foundation has rapidly grown, now encompassing over 30 member organizations and 120 vibrant ecosystem projects. PyTorch itself has become the framework of choice for AI researchers, practitioners, and industry leaders worldwide. Our flagship PyTorch Conference has seen attendance multiply sixfold over just two years, reflecting the community’s tremendous enthusiasm and engagement.&lt;/p&gt;

&lt;p&gt;With new initiatives such as PyTorch Day events, global community meetups, the PyTorch Ambassador Program, Open Source Program Office (OSPO) outreach, the Speaker’s Bureau, and our upcoming training and certification programs, we have significantly deepened our community’s expertise and collaboration capabilities. To sustain and accelerate this momentum, the logical next step was to expand the PyTorch Foundation into an umbrella organization.&lt;/p&gt;

&lt;h2 id=&quot;what-does-an-umbrella-foundation-mean&quot;&gt;What Does an Umbrella Foundation Mean?&lt;/h2&gt;

&lt;p&gt;By transitioning into an umbrella foundation, PyTorch will now host a range of diverse, high-quality AI and ML projects beyond PyTorch Core. These include foundation-hosted projects in two categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Platform Projects&lt;/strong&gt;: Domain-agnostic solutions essential across various stages of the AI lifecycle, such as training, inference, model optimization, and deployment as well as agentic systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vertical Projects&lt;/strong&gt;: Domain-specific projects tailored to particular industries or applications, such as biomedical imaging, protein folding, and geospatial analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Projects under our umbrella gain immediate access to vendor-neutral governance, enhanced visibility, increased funding opportunities, and robust community engagement and support.&lt;/p&gt;

&lt;h2 id=&quot;foundation-hosted-vs-ecosystem-projects&quot;&gt;Foundation-Hosted vs. Ecosystem Projects&lt;/h2&gt;

&lt;p&gt;As we expand, it’s important to clarify the distinction between foundation-hosted and ecosystem projects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Foundation-Hosted Projects&lt;/strong&gt; are projects that fall under the umbrella, they are officially governed and administered under the PyTorch Foundation’s neutral and transparent governance model. Project maintainers continue to oversee their project, and they transfer assets to the Linux Foundation for independent stewardship and adopt an open governance model significantly reducing vendor bias and encouraging broader community contributions and adoption. These projects have greater stability and longevity and integrate with the larger PyTorch community.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ecosystem Projects&lt;/strong&gt; remain independently managed but receive recognition and increased visibility by aligning themselves closely with the PyTorch Foundation community standards. These projects meet specific quality and maturity criteria but retain full independence in governance and asset management.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-join-the-pytorch-ecosystem-or-become-a-foundation-hosted-project&quot;&gt;How to Join the PyTorch Ecosystem or Become a Foundation-Hosted Project&lt;/h2&gt;

&lt;p&gt;We have clearly defined pathways for projects looking to become part of the PyTorch community:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/ecosystem&quot;&gt;Ecosystem Project Status&lt;/a&gt;&lt;/strong&gt;: Projects must meet defined criteria, such as active development, comprehensive documentation, CI/CD infrastructure, clear governance, and community engagement. Approved ecosystem projects benefit from increased exposure and official recognition on the &lt;a href=&quot;https://landscape.pytorch.org/&quot;&gt;PyTorch Landscape&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/foundation-hosted&quot;&gt;Candidate Project Status&lt;/a&gt;&lt;/strong&gt;: Ecosystem projects aspiring to foundation-hosted status can become candidates by securing sponsorship from a PyTorch Foundation &lt;a href=&quot;/tac&quot;&gt;Technical Advisory Council (TAC)&lt;/a&gt; voting member. Candidates receive guidance on meeting all necessary governance, technical, and strategic criteria.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-fdn/foundation-hosted&quot;&gt;Foundation-Hosted Project Status&lt;/a&gt;&lt;/strong&gt;: Candidate projects demonstrating high maturity, stability, multi-platform support, security best practices, and strategic value to the PyTorch community can be approved by the TAC. These projects gain extensive benefits, including neutral trademark hosting, foundation support, marketing and events resources, governance guidance, and strategic funding opportunities.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ensuring-long-term-success-and-innovation&quot;&gt;Ensuring Long-Term Success and Innovation&lt;/h2&gt;

&lt;p&gt;By expanding our scope to become an umbrella foundation, the PyTorch Foundation is uniquely positioned to enhance collaboration, innovation, and sustained growth across the entire AI community. Our mission is clear: create a vendor-neutral, open source environment where the best AI and ML tools can thrive, benefiting users, contributors, and industry stakeholders worldwide.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“PyTorch is absolutely the foundation of the innovation happening in AI today and with projects like Llama, ChatGPT, and hundreds of thousands of open projects built on PyTorch, it has cemented itself as a critical ingredient to the world of AI. This move to create an umbrella foundation enables PyTorch to significantly expand its ecosystem both horizontally and vertically in this new era of agentic systems. I am very excited about this opportunity to take the PyTorch community to the next level!” - Joe Spisak, Product Director for PyTorch at Meta.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“PyTorch sits at the very core of AI today. Meanwhile, the depth of the AI stack has grown dramatically—evolving from enabling accelerated compute to powering fully autonomous systems. Broadening the PyTorch Foundation is a key step in keeping the AI revolution open and accessible to all, across the stack and aligned with the principles PyTorch was built on.” - Luca Antiga, CTO at Lightning AI.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We are incredibly optimistic about the opportunities ahead and excited to welcome new projects into our growing family. The PyTorch Foundation remains deeply committed to driving AI innovation forward, and together, we will continue to build the future of open source artificial intelligence.&lt;/p&gt;

&lt;p&gt;Stay tuned for more updates, announcements, and opportunities to participate!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matt White, Executive Director, PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, I am thrilled to announce a significant milestone for the PyTorch Foundation: we are expanding our scope to become an umbrella foundation, allowing us to host additional projects. This expansion positions the PyTorch Foundation to foster a broader ecosystem of high-value, trusted, and innovative AI projects that cater to all stages of the AI lifecycle—from training and inference to industry-specific applications.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Large Scale Training and Convergence with PyTorch Float8 Rowwise on Crusoe 2K H200s</title>
      <link href="https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe/" rel="alternate" type="text/html" title="Accelerating Large Scale Training and Convergence with PyTorch Float8 Rowwise on Crusoe 2K H200s" />
      <published>2025-04-28T00:00:00-07:00</published>
      <updated>2025-04-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-training-float8-rowwise-crusoe/">&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;: Less Wright, Hamid Shojanazeri, Vasiliy Kuznetsov, Daniel Vega-Myhre, Gokul Nadathur, Will Constable, Tianyu Liu, Tristan Rice, Driss Guessous, Josh Fromm, Luca Wehrstedt, Jiecao Yu
&lt;strong&gt;Crusoe&lt;/strong&gt;: Ethan Petersen, Martin Cala, Chip Smith&lt;/p&gt;

&lt;p&gt;Working with &lt;a href=&quot;http://Crusoe.AI&quot;&gt;Crusoe.AI&lt;/a&gt; we were provided access to one of their new 2K H200 clusters in Iceland, which enabled us to showcase training accelerations of 34 - 43% at scale by leveraging TorchTitan’s HSDP2 and TorchAO’s new float8 rowwise, with comparable convergence and stability vs BF16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg1.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this post we detail the synergy of H200’s with PyTorch’s new Float8 rowwise training with TorchTitan’s FSDP2/HSDP2 and CP at scale.&lt;/p&gt;

&lt;h2 id=&quot;background---what-is-an-h200&quot;&gt;Background - what is an H200?&lt;/h2&gt;

&lt;p&gt;H200’s are an ‘enhanced’ H100, offering the exact same compute as an H100, but with two additional improvements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Larger global memory, 141GiB HBM3e vs the standard 80GiB HBM3&lt;/li&gt;
  &lt;li&gt;Memory bandwidth is ~43% faster with 4.8TB/s vs 3.35 TB/s.  The faster memory transfer has an outsized effect on training speed, especially for PyTorch’s AsyncTP.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-pytorch-float8-rowwise&quot;&gt;What is PyTorch Float8 rowwise?&lt;/h2&gt;

&lt;p&gt;Float 8 Rowwise is a finer grained resolution for Float8 vs the previous ‘tensor wise’ Float8.  It is designed to ensure finer grained accuracy to support larger workloads that tend to become more sensitive to quantization at scale and as training progresses.&lt;/p&gt;

&lt;p&gt;There are two key improvements with Float8 rowwise:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each row now maintains its own scaling factor versus a single scaling factor for the entire tensor, thus improving quantization precision.  Finer grained scaling per row helps reduce the effect of outliers (extreme values that force the quantization scaling factor to stretch and degrade the precision of the normally distributed values) and thus ensures better precision.&lt;/li&gt;
  &lt;li&gt;The scaling factor itself is now implemented by rounding down to the nearest power of 2. This has been shown to help reduce quantization errors when multiplying/dividing by the scaling factor as well as ensuring large values remain scaled to the same value in both the forward and backward passes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that other large scale models have been trained using Float8 at 2K scale with a combination of 1x128 groupwise and 128x128 blockwise, with power of 2 scaling factors.  They had the same goal of improving Float8’s precision for supporting large scale training.&lt;/p&gt;

&lt;p&gt;Thus, Float8 rowwise offers a similar promise to enable Float8 for very large scale training, but we wanted to provide proof of stability and convergence at scale, which training on the Crusoe H200 2k cluster provided initial verification thereof.&lt;/p&gt;

&lt;h2 id=&quot;showcasing-float8-rowwise-loss-convergence-vs-bf16-at-1600-and-1920-gpu-scale&quot;&gt;Showcasing Float8 Rowwise Loss convergence vs BF16 at 1600 and 1920 GPU Scale:&lt;/h2&gt;

&lt;p&gt;In order to verify comparable loss convergence, we ran two separate runs at both 1920 and then 1600 (1.6k) gpu scale using TorchTitan and Lllama3 70B.  The 1.6K GPU runs were set for 2.5k iterations, using TorchTitans’ HSDP2 and Context Parallel to enable 2D parallelism.&lt;/p&gt;

&lt;p&gt;The loss convergence tests were run using Titan’s deterministic mode - this mode effectively freezes most potential sources of variation from run to run, and thus helps ensure that the only substantial change is what we want to test, namely the loss convergence and loss curves of BF16 vs Float8 Rowwise.&lt;/p&gt;

&lt;p&gt;Note that deterministic mode also slows down training speed because various kernels will not be autotuned to maximize throughput (otherwise we risk using different kernels between runs and introducing variance).&lt;/p&gt;

&lt;p&gt;Two runs were completed, one with BF16 and the other with Float8 Rowwise.&lt;/p&gt;

&lt;p&gt;Both runs completed their assigned 2.5k iters without issue, showcasing the Crusoe cluster stability, with FP8 completing at exactly 24 hours and BF16 finishing after 31 hours, 19 minutes.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;DType
   &lt;/td&gt;
   &lt;td&gt;Time / Iters
   &lt;/td&gt;
   &lt;td&gt;Loss
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BF16
   &lt;/td&gt;
   &lt;td&gt;24 hours
   &lt;/td&gt;
   &lt;td&gt;3.15453
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;Float8 Rowwise
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;24 hours
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;2.86386
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BF16
   &lt;/td&gt;
   &lt;td&gt;31 hours, 19 minutes / 2.5K
   &lt;/td&gt;
   &lt;td&gt;2.88109
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;Float8 Rowwise
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;24 hours / 2.5K
   &lt;/td&gt;
   &lt;td style=&quot;color: blue;&quot;&gt;2.86386
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;At the 24 hour mark, Float8 completed 2.5K iterations showcasing the comparative speed up (even in deterministic mode) of float8 training.  At the 24 hour mark, Float8 enabled a &lt;strong&gt;+9.21%&lt;/strong&gt; relative improvement in loss compared to BF16 for the same 24 hours of large scale training time.&lt;/p&gt;

&lt;p&gt;After 31 hours, 19 minutes, the BF16 run finally completed its 2.5k iters.&lt;/p&gt;

&lt;p&gt;The final loss numbers:&lt;br /&gt;
BF16 = &lt;strong&gt;2.88109&lt;/strong&gt;	
Float8 = &lt;strong&gt;2.86386&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From the loss curves we observed very similar curves at the first and last ⅓ and then a turbulent zone in the middle where both showed similar spikes, but with a slight skew to the relative timing of the spikes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg2.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a result of this, we can see that PyTorch’s Float8 rowwise offers similar convergence but over 33% speedup for the same amount of training time.&lt;/p&gt;

&lt;h2 id=&quot;long-term-training-stability-with-float8-rowwise&quot;&gt;Long Term Training stability with Float8 Rowwise&lt;/h2&gt;

&lt;p&gt;Beyond showcasing comparable convergence, we also wanted to show longer term training stability with Float8 and thus we launched a 4 day, 15K run at 256 scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg3.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown above, Float8 training ran for over 100 hours with no issues, highlighting the long term stability of Float8 Rowwise.&lt;/p&gt;

&lt;h2 id=&quot;determinism-in-torchtitan&quot;&gt;Determinism in TorchTitan&lt;/h2&gt;

&lt;p&gt;To verify determinism and to see if the spikiness in the longer runs was from scale, we also ran a smaller run comprising of 2 runs of BF16, and 1 run of Float8 at 256 scale, and with HSDP2 only (i.e. without 2D Context parallel).&lt;/p&gt;

&lt;p&gt;In this case both BF16 runs had identical curves and final loss, and we saw a similar spikiness zone for all three runs.&lt;/p&gt;

&lt;p&gt;At the 2K iteration mark, both Float8 and BF16 ending at nearly identical points:&lt;br /&gt;
BF16 *2 = &lt;strong&gt;3.28538&lt;/strong&gt;&lt;br /&gt;
Float8 rowwise = &lt;strong&gt;3.28203&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg4.png&quot; alt=&quot;line chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above result confirms that neither CP nor scale (2k) are responsible for spikiness in the loss as we saw similar effect at 256 scale as well. The most likely explanation for the loss spikes could be content distribution in the dataset.&lt;/p&gt;

&lt;p&gt;For the sake of determinism, the experiments were run with a serialized C4 dataset (not shuffled), meaning the spikes could be from encountering new content within the dataset.&lt;/p&gt;

&lt;h2 id=&quot;net-speedups-at-various-scales-with-float8-rowwise&quot;&gt;Net speedups at various Scales with Float8 rowwise:&lt;/h2&gt;

&lt;p&gt;We performed shorter runs at various GPU scales to understand how Float8 Rowwise would scale in terms of training acceleration as cluster sizes expanded.  Doubling in scale from 960 to 1920, Float8 continued to deliver impressive training speedups, with a range of over 34-43% gains compared to BF16. We also want to note that scaling from 1k to 2k GPUs communication overhead likely kicked in and we observed a 4% hit on throughput with BF16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg5.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the longer training runs at scale above, Float8 rowwise delivered substantial speedups with equal or even slightly improved loss endpoints while delivering 34% speedups at 1920 (DeepSeek) scale.&lt;/p&gt;

&lt;h2 id=&quot;how-can-i-use-float8-rowwise-in-my-training&quot;&gt;How can I use Float8 Rowwise in my training?&lt;/h2&gt;

&lt;p&gt;Float8 Rowwise is available now for you to use in your large scale training.  It is packaged in &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;TorchAO’s&lt;/a&gt; latest builds (0.9 and higher) and integrated into &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;TorchTitan&lt;/a&gt; natively if you want to get up and running quickly.&lt;/p&gt;

&lt;p&gt;To activate Float8 Rowwise in TorchTitan:&lt;/p&gt;

&lt;p&gt;First enable the model converter to hotswap the nn.linears into float8 linear layers in your models .toml file - see line 29:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg6.png&quot; alt=&quot;code&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Secondly, specify the ‘rowwise’ float8 recipe - see line 72:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-training-float8-rowwise-crusoe/fg7.png&quot; alt=&quot;code&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that you have three choices for the ‘recipe_name’:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rowwise which is the recommended default,&lt;/li&gt;
  &lt;li&gt;tensorwise (the older style float8) and&lt;/li&gt;
  &lt;li&gt;rowwise_with_gw_hp.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The gw_hp rowwise option keeps the gradients to the weights in BF16 precision during the backwards pass, and this can further enhance float8 precision for extremely sensitive workloads.  But, it can ironically be a bit more performant than generic rowwise if the majority of the matmul sizes in your model are smaller (with an estimated tipping point at roughly 13-16K dimensions on H100).&lt;/p&gt;

&lt;p&gt;Thus while we recommend rowwise as the default, it may be worth comparing with gw_hp on your model to verify which provides the best performance, with an upside of even greater precision.&lt;/p&gt;

&lt;p&gt;By toggling the model converter on and off with a #, you can directly compare training acceleration between BF16 and Float8 Rowwise to understand the potential speedups for your own training.&lt;/p&gt;

&lt;h2 id=&quot;future-updates&quot;&gt;Future Updates:&lt;/h2&gt;

&lt;p&gt;We’ll have an additional update coming showcasing multiple improvements for Pipeline Parallel and Async Distributed Checkpointing so please stay tuned.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta and Crusoe</name>
        
        
      </author>

      

      

      
        <summary type="html">Meta: Less Wright, Hamid Shojanazeri, Vasiliy Kuznetsov, Daniel Vega-Myhre, Gokul Nadathur, Will Constable, Tianyu Liu, Tristan Rice, Driss Guessous, Josh Fromm, Luca Wehrstedt, Jiecao Yu Crusoe: Ethan Petersen, Martin Cala, Chip Smith</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate PyTorch 2.7 on Intel® GPUs</title>
      <link href="https://pytorch.org/blog/pytorch-2-7-intel-gpus/" rel="alternate" type="text/html" title="Accelerate PyTorch 2.7 on Intel® GPUs" />
      <published>2025-04-25T00:00:00-07:00</published>
      <updated>2025-04-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7-intel-gpus</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7-intel-gpus/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/pytorch-2-7/&quot;&gt;PyTorch 2.7&lt;/a&gt; continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.&lt;/p&gt;

&lt;h2 id=&quot;incremental-improvements-of-intel-gpu-support-in-pytorch&quot;&gt;Incremental improvements of Intel GPU support in PyTorch&lt;/h2&gt;

&lt;p&gt;Since PyTorch 2.4, we’ve made steady improvements to Intel GPU support with each release. With PyTorch 2.7, we are excited to share that we have established a solid foundation to have Intel GPU work in both graph mode (torch.compile) and eager mode on Windows and Linux. This includes a wide range of Intel GPU products, many of which you may already access. We hope these enhancements will unlock more ubiquitous hardware for your AI research and development.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over time, we have expanded Intel GPU Support across Windows and Linux, including these products:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html&quot;&gt;Intel® Arc™ A-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/support/articles/000097599/processors.html&quot;&gt;Intel® Core™ Ultra Processors with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-series-2-mobile-product-brief.html&quot;&gt;Intel® Core™ Ultra Mobile Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-desktop-processors-series-2-brief.html&quot;&gt;Intel® Core™ Ultra Desktop Processors (Series 2) with Intel Arc Graphics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html&quot;&gt;Intel® Data Center GPU Max Series&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Simpler installation&lt;/a&gt; of torch-xpu PIP wheels and an effortless setup experience.&lt;/li&gt;
  &lt;li&gt;High ATen operation coverage with SYCL and oneDNN for smooth eager mode support with functionality and performance.&lt;/li&gt;
  &lt;li&gt;Notable speedups with torch.compile through default TorchInductor and Triton backend, proved by measurable performance gains with Hugging Face, TIMM, and TorchBench benchmarks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out the detailed advancements in these related release blogs:&lt;a href=&quot;https://pytorch.org/blog/intel-gpus-pytorch-2-4/&quot;&gt; PyTorch 2.4&lt;/a&gt;,&lt;a href=&quot;https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/&quot;&gt; PyTorch 2.5&lt;/a&gt;, and&lt;a href=&quot;https://pytorch.org/blog/unlocking-pt-2-6-intel/&quot;&gt; PyTorch 2.6&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-new-in-pytorch-27&quot;&gt;What’s New in PyTorch 2.7&lt;/h2&gt;

&lt;p&gt;These are the features in PyTorch 2.7  that were added to help accelerate performance on Intel GPUs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve scaled dot-product attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;br /&gt;
With the new SDPA optimization for Intel GPUs on PyTorch 2.7, Stable Diffusion float16 inference achieved up to 3x gain over PyTorch 2.6 release on Intel® Arc™ B580 Graphics and Intel® Core™ Ultra 7 Processor 258V with Intel® Arc™ Graphics 140V on eager mode. See Figure 1 below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg1.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1. PyTorch 2.7 Stable Diffusion Performance Gains Over PyTorch 2.6&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux. With this, Intel GPUs became the first accelerator to support torch.compile on Windows. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; Windows tutorial&lt;/a&gt; for details.&lt;br /&gt;
Graph model (torch.compile) is enabled in Windows 11 for the first time across Intel GPUs, delivering the performance advantages over eager mode as on Linux by PyTorch 2.7. The latest performance data was measured on top of PyTorch Dynamo Benchmarking Suite using Intel® Arc™ B580 Graphics on Windows showcase torch.compile speedup ratio over eager mode as shown in Figure 2. Both training and inference achieved similar significant improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-2-7-intel-gpus/fg2.png&quot; alt=&quot;chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2. Torch.compile Performance Gains Over Eager Mode on Windows&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide full graph mode quantization pipelines with enhanced computational efficiency. Refer to&lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt; PT2E tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable AOTInductor and torch.export on Linux to simplify deployment workflows. Refer to&lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt; AOTInductor tutorial&lt;/a&gt; for details.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis. Refer to the&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#pytorch-profiler&quot;&gt; PyTorch profiler tutorial&lt;/a&gt; for details.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Review the &lt;a href=&quot;https://pytorch.org/docs/2.7/notes/get_start_xpu.html&quot;&gt;Getting Started on Intel GPU Guide&lt;/a&gt; for a tour of the environment setup and a quick start on Intel GPUs.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Looking ahead, we will continue the Intel GPU upstream efforts in future PyTorch releases to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attain state-of-the-art PyTorch-native performance to showcase competitive GEMM computational efficiency for torch.compile, and enhance performance for LLM models through FlexAttention and lower precision data types.&lt;/li&gt;
  &lt;li&gt;Broaden feature compatibility by delivering distributed XCCL backend support for Intel® Data Center GPU Max Series.&lt;/li&gt;
  &lt;li&gt;Expand accelerator support across core PyTorch ecosystem components including torchao, torchtune, and torchtitan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow along in the &lt;a href=&quot;https://dev-discuss.pytorch.org/t/intel-gpu-cpu-enabling-status-and-feature-plan-2025-h1-update/2913&quot;&gt;PyTorch Dev Discussion&lt;/a&gt; to learn more about Intel GPU &amp;amp; CPU enabling status and features. As we get further along, we will create tickets on GitHub to document our progress.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blog, we reviewed the Intel GPU upstream progress starting in PyTorch 2.4 and highlighted the new features of PyTorch 2.7 that accelerate AI workload performance across various Intel GPUs. These new features, especially SDPA on Windows, achieved up to 3x inference (Stable Diffusion, float16) gain over PyTorch 2.6 release on Intel Arc B580 Graphics and Intel Core Ultra 7 Processor 258V with Intel Arc Graphics 140V. Also, torch.compile on Windows delivers similar performance advantages over eager mode on Dynamo benchmarks as on Linux.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We want to thank the following PyTorch maintainers for their technical discussions and insights: &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;h2 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h2&gt;

&lt;p&gt;Measurement on Intel Core Ultra 7 258V: 2200 MHz, 8 Core(s), 8 Logical Processor(s) with Intel Arc 140V GPU (16GB), GPU memory 18.0 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Pro - 24H2. And Intel Core Ultra 5 245KF: 4200 MHz, 14 Core(s), 14 Logical Processor(s), Intel Arc B580 Graphics, dedicated GPU memory 12.0 GB, shared GPU memory 15.8 GB, using Intel Graphics Driver 32.0.101.6647 (WHQL Certified), Windows 11 Enterprise LTSC - 24H2. Test by Intel on Apr 8th, 2025.&lt;/p&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;ai-disclaimer&quot;&gt;AI Disclaimer&lt;/h2&gt;

&lt;p&gt;AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the Intel PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.7 continues to deliver significant functionality and performance enhancements on Intel® GPU architectures to streamline AI workflows. Application developers and researchers seeking to fine-tune, inference and develop PyTorch models on Intel GPUs will now have a consistent user experience across various operating systems, including Windows, Linux and Windows Subsystem for Linux (WSL2). This is made possible through improved installation, eager mode script debugging, a performance profiler, and graph model (torch.compile) deployment. As a result, developers have greater options with a unified GPU programming paradigm for both front-end and back-end development.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.7 Release</title>
      <link href="https://pytorch.org/blog/pytorch-2-7/" rel="alternate" type="text/html" title="PyTorch 2.7 Release" />
      <published>2025-04-23T00:00:00-07:00</published>
      <updated>2025-04-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-7</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-7/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.7 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.7.0&quot;&gt;release notes&lt;/a&gt;)! This release features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;support for the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/&quot;&gt;NVIDIA Blackwell GPU architecture&lt;/a&gt; and pre-built wheels for &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA 12.8&lt;/a&gt; across Linux x86 and arm64 architectures.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; support for Torch Function Modes which enables users to override any *torch.** operation  to implement custom user-defined behavior.&lt;/li&gt;
  &lt;li&gt;Mega Cache which allows users to have end-to-end portable caching for torch;&lt;/li&gt;
  &lt;li&gt;new features for FlexAttention - LLM first token processing, LLM throughput mode optimization and Flex Attention for Inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release is composed of 3262 commits from 457 contributors since PyTorch 2.6. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.7. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Torch.Compile support for Torch Function Modes
   &lt;/td&gt;
   &lt;td&gt;NVIDIA Blackwell Architecture Support
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Mega Cache
   &lt;/td&gt;
   &lt;td&gt;PyTorch Native Context Parallel
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Enhancing Intel GPU Acceleration
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;first token processing&lt;/span&gt; on x86 CPUs 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FlexAttention LLM &lt;span style=&quot;text-decoration:underline;&quot;&gt;throughput mode optimization&lt;/span&gt; on x86 CPUs
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Foreach Map
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Flex Attention for Inference
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Prologue Fusion Support in Inductor
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile-support-for-torch-function-modes&quot;&gt;[Beta] Torch.Compile support for Torch Function Modes&lt;/h3&gt;

&lt;p&gt;This feature enables users to override any *torch.** operation to implement custom user-defined behavior. For example, ops can be rewritten to accommodate a specific backend. This is used in FlexAttention to re-write indexing ops.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_torch_function_modes.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-mega-cache&quot;&gt;[Beta] Mega Cache&lt;/h3&gt;

&lt;p&gt;Mega Cache allows users to have end-to-end portable caching for torch. The intended use case is after compiling and executing a model, the user calls &lt;em&gt;torch.compiler.save_cache_artifacts()&lt;/em&gt; which will return the compiler artifacts in a portable form. Later, potentially on a different machine, the user may call &lt;em&gt;torch.compiler.load_cache_artifacts()&lt;/em&gt; with these artifacts to pre-populate the torch.compile caches in order to jump-start their cache.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html#torch-compile-end-to-end-caching-mega-cache&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-nvidia-blackwell-architecture-support&quot;&gt;[Prototype] NVIDIA Blackwell Architecture Support&lt;/h3&gt;

&lt;p&gt;PyTorch 2.7 introduces support for NVIDIA’s new Blackwell GPU architecture and ships pre-built wheels for CUDA 12.8. For more details on CUDA 12.8 see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html&quot;&gt;CUDA Toolkit Release&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Core components and libraries including cuDNN, NCCL, and CUTLASS have been upgraded to ensure compatibility with Blackwell platforms.&lt;/li&gt;
  &lt;li&gt;PyTorch 2.7 includes Triton 3.3, which adds support for the Blackwell architecture with torch.compile compatibility.&lt;/li&gt;
  &lt;li&gt;To utilize these new features, install PyTorch with CUDA 12.8 using: &lt;em&gt;pip install torch==2.7.0 –index-url https://download.pytorch.org/whl/cu128&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More context can also be found &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/145949&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-pytorch-native-context-parallel&quot;&gt;[Prototype] PyTorch Native Context Parallel&lt;/h3&gt;

&lt;p&gt;PyTorch Context Parallel API allows users to create a Python context so that every *torch.nn.functional.scaled_dot_product_attention() *call within will run with context parallelism. Currently,  PyTorch Context Parallel supports 3 attention backends: 1. Flash attention; 2. Efficient attention;  and 3. cuDNN attention.&lt;/p&gt;

&lt;p&gt;As an example, this is &lt;a href=&quot;https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082&quot;&gt;used within TorchTitan as the Context Parallel solution for LLM training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/prototype/context_parallel.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-enhancing-intel-gpu-acceleration&quot;&gt;[Prototype] Enhancing Intel GPU Acceleration&lt;/h3&gt;

&lt;p&gt;This latest release introduces enhanced performance optimizations for Intel GPU architectures. These improvements accelerate workloads across various Intel GPUs through the following key enhancements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable torch.compile on Windows 11 for Intel GPUs, delivering the performance advantages over eager mode as on Linux.&lt;/li&gt;
  &lt;li&gt;Optimize the performance of PyTorch 2 Export Post Training Quantization (PT2E) on Intel GPU to provide a full graph mode quantization pipelines with enhanced computational efficiency.&lt;/li&gt;
  &lt;li&gt;Improve Scaled Dot-Product Attention (SDPA) inference performance with bfloat16 and float16 to accelerate attention-based models on Intel GPUs.&lt;/li&gt;
  &lt;li&gt;Enable AOTInuctor and torch.export on Linux to simplify deployment workflows.&lt;/li&gt;
  &lt;li&gt;Implement more Aten operators to enhance the continuity of operators execution on Intel GPU and increase the performance on Intel GPU in eager mode.&lt;/li&gt;
  &lt;li&gt;Enable profiler on both Windows and Linux to facilitate model performance analysis.&lt;/li&gt;
  &lt;li&gt;Expand the Intel GPUs support to &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra Series 2 with Intel® Arc™ Graphics&lt;/a&gt;, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html&quot;&gt;Intel® Arc™ B-Series graphics&lt;/a&gt; on both Windows and Linux.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See also the tutorials &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_xpu_inductor.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-first-token-processing-on-x86-cpus&quot;&gt;[Prototype] FlexAttention LLM first token processing on x86 CPUs&lt;/h3&gt;

&lt;p&gt;FlexAttention x86 CPU support was first introduced in PyTorch 2.6, offering optimized implementations — such as PageAttention, which is critical for LLM inference—via the TorchInductor C++ backend. In PyTorch 2.7, more attention variants for first token processing of LLMs are supported. With this feature, users can have a smoother experience running FlexAttention on x86 CPUs, replacing specific &lt;em&gt;scaled_dot_product_attention&lt;/em&gt; operators with a unified FlexAttention API, and benefiting from general support and good performance when using torch.compile.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flexattention-llm-throughput-mode-optimization&quot;&gt;[Prototype] FlexAttention LLM throughput mode optimization&lt;/h3&gt;

&lt;p&gt;The performance of FlexAttention on x86 CPUs for LLM inference throughput scenarios has been further improved by adopting the new C++ micro-GEMM template ability. This addresses the performance bottlenecks for large batch size scenarios present in PyTorch 2.6. With this enhancement, users can transparently benefit from better performance and a smoother experience when using FlexAttention APIs and torch.compile for LLM throughput serving on x86 CPUs.&lt;/p&gt;

&lt;h3 id=&quot;prototype-foreach-map&quot;&gt;[Prototype] Foreach Map&lt;/h3&gt;

&lt;p&gt;This feature uses torch.compile to allow users to apply any pointwise or user-defined function (e.g. torch.add) to lists of tensors, akin to the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops. The main advantage over the existing *torch.&lt;em&gt;foreach&lt;/em&gt;** ops is that any mix of scalars or lists of tensors can be supplied as arguments, and even user-defined python functions can be lifted to apply to lists of tensors. Torch.compile will automatically generate a horizontally fused kernel for optimal performance.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://pytorch.org/tutorials/recipes/foreach_map.html&quot;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flex-attention-for-inference&quot;&gt;[Prototype] Flex Attention for Inference&lt;/h3&gt;

&lt;p&gt;In release 2.5.0, &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;FlexAttention&lt;/a&gt;* torch.nn.attention.flex_attention*  was introduced for ML researchers who’d like to customize their attention kernels without writing kernel code. This update introduces a decoding backend optimized for inference, supporting GQA and PagedAttention, along with feature updates including nested jagged tensor support, performance tuning guides and trainable biases support.&lt;/p&gt;

&lt;h3 id=&quot;prototype-prologue-fusion-support-in-inductor&quot;&gt;[Prototype] Prologue Fusion Support in Inductor&lt;/h3&gt;

&lt;p&gt;Prologue fusion optimizes matrix multiplication (matmul) operations by fusing operations that come before the matmul into the matmul kernel itself, improving performance by reducing global memory bandwidth.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.7 (release notes)! This release features:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/" rel="alternate" type="text/html" title="Accelerating Whisper on Arm with PyTorch and Hugging Face Transformers" />
      <published>2025-04-08T00:00:00-07:00</published>
      <updated>2025-04-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-whisper-arm-w-transformers</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-whisper-arm-w-transformers/">&lt;p&gt;Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.&lt;/p&gt;

&lt;p&gt;A new Arm Learning Path is now available that explains how to accelerate Whisper on Arm-based cloud instances using PyTorch and Hugging Face transformers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Run Whisper on Arm?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Arm processors are popular in cloud infrastructure for their efficiency, performance, and cost-effectiveness. With major cloud providers such as AWS, Azure, and Google Cloud offering Arm-based instances, running machine learning workloads on this architecture is becoming increasingly attractive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What You’ll Learn&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/&quot;&gt;Arm Learning Path&lt;/a&gt; provides a structured approach to setting up and accelerating Whisper on Arm-based cloud instances. Here’s what you cover:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Set Up Your Environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before running Whisper, you must set up your development environment. The learning path walks you through setting up an Arm-based cloud instance and installing all dependencies, such as PyTorch, Transformers, and ffmpeg.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Run Whisper with PyTorch and Hugging Face Transformers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the environment is ready, you will use the Hugging Face transformer library with PyTorch to load and execute Whisper for speech-to-text conversion. The tutorial provides a step-by-step approach for processing audio files and generating audio transcripts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Measure and Evaluate Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To ensure efficient execution, you learn how to measure transcription speeds and compare different optimization techniques. The guide provides insights into interpreting performance metrics and making informed decisions on your deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try it Yourself&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Upon completion of this tutorial, you know how to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deploy Whisper on an Arm-based cloud instance.&lt;/li&gt;
  &lt;li&gt;Implement performance optimizations for efficient execution.&lt;/li&gt;
  &lt;li&gt;Evaluate transcription speeds and optimize further based on results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Try the live demo today&lt;/strong&gt; and see audio transcription in action on Arm: &lt;a href=&quot;https://learn.arm.com/learning-paths/servers-and-cloud-computing/whisper/_demo/&quot;&gt;Whisper on Arm Demo&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pareena Verma, Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">Automatic speech recognition (ASR) has revolutionized how we interact with technology, clearing the way for applications like real-time audio transcription, voice assistants, and accessibility tools. OpenAI Whisper is a powerful model for ASR, capable of multilingual speech recognition and translation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Day France 2025: Call For Proposals Open</title>
      <link href="https://pytorch.org/blog/pt-day-france-cfp/" rel="alternate" type="text/html" title="PyTorch Day France 2025: Call For Proposals Open" />
      <published>2025-04-03T00:00:00-07:00</published>
      <updated>2025-04-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-day-france-cfp</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-day-france-cfp/">&lt;p&gt;We’re pleased to announce &lt;strong&gt;&lt;a href=&quot;https://events.linuxfoundation.org/pytorch-day-france/&quot;&gt;PyTorch Day France 2025&lt;/a&gt;&lt;/strong&gt;, a dedicated gathering of the PyTorch community held &lt;strong&gt;7 May 2025&lt;/strong&gt; in &lt;strong&gt;Paris, France&lt;/strong&gt;. Proudly hosted by the &lt;strong&gt;PyTorch Foundation&lt;/strong&gt; and co-located with &lt;strong&gt;&lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris 2025&lt;/a&gt;&lt;/strong&gt;, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.&lt;/p&gt;

&lt;p&gt;Whether you’re building cutting-edge models or contributing to the ecosystem, PyTorch Day France is your opportunity to connect, collaborate, and help shape the future of deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-day-cfp.png&quot; alt=&quot;PT Day CFP&quot; style=&quot;max-width:600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-attend&quot;&gt;Why Attend?&lt;/h2&gt;

&lt;p&gt;Set in the vibrant atmosphere of STATION F, the world’s largest startup campus, PyTorch Day France will offer a full day of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insightful Technical Talks&lt;/li&gt;
  &lt;li&gt;Interactive Discussions&lt;/li&gt;
  &lt;li&gt;Engaging Poster Sessions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The event is designed to foster open exchange across the PyTorch ecosystem, providing a space to learn from peers, share practical insights, and explore the latest research and applications in AI.&lt;/p&gt;

&lt;h2 id=&quot;submit-a-proposal&quot;&gt;Submit a Proposal&lt;/h2&gt;

&lt;p&gt;We are currently accepting proposals for talks. If you have a project, idea, or research story you’d like to share with the PyTorch community, we want to hear from you.&lt;/p&gt;

&lt;p&gt;📩 Email your &lt;strong&gt;talk title and abstract&lt;/strong&gt; to &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt; for consideration.&lt;/p&gt;

&lt;h2 id=&quot;registration&quot;&gt;Registration&lt;/h2&gt;

&lt;p&gt;To register for PyTorch Day France, please visit the &lt;strong&gt;GOSIM AI Paris website&lt;/strong&gt;, and use the code PYTORCHFRIEND to receive 25% off.&lt;/p&gt;

&lt;p&gt;👉 &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;https://paris2025.gosim.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We encourage early registration to secure your spot and ensure access to both PyTorch Day France and the broader GOSIM AI Paris programming.&lt;/p&gt;

&lt;h2 id=&quot;venue&quot;&gt;Venue&lt;/h2&gt;

&lt;p&gt;STATION F&lt;br /&gt;
5 Parv. Alan Turing, 75013 Paris, France&lt;br /&gt;
A landmark of innovation and entrepreneurship in the heart of Paris.&lt;/p&gt;

&lt;h2 id=&quot;travel-and-accommodations&quot;&gt;Travel and Accommodations&lt;/h2&gt;

&lt;p&gt;Participants are responsible for their own travel and lodging. For those arriving internationally, Paris Charles de Gaulle Airport is approximately 38.4 km from STATION F. Additional information about accommodations and transportation may be available on the &lt;a href=&quot;https://paris2025.gosim.org/&quot;&gt;GOSIM AI Paris website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions?&lt;/h2&gt;

&lt;p&gt;For any inquiries, please contact us at &lt;a href=&quot;mailto:pytorchevents@linuxfoundation.org&quot;&gt;pytorchevents@linuxfoundation.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We look forward to welcoming the PyTorch community to Paris this May for a day of collaboration, learning, and open source AI innovation.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re pleased to announce PyTorch Day France 2025, a dedicated gathering of the PyTorch community held 7 May 2025 in Paris, France. Proudly hosted by the PyTorch Foundation and co-located with GOSIM AI Paris 2025, this event will bring together developers, researchers, and practitioners driving innovation in open source AI and machine learning.</summary>
      

      
      
    </entry>
  
</feed>



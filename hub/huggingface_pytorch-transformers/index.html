<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch-Transformers | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('create', 'UA-117752657-2', 'auto', 'newCampaignTracker');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>

  <body class="hub hub-detail">
    <div class="container-fluid header-holder hub-header">
  <div class="container">
    

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item active">

      <div class="ecosystem-dropdown">
        <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
          Ecosystem
        </a>
        <div class="ecosystem-dropdown-menu">
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class=dropdown-title>Tools & Libraries</span>
            <p>Explore the ecosystem of tools and libraries</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item ">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">

      <div class="resources-dropdown">
        <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

  </div>
</div>


    <div class="main-background hub-background hub-detail-background"></div>

    <div class="jumbotron jumbotron-fluid">
      <div class="container">
        <span class="detail-arrow">
          
            <a href="/hub/research-models"><</a>
          
        </span>
        <h1>
          PyTorch-Transformers
        </h1>

        <div class="row">
          <div class="col-md-6">
            <p class="detail-lead">By HuggingFace Team </p>
          </div>

          <div class="col-md-6">
            <p class="detail-lead lead-summary">PyTorch implementations of popular NLP Transformers</p>
            <a href="https://github.com/huggingface/pytorch-transformers.git"><button class="btn btn-lg with-right-white-arrow detail-github-link">View on Github</button></a>
            <a href="https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb"><button class="btn btn-lg with-right-white-arrow detail-colab-link">Open on Google Colab</button></a>
          </div>
        </div>
      </div>
    </div>

    <div class="main-content-wrapper">
      <div class="main-content">
        <div class="container">
          <div class="row">
            <div class="col-md-4">
              <img src="/assets/images/no-image" data-image-name="no-image" class="featured-image img-fluid">
              <img src="/assets/images/no-image" data-image-name="no-image" class="featured-image img-fluid">
            </div>
            <div class="col-md-8">
              <article class="pytorch-article">
                <h1 id="model-description">Model Description</h1>

<p>PyTorch-Transformers (formerly known as <code class="highlighter-rouge">pytorch-pretrained-bert</code>) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).</p>

<p>The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:</p>

<ol>
  <li><strong><a href="https://github.com/google-research/bert">BERT</a></strong> (from Google) released with the paper <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>
  <li><strong><a href="https://github.com/openai/finetune-transformer-lm">GPT</a></strong> (from OpenAI) released with the paper <a href="https://blog.openai.com/language-unsupervised/">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</li>
  <li><strong><a href="https://blog.openai.com/better-language-models/">GPT-2</a></strong> (from OpenAI) released with the paper <a href="https://blog.openai.com/better-language-models/">Language Models are Unsupervised Multitask Learners</a> by Alec Radford<em>, Jeffrey Wu</em>, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</li>
  <li><strong><a href="https://github.com/kimiyoung/transformer-xl">Transformer-XL</a></strong> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai<em>, Zhilin Yang</em>, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</li>
  <li><strong><a href="https://github.com/zihangdai/xlnet/">XLNet</a></strong> (from Google/CMU) released with the paper <a href="https://arxiv.org/abs/1906.08237">​XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang<em>, Zihang Dai</em>, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</li>
  <li><strong><a href="https://github.com/facebookresearch/XLM/">XLM</a></strong> (from Facebook) released together with the paper <a href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li>
  <li><strong><a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta">RoBERTa</a></strong> (from Facebook), released together with the paper a <a href="https://arxiv.org/abs/1907.11692">Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li>
  <li><strong><a href="https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation">DistilBERT</a></strong> (from HuggingFace), released together with the blogpost <a href="https://medium.com/huggingface/distilbert-8cf3380435b">Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT</a> by Victor Sanh, Lysandre Debut and Thomas Wolf.</li>
</ol>

<p>The components available here are based on the <code class="highlighter-rouge">AutoModel</code> and <code class="highlighter-rouge">AutoTokenizer</code> classes of the <code class="highlighter-rouge">pytorch-transformers</code> library.</p>

<h1 id="requirements">Requirements</h1>

<p>Unlike most other PyTorch Hub models, BERT requires a few additional Python packages to be installed.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install tqdm boto3 requests regex sentencepiece sacremoses
</code></pre></div></div>

<h1 id="usage">Usage</h1>

<p>The available methods are the following:</p>
<ul>
  <li><code class="highlighter-rouge">config</code>: returns a configuration item corresponding to the specified model or pth.</li>
  <li><code class="highlighter-rouge">tokenizer</code>: returns a tokenizer corresponding to the specified model or path</li>
  <li><code class="highlighter-rouge">model</code>: returns a model corresponding to the specified model or path</li>
  <li><code class="highlighter-rouge">modelWithLMHead</code>: returns a model with a language modeling head corresponding to the specified model or path</li>
  <li><code class="highlighter-rouge">modelForSequenceClassification</code>: returns a model with a sequence classifier corresponding to the specified model or path</li>
  <li><code class="highlighter-rouge">modelForQuestionAnswering</code>: returns a model with  a question answering head corresponding to the specified model or path</li>
</ul>

<p>All these methods share the following argument: <code class="highlighter-rouge">pretrained_model_or_path</code>, which is a string identifying a pre-trained model or path from which an instance will be returned. There are several checkpoints available for each model, which are detailed below:</p>

<p>The available models are listed on the <a href="https://huggingface.co/pytorch-transformers/pretrained_models.html">pytorch-transformers documentation, pre-trained models section</a>.</p>

<h1 id="documentation">Documentation</h1>

<p>Here are a few examples detailing the usage of each available method.</p>

<h2 id="tokenizer">Tokenizer</h2>

<p>The tokenizer object allows the conversion from character strings to tokens understood by the different models. Each model has its own tokenizer, and some tokenizing methods are different across tokenizers. The complete documentation can be found <a href="https://huggingface.co/pytorch-transformers/main_classes/tokenizer.html">here</a>.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'tokenizer'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>    <span class="c"># Download vocabulary from S3 and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'tokenizer'</span><span class="p">,</span> <span class="s">'./test/bert_saved_model/'</span><span class="p">)</span>  <span class="c"># E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`</span>
</code></pre></div></div>

<h2 id="models">Models</h2>

<p>The model object is a model instance inheriting from a <code class="highlighter-rouge">nn.Module</code>. Each model is accompanied by their saving/loading methods, either from a local file or directory, or from a pre-trained configuration (see previously described <code class="highlighter-rouge">config</code>). Each model works differently, a complete overview of the different models can be found in the <a href="https://huggingface.co/pytorch-transformers/pretrained_models.html">documentation</a>.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>    <span class="c"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'./test/bert_model/'</span><span class="p">)</span>  <span class="c"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="bp">True</span>
<span class="c"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="models-with-a-language-modeling-head">Models with a language modeling head</h2>

<p>Previously mentioned <code class="highlighter-rouge">model</code> instance with an additional language modeling head.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelWithLMHead'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>    <span class="c"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelWithLMHead'</span><span class="p">,</span> <span class="s">'./test/bert_model/'</span><span class="p">)</span>  <span class="c"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelWithLMHead'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="bp">True</span>
<span class="c"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelWithLMHead'</span><span class="p">,</span> <span class="s">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="models-with-a-sequence-classification-head">Models with a sequence classification head</h2>

<p>Previously mentioned <code class="highlighter-rouge">model</code> instance  with an additional sequence classification head.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForSequenceClassification'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>    <span class="c"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForSequenceClassification'</span><span class="p">,</span> <span class="s">'./test/bert_model/'</span><span class="p">)</span>  <span class="c"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForSequenceClassification'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="bp">True</span>
<span class="c"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForSequenceClassification'</span><span class="p">,</span> <span class="s">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="models-with-a-question-answering-head">Models with a question answering head</h2>

<p>Previously mentioned <code class="highlighter-rouge">model</code> instance  with an additional question answering head.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForQuestionAnswering'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>    <span class="c"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForQuestionAnswering'</span><span class="p">,</span> <span class="s">'./test/bert_model/'</span><span class="p">)</span>  <span class="c"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForQuestionAnswering'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># Update configuration during loading</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="bp">True</span>
<span class="c"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForQuestionAnswering'</span><span class="p">,</span> <span class="s">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="configuration">Configuration</h2>

<p>The configuration is optional. The configuration object holds information concerning the model, such as the number of heads/layers, if the model should output attentions or hidden states, or if it should be adapted for TorchScript. Many parameters are available, some specific to each model. The complete documentation can be found <a href="https://huggingface.co/pytorch-transformers/main_classes/configuration.html">here</a>.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>  <span class="c"># Download configuration from S3 and cache.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'./test/bert_saved_model/'</span><span class="p">)</span>  <span class="c"># E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'./test/bert_saved_model/my_configuration.json'</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="bp">True</span>
<span class="n">config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="bp">True</span>
<span class="k">assert</span> <span class="n">unused_kwargs</span> <span class="o">==</span> <span class="p">{</span><span class="s">'foo'</span><span class="p">:</span> <span class="bp">False</span><span class="p">}</span>

<span class="c"># Using the configuration with a model</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'config'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'bert-base-uncased'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="c"># Model will now output attentions and hidden states as well</span>

</code></pre></div></div>

<h1 id="example-usage">Example Usage</h1>

<p>Here is an example on how to tokenize the input text to be fed as input to a BERT model, and then get the hidden states computed by such a model or predict masked tokens using language modeling BERT model.</p>

<h2 id="first-tokenize-the-input">First, tokenize the input</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'tokenizer'</span><span class="p">,</span> <span class="s">'bert-base-cased'</span><span class="p">)</span>

<span class="n">text_1</span> <span class="o">=</span> <span class="s">"Who was Jim Henson ?"</span>
<span class="n">text_2</span> <span class="o">=</span> <span class="s">"Jim Henson was a puppeteer"</span>

<span class="c"># Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text_1</span><span class="p">,</span> <span class="n">text_2</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="using-bertmodel-to-encode-the-input-sentence-in-a-sequence-of-last-layer-hidden-states">Using <code class="highlighter-rouge">BertModel</code> to encode the input sentence in a sequence of last layer hidden-states</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define sentence A and B indices associated to 1st and 2nd sentences (see paper)</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c"># Convert inputs to PyTorch tensors</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'model'</span><span class="p">,</span> <span class="s">'bert-base-cased'</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">encoded_layers</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="using-modelwithlmhead-to-predict-a-masked-token-with-bert">Using <code class="highlighter-rouge">modelWithLMHead</code> to predict a masked token with BERT</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Mask a token that we will try to predict back with `BertForMaskedLM`</span>
<span class="n">masked_index</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">indexed_tokens</span><span class="p">[</span><span class="n">masked_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>

<span class="n">masked_lm__model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelWithLMHead'</span><span class="p">,</span> <span class="s">'bert-base-cased'</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">masked_lm__model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>

<span class="c"># Get the predicted token</span>
<span class="n">predicted_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">masked_index</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predicted_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">([</span><span class="n">predicted_index</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">predicted_token</span> <span class="o">==</span> <span class="s">'Jim'</span>
</code></pre></div></div>

<h2 id="using-modelforquestionanswering-to-do-question-answering-with-bert">Using <code class="highlighter-rouge">modelForQuestionAnswering</code> to do question answering with BERT</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">question_answering_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForQuestionAnswering'</span><span class="p">,</span> <span class="s">'bert-large-uncased-whole-word-masking-finetuned-squad'</span><span class="p">)</span>
<span class="n">question_answering_tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'tokenizer'</span><span class="p">,</span> <span class="s">'bert-large-uncased-whole-word-masking-finetuned-squad'</span><span class="p">)</span>

<span class="c"># The format is paragraph first and then question</span>
<span class="n">text_1</span> <span class="o">=</span> <span class="s">"Jim Henson was a puppeteer"</span>
<span class="n">text_2</span> <span class="o">=</span> <span class="s">"Who was Jim Henson ?"</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">question_answering_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text_1</span><span class="p">,</span> <span class="n">text_2</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>

<span class="c"># Predict the start and end positions logits</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span> <span class="o">=</span> <span class="n">question_answering_model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>

<span class="c"># get the highest prediction</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">question_answering_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">indexed_tokens</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">start_logits</span><span class="p">):</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">end_logits</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">answer</span> <span class="o">==</span> <span class="s">"puppeteer"</span>

<span class="c"># Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)</span>
<span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">12</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">14</span><span class="p">])</span>
<span class="n">multiple_choice_loss</span> <span class="o">=</span> <span class="n">question_answering_model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span><span class="o">=</span><span class="n">end_positions</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="using-modelforsequenceclassification-to-do-paraphrase-classification-with-bert">Using <code class="highlighter-rouge">modelForSequenceClassification</code> to do paraphrase classification with BERT</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequence_classification_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'modelForSequenceClassification'</span><span class="p">,</span> <span class="s">'bert-base-cased-finetuned-mrpc'</span><span class="p">)</span>
<span class="n">sequence_classification_tokenizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'huggingface/pytorch-transformers'</span><span class="p">,</span> <span class="s">'tokenizer'</span><span class="p">,</span> <span class="s">'bert-base-cased-finetuned-mrpc'</span><span class="p">)</span>

<span class="n">text_1</span> <span class="o">=</span> <span class="s">"Jim Henson was a puppeteer"</span>
<span class="n">text_2</span> <span class="o">=</span> <span class="s">"Who was Jim Henson ?"</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">sequence_classification_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text_1</span><span class="p">,</span> <span class="n">text_2</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>

<span class="c"># Predict the sequence classification logits</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">seq_classif_logits</span> <span class="o">=</span> <span class="n">sequence_classification_model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">)</span>

<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">seq_classif_logits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">predicted_labels</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c"># In MRPC dataset this means the two sentences are not paraphrasing each other</span>

<span class="c"># Or get the sequence classification loss (set model to train mode before if used for training)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">seq_classif_loss</span> <span class="o">=</span> <span class="n">sequence_classification_model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">segments_tensors</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

                <a href="https://github.com/pytorch/hub/issues"><button class="btn btn-lg hub-feedback-button hub-flag">Not Working?</button></a>
              </article>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
        </div>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="">
          <a href="/features">Features</a>
        </li>

        <li class="">
          <a href="/ecosystem">Ecosystem</a>
        </li>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/hub">PyTorch Hub</a>
        </li>

        <li class="">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li>
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/resources">Resources</a>
        </li>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


  </body>
</html>

<script src="/assets/hub-detail.js"></script>

<!DOCTYPE html>
<html lang="en">
  <head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Ecosystem Day 2021 | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Ecosystem Day 2021" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Ecosystem Day 2021" />
<meta name="twitter:description" content="" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>

  <body class="ecosystem">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="hello-bar">
  <div class="container">
    Join us at PyTorch Conference in San Francisco, October 22-23. CFP open now! <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
  </div>
</div>
<div class="container-fluid header-holder ecosystem-header">
  <div class="container">
    

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
          <a class="nav-dropdown-item" href="/new">
            <span class="dropdown-title">New to PyTorch Foundation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/join-ecosystem">
            <span class="dropdown-title">Join the Ecosystem</span>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2024">
            <span class="dropdown-title">Contributor Awards - 2024</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
          <a class="nav-dropdown-item" target="_blank" href="https://pytorch.org/executorch/stable/index.html">
            <span class="dropdown-title">ExecuTorch Documentation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/newsletter">
            <span class=dropdown-title>Newsletter</span>
            <p>Stay up-to-date with the latest updates</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
          <a class="nav-dropdown-item" href="/credits">
            <span class=dropdown-title>Cloud Credit Program</span>
          </a>
          <a class="nav-dropdown-item" href="/tac">
            <span class=dropdown-title>Technical Advisory Council</span>
          </a>
          <a class="nav-dropdown-item" href="/staff">
            <span class=dropdown-title>Staff</span>
          </a>
          <a class="nav-dropdown-item" href="/contact-us">
            <span class=dropdown-title>Contact Us</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

  </div>
</div>


    <div class="main-background features-background"></div>

    <div class="jumbotron jumbotron-fluid contributor-jumbotron">
  <div class="contributor-jumbo-text container">
    <h1>PyTorch Ecosystem Day</h1>
    <h1 class="lead">2021</h1>
  </div>
</div>

<div class="main-content-wrapper">
  <div class="main-content">
    <div class="container">
      <p class="lead">
        Thank you to the incredible PyTorch Community for making the first ever
        PyTorch Ecosystem Day a success! Ecosystem Day was hosted on Gather.Town
        utilizing an auditorium, exhibition hall, and breakout rooms for
        partners to reserve for talks, demos, or tutorials. In order to cater to
        the global community, the event held two sessions: a morning session
        from 8am PT - 1pm PT and an evening session from 3:00pm -7:00pm PT. The
        day was filled with discussions on new developments, trends and
        challenges showcased through 71 posters, 32 breakout sessions and 6
        keynote speakers.
      </p>
      <p class="lead">
        Special thanks to our 6 keynote speakers: Piotr Bialecki, Ritchie Ng,
        Miquel Farré, Joe Spisak, Geeta Chauhan, and Suraj Subramanian. You can
        find the opening talks here:
      </p>
      <ul class="lead">
        <li>
          <a href="https://www.youtube.com/watch?v=MYE01-XaSZA"
            >Morning Session</a
          >
        </li>
        <li>
          <a href="https://www.youtube.com/watch?v=CjU_6OaYKpw"
            >Evening Session</a
          >
        </li>
      </ul>
      <div class="input-group mb-3">
        <input
          type="text"
          id="pted-filter"
          placeholder="Filter..."
          class="form-control"
        />
      </div>
      <hr />
      <div class="row">
        <h1>Posters</h1>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K1.png">Bring quantum machine learning to PyTorch with PennyLane</a>
              
            </h5>
            <h6 class="card-subtitle">Josh Izaac, Thomas Bromley</h6>
            <p class="card-text">PennyLane allows you to train quantum circuits just like neural networks!, This poster showcases how PennyLane can be interfaced with PyTorch to enable training of quantum and hybrid machine learning models. The outputs of a quantum circuit are provided as a Torch tensor with a defined gradient. We highlight how this functionality can be used to explore new paradigms in machine learning, including the use of hybrid models for transfer learning.</p>
            
            <p class="card-text">
              <a href="http://pennylane.ai">http://pennylane.ai</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platform, Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A4.png">PyTorch development in VS Code</a>
              
            </h5>
            <h6 class="card-subtitle">Jeffrey Mew</h6>
            <p class="card-text">Visual Studio Code, a free cross-platform lightweight code editor, has become the most popular among Python developers for both web and machine learning projects. We will be walking you through an end to end PyTorch project to showcase what VS Code has a lot to offer to PyTorch developers to boost their productivity.
 
Firstly, get your PyTorch project quickly up and running with VS Code's environment/dependency management and built-in Jupyter Notebook support. Secondly, breeze through coding with help from our AI-powered IntelliSense. When it's time to run your code, use the built-in Tensorboard integration to monitor your training along with the integrated PyTorch profiler to analyze and debug your code. Once you're ready for the cloud, VS Code has Azure service integration to allow you to scale your model training and deployment, along with deployment.
 
Combing the power of the code editor with easy access to the Azure services, VS Code can be the one-stop shop for any developers looking to build machine learning models with PyTorch.</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A5.png">Upcoming features in TorchScript</a>
              
            </h5>
            <h6 class="card-subtitle">Yanan Cao, Harry Kim, Jason Ansel</h6>
            <p class="card-text">TorchScript is the bridge between PyTorch's flexible eager mode to more deterministic and performant graph mode suitable for production deployment. As part of PyTorch 1.9 release, TorchScript will launch a few features that we'd like to share with you earlier, including a) a new formal language specification that defines the exact subset of Python/PyTorch features supported in TorchScript; b) Profile-Directed Typing that reduces the burden of converting a loosely-typed eager model into a strictly-typed TorchScript model; c) A TorchScript profiler that can shed light on performance characteristics of TorchScript model. We are constantly making improvements to make TorchScript easier to use and more performant.</p>
            
            <p class="card-text">
              <a href="http://fb.me/torchscript">http://fb.me/torchscript</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               Quantization-Aware Training with Brevitas 
            </h5>
            <h6 class="card-subtitle">Alessandro Pappalardo</h6>
            <p class="card-text">Brevitas is an open-source PyTorch library for quantization-aware training. Thanks to its flexible design at multiple levels of abstraction, Brevitas generalizes the typical uniform affine quantization paradigm adopted in the deep learning community under a common set of unified APIs. Brevitas provides a platform to both ML practitioners and researchers to either apply built-in state-of-the-art techniques in training for reduced-precision inference, or to implement novel quantization-aware training algorithms. Users can target supported inference toolchains, such as onnxruntime, TVM, Vitis AI, FINN or PyTorch itself, or experiment with hypothetical target hardware platforms. In particular, when combined with the flexibility of Xilinx FPGAs through the FINN toolchain, Brevitas supports the co-design of novel hardware building blocks in a machine-learning driven fashion. Within Xilinx, Brevitas has been adopted by various research projects concerning quantized neural networks, as well as in large scale deployments targeting custom programmable logic accelerators.</p>
            
            <p class="card-text">
              <a href="https://github.com/Xilinx/brevitas/">https://github.com/Xilinx/brevitas/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-B5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B5.png">PyTorch Quantization: FX Graph Mode Quantization</a>
              
            </h5>
            <h6 class="card-subtitle">Jerry Zhang, Vasiliy Kuznetsov, Raghuraman Krishnamoorthi</h6>
            <p class="card-text">Quantization is a common model optimization technique to speedup runtime of a model by upto 4x,  with a possible slight loss of accuracy. Currently, PyTorch support Eager Mode Quantization. FX Graph Mode Quantization improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process. To use FX Graph Mode Quantization, one might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx).</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization">https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C4.png">Accelerate deployment of deep learning models in production with Amazon EC2 Inf1 and TorchServe containers</a>
              
            </h5>
            <h6 class="card-subtitle">Fabio Nonato</h6>
            <p class="card-text"> Deep learning models can have game-changing impact on machine learning applications. However, deploying and managing deep learning models in production is complex and requires considerable engineering effort - from  building custom inferencing APIs and scaling prediction services, to securing applications, while still leveraging the latest ML frameworks and hardware technology.  Amazon EC2 Inf1 instances powered by AWS Inferentia deliver the highest performance and lowest cost machine learning inference in the cloud. Developers can deploy their deep-learning models to Inf1 instances using the AWS Neuron SDK that is natively integrated with PyTorch.
 
Attend this poster session to learn how you can optimize and accelerate the deployment of your deep learning models in production using Inf1 instances and TorchServe containers. You will learn how to deploy TorchScript models on Inf1 and optimize your models with minimal code changes with features such as NeuronCore Groups and NeuronCore Pipeline, to meet your throughput and latency requirements. You can directly integrate these model level optimizations into the inference endpoint using TorchServe.
 
We will also deep dive into how we optimized performance of  a natural language processing endpoint and showcase the workflow for deploying the optimized model using TorchServe containers on Amazon ECS.</p>
            
            <p class="card-text">
              <a href="https://bit.ly/3mQVowk">https://bit.ly/3mQVowk</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C5.png">Torch.fx</a>
              
            </h5>
            <h6 class="card-subtitle">James Reed, Zachary DeVito, Ansley Ussery, Horace He, Michael Suo</h6>
            <p class="card-text">FX is a toolkit for writing Python-to-Python transforms over PyTorch code.
FX consists of three parts:
> Symbolic Tracing – a method to extract a representation of the program by running it with "proxy" values.
> Graph-based Transformations – FX provides an easy-to-use Python-based Graph API for manipulating the code.
> Python code generation – FX generates valid Python code from graphs and turns that code into executable Python `nn.Module` instances.</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/docs/stable/fx.html">https://pytorch.org/docs/stable/fx.html</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D4.png">AI Model Efficiency Toolkit (AIMET)</a>
              
            </h5>
            <h6 class="card-subtitle">Abhijit Khobare, Murali Akula, Tijmen Blankevoort, Harshita Mangal, Frank Mayer, Sangeetha Marshathalli Siddegowda, Chirag Patel, Vinay Garg, Markus Nagel</h6>
            <p class="card-text">AI is revolutionizing industries, products, and core capabilities by delivering dramatically enhanced experiences. However, the deep neural networks of today use too much memory, compute, and energy. To make AI truly ubiquitous, it needs to run on the end device within a tight power and thermal budget. Quantization and compression help address these issues. In this tutorial, we'll discuss:
The existing quantization and compression challenges
Our research in novel quantization and compression techniques to overcome these challenges
How developers and researchers can implement these techniques through the AI Model Efficiency Toolkit</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Compiler & Transform & Production</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H8.png">Pytorch via SQL commands: A flexible, modular AutoML framework that democratizes ML for database users</a>
              
            </h5>
            <h6 class="card-subtitle">Natasha Seelam, Patricio Cerda-Mardini, Cosmo Jenytin, Jorge Torres</h6>
            <p class="card-text">Pytorch enables building models with complex inputs and outputs, including time-series data, text and audiovisual data. However, such models require expertise and time to build, often spent on tedious tasks like cleaning the data or transforming it into a format that is expected by the models.

Thus, pre-trained models are often used as-is when a researcher wants to experiment only with a specific facet of a problem. See, as examples, FastAI's work into optimizers, schedulers, and gradual training through pre-trained residual models, or NLP projects with Hugging Face models as their backbone.

We think that, for many of these problems, we can automatically generate a "good enough" model and data-processing pipeline from just the raw data and the endpoint. To address this situation, we are developing MindsDB, an open-source, PyTorch-based ML platform that works inside databases via SQL commands. It is built with a modular approach, and in this talk we are going to focus on Lightwood, the stand-alone core component that performs machine learning automation on top of the PyTorch framework.

Lightwood automates model building into 5 stages: (1) classifying each feature into a "data type", (2) running statistical analyses on each column of a dataset, (3) fitting multiple models to normalize, tokenize, and generate embeddings for each feature, (4) deploying the embeddings to fit a final estimator, and (5) running an analysis on the final ensemble to evaluate it and generate a confidence model. It can generate quick "baseline" models to benchmark performance for any custom encoder representation of a data type and can also serve as scaffolding for investigating new hypotheses (architectures, optimizers, loss-functions, hyperparameters, etc).

We aim to present our benchmarks covering wide swaths of problem types and illustrate how Lightwood can be useful for researchers and engineers through a hands-on demo.</p>
            
            <p class="card-text">
              <a href="https://mindsdb.com">https://mindsdb.com</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Database & AI Accelerators</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J8.png">PyTorch on Supercomputers Simulations and AI at Scale with SmartSim</a>
              
            </h5>
            <h6 class="card-subtitle">Sam Partee , Alessandro Rigazzi, Mathew Ellis, Benjamin Rob</h6>
            <p class="card-text">SmartSim is an open source library dedicated to enabling online analysis and Machine Learning (ML) for traditional High Performance Computing (HPC) simulations. Clients are provided in common HPC simulation languages, C/C++/Fortran, that enable simulations to perform inference requests in parallel on large HPC systems. SmartSim utilizes the Redis ecosystem to host and serve PyTorch models alongside simulations. We present a use case of SmartSim where a global ocean simulation, used in climate modeling, is augmented with a PyTorch model to resolve quantities of eddy kinetic energy within the simulation.</p>
            
            <p class="card-text">
              <a href="https://github.com/CrayLabs/SmartSim">https://github.com/CrayLabs/SmartSim</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Database & AI Accelerators</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I8.png">Model agnostic confidence estimation with conformal predictors for AutoML</a>
              
            </h5>
            <h6 class="card-subtitle">Patricio Cerda-Mardini, Natasha Seelam</h6>
            <p class="card-text">Many domains leverage the extraordinary predictive performance of machine learning algorithms. However, there is an increasing need for transparency of these models in order to justify deploying them in applied settings. Developing trustworthy models is a great challenge, as they are usually optimized for accuracy, relegating the fit between the true and predicted distributions to the background [1]. This concept of obtaining predicted probability estimates that match the true likelihood is also known as calibration.

Contemporary ML models generally exhibit poor calibration. There are several methods that aim at producing calibrated ML models [2, 3]. Inductive conformal prediction (ICP) is a simple yet powerful framework to achieve this, offering strong guarantees about the error rates of any machine learning model [4]. ICP provides confidence scores and turns any point prediction into a prediction region through nonconformity measures, which indicate the degree of inherent strangeness a data point presents when compared to a calibration data split.

In this work, we discuss the integration of ICP with MindsDB --an open source AutoML framework-- successfully replacing its existing quantile loss approach for confidence estimation capabilities.
Our contribution is threefold. First, we present a study on the effect of a "self-aware" neural network normalizer in the width of predicted region sizes (also known as efficiency) when compared to an unnormalized baseline. Our benchmarks consider results for over 30 datasets of varied domains with both categorical and numerical targets. Second, we propose an algorithm to dynamically determine the confidence level based on a target size for the predicted region, effectively prioritizing efficiency over a minimum error rate. Finally, we showcase the results of a nonconformity measure specifically tailored for small datasets.

References:
[1] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). On Calibration of Modern Neural Networks. ArXiv, abs/1706.04599.
[2] Naeini, M., Cooper, G., & Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 2015, 2901-2907 .
[3] Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., & Wilson, A. (2019). A Simple Baseline for Bayesian Uncertainty in Deep Learning. NeurIPS.
[4] Papadopoulos, H., Vovk, V., & Gammerman, A. (2007). Conformal Prediction with Neural Networks. 19th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2007), 2, 388-395.</p>
            
            <p class="card-text">
              <a href="https://mindsdb.com">https://mindsdb.com</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Database & AI Accelerators</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K8.png">Enabling PyTorch on AMD Instinct™ GPUs with the AMD ROCm™ Open Software Platform</a>
              
            </h5>
            <h6 class="card-subtitle">Derek Bouius</h6>
            <p class="card-text">AMD Instinct GPUs are enabled with the upstream PyTorch repository via the ROCm open software platform. Now users can also easily download the installable Python package, built from the upstream PyTorch repository and hosted on pytorch.org. Notably, it includes support for distributed training across multiple GPUs and supports accelerated mixed precision training. AMD also provides hardware support for the PyTorch community build to help develop and maintain new features. This poster will highlight some of the work that has gone into enabling PyTorch support.</p>
            
            <p class="card-text">
              <a href="https://www.amd.com/rocm">https://www.amd.com/rocm</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Database & AI Accelerators</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-E1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E1.png">DeepSpeed: Shattering barriers of deep learning speed & scale</a>
              
            </h5>
            <h6 class="card-subtitle">DeepSpeed Team Microsoft Corporation</h6>
            <p class="card-text">In the poster (and a talk during the breakout session), we will present three aspects of DeepSpeed (https://github.com/microsoft/DeepSpeed), a deep learning optimization library based on PyTorch framework: 1) How we overcome the GPU memory barrier by ZeRO-powered data parallelism. 2) How we overcome the network bandwidth barrier by 1-bit Adam and 1-bit Lamb compressed optimization algorithms. 3) How we overcome the usability barrier by integration with Azure ML, HuggingFace, and PyTorch Lightning.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               Dask PyTorch DDP: A new library bringing Dask parallelization to PyTorch training 
            </h5>
            <h6 class="card-subtitle">Stephanie Kirmer, Hugo Shi</h6>
            <p class="card-text">We have developed a library that helps simplify the task of multi-machine parallel training for PyTorch models, bringing together the power of PyTorch DDP with Dask for parallelism on GPUs. Our poster describes the library and its core function, and demonstrates how the multi-machine training process works in practice.</p>
            
            <p class="card-text">
              <a href="https://github.com/saturncloud/dask-pytorch-ddp">https://github.com/saturncloud/dask-pytorch-ddp</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-E3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E3.png">Optimising Physics Informed Neural Networks.</a>
              
            </h5>
            <h6 class="card-subtitle">Vignesh Gopakumar</h6>
            <p class="card-text">Solving PDEs using Neural Networks are often ardently laborious as it requires training towards a well-defined solution, i.e. global minima for a network architecture - objective function combination. For a family of complex PDEs, Physics Informed neural networks won't offer much in comparison to traditional numerical methods as their global minima becomes more and more intractable. We propose a modified approach that hinges on continual and parametrised learning that can create more general PINNs that can solve for a variety of PDE scenarios rather than solving for a well-defined case. We believe that this brings Neural Network based PDE solvers in comparison to numerical solvers.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-F1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F1.png">FairScale-A general purpose modular PyTorch library for  high performance and large scale training</a>
              
            </h5>
            <h6 class="card-subtitle">Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Benjamin Lefaudeux, Vitaliy Liptchinsky, Naman Goyal, Siddhardth Goyal, Myle Ott, Sam Sheifer, Anjali Sridhar, Min Xu</h6>
            <p class="card-text">FairScale is a library that extends basic PyTorch capabilities while adding new SOTA techniques for high performance and large scale training on one or multiple machines. FairScale makes available the latest distributed training techniques in the form of composable modules and easy to use APIs.

Machine Learning (ML) training at scale traditionally means data parallelism to reduce training time by using multiple devices to train on larger batch size. Nevertheless, with the recent increase of ML models sizes data parallelism is no longer enough to satisfy all "scaling" needs. FairScale provides several options to overcome some of the limitations to scale.

For scaling training that is bottlenecked by memory (optimizer state, intermediate activations, parameters), FairScale provides APIs that have implemented optimizer, gradient and parameter sharding. This will allow users to train large models using devices in a more memory efficient manner.

To overcome the memory required for large models FairScale provides various flavors of pipeline and model parallelism, MOE (Mixture Of Experts) layer, and Offload models. Those methods allow to perform computation only of shards of the models across multiple devices with micro batches of data to maximize device efficiency.

FairScale also provides modules to aid users to scale batch size effectively without changing their existing learning rate hyperparameter - AdaScale - and save memory with checkpoint activation of intermediate layers.

FairScale has also been integrated into Pytorch Lightening, HuggingFace, FairSeq, VISSL, and MMF to enable users of those frameworks to take advantage of its features.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-F2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F2.png">AdaptDL: An Open-Source Resource-Adaptive Deep Learning Training/Scheduling Framework</a>
              
            </h5>
            <h6 class="card-subtitle">Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, Eric P. Xing</h6>
            <p class="card-text">AdaptDL is an open source framework and scheduling algorithm that directly optimizes cluster-wide training performance and resource utilization. By elastically re-scaling jobs, co-adapting batch sizes and learning rates, and avoiding network interference, AdaptDL improves shared-cluster training compared with alternative schedulers. AdaptDL can automatically determine the optimal number of resources given a job's need. It will efficiently add or remove resources dynamically to ensure the highest-level performance. The AdaptDL scheduler will automatically figure out the most efficient number of GPUs to allocate to your job, based on its scalability. When the cluster load is low, your job can dynamically expand to take advantage of more GPUs. AdaptDL offers an easy-to-use API to make existing PyTorch training code elastic with adaptive batch sizes and learning rates.
Showcase: Distributed training and Data Loading</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               Accelerate PyTorch large model training with ONNX Runtime: just add one line of code! 
            </h5>
            <h6 class="card-subtitle">Natalie Kershaw</h6>
            <p class="card-text">As deep learning models, especially transformer models get bigger and bigger, reducing training time becomes both a financial and environmental imperative. ONNX Runtime can accelerate large-scale distributed training of PyTorch transformer models with a one-line code change (in addition to import statements ;-)) Adding in the DeepSpeed library improves training speed even more.

With the new ORTModule API, you wrap an existing torch.nn.Module, and have us automatically: export the model as an ONNX computation graph; compile and optimize it with ONNX Runtime; and integrate it into your existing training script.

In this poster, we demonstrate how to fine-tune a popular HuggingFace model and show the performance improvement, on a multi-GPU cluster in the Azure Machine Learning cloud service.</p>
            
            <p class="card-text">
              <a href="https://aka.ms/pytorchort">https://aka.ms/pytorchort</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/G2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-G2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/G2.png">PyTorch/XLA with new Cloud TPU VMs and Profiler</a>
              
            </h5>
            <h6 class="card-subtitle">Jack Cao, Daniel Sohn, Zak Stone, Shauheen Zahirazami</h6>
            <p class="card-text">PyTorch / XLA enables users to train PyTorch models on XLA devices including Cloud TPUs. Cloud TPU VMs now provide direct access to TPU host machines and hence offer much greater flexibility in addition to making debugging easier and reducing data transfer overheads. PyTorch / XLA has now full support for this new architecture. A new profiling tool has also been developed to enable better profiling of PyTorch / XLA. These improvements not only make it much easier to develop models but also reduce the cost of large-scale PyTorch / XLA training runs on Cloud TPUs.</p>
            
            <p class="card-text">
              <a href="http://goo.gle/pt-xla-tpuvm-signup">http://goo.gle/pt-xla-tpuvm-signup</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Distributed Training</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-E4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E4.png">PyTorch Lightning: Deep Learning without the Boilerplate</a>
              
            </h5>
            <h6 class="card-subtitle">Ari Bornstein</h6>
            <p class="card-text">PyTorch Lightning reduces the engineering boilerplate and resources required to implement state-of-the-art AI. Organizing PyTorch code with Lightning enables seamless training on multiple-GPUs, TPUs, CPUs, and the use of difficult to implement best practices such as model sharding, 16-bit precision, and more, without any code changes. In this poster, we will use practical Lightning examples to demonstrate how to train Deep Learning models with less boilerplate.</p>
            
            <p class="card-text">
              <a href="https://www.pytorchlightning.ai/">https://www.pytorchlightning.ai/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Frontend & Experiment Manager</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-E5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/E5.png">Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology</a>
              
            </h5>
            <h6 class="card-subtitle">Jiong Gong, Nikita Shustrov, Eikan Wang, Jianhui Li, Vitaly Fedyunin</h6>
            <p class="card-text">Intel and Facebook collaborated to enable BF16, a first-class data type in PyTorch, and a data type that are accelerated natively with the 3rd Gen Intel® Xeon® scalable processors. This poster introduces the latest SW advancements added in Intel Extension for PyTorch (IPEX) on top of PyTorch and the oneAPI DNN library for ease-of-use and high-performance BF16 DL compute on CPU. With these SW advancements, we demonstrated ease-of-use IPEX user-facing API, and we also showcased 1.55X-2.42X speed-up with IPEX BF16 training over FP32 with the stock PyTorch and 1.40X-4.26X speed-up with IPEX BF16 inference over FP32 with the stock PyTorch.</p>
            
            <p class="card-text">
              <a href="https://github.com/intel/intel-extension-for-pytorch">https://github.com/intel/intel-extension-for-pytorch</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Frontend & Experiment Manager</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-F4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F4.png">TorchStudio, a machine learning studio software based on PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Robin Lobel</h6>
            <p class="card-text">TorchStudio is a standalone software based on PyTorch and LibTorch. It aims to simplify the creation, training and iterations of PyTorch models. It runs locally on Windows, Ubuntu and macOS. It can load, analyze and explore PyTorch datasets from the TorchVision or TorchAudio categories, or custom datasets with any number of inputs and outputs. PyTorch models can then be loaded and written from scratch, analyzed, and trained using local hardware. Trainings can be run simultaneously and compared to identify the best performing models, and export them as a trained TorchScript or ONNX model.</p>
            
            <p class="card-text">
              <a href="https://torchstudio.ai/">https://torchstudio.ai/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Frontend & Experiment Manager</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-F5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/F5.png">Hydra Framework</a>
              
            </h5>
            <h6 class="card-subtitle">Jieru Hu, Omry Yadan </h6>
            <p class="card-text">Hydra is an open source framework for configuring and launching research Python applications. Key features: - Compose and override your config dynamically to get the perfect config for each run - Run on remote clusters like SLURM and AWS without code changes - Perform basic greed search and hyper parameter optimization without code changes - Command line tab completion for your dynamic config And more.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Frontend & Experiment Manager</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/G4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-G4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/G4.png">PyTorch-Ignite: training common things easy and the hard things possible</a>
              
            </h5>
            <h6 class="card-subtitle">Victor Fomin, Sylvain Desroziers, Taras Savchyn</h6>
            <p class="card-text">This poster intends to give a brief but illustrative overview of what PyTorch-Ignite can offer for Deep Learning enthusiasts, professionals and researchers. Following the same philosophy as PyTorch, PyTorch-Ignite aims to keep it simple, flexible and extensible but performant and scalable. Throughout this poster, we will introduce the basic concepts of PyTorch-Ignite, its API and features it offers. We also assume that the reader is familiar with PyTorch.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Frontend & Experiment Manager</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H4.png">Farabio - Deep Learning Toolkit for Biomedical Imaging</a>
              
            </h5>
            <h6 class="card-subtitle">Sanzhar Askaruly, Nurbolat Aimakov, Alisher Iskakov, Hyewon Cho</h6>
            <p class="card-text">Deep learning has transformed many aspects of industrial pipelines recently. Scientists involved in biomedical imaging research are also benefiting from the power of AI to tackle complex challenges. Although the academic community has widely accepted image processing tools, such as scikit-image, ImageJ, there is still a need for a tool which integrates deep learning into biomedical image analysis. We propose a minimal, but convenient Python package based on PyTorch with common deep learning models, extended by flexible trainers and medical datasets.</p>
            
            <p class="card-text">
              <a href="https://github.com/tuttelikz/farabio">https://github.com/tuttelikz/farabio</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H5.png">MONAI: A Domain Specialized Library for Healthcare Imaging</a>
              
            </h5>
            <h6 class="card-subtitle">Michael Zephyr, Prerna Dogra Richard Brown, Wenqi Li, Eric Kerfoot</h6>
            <p class="card-text">Healthcare image analysis for both radiology and pathology is increasingly being addressed with deep-learning-based solutions. These applications have specific requirements to support various imaging modalities like MR, CT, ultrasound, digital pathology, etc. It is a substantial effort for researchers in the field to develop custom functionalities to handle these requirements. Consequently, there has been duplication of effort, and as a result, researchers have incompatible tools, which makes it hard to collaborate.
 
MONAI stands for Medical Open Network for AI. Its mission is to accelerate the development of healthcare imaging solutions by providing domain-specialized building blocks and a common foundation for the community to converge in a native PyTorch paradigm.</p>
            
            <p class="card-text">
              <a href="https://monai.io/">https://monai.io/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I4.png">How theator Built a Continuous Training Framework to Scale Up Its Surgical Intelligence Platform</a>
              
            </h5>
            <h6 class="card-subtitle">Shai Brown, Daniel Neimark, Maya Zohar, Omri Bar, Dotan Asselmann</h6>
            <p class="card-text">Theator is re-imagining surgery with a Surgical Intelligence platform that leverages highly advanced AI, specifically machine learning and computer vision technology, to analyze every step, event, milestone, and critical junction of surgical procedures.

Our platform analyzes lengthy surgical procedure videos and extracts meaningful information, providing surgeons with highlight reels of key moments in an operation, enhanced by annotations.

As the team expanded, we realized that we were spending too much time manually running model training and focusing on DevOps tasks and not enough time dedicated to core research.

To face this, we build an automation framework composed of multiple training pipelines using PyTorch and ClearML. Our framework automates and manages our entire process, from model development to deployment to continuous training for model improvement.

New data is now immediately processed and fed directly into training pipelines – speeding up workflow, minimizing human error, and freeing up our research team for more important tasks. Thus, enabling us to scale our ML operation and deliver better models for our end users.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I5.png">Q&Aid: A Conversation Agent Powered by PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Cebere Bogdan, Cebere Tudor, Manolache Andrei, Horia Paul-Ion</h6>
            <p class="card-text">We present Q&Aid, a conversation agent that relies on a series of machine learning models to filter, label, and answer medical questions based on a provided image and text inputs. Q&Aid is simplifying the hospital logic backend by standardizing it to a Health Intel Provider (HIP). A HIP is a collection of models trained on local data that receives text and visual input, afterward filtering, labeling, and feeding the data to the right models and generating at the end output for the aggregator. Any hospital is identified as a HIP holding custom models and labeling based on its knowledge. The hospitals are training and fine-tuning their models, such as a Visual Question Answering (VQA) model, on private data (e.g. brain anomaly segmentation). We aggregate all of the tasks that the hospitals can provide into a single chat app, offering the results to the user. When the chat ends, the transcript is forwarded to each hospital, a doctor being in charge of the final decision.</p>
            
            <p class="card-text">
              <a href="https://qrgo.page.link/d1fQk">https://qrgo.page.link/d1fQk</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J4.png">Sleepbot: Multi-signal Sleep Stage Classifier AI for hospital and home</a>
              
            </h5>
            <h6 class="card-subtitle">Jaden Hong, Kevin Tran, Tyler Lee, Paul Lee, Freddie Cha, Louis Jung, Dr. Jung Kyung Hong, Dr. In-Young Yoon, David Lee</h6>
            <p class="card-text">Sleep disorders and insomnia are now regarded as a worldwide problem. Roughly 62% of adults worldwide feel that they don't sleep well. However, sleep is difficult to track so it's not easy to get suitable treatment to improve your sleep quality. Currently, the PSG (Polysomnography) is the only way to evaluate the sleep quality accurately but it's expensive and often inaccurate due to the first night effect. 

We propose a multi-signal sleep stage classifier for contactless sleep tracking: Sleepbot. By automating the manual PSG reading and providing explainable analysis, Sleepbot opens a new possibility to apply sleep staging AI in both home and hospital. With sound recorded by a smartphone app and RF-sensed signal measured by Asleep's non-contact sleep tracker, Sleepbot provides a clinical level of sleep stage classification. 

Sleepbot achieved 85.5 % accuracy in 5-class (Wake, N1, N2, N3, Rem) using PSG signals measured from 3,700 subjects and 77 % accuracy in 3-class (Wake, Sleep, REM) classification using only sound data measured from 1,2000 subjects.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               PyMDE: Minimum-Distortion Embedding 
            </h5>
            <h6 class="card-subtitle">Akshay Agrawal, Alnur Ali, Stephen Boyd</h6>
            <p class="card-text">We present a unifying framework for the vector embedding problem: given a set of items and some known relationships between them, we seek a representation of the items by vectors, possibly subject to some constraints (e.g., requiring the vectors to have zero mean and identity covariance). We want the vectors associated with similar items to be near each other, and vectors associated with dissimilar items to not be near, measured in Euclidean distance. We formalize this by introducing distortion functions, defined for some pairs of the items. Our goal is to choose an embedding that minimizes the total distortion, subject to the constraints. We call this the minimum-distortion embedding (MDE) problem. The MDE framework generalizes many well-known embedding methods, such as PCA, the Laplacian eigenmap, multidimensional scaling, UMAP, and others, and also includes new types of embeddings.

Our accompanying software library, PyMDE, makes it easy for users to specify and approximately solve MDE problems, enabling experimentation with well-known and custom embeddings alike. By making use of automatic differentiation and hardware acceleration via PyTorch, we are able to scale to very large embedding problems. We will showcase examples of embedding real datasets, including an academic co-authorship network, single-cell mRNA transcriptomes, US census data, and population genetics.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K4.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K4.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K4.png">TorchIO: Pre-Processing & Augmentation of Medical Images for Deep Learning Applications</a>
              
            </h5>
            <h6 class="card-subtitle">Fernando Pérez-García, Rachel Sparks, Sébastien Ourselin</h6>
            <p class="card-text">Processing of medical images such as MRI or CT presents unique challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment of volumes.

We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be composed, reproduced, traced and extended. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.

TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages open science, as it supports reproducibility and is version controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K5.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K5.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K5.png">Deep Learning Based Model to Predict Covid19 Patients' Outcomes on Admission</a>
              
            </h5>
            <h6 class="card-subtitle">Laila Rasmy, Ziqian Xie, Degui Zhi</h6>
            <p class="card-text">With the extensive use of electronic records and the availability of historical patient information, predictive models that can help identify patients at risk based on their history at an early stage can be a valuable adjunct to clinician judgment. Deep learning models can better predict patients' outcomes by consuming their medical history regardless of the length and the complexity of such data. We used our Pytorch_EHR framework to train a model that can predict COVID-19 patient's health outcomes on admission. We used the Cerner Real-world COVID-19 (Q2) cohort which included information for 117,496 COVID patients from 62 health systems. We used a cohort of 55,068 patients and defined our outcomes including mortality, intubation, and hospitalization longer than 3 days as binary outcomes. We feed the model with all diagnoses, medication, laboratory results, and other clinical events information available before or on their first COVID-19 encounter admission date. We kept the data preprocessing at a minimum for convenience and practicality relying on the embedding layer that learns features representations from the large training set. Our model showed improved performance compared to other baseline machine learning models like logistic regression (LR). For in-hospital mortality, our model showed AUROC of 89.5%, 90.6%, and 84.3% for in-hospital mortality, intubation, and hospitalization for more than 3 days, respectively versus LR which showed 82.8%, 83.2%, and 76.8%</p>
            
            <p class="card-text">
              <a href="https://github.com/ZhiGroup/pytorch_ehr">https://github.com/ZhiGroup/pytorch_ehr</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Medical & Healthcare</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A1.png"> Rolling out Transformers with TorchScript and Inferentia</a>
              
            </h5>
            <h6 class="card-subtitle">Binghui Ouyang, Alexander O’Connor </h6>
            <p class="card-text">While Transformers have brought unprecedented improvements in the accuracy and ease of developing NLP applications, their deployment remains challenging due to the large size of the models and their computational complexity. 
 Indeed, until recently is has been a widespread misconception that hosting high-performance transformer-based models was prohibitively expensive, and technically challenging. Fortunately, recent advances in both the PyTorch ecosystem and in custom hardware for inference have created a world where models can be deployed in a cost-effective, scalable way, without the need for complex engineering.

In this presentation, we will discuss the use of PyTorch and AWS Inferentia to deploy production-scale models in chatbot intent classification - a particularly relevant and demanding scenario. 

Autodesk deploys a number of transformer based models to solve customer support issues across our channels, and our ability to provide a flexible, high-quality machine learning solution is supported by leveraging cutting-edge technology such as transformer based classification. Our chatbot, AVA, responds to tens of thousands of customer interactions monthly, and we are evolving our architecture to be supported by customer inference.

We will discuss our experience of piloting transformer-based intent models, and present a workflow for going from data to deployment for similar projects.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A2.png">PyTorchTS: PyTorch Probabilistic Time Series Forecasting Framework</a>
              
            </h5>
            <h6 class="card-subtitle">Kashif Rasul</h6>
            <p class="card-text">PyTorchTS is a PyTorch based Probabilistic Time Series forecasting framework that comes with state of the art univariate and multivariate models.</p>
            
            <p class="card-text">
              <a href="https://github.com/zalandoresearch/pytorch-ts">https://github.com/zalandoresearch/pytorch-ts</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A3.png">MMF: A modular framework for multimodal research</a>
              
            </h5>
            <h6 class="card-subtitle">Sasha Sheng, Amanpreet Singh</h6>
            <p class="card-text">MMF is designed from ground up to let you focus on what matters -- your model -- by providing boilerplate code for distributed training, common datasets and state-of-the-art pretrained baselines out-of-the-box. MMF is built on top of PyTorch that brings all of its power in your hands. MMF is not strongly opinionated. So you can use all of your PyTorch knowledge here. MMF is created to be easily extensible and composable. Through our modular design, you can use specific components from MMF that you care about. Our configuration system allows MMF to easily adapt to your needs.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               AllenNLP: An NLP research library for developing state-of-the-art models 
            </h5>
            <h6 class="card-subtitle">Dirk Groeneveld, Akshita Bhagia, Pete Walsh, Michael Schmitz</h6>
            <p class="card-text">An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.</p>
            
            <p class="card-text">
              <a href="https://github.com/allenai/allennlp">https://github.com/allenai/allennlp</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-B2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B2.png">Project Spock at Tubi: Understanding Content using Deep Learning for NLP</a>
              
            </h5>
            <h6 class="card-subtitle">John Trenkle, Jaya Kawale & Tubi ML team</h6>
            <p class="card-text">Tubi is one of the leading platforms providing free high-quality streaming movies and TV shows to a worldwide audience. We embrace a data-driven approach and leverage advanced machine learning techniques using PyTorch to enhance our platform and business in any way we can.  The Three Pillars of AVOD are the guiding principle for our work.  The Pillars are 
Content: all the titles we maintain in our library
Audience: everyone who watches titles on Tubi
Advertising: ads shown to viewers on behalf of brands

In this poster, we'll focus on the Content aspect with more details for the various use cases especially Content Understanding. Content is an important pillar of Tubi since to be successful, we need to look at existing titles and beyond what we already have and attempt to understand all of the titles out in the wild and how they could benefit our platform in some fashion. Content Understanding revolves around digesting a rich collection of 1st- and 3rd-party data in structured (metadata) and unstructured (text) forms and developing representations that capture the essence of those Titles. With the analogy of linear algebra, we can say we are attempting to project Title vectors from the universe to our tubiverse with as much fidelity as possible in order to ascertain potential value for each target use case. We will describe several techniques to understand content better using Pytorch.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-B3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B3.png">RL Based Performance Optimization of Deep Neural Networks</a>
              
            </h5>
            <h6 class="card-subtitle">Benoit Steiner, Chris Cummins, Horace He, Hugh Leather</h6>
            <p class="card-text">As the usage of machine learning techniques is becoming ubiquitous, the efficient execution of neural networks is crucial to many applications. Frameworks, such as Halide and TVM, separate the algorithmic representation of
the deep learning model from the schedule that determines its implementation. Finding good schedules, however, remains extremely challenging. Auto-tuning methods, which search the space of valid schedules and execute each candidate on the hardware, identify some of the best performing schedules, but the search can take hours, hampering the productivity of deep learning practitioners. What is needed is a method that achieves a similar performance without extensive search, delivering the needed efficiency quickly.

Using PyTorch, we model the scheduling process as a sequence of optimization choices, and implement a new technique to accurately predict the expected performance of a partial schedule using a LSTM over carefully engineered features that describe each DNN operator and their current scheduling choices. Leveraging these predictions we are able to make these optimization decisions greedily and, without any executions on the target hardware, rapidly identify an efficient schedule.
This techniques enables to find schedules that improve the execution performance of deep neural networks by 2.6× over Halide and 1.5× over TVM. Moreover, our technique completes in seconds instead of hours, making it  possible to include it as a new backend for PyTorch itself.</p>
            
            <p class="card-text">
              <a href="http://facebook.ai">http://facebook.ai</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C1.png">A Data-Centric Framework for Composable NLP</a>
              
            </h5>
            <h6 class="card-subtitle">Zhenghong Liu</h6>
            <p class="card-text">Forte is an open-source toolkit for building Natural Language Processing workflows via assembling state-of-the-art NLP and ML technologies. This toolkit features composable pipeline, cross-task interaction, adaptable data-model interfaces. The highly composable design allows users to build complex NLP pipelines of a wide range of tasks including document retrieval, information extraction, and text generation by combining existing toolkits or customized PyTorch models. The cross-task interaction ability allows developers to utilize the results from individual tasks to make informed decisions. The data-model interface helps developers to focus on building reusable PyTorch models by abstracting out domain and preprocessing details. We show that Forte can be used to build complex pipelines, and the resulting pipeline can be easily adapted to different domains and tasks with small changes in the code.</p>
            
            <p class="card-text">
              <a href="https://github.com/asyml/forte">https://github.com/asyml/forte</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C2.png">Environments and Baselines for Multitask Reinforcement Learning</a>
              
            </h5>
            <h6 class="card-subtitle">Shagun Sodhani, Amy Zhang, Ludovic Denoyer, Pierre-Alexandre Kamienny, Olivier Delalleau</h6>
            <p class="card-text">The two key components in a multi-task RL codebase are (i) Multi-task RL algorithms and (ii) Multi-task RL environments. We develop open-source libraries for both components. [MTRL](https://github.com/facebookresearch/mtrl) provides components to implement multi-task RL algorithms, and [MTEnv](https://github.com/facebookresearch/mtenv) is a library to interface with existing multi-task RL environments and create new ones.

MTRL has two building blocks: (i) single task policy and (ii) components to augment the single-task policy for multi-task setup. The ideal workflow is to start with a base policy and add multi-task components as they seem fit. MTRL enables algorithms like GradNorm, Distral, HiPBMDP, PCGrad, Soft Modularization, etc.

MTEnv is an effort to standardize multi-task RL environments and provide better benchmarks. We extend the Gym API to support multiple tasks, with two guiding principles: (i) Make minimal changes to the Gym Interface (which the community is very familiar with) and (ii) Make it easy to port existing environments to MTEnv. Additionally, we provide a collection of commonly used multi-task RL environments (Acrobot, Cartpole, Multitask variant of DeepMind Control Suite, Meta-World, Multi-armed Bandit, etc.). The RL practitioner can combine its own environments with the MTEnv wrappers to add multi-task support with a small code change.

MTRL and MTEnv are used in several ongoing/published works at FAIR.</p>
            
            <p class="card-text">
              <a href="http://qr.w69b.com/g/tGZSFw33G">http://qr.w69b.com/g/tGZSFw33G</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C3.png">The Hugging Face Ecosystem</a>
              
            </h5>
            <h6 class="card-subtitle">Lysandre Debut, Sylvain Gugger, Quentin Lhoest </h6>
            <p class="card-text">Transfer learning has become the norm to get state-of-the-art results in NLP. Hugging Face provides you with tools to help you on every step along the way:

- A free git-based shared hub with more than 7,500 PyTorch checkpoints, and more than 800 NLP datasets.
- The ? Datasets library, to easily download the dataset, manipulate it and prepare it.
- The ? Tokenizers library, that provides ultra-fast tokenizers backed by Rust, and converts text in PyTorch tensors.
- The ? Transformers library, providing more than 45 PyTorch implementations of Transformer architectures as simple nn.Module as well as a training API.
- The ? Accelerate library, a non-intrusive API that allows you to run your raw training loop on any distributed setup.

The pipeline is then simply a six-step process: select a pretrained model from the hub, handle the data with Datasets, tokenize the text with Tokenizers, load the model with Transformers, train it with the Trainer or your own loop powered by Accelerate, before sharing your results with the community on the hub.</p>
            
            <p class="card-text">
              <a href="https://huggingface.co/models">https://huggingface.co/models</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D1.png"> Asteroid: the Pytorch-based Audio Source Separation Toolkit for Researchers</a>
              
            </h5>
            <h6 class="card-subtitle">Manuel Pariente, Samuele Cornell, Jonas Haag, Joris Cosentino, Michel Olvera, Fabian-Robert Stöter, Efthymios Tzinis</h6>
            <p class="card-text">Asteroid is an audio source separation toolkit built with PyTorch and PyTorch-Lightning. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, recipes on common audio source separation datasets are provided, including all the steps from data download\preparation through training to evaluation as well as many current state-of-the-art DNN models. Asteroid exposes all levels of granularity to the user from simple layers to complete ready-to-use models. Our pretrained models are hosted on the asteroid-models community in Zenodo and on the Huggingface model Hub. Loading and using pretrained models is trivial and sharing them is also made easy with asteroid's CLI.","poster_showcase":"Audio Source Separation, Speech Processing, Deep Learning","email":"cornellsamuele@gmail.com"}</p>
            
            <p class="card-text">
              <a href="https://asteroid-team.github.io/">https://asteroid-team.github.io/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D2.png">rlstructures: A Lightweight Python Library for Reinforcement Learning Research</a>
              
            </h5>
            <h6 class="card-subtitle">Ludovic Denoyer, Danielle Rothermel, Xavier Martinet</h6>
            <p class="card-text">RLStructures is a lightweight Python library that provides simple APIs as well as data structures that make as few assumptions as possible about the structure of your agent or your task, while allowing for transparently executing multiple policies on multiple environments in parallel (incl. multiple GPUs). It thus facilitates the implementation of RL algorithms while avoiding complex abstractions.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D3.png">MBRL-Lib: a PyTorch toolbox for model-based reinforcement learning research</a>
              
            </h5>
            <h6 class="card-subtitle">Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, Roberto Calandra</h6>
            <p class="card-text">Model-based reinforcement learning (MBRL) is an active area of research with enormous potential. In contrast to model-free RL, MBRL algorithms solve tasks by learning a predictive model of the task dynamics, and use this model to predict the future and facilitate decision making. Many researchers have argued that MBRL can result in lower sample complexity, better generalization, as well as safer and more interpretable decisions. However, despite the surge in popularity and great potential of MBRL, there is currently no widely accepted library for facilitating research in this area. Since MBRL methods often involve the interplay of complex components such as probabilistic ensembles, latent variable models, planning algorithms, and even model-free methods, the lack of such a library raises the entry bar to the field and slows down research efforts. In this work we aim to solve this problem by introducing MBRL-Lib, a modular PyTorch toolbox specifically designed for facilitating research on model-based reinforcement learning. MBRL-Lib provides interchangeable options for training dynamics models and running planning algorithms, which can then be used in a mix and match fashion to create novel MBRL methods. The library also provides a set of utility functions to run common MBRL tasks, as well a set of diagnostics tools to identify potential issues while training dynamics models and control algorithms.</p>
            
            <p class="card-text">
              <a href="https://github.com/facebookresearch/mbrl-lib">https://github.com/facebookresearch/mbrl-lib</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">NLP & Multimodal, RL & Time Series</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H6.png">Introducing New PyTorch Profiler</a>
              
            </h5>
            <h6 class="card-subtitle">Geeta Chauhan, Gisle Dankel, Elena Neroslavaskaya</h6>
            <p class="card-text">Analyzing and improving large-scale deep learning model performance is an ongoing challenge that continues to grow in importance as the model sizes increase. Microsoft and Facebook collaborated to create a native PyTorch performance debugging tool called PyTorch Profiler. The profiler builds on the PyTorch autograd profiler foundation, adds a new high fidelity GPU profiling engine, and out-of-the-box bottleneck analysis tool in Tensorboard. New Profiler delivers the simplest experience available to date where users can profile their models without installing any additional packages and see results immediately in Tensorboard. Until today, beginner users of PyTorch may not have attempted to profile their models due to the task complexity. With the new bottleneck analysis tool, they will find profiling easy and accessible. Experienced users will be delighted by the detailed trace views which illustrate GPU kernel execution events and their relationship to the PyTorch operations. Come learn how to profile your PyTorch models using this new delightfully simple tool.</p>
            
            <p class="card-text">
              <a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool">https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Performance & Profiler</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I6.png">TRTorch: A Compiler for TorchScript Targeting NVIDIA GPUs with TensorRT</a>
              
            </h5>
            <h6 class="card-subtitle">Naren Dasan</h6>
            <p class="card-text">For experimentation and the development of machine learning models, few tools are as approachable as PyTorch. However, when moving from research to production, some of the features that make PyTorch great for development make it hard to deploy. With the introduction of TorchScript, PyTorch has solid tooling for addressing some of the problems of deploying PyTorch models. TorchScript removes the dependency on Python and produces portable, self contained, static representations of code and weights. But in addition to portability, users also look to optimize performance in deployment. When deploying on NVIDIA GPUs, TensorRT, NVIDIA's deep learning optimizer, provides the capability to maximize performance of workloads by tuning the execution of models for specific target hardware. TensorRT also provides tooling for conducting further optimization through mixed and reduced precision execution and post training quantization (PTQ). We present TRTorch, a compiler for PyTorch and TorchScript targeting NVIDIA GPUs, which combines the usability of PyTorch with the performance of TensorRT and allows users to fully optimize their inference workloads without leaving the PyTorch ecosystem. It also simplifies conducting complex optimizations like PTQ by leveraging common PyTorch tooling. TRTorch can be used directly from PyTorch as a TorchScript Backend, embedded in an application or used from the command line to easily increase the performance of inference applications.</p>
            
            <p class="card-text">
              <a href="https://nvidia.github.io/TRTorch/">https://nvidia.github.io/TRTorch/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Performance & Profiler</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J6.png">WeightWatcher: A Diagnostic Tool for DNNs</a>
              
            </h5>
            <h6 class="card-subtitle">Charles H. Martin</h6>
            <p class="card-text">WeightWatcher (WW) is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data. It can be used to: analyze pre/trained pyTorch models; 
inspect models that are difficult to train; gauge improvements in model performance; predict test accuracies across different models; and detect potential problems when compressing or fine-tuning pretrained models.

WeightWatcher is based on theoretical research (done in\-joint with UC Berkeley) into "Why Deep Learning Works", using ideas from Random Matrix Theory (RMT), Statistical Mechanics, and Strongly Correlated Systems.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Performance & Profiler</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               Constrained Optimization in PyTorch 1.9 Through Parametrizations 
            </h5>
            <h6 class="card-subtitle">Mario Lezcano-Casado</h6>
            <p class="card-text">"This poster presents the ""parametrizations"" feature that will be added to PyTorch in 1.9.0.
This feature allows for a simple implementation of methods like pruning, weight_normalization or spectral_normalization.
More generally, it implements a way to have ""computed parameters"". This means that we replace a parameter `weight` in a layer with `f(weight)`, where `f` is an arbitrary module. In other words, after putting a parametrization `f` on `layer.weight`, `layer.weight` will return `f(weight)`.
They implement a caching system, so that the value `f(weight)` is computed just once during the forward pass.
A module that implements a parametrisation may also have a `right_inverse` method. If this method is present, it is possible to assign to a parametrised tensor. This is useful when initialising a parametrised tensor.
This feature can be seen as a first step towards invertible modules. In particular, it may also help making distributions first-class citizens in PyTorch.
Parametrisations also allows for a simple implementation of constrained optimisation. From this perspective, parametrisation maps an unconstrained tensor to a constrained space such as the space of orthogonal matrices, SPD matrices, low-rank matrices... This approach is implemented in the library GeoTorch (https://github.com/Lezcano/geotorch/)."</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Performance & Profiler</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H1.png">Distributed Pytorch with Ray</a>
              
            </h5>
            <h6 class="card-subtitle">Richard Liaw, Kai Fricke, Amog Kamsetty, Michael Galarnyk</h6>
            <p class="card-text">Ray is a popular framework for distributed Python that can be paired with PyTorch to rapidly scale machine learning applications. Ray contains a large ecosystem of applications and libraries that leverage and integrate with Pytorch. This includes Ray Tune, a Python library for experiment execution and hyperparameter tuning at any scale; RLlib, a state-of-the-art library for reinforcement learning; and Ray Serve, a library for scalable model serving. Together, Ray and Pytorch are becoming the core foundation for the next generation of production machine learning platforms.</p>
            
            <p class="card-text">
              <a href="https://ray.io/">https://ray.io/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H2.png">Avalanche: an End-to-End Library for Continual Learning based on PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Vincenzo Lomonaco, Lorenzo Pellegrini Andrea Cossu, Antonio Carta, Gabriele Graffieti</h6>
            <p class="card-text">Learning continually from non-stationary data stream is a long sought goal of machine learning research. Recently, we have witnessed a renewed and fast-growing interest in Continual Learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose an open-source, end-to-end library for continual learning based on PyTorch that may provide a shared and collaborative code-base for fast prototyping, training and reproducible evaluation of continual learning algorithms.</p>
            
            <p class="card-text">
              <a href="https://avalanche.continualai.org">https://avalanche.continualai.org</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-H3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/H3.png">PyTorch on IBM Z and LinuxONE (s390x)</a>
              
            </h5>
            <h6 class="card-subtitle">Hong Xu</h6>
            <p class="card-text">IBM Z is a hardware product line for mission-critical applications, such as finance and health applications. It employs its own CPU architecture, which PyTorch does not officially support. In this poster, we discuss why it is important to support PyTorch on Z. Then, we show our prebuilt minimal PyTorch package for IBM Z. Finally, we demonstrate our continuing commitment to make more PyTorch features available on IBM Z.</p>
            
            <p class="card-text">
              <a href="https://codait.github.io/pytorch-on-z">https://codait.github.io/pytorch-on-z</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I1.png">The Fundamentals of MLOps for R&D: Orchestration, Automation, Reproducibility</a>
              
            </h5>
            <h6 class="card-subtitle">Dr. Ariel Biller</h6>
            <p class="card-text">Both from sanity considerations and the productivity perspective, Data Scientists, ML engineers, Graduate students, and other research-facing roles are all starting to adopt best-practices from production-grade MLOps.

However, most toolchains come with a hefty price of extra code and maintenance, which reduces the actual time available for R&D. We will show an alternative approach using ClearML, the open-source MLOps solution.

In this "best-practices" poster, we will overview the "must-haves" of R&D-MLOPs: 
Orchestration, Automation, and Reproducibility. These enable easy remote execution through magically reproducible setups and even custom, reusable, bottom-up pipelines.

We will take a single example and schematically transform it from the "as downloaded from GitHub" stage to a fully-fledged, scalable, version-controlled, parameterizable R&D pipeline. We will measure the number of changes needed to the codebase and provide evidence of real low-cost integration. All code, logs, and metrics will be available as supporting information.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I2.png">FairTorch: Aspiring to Mitigate the Unfairness of Machine Learning Models</a>
              
            </h5>
            <h6 class="card-subtitle">Masashi Sode, Akihiko Fukuchi, Yoki Yabe, Yasufumi Nakata</h6>
            <p class="card-text">Is your machine learning model fair enough to be used in your system? What if a recruiting AI discriminates on gender and race? What if the accuracy of medical AI depends on a person's annual income or on the GDP of the country where it is used? Today's AI has the potential to cause such problems. In recent years, fairness in machine learning has received increasing attention. If current machine learning models used for decision making may cause unfair discrimination, developing a fair machine learning model is an important goal in many areas, such as medicine, employment, and politics. Despite the importance of this goal to society, as of 2020, there was no PyTorch¹ project incorporating fairness into a machine learning model. To solve this problem, we created FairTorch at the PyTorch Summer Hackathon 2020.

FairTorch provides a tool to mitigate the unfairness of machine learning models. A unique feature of our tool is that it allows you to add a fairness constraint to your model by adding only a few lines of code, using the fairness criteria provided in the library.</p>
            
            <p class="card-text">
              <a href="https://github.com/wbawakate/fairtorch">https://github.com/wbawakate/fairtorch</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-I3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/I3.png">TorchDrift: Drift Detection for PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Thomas Viehmann, Luca Antiga</h6>
            <p class="card-text">When machine learning models are deployed to solve a given task, a crucial question is whether they are actually able to perform as expected. TorchDrift addresses one aspect of the answer, namely drift detection, or whether the information flowing through our models - either probed at the input, output or somewhere in-between - is still consistent with the one it was trained and evaluated on. In a nutshell, TorchDrift is designed to be plugged into PyTorch models and check whether they are operating within spec.
TorchDrift's principles apply PyTorch's motto _from research to production_ to drift detection: We provide a library of methods that canbe used as baselines or building blocks for drift detection research, as well as provide practitioners deploying PyTorch models in production with up-to-date methods and educational material for building the necessary statistical background. Here we introduce TorchDrift with an example illustrating the underlying two-sample tests. We show how TorchDrift can be integrated in high-performance runtimes such as TorchServe or RedisAI, to enable drift detection in real-world applications thanks to the PyTorch JIT.</p>
            
            <p class="card-text">
              <a href="https://torchdrift.org/">https://torchdrift.org/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J1.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J1.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J1.png">Ouroboros: MLOps for Automated Driving</a>
              
            </h5>
            <h6 class="card-subtitle">Quincy Chen, Arjun Bhargava, Sudeep Pillai, Marcus Pan, Chao Fang, Chris   Ochoa, Adrien Gaidon, Kuan-Hui Lee, Wolfram Burgard</h6>
            <p class="card-text">Modern machine learning for autonomous vehicles requires a fundamentally different infrastructure and production lifecycle from their standard software continuous-integration/continuous-deployment counterparts. At Toyota Research Institute (TRI), we have developed ​Ouroboros​ - a modern ML platform that supports the end-to-end lifecycle of all ML models delivered to TRI's autonomous vehicle fleets. We envision that all ML models delivered to our fleet undergo a systematic and rigorous treatment. Ouroboros delivers several essential features including:
a. ML dataset governance and infrastructure-as-code​ that ensures the traceability, reproducibility, standardization, and fairness for all ML datasets and models procedurally generated and delivered to the TRI fleet.
b. Unified ML dataset and model management:​ An unified and streamlined workflow for ML dataset curation, label management, and model development that supports several key ML models delivered to the TRI fleet today
c. A Large-scale Multi-task, Multi-modal Dataset for Automated Driving​ that supports the development of various models today, including 3D object detection, 2D object detection, 2D BeVFlow, Panoptic Segmentation;
d. Orchestrated ML workflows​ to stand up scalable ML applications such as push-button re-training solutions, ML CI/CDs pipelines, Dataset Curation workflows, Auto-labelling pipelines, leveraging the most up-to-date cloud tools available. along their lifecycles, ensuring strong governance on building reusable, reproducible, robust, traceable, and fair ML models for the production driving setting. By following the best MLOps practices, we expect our platform to lay the foundation for continuous life-long learning in our autonomous vehicle fleets and accelerate the transition from research to production.</p>
            
            <p class="card-text">
              <a href="https://github.com/TRI-ML">https://github.com/TRI-ML</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J2.png">carefree-learn: Tabular Datasets ❤️ PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Yujian He</h6>
            <p class="card-text">carefree-learn makes PyTorch accessible to people who are familiar with machine learning but not necessarily PyTorch. By having already implemented all the pre-processing and post-processing under the hood, users can focus on implementing the core machine learning algorithms / models with PyTorch and test them on various datasets. By having designed the whole structure carefully, users can easily customize every block in the whole pipeline, and can also 'combine' the implemented blocks to 'construct' new models without efforts. By having carefully made abstractions users can adapt it to their specific down-stream tasks, such as quantitative trading (in fact I've already implemented one for my company and it works pretty well XD). carefree-learn handles distributed training carefully, so users can either run multiple tasks at the same time, or run a huge model with DDP in one line of code. carefree-learn also integrates with mlflow and supports exporting to ONNX, which means it is ready for production to some extend.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-J3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/J3.png">OpenMMLab: An Open-Source Algorithm Platform for Computer Vision</a>
              
            </h5>
            <h6 class="card-subtitle">Wenwei Zhang</h6>
            <p class="card-text">OpenMMLab project builds open-source toolboxes for Artificial Intelligence (AI). It aims to 1) provide high-quality codebases to reduce the difficulties in algorithm reimplementation; 2) provide a complete research platform to accelerate the research production; and 3) shorten the gap between research production to the industrial applications. Based on PyTorch, OpenMMLab develops MMCV to provide unified abstract training APIs and common utils, which serves as a foundation of 15+ toolboxes and 40+ datasets.

Since the initial release in October 2018, OpenMMLab has released 15+ toolboxes that cover 10+ directions, implement 100+ algorithms, and contain 1000+ pre-trained models. With a tighter collaboration with the community, OpenMMLab will release more toolboxes with more flexible and easy-to-use training frameworks in the future.</p>
            
            <p class="card-text">
              <a href="https://openmmlab.com/">https://openmmlab.com/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K2.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K2.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K2.png">Catalyst – Accelerated deep learning R&D</a>
              
            </h5>
            <h6 class="card-subtitle">Sergey Kolesnikov</h6>
            <p class="card-text">For the last three years, Catalyst-Team and collaborators have been working on Catalyst  -  a high-level PyTorch framework Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. You get metrics, model checkpointing, advanced logging, and distributed training support without the boilerplate and low-level bugs.</p>
            
            <p class="card-text">
              <a href="https://catalyst-team.com">https://catalyst-team.com</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K3.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-K3.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/K3.png">High-fidelity performance metrics for generative models in PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Anton Obukhov</h6>
            <p class="card-text">Evaluation of generative models such as GANs is an important part of deep learning research. In 2D image generation, three approaches became widely spread: Inception Score, Fréchet Inception Distance, and Kernel Inception Distance. Despite having a clear mathematical and algorithmic description, these metrics were initially implemented in TensorFlow and inherited a few properties of the framework itself, such as a specific implementation of the interpolation function. These design decisions were effectively baked into the evaluation protocol and became an inherent part of the specification of the metrics. As a result, researchers wishing to compare against state of the art in generative modeling are forced to perform an evaluation using the original metric authors' codebases. Reimplementations of metrics in PyTorch and other frameworks exist, but they do not provide a proper level of fidelity, thus making them unsuitable for reporting results and comparing them to other methods. This software aims to provide epsilon-exact implementations of the said metrics in PyTorch and remove inconveniences associated with generative model evaluation and development. All the evaluation pipeline steps are correctly tested, with relative errors and sources of remaining non-determinism summarized in sections below.
TLDR; fast and reliable GAN evaluation in PyTorch</p>
            
            <p class="card-text">
              <a href="https://github.com/toshas/torch-fidelity">https://github.com/toshas/torch-fidelity</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Platforms & Ops & Tools</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A6.png">Using Satellite Imagery to Identify Oceanic Oil Pollution</a>
              
            </h5>
            <h6 class="card-subtitle">Jona Raphael (jona@skytruth.org), Ben Eggleston, Ryan Covington, Tatianna Evanisko, John Amos</h6>
            <p class="card-text">Operational oil discharges from ships, also known as "bilge dumping," have been identified as a major source of petroleum products entering our oceans, cumulatively exceeding the largest oil spills, such as the Exxon Valdez and Deepwater Horizon spills, even when considered over short time spans. However, we still don't have a good estimate of
● How much oil is being discharged;
● Where the discharge is happening;
● Who the responsible vessels are.
This makes it difficult to prevent and effectively respond to oil pollution that can damage our marine and coastal environments and economies that depend on them.

In this poster we will share SkyTruth's recent work to address these gaps using machine learning tools to detect oil pollution events and identify the responsible vessels when possible. We use a convolutional neural network (CNN) in a ResNet-34 architecture to perform pixel segmentation on all incoming Sentinel-1 synthetic aperture radar (SAR) imagery to classify slicks. Despite the satellites' incomplete oceanic coverage, we have been detecting an average of 135 vessel slicks per month, and have identified several geographic hotspots where oily discharges are occurring regularly. For the images that capture a vessel in the act of discharging oil, we rely on an Automatic Identification System (AIS) database to extract details about the ships, including vessel type and flag state. We will share our experience
● Making sufficient training data from inherently sparse satellite image datasets;
● Building a computer vision model using PyTorch and fastai;
● Fully automating the process in the Amazon Web Services (AWS) cloud.
The application has been running continuously since August 2020, has processed over 380,000 Sentinel-1 images, and has populated a database with more than 1100 high-confidence slicks from vessels. We will be discussing preliminary results from this dataset and remaining challenges to be overcome.
Learn more at https://skytruth.org/bilge-dumping/</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A7.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A7.png">UPIT: A fastai Package for Unpaired Image-to-Image Translation</a>
              
            </h5>
            <h6 class="card-subtitle">Tanishq Abraham</h6>
            <p class="card-text">Unpaired image-to-image translation algorithms have been used for various computer vision tasks like style transfer and domain adaption. Such algorithms are highly attractive because they alleviate the need for the collection of paired datasets. In this poster, we demonstrate UPIT, a novel fastai/PyTorch package (built with nbdev) for unpaired image-to-image translation. It implements various state-of-the-art unpaired image-to-image translation algorithms such as CycleGAN, DualGAN, UNIT, and more. It enables simple training and inference on unpaired datasets. It also comes with implementations of commonly used metrics like FID, KID, and LPIPS. It also comes with Weights-and-Biases integration for easy experiment tracking. Since it is built on top of fastai and PyTorch, it comes with support for mixed-precision and multi-GPU training. It is highly flexible, and custom dataset types, models, and metrics can be used as well. With UPIT, training and applying unpaired image-to-image translation only takes a few lines of code.</p>
            
            <p class="card-text">
              <a href="https://github.com/tmabraham/UPIT">https://github.com/tmabraham/UPIT</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-A8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/A8.png">PyTorchVideo: A Deep Learning Library for Video Understanding</a>
              
            </h5>
            <h6 class="card-subtitle">Aaron Adcock, Bo Xiong, Christoph Feichtenhofer, Haoqi Fan, Heng Wang, Kalyan Vasudev Alwala, Matt Feiszli, Tullie Murrell, Wan-Yen Lo, Yanghao Li, Yilei Li, Zhicheng Yan </h6>
            <p class="card-text">PyTorchVideo is the new Facebook AI deep learning library for video understanding research. It contains variety of state of the art pretrained video models, dataset, augmentation, tools for video understanding. PyTorchVideo provides efficient video components on accelerated inference on mobile device.</p>
            
            <p class="card-text">
              <a href="https://pytorchvideo.org/">https://pytorchvideo.org/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <h5 class="card-title">
               Deep Learning Enables Fast and Dense Single-Molecule Localization with High Accuracy 
            </h5>
            <h6 class="card-subtitle">A. Speiser, L-R. Müller, P. Hoess, U. Matti, C. J. Obara, J. H. Macke, J. Ries, S. C. Turaga</h6>
            <p class="card-text">Single-molecule localization microscopy (SMLM) has had remarkable success in imaging cellular structures with nanometer resolution, but the need for activating only single isolated emitters limits imaging speed and labeling density. Here, we overcome this major limitation using deep learning. We developed DECODE, a computational tool that can localize single emitters at high density in 3D with the highest accuracy for a large range of imaging modalities and conditions. In a public software benchmark competition, it outperformed all other fitters on 12 out of 12 data-sets when comparing both detection accuracy and localization error, often by a substantial margin. DECODE allowed us to take live-cell SMLM data with reduced light exposure in just 3 seconds and to image microtubules at ultra-high labeling density. Packaged for simple installation and use, DECODE will enable many labs to reduce imaging times and increase localization density in SMLM.</p>
            
            <p class="card-text">
              <a href="http://github.com/turagalab/decode">http://github.com/turagalab/decode</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-B7.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B7.png">A Robust PyTorch Trainable Entry Convnet Layer in Fourier Domain</a>
              
            </h5>
            <h6 class="card-subtitle">Abraham Sánchez, Guillermo Mendoza, E. Ulises Moya-Sánchez</h6>
            <p class="card-text">We draw inspiration from the cortical area V1. We try to mimic their main processing properties by means of: quaternion local phase/orientation to compute lines and edges detection in a specific direction. We analyze how this layer is robust by its greometry to large illumination and brightness changes.</p>
            
            <p class="card-text">
              <a href="https://gitlab.com/ab.sanchezperez/pytorch-monogenic">https://gitlab.com/ab.sanchezperez/pytorch-monogenic</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-B8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/B8.png">PyroNear: Embedded Deep Learning for Early Wildfire Detection</a>
              
            </h5>
            <h6 class="card-subtitle">François-Guillaume Fernandez, Mateo Lostanlen, Sebastien Elmaleh, Bruno Lenzi, Felix Veith, and more than 15+ contributors</h6>
            <p class="card-text">"PyroNear is non-profit organization composed solely of volunteers which was created in late 2019. Our core belief is that recent technological developments can support the cohabitation between mankind & its natural habitat. We strive towards high-performing, accessible & affordable tech-solutions for protection against natural hazards. More specifically, our first efforts are focused on wildfire protection by increasing the coverage of automatic detection systems.

Our ongoing initiative has now gathered dozens of volunteers to put up the following main contributions:
- Computer Vision: compiling open-source models and datasets (soon to be published) for vision tasks related to wildfire detection
- Edge Computing: developing an affordable physical prototype running our PyTorch model on a Raspberry Pi
- End-to-end detection workflow: building a responsible end-to-end system for large scale detection and alert management (API, front-end monitoring platform)
- Deployment: working with French firefighter departments to gather field knowledge and conduct a test phase over the incoming European summer."
PyTorch3D is a modular and optimized library for 3D Deep Learning with PyTorch. It includes support for: data structures for heterogeneous batching of 3D data (Meshes, Point clouds and Volumes), optimized 3D operators and loss functions (with custom CUDA kernels), a modular differentiable rendering API for Meshes, Point clouds and Implicit functions, as well as several other tools for 3D Deep Learning.</p>
            
            <p class="card-text">
              <a href="https://github.com/pyronear">https://github.com/pyronear</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C6.png">PyTorch3D: Fast, Flexible, 3D Deep Learning  </a>
              
            </h5>
            <h6 class="card-subtitle">Nikhila Ravi, Jeremy Reizenstein, David Novotny, Justin Johnson, Georgia Gkioxari, Roman Shapovalov, Patrick Labatut, Wan-Yen Lo</h6>
            <p class="card-text">PyTorch3D is a modular and optimized library for 3D Deep Learning with PyTorch. It includes support for: data structures for heterogeneous batching of 3D data (Meshes, Point clouds and Volumes), optimized 3D operators and loss functions (with custom CUDA kernels), a modular differentiable rendering API for Meshes, Point clouds and Implicit functions, as well as several other tools for 3D Deep Learning.</p>
            
            <p class="card-text">
              <a href="https://arxiv.org/abs/2007.08501">https://arxiv.org/abs/2007.08501</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C7.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C7.png">Kornia: an Open Source Differentiable Computer Vision Library for PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">E. Riba, J. Shi, D. Mishkin, L. Ferraz, A. Nicolao</h6>
            <p class="card-text">This work presents Kornia, an open source computer vision library built upon a set of differentiable routines and modules that aims to solve generic computer vision problems. The package uses PyTorch as its main backend, not only for efficiency but also to take advantage of the reverse auto-differentiation engine to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be integrated into neural networks to train models to perform a wide range of operations including image transformations,camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations on graphical processing units, generating faster systems. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.</p>
            
            <p class="card-text">
              <a href="http://www.kornia.org">http://www.kornia.org</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-C8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/C8.png">NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in PyTorch</a>
              
            </h5>
            <h6 class="card-subtitle">Thomas George</h6>
            <p class="card-text">Fisher Information Matrices (FIM) and Neural Tangent Kernels (NTK) are useful tools in a number of diverse applications related to neural networks. Yet these theoretical tools are often difficult to implement using current libraries for practical size networks, given that they require per-example gradients, and a large amount of memory since they scale as the number of parameters (for the FIM) or the number of examples x cardinality of the output space (for the NTK). NNGeometry is a PyTorch library that offers a high level API for computing various linear algebra operations such as matrix-vector products, trace, frobenius norm, and so on, where the matrix is either the FIM or the NTK, leveraging recent advances in approximating these matrices.</p>
            
            <p class="card-text">
              <a href="https://github.com/tfjgeorge/nngeometry/">https://github.com/tfjgeorge/nngeometry/</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D6.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D6.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D6.png">CompressAI: a research library and evaluation platform for end-to-end compression  </a>
              
            </h5>
            <h6 class="card-subtitle">Bégaint J., Racapé F., Feltman S., Pushparaja A.</h6>
            <p class="card-text">CompressAI is a PyTorch library that provides custom operations, layers, modules and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, CompressAI includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. State-of-the-art end-to-end compression models have been reimplemented in PyTorch and trained from scratch, reproducing published results and allowing further research in the domain.</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D7.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D7.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D7.png">pystiche: A Framework for Neural Style Transfer</a>
              
            </h5>
            <h6 class="card-subtitle">Philip Meier, Volker Lohweg</h6>
            <p class="card-text">The seminal work of Gatys, Ecker, and Bethge gave birth to the field of _Neural Style Transfer_ (NST) in 2016. An NST describes the merger between the content and artistic style of two arbitrary images. This idea is nothing new in the field of Non-photorealistic rendering (NPR). What distinguishes NST from traditional NPR approaches is its generality: an NST only needs a single arbitrary content and style image as input and thus "makes -- for the first time -- a generalized style transfer practicable". Besides peripheral tasks, an NST at its core is the definition of an optimization criterion called _perceptual loss_, which estimates the perceptual quality of the stylized image. Usually the perceptual loss comprises a deep neural network that needs to supply encodings of images from various depths. 

`pystiche` is a library for NST written in Python and built upon PyTorch. It provides modular and efficient implementations for commonly used perceptual losses as well as neural net architectures. This enables users to mix current state-of-the-art techniques with new ideas with ease. This poster will showcase the core concepts of `pystiche` that will enable other researchers as well as lay persons to got an NST running in minutes.</p>
            
            <p class="card-text">
              <a href="https://github.com/pmeier/pystiche">https://github.com/pmeier/pystiche</a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
        <div class="card">
          <div class="card-body">
            
            <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D8.png"
              ><img src="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/thumb-D8.png"
            /></a>
            
            <h5 class="card-title">
              
              <a href="https://s3.amazonaws.com/assets.pytorch.org/pted2021/posters/D8.png"> GaNDLF – A Generally Nuanced Deep Learning Framework for Clinical Imaging Workflows</a>
              
            </h5>
            <h6 class="card-subtitle">Siddhish Thakur</h6>
            <p class="card-text"> Deep Learning (DL) has greatly highlighted the potential impact of optimized machine learning in both the scientific
and clinical communities. The advent of open-source DL libraries from major industrial entities, such as TensorFlow
(Google), PyTorch (Facebook), further contributes to DL promises on the democratization of computational analytics. However, increased technical and specialized background is required to develop DL algorithms, and the variability of implementation details hinders their reproducibility. Towards lowering the barrier and making the mechanism of DL development, training, and inference more stable, reproducible, and scalable, without requiring an extensive technical background, this manuscript proposes the Generally Nuanced Deep Learning Framework (GaNDLF). With built-in support for k-fold cross-validation, data augmentation, multiple modalities and output classes, and multi-GPU training, as well as the ability to work with both radiographic and histologic imaging, GaNDLF aims to provide an end-to-end solution for all DL-related tasks, to tackle problems in medical imaging and provide a robust application framework for deployment in clinical workflows.

Keywords: Deep Learning, Framework, Segmentation, Regression, Classification, Cross-validation, Data
augmentation, Deployment, Clinical, Workflows</p>
            
            <p class="card-text">
              <a href=""></a>
            </p>
            
            <p class="card-text">
              <small class="text-muted">Vision</small>
            </p>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</div>
<script type="text/javascript">
  // this isn't efficient please don't use for more than a few hundred items
  $("#pted-filter").on("keyup", function () {
    var input = $(this).val().toLowerCase();

    $(".card").filter(function () {
      $(this).toggle($(this).text().toLowerCase().indexOf(input) > -1);
    });
  });
</script>


    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p
        class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
    
    
        <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
        <script>
          hbspt.forms.create({
            region: "na1",
            portalId: "8112310",
            formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
          });
        </script>
        
    
      <p
        class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
        
    </div>
    


    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA" target="_blank" title="PyTorch Slack">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack"><path fill="currentColor" d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z"></path></svg>
        </a></li>
        <li><a href="/wechat" title="PyTorch on WeChat">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat"><path fill="currentColor" d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z"></path><path fill="currentColor" d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z"></path></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
          <li>
            <a href="/new">New to PyTorch Foundation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Tools</a>
          </li>
          <li>
            <a href="/join-ecosystem">Join the Ecosystem</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2024">Contributor Awards - 2024</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
          <li>
            <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
          <li>
            <a href="/newsletter">Newsletter</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="/credits">Cloud Credit Program</a>
          </li>
          <li>          
            <a href="/tac">Technical Advisory Council</a>
          </li>
          <li>
            <a href="/staff">Staff</a>
          </li>
          <li>
            <a href="/contact-us">Contact Us</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


  </body>
</html>

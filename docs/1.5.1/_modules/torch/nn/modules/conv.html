


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.conv &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='http://pytorch.org/docs/versions.html'>1.5.1 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc/index.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchvision/index.html">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.conv</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_reverse_repeat_tuple</span>
<span class="kn">from</span> <span class="nn">..._jit_internal</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>


<span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="s1">&#39;groups&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="s1">&#39;in_channels&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;out_channels&#39;</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">]</span>
    <span class="vm">__annotations__</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;in_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="n">valid_padding_modes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="s1">&#39;replicate&#39;</span><span class="p">,</span> <span class="s1">&#39;circular&#39;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_padding_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode must be one of </span><span class="si">{}</span><span class="s2">, but got padding_mode=&#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">valid_padding_modes</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">transposed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">output_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="c1"># `_reversed_padding_repeated_twice` is the padding to be passed to</span>
        <span class="c1"># `F.pad` if needed (e.g., for non-zero padding types that are</span>
        <span class="c1"># implemented as two ops: padding + conv). `F.pad` accepts paddings in</span>
        <span class="c1"># reverse order than the dimension.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span> <span class="o">=</span> <span class="n">_reverse_repeat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
            <span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{in_channels}</span><span class="s1">, </span><span class="si">{out_channels}</span><span class="s1">, kernel_size=</span><span class="si">{kernel_size}</span><span class="s1">&#39;</span>
             <span class="s1">&#39;, stride=</span><span class="si">{stride}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding=</span><span class="si">{padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, dilation=</span><span class="si">{dilation}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, output_padding=</span><span class="si">{output_padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, groups=</span><span class="si">{groups}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, bias=False&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding_mode=</span><span class="si">{padding_mode}</span><span class="s1">&#39;</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;padding_mode&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>


<div class="viewcode-block" id="Conv1d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.Conv1d">[docs]</a><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size</span>
<span class="sd">    :math:`(N, C_{\text{in}}, L)` and output :math:`(N, C_{\text{out}}, L_{\text{out}})` can be</span>
<span class="sd">    precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)</span>
<span class="sd">        \star \text{input}(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid `cross-correlation`_ operator,</span>
<span class="sd">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="sd">    :math:`L` is a length of signal sequence.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="sd">      number or a one-element tuple.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both sides</span>
<span class="sd">      for :attr:`padding` number of points.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="sd">      known as the Ã  trous algorithm. It is harder to describe, but this `link`_</span>
<span class="sd">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters,</span>
<span class="sd">          of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        Depending of the size of your kernel, several (of the last)</span>
<span class="sd">        columns of the input might be lost, because it is a valid</span>
<span class="sd">        `cross-correlation`_, and not a full `cross-correlation`_.</span>
<span class="sd">        It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})`.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="sd">            elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, L_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, L_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}</span>
<span class="sd">                        \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})`.</span>
<span class="sd">            The values of these weights are sampled from</span>
<span class="sd">            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape</span>
<span class="sd">            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.Conv2d">[docs]</a><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size</span>
<span class="sd">    :math:`(N, C_{\text{in}}, H, W)` and output :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})`</span>
<span class="sd">    can be precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</span>


<span class="sd">    where :math:`\star` is the valid 2D `cross-correlation`_ operator,</span>
<span class="sd">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="sd">    :math:`H` is a height of input planes in pixels, and :math:`W` is</span>
<span class="sd">    width in pixels.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="sd">      number or a tuple.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for :attr:`padding` number of points for each dimension.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="sd">      known as the Ã  trous algorithm. It is harder to describe, but this `link`_</span>
<span class="sd">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters, of size:</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the height and width dimension</span>
<span class="sd">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="sd">          and the second `int` for the width dimension</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,</span>
<span class="sd">                         then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span></div>

<div class="viewcode-block" id="Conv3d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.Conv3d">[docs]</a><span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`</span>
<span class="sd">    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out(N_i, C_{out_j}) = bias(C_{out_j}) +</span>
<span class="sd">                                \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid 3D `cross-correlation`_ operator</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for :attr:`padding` number of points for each dimension.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters, of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension</span>
<span class="sd">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="sd">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]</span>
<span class="sd">                    \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]</span>
<span class="sd">                    \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]</span>
<span class="sd">                    \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,</span>
<span class="sd">                         then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_ConvTransposeNd</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only &quot;zeros&quot; padding mode is supported for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_output_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
        <span class="c1"># type: (Tensor, Optional[List[int]], List[int], List[int], List[int]) -&gt; List[int]</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>  <span class="c1"># converting to list if was not already</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">k</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;output_size must have </span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements (got </span><span class="si">{}</span><span class="s2">)&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>

            <span class="n">min_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="n">max_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">dim_size</span> <span class="o">=</span> <span class="p">((</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span>
                            <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
                <span class="n">min_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)</span>
                <span class="n">max_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)):</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">min_size</span> <span class="o">=</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">max_size</span> <span class="o">=</span> <span class="n">max_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;</span> <span class="n">min_size</span> <span class="ow">or</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">max_size</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                        <span class="s2">&quot;requested an output size of </span><span class="si">{}</span><span class="s2">, but valid sizes range &quot;</span>
                        <span class="s2">&quot;from </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2"> (for an input of </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">output_size</span><span class="p">,</span> <span class="n">min_sizes</span><span class="p">,</span> <span class="n">max_sizes</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]))</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="ConvTranspose1d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConvTranspose1d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv1d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, L_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, L_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}</span>
<span class="sd">                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels).</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># type: (Tensor, Optional[List[int]]) -&gt; Tensor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose1d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConvTranspose2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConvTranspose2d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv2d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="sd">    can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the height and width dimensions</span>
<span class="sd">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="sd">          and the second `int` for the width dimension</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">        &gt;&gt;&gt; # exact output size can be also specified as an argument</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)</span>
<span class="sd">        &gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; h = downsample(input)</span>
<span class="sd">        &gt;&gt;&gt; h.size()</span>
<span class="sd">        torch.Size([1, 16, 6, 6])</span>
<span class="sd">        &gt;&gt;&gt; output = upsample(h, output_size=input.size())</span>
<span class="sd">        &gt;&gt;&gt; output.size()</span>
<span class="sd">        torch.Size([1, 16, 12, 12])</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># type: (Tensor, Optional[List[int]]) -&gt; Tensor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose2d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConvTranspose3d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConvTranspose3d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose3d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image composed of several input</span>
<span class="sd">    planes.</span>
<span class="sd">    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,</span>
<span class="sd">    and sums over the outputs from all input feature planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv3d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="sd">    can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions</span>
<span class="sd">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="sd">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">              D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]</span>
<span class="sd">                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</span>


<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># type: (Tensor, Optional[List[int]]) -&gt; Tensor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose3d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<span class="c1"># TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.</span>
<span class="c1">#</span>
<span class="c1"># `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used</span>
<span class="c1"># with `_ConvNd` to construct actual module classes that implements conv</span>
<span class="c1"># transpose ops:</span>
<span class="c1">#</span>
<span class="c1">#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):</span>
<span class="c1">#       ...</span>
<span class="c1">#</span>
<span class="c1"># In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper</span>
<span class="c1"># subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)</span>
<span class="c1"># use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias</span>
<span class="c1"># for BC, because it is cheap and easy for us to do so, even though that</span>
<span class="c1"># `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as</span>
<span class="c1"># above would still work).</span>
<span class="k">class</span> <span class="nc">_ConvTransposeMixin</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;_ConvTransposeMixin is a deprecated internal class. &quot;</span>
            <span class="s2">&quot;Please consider using public APIs.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvTransposeMixin</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># TODO: Conv2dLocal</span>
<span class="c1"># TODO: Conv2dMap</span>
<span class="c1"># TODO: ConvTranspose2dMap</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
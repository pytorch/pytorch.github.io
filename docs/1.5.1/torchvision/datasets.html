


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchvision.datasets &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/torchvision/datasets.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="torchvision.io" href="io.html" />
    <link rel="prev" title="torchvision" href="index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='http://pytorch.org/docs/versions.html'>1.5.1 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rpc/index.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">torchvision</a> &gt;</li>
        
      <li>torchvision.datasets</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/torchvision/datasets.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torchvision-datasets">
<h1>torchvision.datasets<a class="headerlink" href="#torchvision-datasets" title="Permalink to this headline">¶</a></h1>
<p>All datasets are subclasses of <a class="reference internal" href="../data.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></a>
i.e, they have <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> and <code class="docutils literal notranslate"><span class="pre">__len__</span></code> methods implemented.
Hence, they can all be passed to a <a class="reference internal" href="../data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a>
which can load multiple samples parallelly using <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code> workers.
For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">imagenet_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageNet</span><span class="p">(</span><span class="s1">&#39;path/to/imagenet_root/&#39;</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">imagenet_data</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">nThreads</span><span class="p">)</span>
</pre></div>
</div>
<p>The following datasets are available:</p>
<div class="contents local topic" id="datasets">
<p class="topic-title">Datasets</p>
<ul class="simple">
<li><p><a class="reference internal" href="#mnist" id="id18">MNIST</a></p></li>
<li><p><a class="reference internal" href="#fashion-mnist" id="id19">Fashion-MNIST</a></p></li>
<li><p><a class="reference internal" href="#kmnist" id="id20">KMNIST</a></p></li>
<li><p><a class="reference internal" href="#emnist" id="id21">EMNIST</a></p></li>
<li><p><a class="reference internal" href="#qmnist" id="id22">QMNIST</a></p></li>
<li><p><a class="reference internal" href="#fakedata" id="id23">FakeData</a></p></li>
<li><p><a class="reference internal" href="#coco" id="id24">COCO</a></p>
<ul>
<li><p><a class="reference internal" href="#captions" id="id25">Captions</a></p></li>
<li><p><a class="reference internal" href="#detection" id="id26">Detection</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#lsun" id="id27">LSUN</a></p></li>
<li><p><a class="reference internal" href="#imagefolder" id="id28">ImageFolder</a></p></li>
<li><p><a class="reference internal" href="#datasetfolder" id="id29">DatasetFolder</a></p></li>
<li><p><a class="reference internal" href="#imagenet" id="id30">ImageNet</a></p></li>
<li><p><a class="reference internal" href="#cifar" id="id31">CIFAR</a></p></li>
<li><p><a class="reference internal" href="#stl10" id="id32">STL10</a></p></li>
<li><p><a class="reference internal" href="#svhn" id="id33">SVHN</a></p></li>
<li><p><a class="reference internal" href="#phototour" id="id34">PhotoTour</a></p></li>
<li><p><a class="reference internal" href="#sbu" id="id35">SBU</a></p></li>
<li><p><a class="reference internal" href="#flickr" id="id36">Flickr</a></p></li>
<li><p><a class="reference internal" href="#voc" id="id37">VOC</a></p></li>
<li><p><a class="reference internal" href="#cityscapes" id="id38">Cityscapes</a></p></li>
<li><p><a class="reference internal" href="#sbd" id="id39">SBD</a></p></li>
<li><p><a class="reference internal" href="#usps" id="id40">USPS</a></p></li>
<li><p><a class="reference internal" href="#kinetics-400" id="id41">Kinetics-400</a></p></li>
<li><p><a class="reference internal" href="#hmdb51" id="id42">HMDB51</a></p></li>
<li><p><a class="reference internal" href="#ucf101" id="id43">UCF101</a></p></li>
<li><p><a class="reference internal" href="#celeba" id="id44">CelebA</a></p></li>
</ul>
</div>
<p>All the datasets have almost similar API. They all have two common arguments:
<code class="docutils literal notranslate"><span class="pre">transform</span></code> and  <code class="docutils literal notranslate"><span class="pre">target_transform</span></code> to transform the input and target respectively.</p>
<div class="section" id="mnist">
<h2><a class="toc-backref" href="#id18">MNIST</a><a class="headerlink" href="#mnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.MNIST">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">MNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/mnist.html#MNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.MNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">MNIST/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fashion-mnist">
<h2><a class="toc-backref" href="#id19">Fashion-MNIST</a><a class="headerlink" href="#fashion-mnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.FashionMNIST">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">FashionMNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/mnist.html#FashionMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.FashionMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">Fashion-MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">Fashion-MNIST/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="kmnist">
<h2><a class="toc-backref" href="#id20">KMNIST</a><a class="headerlink" href="#kmnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.KMNIST">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">KMNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/mnist.html#KMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.KMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/rois-codh/kmnist">Kuzushiji-MNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">KMNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">KMNIST/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="emnist">
<h2><a class="toc-backref" href="#id21">EMNIST</a><a class="headerlink" href="#emnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.EMNIST">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">EMNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/mnist.html#EMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.EMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist">EMNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">EMNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">EMNIST/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>split</strong> (<em>string</em>) – The dataset has 6 different splits: <code class="docutils literal notranslate"><span class="pre">byclass</span></code>, <code class="docutils literal notranslate"><span class="pre">bymerge</span></code>,
<code class="docutils literal notranslate"><span class="pre">balanced</span></code>, <code class="docutils literal notranslate"><span class="pre">letters</span></code>, <code class="docutils literal notranslate"><span class="pre">digits</span></code> and <code class="docutils literal notranslate"><span class="pre">mnist</span></code>. This argument specifies
which one to use.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="qmnist">
<h2><a class="toc-backref" href="#id22">QMNIST</a><a class="headerlink" href="#qmnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.QMNIST">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">QMNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">what=None</em>, <em class="sig-param">compat=True</em>, <em class="sig-param">train=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/mnist.html#QMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.QMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/facebookresearch/qmnist">QMNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset whose <a href="#id5"><span class="problematic" id="id6">``</span></a>processed’’
subdir contains torch binary files with the datasets.</p></li>
<li><p><strong>what</strong> (<em>string</em><em>,</em><em>optional</em>) – Can be ‘train’, ‘test’, ‘test10k’,
‘test50k’, or ‘nist’ for respectively the mnist compatible
training set, the 60k qmnist testing set, the 10k qmnist
examples that match the mnist testing set, the 50k
remaining qmnist testing examples, or all the nist
digits. The default is to select ‘train’ or ‘test’
according to the compatibility argument ‘train’.</p></li>
<li><p><strong>compat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>,</em><em>optional</em>) – A boolean that says whether the target
for each example is class number (for compatibility with
the MNIST dataloader) or a torch vector containing the
full qmnist information. Default=True.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from
the internet and puts it in root directory. If dataset is
already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that
takes in an PIL image and returns a transformed
version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform
that takes in the target and transforms it.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>,</em><em>optional</em><em>,</em><em>compatibility</em>) – When argument ‘what’ is
not specified, this boolean decides whether to load the
training set ot the testing set.  Default: True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fakedata">
<h2><a class="toc-backref" href="#id23">FakeData</a><a class="headerlink" href="#fakedata" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.FakeData">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">FakeData</code><span class="sig-paren">(</span><em class="sig-param">size=1000</em>, <em class="sig-param">image_size=(3</em>, <em class="sig-param">224</em>, <em class="sig-param">224)</em>, <em class="sig-param">num_classes=10</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">random_offset=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/fakedata.html#FakeData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.FakeData" title="Permalink to this definition">¶</a></dt>
<dd><p>A fake dataset that returns randomly generated images and returns them as PIL images</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the dataset. Default: 1000 images</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Size if the returned images. Default: (3, 224, 224)</p></li>
<li><p><strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Number of classes in the datset. Default: 10</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>random_offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Offsets the index-based random seed used to
generate each image. Default: 0</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="coco">
<h2><a class="toc-backref" href="#id24">COCO</a><a class="headerlink" href="#coco" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These require the <a class="reference external" href="https://github.com/pdollar/coco/tree/master/PythonAPI">COCO API to be installed</a></p>
</div>
<div class="section" id="captions">
<h3><a class="toc-backref" href="#id25">Captions</a><a class="headerlink" href="#captions" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torchvision.datasets.CocoCaptions">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">CocoCaptions</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">annFile</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/coco.html#CocoCaptions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoCaptions" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://mscoco.org/dataset/#captions-challenge2015">MS Coco Captions</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</p></li>
<li><p><strong>annFile</strong> (<em>string</em>) – Path to json annotation file.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dset</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">CocoCaptions</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;dir where images are&#39;</span><span class="p">,</span>
                        <span class="n">annFile</span> <span class="o">=</span> <span class="s1">&#39;json annotation file&#39;</span><span class="p">,</span>
                        <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of samples: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cap</span><span class="p">))</span>
<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">cap</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="c1"># load 4th sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image Size: &quot;</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Number</span> <span class="n">of</span> <span class="n">samples</span><span class="p">:</span> <span class="mi">82783</span>
<span class="n">Image</span> <span class="n">Size</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="n">L</span><span class="p">,</span> <span class="mi">427</span><span class="n">L</span><span class="p">,</span> <span class="mi">640</span><span class="n">L</span><span class="p">)</span>
<span class="p">[</span><span class="sa">u</span><span class="s1">&#39;A plane emitting smoke stream flying over a mountain.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A plane darts across a bright blue sky behind a mountain covered in snow&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A plane leaves a contrail above the snowy mountain top.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A mountain that has a plane flying overheard in the distance.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A mountain view with a plume of smoke in the background&#39;</span><span class="p">]</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchvision.datasets.CocoCaptions.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/coco.html#CocoCaptions.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoCaptions.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple (image, target). target is a list of captions for the image.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="detection">
<h3><a class="toc-backref" href="#id26">Detection</a><a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torchvision.datasets.CocoDetection">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">CocoDetection</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">annFile</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/coco.html#CocoDetection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoDetection" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://mscoco.org/dataset/#detections-challenge2016">MS Coco Detection</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</p></li>
<li><p><strong>annFile</strong> (<em>string</em>) – Path to json annotation file.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.CocoDetection.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/coco.html#CocoDetection.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoDetection.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple (image, target). target is the object returned by <code class="docutils literal notranslate"><span class="pre">coco.loadAnns</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="lsun">
<h2><a class="toc-backref" href="#id27">LSUN</a><a class="headerlink" href="#lsun" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.LSUN">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">LSUN</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">classes='train'</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/lsun.html#LSUN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.LSUN" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.yf.io/p/lsun">LSUN</a> dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory for the database files.</p></li>
<li><p><strong>classes</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – One of {‘train’, ‘val’, ‘test’} or a list of
categories to load. e,g. [‘bedroom_train’, ‘church_outdoor_train’].</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.LSUN.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/lsun.html#LSUN.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.LSUN.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple (image, target) where target is the index of the target category.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="imagefolder">
<h2><a class="toc-backref" href="#id28">ImageFolder</a><a class="headerlink" href="#imagefolder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.ImageFolder">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">ImageFolder</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">loader=&lt;function default_loader&gt;</em>, <em class="sig-param">is_valid_file=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/folder.html#ImageFolder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.ImageFolder" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic data loader where the images are arranged in this way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">png</span>

<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="mf">123.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory path.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>loader</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function to load an image given its path.</p></li>
<li><p><strong>is_valid_file</strong> – A function that takes path of an Image file
and check if the file is a valid file (used to check of corrupt files)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.ImageFolder.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="headerlink" href="#torchvision.datasets.ImageFolder.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(sample, target) where target is class_index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="datasetfolder">
<h2><a class="toc-backref" href="#id29">DatasetFolder</a><a class="headerlink" href="#datasetfolder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.DatasetFolder">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">DatasetFolder</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">loader</em>, <em class="sig-param">extensions=None</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">is_valid_file=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/folder.html#DatasetFolder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.DatasetFolder" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic data loader where the samples are arranged in this way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">ext</span>

<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="mf">123.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">ext</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory path.</p></li>
<li><p><strong>loader</strong> (<em>callable</em>) – A function to load a sample given its path.</p></li>
<li><p><strong>extensions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><em>string</em><em>]</em>) – A list of allowed extensions.
both extensions and is_valid_file should not be passed.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in
a sample and returns a transformed version.
E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code> for images.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes
in the target and transforms it.</p></li>
<li><p><strong>is_valid_file</strong> – A function that takes path of a file
and check if the file is a valid file (used to check of corrupt files)
both extensions and is_valid_file should not be passed.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.DatasetFolder.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/folder.html#DatasetFolder.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.DatasetFolder.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(sample, target) where target is class_index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="imagenet">
<h2><a class="toc-backref" href="#id30">ImageNet</a><a class="headerlink" href="#imagenet" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.ImageNet">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">ImageNet</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/imagenet.html#ImageNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.ImageNet" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://image-net.org/">ImageNet</a> 2012 Classification Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the ImageNet Dataset.</p></li>
<li><p><strong>split</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">val</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>loader</strong> – A function to load an image given its path.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This requires <cite>scipy</cite> to be installed</p>
</div>
</div>
<div class="section" id="cifar">
<h2><a class="toc-backref" href="#id31">CIFAR</a><a class="headerlink" href="#cifar" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.CIFAR10">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">CIFAR10</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/cifar.html#CIFAR10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR10" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">cifar-10-batches-py</span></code> exists or will be saved to if download is set to True.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from training set, otherwise
creates from test set.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.CIFAR10.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/cifar.html#CIFAR10.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR10.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.CIFAR100">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">CIFAR100</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/cifar.html#CIFAR100"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR100" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a> Dataset.</p>
<p>This is a subclass of the <cite>CIFAR10</cite> Dataset.</p>
</dd></dl>

</div>
<div class="section" id="stl10">
<h2><a class="toc-backref" href="#id32">STL10</a><a class="headerlink" href="#stl10" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.STL10">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">STL10</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">folds=None</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/stl10.html#STL10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.STL10" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://cs.stanford.edu/~acoates/stl10/">STL10</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">stl10_binary</span></code> exists.</p></li>
<li><p><strong>split</strong> (<em>string</em>) – One of {‘train’, ‘test’, ‘unlabeled’, ‘train+unlabeled’}.
Accordingly dataset is selected.</p></li>
<li><p><strong>folds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – <p>One of {0-9} or None.
For training, loads one of the 10 pre-defined folds of 1k samples for the</p>
<blockquote>
<div><p>standard evaluation procedure. If no value is passed, loads the 5k samples.</p>
</div></blockquote>
</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.STL10.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/stl10.html#STL10.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.STL10.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="svhn">
<h2><a class="toc-backref" href="#id33">SVHN</a><a class="headerlink" href="#svhn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SVHN">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">SVHN</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/svhn.html#SVHN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SVHN" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://ufldl.stanford.edu/housenumbers/">SVHN</a> Dataset.
Note: The SVHN dataset assigns the label <cite>10</cite> to the digit <cite>0</cite>. However, in this Dataset,
we assign the label <cite>0</cite> to the digit <cite>0</cite> to be compatible with PyTorch loss functions which
expect the class labels to be in the range <cite>[0, C-1]</cite></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class needs <a class="reference external" href="https://docs.scipy.org/doc/">scipy</a> to load data from <cite>.mat</cite> format.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">SVHN</span></code> exists.</p></li>
<li><p><strong>split</strong> (<em>string</em>) – One of {‘train’, ‘test’, ‘extra’}.
Accordingly dataset is selected. ‘extra’ is Extra training set.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.SVHN.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/svhn.html#SVHN.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SVHN.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="phototour">
<h2><a class="toc-backref" href="#id34">PhotoTour</a><a class="headerlink" href="#phototour" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.PhotoTour">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">PhotoTour</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">name</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/phototour.html#PhotoTour"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.PhotoTour" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://phototour.cs.washington.edu/patches/default.htm">Learning Local Image Descriptors Data</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are.</p></li>
<li><p><strong>name</strong> (<em>string</em>) – Name of the dataset to load.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.PhotoTour.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/phototour.html#PhotoTour.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.PhotoTour.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(data1, data2, matches)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sbu">
<h2><a class="toc-backref" href="#id35">SBU</a><a class="headerlink" href="#sbu" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SBU">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">SBU</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/sbu.html#SBU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBU" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.cs.virginia.edu/~vicente/sbucaptions/">SBU Captioned Photo</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where tarball
<code class="docutils literal notranslate"><span class="pre">SBUCaptionedPhotoDataset.tar.gz</span></code> exists.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.SBU.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/sbu.html#SBU.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBU.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is a caption for the photo.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="flickr">
<h2><a class="toc-backref" href="#id36">Flickr</a><a class="headerlink" href="#flickr" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.Flickr8k">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">Flickr8k</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">ann_file</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/flickr.html#Flickr8k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr8k" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://hockenmaier.cs.illinois.edu/8k-pictures.html">Flickr8k Entities</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</p></li>
<li><p><strong>ann_file</strong> (<em>string</em>) – Path to annotation file.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.Flickr8k.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/flickr.html#Flickr8k.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr8k.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple (image, target). target is a list of captions for the image.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.Flickr30k">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">Flickr30k</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">ann_file</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/flickr.html#Flickr30k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr30k" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/">Flickr30k Entities</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</p></li>
<li><p><strong>ann_file</strong> (<em>string</em>) – Path to annotation file.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.Flickr30k.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/flickr.html#Flickr30k.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr30k.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple (image, target). target is a list of captions for the image.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="voc">
<h2><a class="toc-backref" href="#id37">VOC</a><a class="headerlink" href="#voc" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.VOCSegmentation">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">VOCSegmentation</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">year='2012'</em>, <em class="sig-param">image_set='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/voc.html#VOCSegmentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCSegmentation" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC</a> Segmentation Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the VOC Dataset.</p></li>
<li><p><strong>year</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset year, supports years 2007 to 2012.</p></li>
<li><p><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">trainval</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.VOCSegmentation.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/voc.html#VOCSegmentation.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCSegmentation.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is the image segmentation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.VOCDetection">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">VOCDetection</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">year='2012'</em>, <em class="sig-param">image_set='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/voc.html#VOCDetection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCDetection" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC</a> Detection Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the VOC Dataset.</p></li>
<li><p><strong>year</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset year, supports years 2007 to 2012.</p></li>
<li><p><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">trainval</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.
(default: alphabetic indexing of VOC’s 20 classes).</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>required</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.VOCDetection.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/voc.html#VOCDetection.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCDetection.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is a dictionary of the XML tree.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="cityscapes">
<h2><a class="toc-backref" href="#id38">Cityscapes</a><a class="headerlink" href="#cityscapes" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Requires Cityscape to be downloaded.</p>
</div>
<dl class="class">
<dt id="torchvision.datasets.Cityscapes">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">Cityscapes</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">mode='fine'</em>, <em class="sig-param">target_type='instance'</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/cityscapes.html#Cityscapes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Cityscapes" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.cityscapes-dataset.com/">Cityscapes</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory <code class="docutils literal notranslate"><span class="pre">leftImg8bit</span></code>
and <code class="docutils literal notranslate"><span class="pre">gtFine</span></code> or <code class="docutils literal notranslate"><span class="pre">gtCoarse</span></code> are located.</p></li>
<li><p><strong>split</strong> (<em>string</em><em>, </em><em>optional</em>) – The image split to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code> if mode=”fine”
otherwise <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">train_extra</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The quality mode to use, <code class="docutils literal notranslate"><span class="pre">fine</span></code> or <code class="docutils literal notranslate"><span class="pre">coarse</span></code></p></li>
<li><p><strong>target_type</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – Type of target to use, <code class="docutils literal notranslate"><span class="pre">instance</span></code>, <code class="docutils literal notranslate"><span class="pre">semantic</span></code>, <code class="docutils literal notranslate"><span class="pre">polygon</span></code>
or <code class="docutils literal notranslate"><span class="pre">color</span></code>. Can also be a list to output a tuple with all specified target types.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Get semantic segmentation target</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fine&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="s1">&#39;semantic&#39;</span><span class="p">)</span>

<span class="n">img</span><span class="p">,</span> <span class="n">smnt</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Get multiple targets</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fine&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;instance&#39;</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">,</span> <span class="s1">&#39;polygon&#39;</span><span class="p">])</span>

<span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">inst</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">poly</span><span class="p">)</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Validate on the “coarse” set</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;coarse&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="s1">&#39;semantic&#39;</span><span class="p">)</span>

<span class="n">img</span><span class="p">,</span> <span class="n">smnt</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchvision.datasets.Cityscapes.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/cityscapes.html#Cityscapes.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Cityscapes.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is a tuple of all target types if target_type is a list with more
than one item. Otherwise target is a json object if target_type=”polygon”, else the image segmentation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sbd">
<h2><a class="toc-backref" href="#id39">SBD</a><a class="headerlink" href="#sbd" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SBDataset">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">SBDataset</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">image_set='train'</em>, <em class="sig-param">mode='boundaries'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/sbd.html#SBDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBDataset" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://home.bharathh.info/pubs/codes/SBD/download.html">Semantic Boundaries Dataset</a></p>
<p>The SBD currently contains annotations from 11355 images taken from the PASCAL VOC 2011 dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please note that the train and val splits included with this dataset are different from
the splits in the PASCAL VOC dataset. In particular some “train” images might be part of
VOC2012 val.
If you are interested in testing on VOC 2012 val, then use <cite>image_set=’train_noval’</cite>,
which excludes all val images.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class needs <a class="reference external" href="https://docs.scipy.org/doc/">scipy</a> to load target files from <cite>.mat</cite> format.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the Semantic Boundaries Dataset</p></li>
<li><p><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">val</span></code> or <code class="docutils literal notranslate"><span class="pre">train_noval</span></code>.
Image set <code class="docutils literal notranslate"><span class="pre">train_noval</span></code> excludes VOC 2012 val images.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – Select target type. Possible values ‘boundaries’ or ‘segmentation’.
In case of ‘boundaries’, the target is an array of shape <cite>[num_classes, H, W]</cite>,
where <cite>num_classes=20</cite>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version. Input sample is PIL image and target is a numpy array
if <cite>mode=’boundaries’</cite> or PIL image if <cite>mode=’segmentation’</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="usps">
<h2><a class="toc-backref" href="#id40">USPS</a><a class="headerlink" href="#usps" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.USPS">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">USPS</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/usps.html#USPS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.USPS" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps">USPS</a> Dataset.
The data-format is : [label [index:value ]*256 n] * num_lines, where <code class="docutils literal notranslate"><span class="pre">label</span></code> lies in <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">10]</span></code>.
The value for each pixel lies in <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. Here we transform the <code class="docutils literal notranslate"><span class="pre">label</span></code> into <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">9]</span></code>
and make pixel values in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset to store``USPS`` data files.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">usps.bz2</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">usps.t.bz2</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torchvision.datasets.USPS.__getitem__">
<code class="sig-name descname">__getitem__</code><span class="sig-paren">(</span><em class="sig-param">index</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/usps.html#USPS.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.USPS.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(image, target) where target is index of the target class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="kinetics-400">
<h2><a class="toc-backref" href="#id41">Kinetics-400</a><a class="headerlink" href="#kinetics-400" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.Kinetics400">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">Kinetics400</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">frames_per_clip</em>, <em class="sig-param">step_between_clips=1</em>, <em class="sig-param">frame_rate=None</em>, <em class="sig-param">extensions=('avi'</em>, <em class="sig-param">)</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">_precomputed_metadata=None</em>, <em class="sig-param">num_workers=1</em>, <em class="sig-param">_video_width=0</em>, <em class="sig-param">_video_height=0</em>, <em class="sig-param">_video_min_dimension=0</em>, <em class="sig-param">_audio_samples=0</em>, <em class="sig-param">_audio_channels=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/kinetics.html#Kinetics400"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Kinetics400" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">Kinetics-400</a>
dataset.</p>
<p>Kinetics-400 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the Kinetics-400 Dataset.</p></li>
<li><p><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames in a clip</p></li>
<li><p><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames between each clip</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in a TxHxWxC video
and returns a transformed version.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>video (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>[T, H, W, C])</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="hmdb51">
<h2><a class="toc-backref" href="#id42">HMDB51</a><a class="headerlink" href="#hmdb51" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.HMDB51">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">HMDB51</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">annotation_path</em>, <em class="sig-param">frames_per_clip</em>, <em class="sig-param">step_between_clips=1</em>, <em class="sig-param">frame_rate=None</em>, <em class="sig-param">fold=1</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">_precomputed_metadata=None</em>, <em class="sig-param">num_workers=1</em>, <em class="sig-param">_video_width=0</em>, <em class="sig-param">_video_height=0</em>, <em class="sig-param">_video_min_dimension=0</em>, <em class="sig-param">_audio_samples=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/hmdb51.html#HMDB51"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.HMDB51" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB51</a>
dataset.</p>
<p>HMDB51 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the HMDB51 Dataset.</p></li>
<li><p><strong>annotation_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Path to the folder containing the split files.</p></li>
<li><p><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of frames in a clip.</p></li>
<li><p><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of frames between each clip.</p></li>
<li><p><strong>fold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Which fold to use. Should be between 1 and 3.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates a dataset from the train split,
otherwise from the <code class="docutils literal notranslate"><span class="pre">test</span></code> split.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a TxHxWxC video
and returns a transformed version.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>video (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>[T, H, W, C])</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="ucf101">
<h2><a class="toc-backref" href="#id43">UCF101</a><a class="headerlink" href="#ucf101" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.UCF101">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">UCF101</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">annotation_path</em>, <em class="sig-param">frames_per_clip</em>, <em class="sig-param">step_between_clips=1</em>, <em class="sig-param">frame_rate=None</em>, <em class="sig-param">fold=1</em>, <em class="sig-param">train=True</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">_precomputed_metadata=None</em>, <em class="sig-param">num_workers=1</em>, <em class="sig-param">_video_width=0</em>, <em class="sig-param">_video_height=0</em>, <em class="sig-param">_video_min_dimension=0</em>, <em class="sig-param">_audio_samples=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/ucf101.html#UCF101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.UCF101" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.crcv.ucf.edu/data/UCF101.php">UCF101</a> dataset.</p>
<p>UCF101 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory of the UCF101 Dataset.</p></li>
<li><p><strong>annotation_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – path to the folder containing the split files</p></li>
<li><p><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames in a clip.</p></li>
<li><p><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – number of frames between each clip.</p></li>
<li><p><strong>fold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – which fold to use. Should be between 1 and 3.</p></li>
<li><p><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates a dataset from the train split,
otherwise from the <code class="docutils literal notranslate"><span class="pre">test</span></code> split.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in a TxHxWxC video
and returns a transformed version.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>video (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>[T, H, W, C])</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="celeba">
<h2><a class="toc-backref" href="#id44">CelebA</a><a class="headerlink" href="#celeba" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.CelebA">
<em class="property">class </em><code class="sig-prename descclassname">torchvision.datasets.</code><code class="sig-name descname">CelebA</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">target_type='attr'</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">download=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchvision/datasets/celeba.html#CelebA"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CelebA" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">Large-scale CelebFaces Attributes (CelebA) Dataset</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</p></li>
<li><p><strong>split</strong> (<em>string</em>) – One of {‘train’, ‘valid’, ‘test’, ‘all’}.
Accordingly dataset is selected.</p></li>
<li><p><strong>target_type</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – <p>Type of target to use, <code class="docutils literal notranslate"><span class="pre">attr</span></code>, <code class="docutils literal notranslate"><span class="pre">identity</span></code>, <code class="docutils literal notranslate"><span class="pre">bbox</span></code>,
or <code class="docutils literal notranslate"><span class="pre">landmarks</span></code>. Can also be a list to output a tuple with all specified target types.
The targets represent:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">attr</span></code> (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes
<code class="docutils literal notranslate"><span class="pre">identity</span></code> (int): label for each person (data points with the same identity are the same person)
<code class="docutils literal notranslate"><span class="pre">bbox</span></code> (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)
<code class="docutils literal notranslate"><span class="pre">landmarks</span></code> (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,</p>
<blockquote>
<div><p>righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)</p>
</div></blockquote>
</div></blockquote>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">attr</span></code>. If empty, <code class="docutils literal notranslate"><span class="pre">None</span></code> will be returned as target.</p>
</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="io.html" class="btn btn-neutral float-right" title="torchvision.io" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="torchvision" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torchvision.datasets</a><ul>
<li><a class="reference internal" href="#mnist">MNIST</a></li>
<li><a class="reference internal" href="#fashion-mnist">Fashion-MNIST</a></li>
<li><a class="reference internal" href="#kmnist">KMNIST</a></li>
<li><a class="reference internal" href="#emnist">EMNIST</a></li>
<li><a class="reference internal" href="#qmnist">QMNIST</a></li>
<li><a class="reference internal" href="#fakedata">FakeData</a></li>
<li><a class="reference internal" href="#coco">COCO</a><ul>
<li><a class="reference internal" href="#captions">Captions</a></li>
<li><a class="reference internal" href="#detection">Detection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lsun">LSUN</a></li>
<li><a class="reference internal" href="#imagefolder">ImageFolder</a></li>
<li><a class="reference internal" href="#datasetfolder">DatasetFolder</a></li>
<li><a class="reference internal" href="#imagenet">ImageNet</a></li>
<li><a class="reference internal" href="#cifar">CIFAR</a></li>
<li><a class="reference internal" href="#stl10">STL10</a></li>
<li><a class="reference internal" href="#svhn">SVHN</a></li>
<li><a class="reference internal" href="#phototour">PhotoTour</a></li>
<li><a class="reference internal" href="#sbu">SBU</a></li>
<li><a class="reference internal" href="#flickr">Flickr</a></li>
<li><a class="reference internal" href="#voc">VOC</a></li>
<li><a class="reference internal" href="#cityscapes">Cityscapes</a></li>
<li><a class="reference internal" href="#sbd">SBD</a></li>
<li><a class="reference internal" href="#usps">USPS</a></li>
<li><a class="reference internal" href="#kinetics-400">Kinetics-400</a></li>
<li><a class="reference internal" href="#hmdb51">HMDB51</a></li>
<li><a class="reference internal" href="#ucf101">UCF101</a></li>
<li><a class="reference internal" href="#celeba">CelebA</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
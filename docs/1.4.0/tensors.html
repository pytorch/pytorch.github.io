


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Tensor &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/tensors.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tensor Attributes" href="tensor_attributes.html" />
    <link rel="prev" title="torch.nn.functional" href="nn.functional.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='http://pytorch.org/docs/versions.html'>1.4.0 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/distributed_autograd.html">Distributed Autograd Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/rref.html">Remote Reference Protocol</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/cppdocs/">C++ API</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a></li>
</ul>
<p class="caption"><span class="caption-text">torchaudio Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
</ul>
<p class="caption"><span class="caption-text">torchtext Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.Tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/tensors.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torch-tensor">
<span id="tensor-doc"></span><h1>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">¶</a></h1>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<p>Torch defines nine CPU tensor types and nine GPU tensor types:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 34%" />
<col style="width: 21%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Data type</p></th>
<th class="head"><p>dtype</p></th>
<th class="head"><p>CPU tensor</p></th>
<th class="head"><p>GPU tensor</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.FloatTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.DoubleTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.DoubleTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.HalfTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.HalfTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ByteTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.CharTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.CharTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.short</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ShortTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ShortTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>32-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.int</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.IntTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.IntTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.long</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.LongTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Boolean</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.bool</span></code></p></td>
<td><p><a class="reference internal" href="#torch.BoolTensor" title="torch.BoolTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code></a></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.BoolTensor</span></code></p></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is an alias for the default tensor type (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>).</p>
<p>A tensor can be constructed from a Python <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code> or sequence using the
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">tensor([[ 1.0000, -1.0000],</span>
<span class="go">        [ 1.0000, -1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> and just want to change its <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> flag, use
<a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a> or
<a class="reference internal" href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a> to avoid a copy.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="torch.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code></a>.</p>
</div>
<p>A tensor of specific data type can be constructed by passing a
<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and/or a <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> to a
constructor or tensor creation op:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  0,  0,  0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<p>The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 1,  8,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<p>Use <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.item()</span></code></a> to get a Python number from a tensor containing a
single value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(2.5000)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<p>A tensor can be created with <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad=True</span></code> so that
<a class="reference internal" href="autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> records operations on them for automatic differentiation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[ 2.0000, -2.0000],</span>
<span class="go">        [ 2.0000,  2.0000]])</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on the <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, and
<a class="reference internal" href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a> attributes of a <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, see
<a class="reference internal" href="tensor_attributes.html#tensor-attributes-doc"><span class="std std-ref">Tensor Attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To change an existing tensor’s <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> and/or <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, consider using
<a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a> method on the tensor.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Current implementation of <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> introduces memory overhead,
thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.
If this is your case, consider using one large structure.</p>
</div>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="sig-prename descclassname">torch.</code><code class="sig-name descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>There are a few main ways to create a tensor, depending on your use case.</p>
<ul class="simple">
<li><p>To create a tensor with pre-existing data, use <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</p></li>
<li><p>To create a tensor with specific size, use <code class="docutils literal notranslate"><span class="pre">torch.*</span></code> tensor creation
ops (see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with the same size (and similar types) as another tensor,
use <code class="docutils literal notranslate"><span class="pre">torch.*_like</span></code> tensor creation ops
(see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with similar type but different size as another tensor,
use <code class="docutils literal notranslate"><span class="pre">tensor.new_*</span></code> creation ops.</p></li>
</ul>
<dl class="method">
<dt id="torch.Tensor.new_tensor">
<code class="sig-name descname">new_tensor</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> as the tensor data.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code></a>
or <a class="reference internal" href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code></a>.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="torch.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <a class="reference internal" href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>array_like</em>) – The returned Tensor copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 2,  3]], dtype=torch.int8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_full">
<code class="sig-name descname">new_full</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">fill_value</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_full" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fill_value</strong> (<em>scalar</em>) – the number to fill the output tensor with.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_empty">
<code class="sig-name descname">new_empty</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_ones">
<code class="sig-name descname">new_ones</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>python:int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 1,  1,  1],</span>
<span class="go">        [ 1,  1,  1]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_zeros">
<code class="sig-name descname">new_zeros</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>python:int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_cuda">
<code class="sig-name descname">is_cuda</code><a class="headerlink" href="#torch.Tensor.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.device">
<code class="sig-name descname">device</code><a class="headerlink" href="#torch.Tensor.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Is the <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> where this Tensor is.</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-name descname">grad</code></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to
<a class="reference internal" href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.
The attribute will then contain the gradients computed and future calls to
<a class="reference internal" href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will accumulate (add) gradients into it.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.ndim">
<code class="sig-name descname">ndim</code><a class="headerlink" href="#torch.Tensor.ndim" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.T">
<code class="sig-name descname">T</code><a class="headerlink" href="#torch.Tensor.T" title="Permalink to this definition">¶</a></dt>
<dd><p>Is this Tensor with its dimensions reversed.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of dimensions in <code class="docutils literal notranslate"><span class="pre">x</span></code>,
<code class="docutils literal notranslate"><span class="pre">x.T</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.permute(n-1,</span> <span class="pre">n-2,</span> <span class="pre">...,</span> <span class="pre">0)</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs">
<code class="sig-name descname">abs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs_">
<code class="sig-name descname">abs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos">
<code class="sig-name descname">acos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos_">
<code class="sig-name descname">acos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add" title="Permalink to this definition">¶</a></dt>
<dd><p>add(value=1, other) -&gt; Tensor</p>
<p>See <a class="reference internal" href="torch.html#torch.add" title="torch.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add_">
<code class="sig-name descname">add_</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add_" title="Permalink to this definition">¶</a></dt>
<dd><p>add_(value=1, other) -&gt; Tensor</p>
<p>In-place version of <a class="reference internal" href="#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm">
<code class="sig-name descname">addbmm</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">batch1</em>, <em class="sig-param">batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm_">
<code class="sig-name descname">addbmm_</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">batch1</em>, <em class="sig-param">batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv">
<code class="sig-name descname">addcdiv</code><span class="sig-paren">(</span><em class="sig-param">value=1</em>, <em class="sig-param">tensor1</em>, <em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv_">
<code class="sig-name descname">addcdiv_</code><span class="sig-paren">(</span><em class="sig-param">value=1</em>, <em class="sig-param">tensor1</em>, <em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul">
<code class="sig-name descname">addcmul</code><span class="sig-paren">(</span><em class="sig-param">value=1</em>, <em class="sig-param">tensor1</em>, <em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul_">
<code class="sig-name descname">addcmul_</code><span class="sig-paren">(</span><em class="sig-param">value=1</em>, <em class="sig-param">tensor1</em>, <em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm">
<code class="sig-name descname">addmm</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">mat1</em>, <em class="sig-param">mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm_">
<code class="sig-name descname">addmm_</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">mat1</em>, <em class="sig-param">mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv">
<code class="sig-name descname">addmv</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">mat</em>, <em class="sig-param">vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv_">
<code class="sig-name descname">addmv_</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">mat</em>, <em class="sig-param">vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr">
<code class="sig-name descname">addr</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">vec1</em>, <em class="sig-param">vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr_">
<code class="sig-name descname">addr_</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">vec1</em>, <em class="sig-param">vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.allclose">
<code class="sig-name descname">allclose</code><span class="sig-paren">(</span><em class="sig-param">other</em>, <em class="sig-param">rtol=1e-05</em>, <em class="sig-param">atol=1e-08</em>, <em class="sig-param">equal_nan=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.allclose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.allclose" title="torch.allclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.angle">
<code class="sig-name descname">angle</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.angle" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.angle" title="torch.angle"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.angle()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.apply_">
<code class="sig-name descname">apply_</code><span class="sig-paren">(</span><em class="sig-param">callable</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.apply_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmax">
<code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.argmax" title="torch.argmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmin">
<code class="sig-name descname">argmin</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.argmin" title="torch.argmin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argsort">
<code class="sig-name descname">argsort</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em>, <em class="sig-param">descending=False</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.argsort" title="Permalink to this definition">¶</a></dt>
<dd><p>See :func: <cite>torch.argsort</cite></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin">
<code class="sig-name descname">asin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin_">
<code class="sig-name descname">asin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.as_strided">
<code class="sig-name descname">as_strided</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">stride</em>, <em class="sig-param">storage_offset=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.as_strided" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan">
<code class="sig-name descname">atan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2">
<code class="sig-name descname">atan2</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2_">
<code class="sig-name descname">atan2_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan_">
<code class="sig-name descname">atan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt>
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">gradient=None</em>, <em class="sig-param">retain_graph=None</em>, <em class="sig-param">create_graph=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.backward"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Computes the gradient of current tensor w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the tensor is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionally requires specifying <code class="docutils literal notranslate"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and location, that contains
the gradient of the differentiated function w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to
zero them before calling it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gradient</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>None</em>) – Gradient w.r.t. the
tensor. If it is a tensor, it will be automatically converted
to a Tensor that does not require grad unless <code class="docutils literal notranslate"><span class="pre">create_graph</span></code> is True.
None values can be specified for scalar Tensors or ones that
don’t require grad. If a None value would be acceptable then
this argument is optional.</p></li>
<li><p><strong>retain_graph</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute
the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p></li>
<li><p><strong>create_graph</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will
be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm">
<code class="sig-name descname">baddbmm</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">batch1</em>, <em class="sig-param">batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm_">
<code class="sig-name descname">baddbmm_</code><span class="sig-paren">(</span><em class="sig-param">beta=1</em>, <em class="sig-param">alpha=1</em>, <em class="sig-param">batch1</em>, <em class="sig-param">batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli">
<code class="sig-name descname">bernoulli</code><span class="sig-paren">(</span><em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a result tensor where each <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">result[i]</mtext></mrow><annotation encoding="application/x-tex">\texttt{result[i]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord text"><span class="mord texttt">result[i]</span></span></span></span></span>

</span> is independently
sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">self[i]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{self[i]})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">self[i]</span></span><span class="mclose">)</span></span></span></span>

</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, and the result will have the same <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See <a class="reference internal" href="torch.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli_">
<code class="sig-name descname">bernoulli_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.bernoulli_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="sig-name descname">bernoulli_</code><span class="sig-paren">(</span><em class="sig-param">p=0.5</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">p</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{p})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">p</span></span><span class="mclose">)</span></span></span></span>

</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral
<code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="sig-name descname">bernoulli_</code><span class="sig-paren">(</span><em class="sig-param">p_tensor</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">p_tensor</span></code> should be a tensor containing probabilities to be used for
drawing the binary random number.</p>
<p>The <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>i</mtext><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\text{i}^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.906868em;vertical-align:0em;"></span><span class="mord"><span class="mord text"><span class="mord">i</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.906868em;"><span style="top:-3.12076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span>

</span> element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will be set to a
value sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">p_tensor[i]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{p\_tensor[i]})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">p_tensor[i]</span></span><span class="mclose">)</span></span></span></span>

</span>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, but <code class="xref py py-attr docutils literal notranslate"><span class="pre">p_tensor</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</dd></dl>

<p>See also <a class="reference internal" href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bernoulli()</span></code></a> and <a class="reference internal" href="torch.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bfloat16">
<code class="sig-name descname">bfloat16</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bfloat16()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bfloat16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bincount">
<code class="sig-name descname">bincount</code><span class="sig-paren">(</span><em class="sig-param">weights=None</em>, <em class="sig-param">minlength=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bincount" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bincount" title="torch.bincount"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bincount()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bitwise_not">
<code class="sig-name descname">bitwise_not</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bitwise_not" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_not()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bitwise_not_">
<code class="sig-name descname">bitwise_not_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bitwise_not_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.bitwise_not" title="torch.Tensor.bitwise_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_not()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bitwise_xor">
<code class="sig-name descname">bitwise_xor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bitwise_xor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_xor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bitwise_xor_">
<code class="sig-name descname">bitwise_xor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bitwise_xor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.bitwise_xor" title="torch.Tensor.bitwise_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bmm">
<code class="sig-name descname">bmm</code><span class="sig-paren">(</span><em class="sig-param">batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bool">
<code class="sig-name descname">bool</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bool" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bool()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bool)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.byte">
<code class="sig-name descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.byte" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cauchy_">
<code class="sig-name descname">cauchy_</code><span class="sig-paren">(</span><em class="sig-param">median=0</em>, <em class="sig-param">sigma=1</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cauchy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>π</mi></mfrac><mfrac><mi>σ</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mtext>median</mtext><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25744em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord text"><span class="mord">median</span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

</div></dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil">
<code class="sig-name descname">ceil</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil_">
<code class="sig-name descname">ceil_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.char">
<code class="sig-name descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.char" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cholesky">
<code class="sig-name descname">cholesky</code><span class="sig-paren">(</span><em class="sig-param">upper=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cholesky_inverse">
<code class="sig-name descname">cholesky_inverse</code><span class="sig-paren">(</span><em class="sig-param">upper=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cholesky_inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cholesky_solve">
<code class="sig-name descname">cholesky_solve</code><span class="sig-paren">(</span><em class="sig-param">input2</em>, <em class="sig-param">upper=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cholesky_solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.chunk">
<code class="sig-name descname">chunk</code><span class="sig-paren">(</span><em class="sig-param">chunks</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span> &#x2192; List of Tensors<a class="headerlink" href="#torch.Tensor.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp">
<code class="sig-name descname">clamp</code><span class="sig-paren">(</span><em class="sig-param">min</em>, <em class="sig-param">max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp_">
<code class="sig-name descname">clamp_</code><span class="sig-paren">(</span><em class="sig-param">min</em>, <em class="sig-param">max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clone">
<code class="sig-name descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The copy has the same size and data
type as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike <cite>copy_()</cite>, this function is recorded in the computation graph. Gradients
propagating to the cloned tensor will propagate to the original tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.contiguous">
<code class="sig-name descname">contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a contiguous tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous, this function returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.copy_">
<code class="sig-name descname">copy_</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. It may be of a different data type or reside on a
different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source tensor to copy from</p></li>
<li><p><strong>non_blocking</strong> (<em>bool</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.conj">
<code class="sig-name descname">conj</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.conj" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.conj" title="torch.conj"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos">
<code class="sig-name descname">cos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos_">
<code class="sig-name descname">cos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh">
<code class="sig-name descname">cosh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh_">
<code class="sig-name descname">cosh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cpu">
<code class="sig-name descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CPU memory.</p>
<p>If this object is already in CPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cross">
<code class="sig-name descname">cross</code><span class="sig-paren">(</span><em class="sig-param">other</em>, <em class="sig-param">dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cuda">
<code class="sig-name descname">cuda</code><span class="sig-paren">(</span><em class="sig-param">device=None</em>, <em class="sig-param">non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination GPU device.
Defaults to the current CUDA device.</p></li>
<li><p><strong>non_blocking</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumprod">
<code class="sig-name descname">cumprod</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumsum">
<code class="sig-name descname">cumsum</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.data_ptr">
<code class="sig-name descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dequantize">
<code class="sig-name descname">dequantize</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dequantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.det">
<code class="sig-name descname">det</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.det" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.det" title="torch.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dense_dim">
<code class="sig-name descname">dense_dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.dense_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse COO tensor (i.e., with <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> layout),
this returns the number of dense dimensions. Otherwise, this throws an error.</p>
<p>See also <a class="reference internal" href="#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.sparse_dim()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt>
<code class="sig-name descname">detach</code><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Returns a new Tensor, detached from the current graph.</p>
<p>The result will never require gradient.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returned Tensor shares the same storage with the original one.
In-place modifications on either of them will be seen, and may trigger
errors in correctness checks.
IMPORTANT NOTE: Previously, in-place size / stride / storage changes
(such as <cite>resize_</cite> / <cite>resize_as_</cite> / <cite>set_</cite> / <cite>transpose_</cite>) to the returned tensor
also update the original tensor. Now, these in-place changes will not update the
original tensor anymore, and will instead trigger an error.
For sparse tensors:
In-place indices / values changes (such as <cite>zero_</cite> / <cite>copy_</cite> / <cite>add_</cite>) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.</p>
</div>
</dd></dl>

<dl class="method">
<dt>
<code class="sig-name descname">detach_</code><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag">
<code class="sig-name descname">diag</code><span class="sig-paren">(</span><em class="sig-param">diagonal=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag_embed">
<code class="sig-name descname">diag_embed</code><span class="sig-paren">(</span><em class="sig-param">offset=0</em>, <em class="sig-param">dim1=-2</em>, <em class="sig-param">dim2=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag_embed" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diagflat">
<code class="sig-name descname">diagflat</code><span class="sig-paren">(</span><em class="sig-param">offset=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diagflat" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diagflat" title="torch.diagflat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diagonal">
<code class="sig-name descname">diagonal</code><span class="sig-paren">(</span><em class="sig-param">offset=0</em>, <em class="sig-param">dim1=0</em>, <em class="sig-param">dim2=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diagonal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diagonal" title="torch.diagonal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_diagonal_">
<code class="sig-name descname">fill_diagonal_</code><span class="sig-paren">(</span><em class="sig-param">fill_value</em>, <em class="sig-param">wrap=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fill_diagonal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fill the main diagonal of a tensor that has at least 2-dimensions.
When dims&gt;2, all dimensions of input must be of equal length.
This function modifies the input tensor in-place, and returns the input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fill_value</strong> (<em>Scalar</em>) – the fill value</p></li>
<li><p><strong>wrap</strong> (<em>bool</em>) – the diagonal ‘wrapped’ after N columns for tall matrices.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.digamma">
<code class="sig-name descname">digamma</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.digamma" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.digamma" title="torch.digamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.digamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.digamma_">
<code class="sig-name descname">digamma_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.digamma_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.digamma" title="torch.Tensor.digamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">digamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dim">
<code class="sig-name descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dist">
<code class="sig-name descname">dist</code><span class="sig-paren">(</span><em class="sig-param">other</em>, <em class="sig-param">p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div">
<code class="sig-name descname">div</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div_">
<code class="sig-name descname">div_</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dot">
<code class="sig-name descname">dot</code><span class="sig-paren">(</span><em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.double">
<code class="sig-name descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.double" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eig">
<code class="sig-name descname">eig</code><span class="sig-paren">(</span><em class="sig-param">eigenvectors=False) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eig" title="torch.eig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.element_size">
<code class="sig-name descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.element_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq">
<code class="sig-name descname">eq</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq_">
<code class="sig-name descname">eq_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.equal">
<code class="sig-name descname">equal</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf">
<code class="sig-name descname">erf</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf_">
<code class="sig-name descname">erf_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erf" title="torch.Tensor.erf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc">
<code class="sig-name descname">erfc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc_">
<code class="sig-name descname">erfc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erfc" title="torch.Tensor.erfc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv">
<code class="sig-name descname">erfinv</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv_">
<code class="sig-name descname">erfinv_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp">
<code class="sig-name descname">exp</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp_">
<code class="sig-name descname">exp_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1">
<code class="sig-name descname">expm1</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1_">
<code class="sig-name descname">expm1_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand">
<code class="sig-name descname">expand</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. For the new dimensions, the
size cannot be set to -1.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal notranslate"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>python:int...</em>) – the desired expanded size</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of an expanded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensors, please clone them first.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand_as">
<code class="sig-name descname">expand_as</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.expand_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.expand(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">expand</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exponential_">
<code class="sig-name descname">exponential_</code><span class="sig-paren">(</span><em class="sig-param">lambd=1</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exponential_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the exponential distribution:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msup><mi>e</mi><mrow><mo>−</mo><mi>λ</mi><mi>x</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f(x) = \lambda e^{-\lambda x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8991079999999999em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">λ</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span></span>

</div></dd></dl>

<dl class="method">
<dt id="torch.Tensor.fft">
<code class="sig-name descname">fft</code><span class="sig-paren">(</span><em class="sig-param">signal_ndim</em>, <em class="sig-param">normalized=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fft()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_">
<code class="sig-name descname">fill_</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.flatten">
<code class="sig-name descname">flatten</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">start_dim=0</em>, <em class="sig-param">end_dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>see <a class="reference internal" href="torch.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.flip">
<code class="sig-name descname">flip</code><span class="sig-paren">(</span><em class="sig-param">dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.flip" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.flip" title="torch.flip"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.float">
<code class="sig-name descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.float" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor">
<code class="sig-name descname">floor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor_">
<code class="sig-name descname">floor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod">
<code class="sig-name descname">fmod</code><span class="sig-paren">(</span><em class="sig-param">divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod_">
<code class="sig-name descname">fmod_</code><span class="sig-paren">(</span><em class="sig-param">divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac">
<code class="sig-name descname">frac</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac_">
<code class="sig-name descname">frac_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gather">
<code class="sig-name descname">gather</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge">
<code class="sig-name descname">ge</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge_">
<code class="sig-name descname">ge_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geometric_">
<code class="sig-name descname">geometric_</code><span class="sig-paren">(</span><em class="sig-param">p</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.geometric_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo>=</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>p</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(X=k) = p^{k - 1} (1 - p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.149108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></span>

</div></dd></dl>

<dl class="method">
<dt id="torch.Tensor.geqrf">
<code class="sig-name descname">geqrf</code><span class="sig-paren">(</span><em class="sig-param">) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ger">
<code class="sig-name descname">ger</code><span class="sig-paren">(</span><em class="sig-param">vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.get_device">
<code class="sig-name descname">get_device</code><span class="sig-paren">(</span><em class="sig-param">) -&gt; Device ordinal (Integer</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.get_device" title="Permalink to this definition">¶</a></dt>
<dd><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
For CPU tensors, an error is thrown.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>  <span class="c1"># RuntimeError: get_device is not implemented for type torch.FloatTensor</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt">
<code class="sig-name descname">gt</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt_">
<code class="sig-name descname">gt_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.half">
<code class="sig-name descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.half" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.hardshrink">
<code class="sig-name descname">hardshrink</code><span class="sig-paren">(</span><em class="sig-param">lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.functional.html#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.hardshrink()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.histc">
<code class="sig-name descname">histc</code><span class="sig-paren">(</span><em class="sig-param">bins=100</em>, <em class="sig-param">min=0</em>, <em class="sig-param">max=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ifft">
<code class="sig-name descname">ifft</code><span class="sig-paren">(</span><em class="sig-param">signal_ndim</em>, <em class="sig-param">normalized=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ifft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ifft" title="torch.ifft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ifft()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.imag">
<code class="sig-name descname">imag</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.imag" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.imag" title="torch.imag"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.imag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add_">
<code class="sig-name descname">index_add_</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Accumulate the elements of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by adding
to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is added to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to add</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[  2.,   3.,   4.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  8.,   9.,  10.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  5.,   6.,   7.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add">
<code class="sig-name descname">index_add</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy_">
<code class="sig-name descname">index_copy_</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting
the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is copied to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy">
<code class="sig-name descname">index_copy</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill_">
<code class="sig-name descname">index_fill_</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">val</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">val</span></code> by
selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to fill in</p></li>
<li><p><strong>val</strong> (<em>python:float</em>) – the value to fill with</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[-1.,  2., -1.],</span>
<span class="go">        [-1.,  5., -1.],</span>
<span class="go">        [-1.,  8., -1.]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill">
<code class="sig-name descname">index_fill</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_put_">
<code class="sig-name descname">index_put_</code><span class="sig-paren">(</span><em class="sig-param">indices</em>, <em class="sig-param">value</em>, <em class="sig-param">accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Puts values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using
the indices specified in <a class="reference internal" href="#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code></a> (which is a tuple of Tensors). The
expression <code class="docutils literal notranslate"><span class="pre">tensor.index_put_(indices,</span> <span class="pre">value)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">tensor[indices]</span> <span class="pre">=</span> <span class="pre">value</span></code>. Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<em>tuple of LongTensor</em>) – tensors used to index into <cite>self</cite>.</p></li>
<li><p><strong>value</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor of same dtype as <cite>self</cite>.</p></li>
<li><p><strong>accumulate</strong> (<em>bool</em>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_put">
<code class="sig-name descname">index_put</code><span class="sig-paren">(</span><em class="sig-param">indices</em>, <em class="sig-param">value</em>, <em class="sig-param">accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_put" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-place version of <a class="reference internal" href="#torch.Tensor.index_put_" title="torch.Tensor.index_put_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_put_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_select">
<code class="sig-name descname">index_select</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.indices">
<code class="sig-name descname">indices</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.indices" title="Permalink to this definition">¶</a></dt>
<dd><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse COO tensor (i.e., with <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> layout),
this returns a view of the contained indices tensor. Otherwise, this throws an
error.</p>
<p>See also <a class="reference internal" href="#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.values()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code> for details.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int">
<code class="sig-name descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.int" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int_repr">
<code class="sig-name descname">int_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.int_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a quantized Tensor,
<code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.inverse">
<code class="sig-name descname">inverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.irfft">
<code class="sig-name descname">irfft</code><span class="sig-paren">(</span><em class="sig-param">signal_ndim</em>, <em class="sig-param">normalized=False</em>, <em class="sig-param">onesided=True</em>, <em class="sig-param">signal_sizes=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.irfft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.irfft" title="torch.irfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.irfft()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_contiguous">
<code class="sig-name descname">is_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in C order.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_floating_point">
<code class="sig-name descname">is_floating_point</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_floating_point" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-name descname">is_leaf</code></dt>
<dd><p>All Tensors that have <a class="reference internal" href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a class="reference internal" href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">True</span></code>, they will be leaf Tensors if they were
created by the user. This means that they are not the result of an operation and so
<code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_fn</span></code> is None.</p>
<p>Only leaf Tensors will have their <a class="reference internal" href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during a call to <a class="reference internal" href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>.
To get <a class="reference internal" href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated for non-leaf Tensors, you can use <a class="reference internal" href="autograd.html#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain_grad()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># b was created by the operation that cast a cpu Tensor into a cuda Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># c was created by the addition operation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># e requires gradients and has no operations creating it</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># f requires grad, has no operation creating it</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_pinned">
<code class="sig-name descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this tensor resides in pinned memory.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_set_to">
<code class="sig-name descname">is_set_to</code><span class="sig-paren">(</span><em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_set_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if this object refers to the same <code class="docutils literal notranslate"><span class="pre">THTensor</span></code> object from the
Torch C API as the given tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_shared">
<code class="sig-name descname">is_shared</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.is_shared"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.is_shared" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if tensor is in shared memory.</p>
<p>This is always <code class="docutils literal notranslate"><span class="pre">True</span></code> for CUDA tensors.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_signed">
<code class="sig-name descname">is_signed</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_signed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a signed data type.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_sparse">
<code class="sig-name descname">is_sparse</code><a class="headerlink" href="#torch.Tensor.is_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.item">
<code class="sig-name descname">item</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; number<a class="headerlink" href="#torch.Tensor.item" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of this tensor as a standard Python number. This only works
for tensors with one element. For other cases, see <a class="reference internal" href="#torch.Tensor.tolist" title="torch.Tensor.tolist"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tolist()</span></code></a>.</p>
<p>This operation is not differentiable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.kthvalue">
<code class="sig-name descname">kthvalue</code><span class="sig-paren">(</span><em class="sig-param">k</em>, <em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; (Tensor</em>, <em class="sig-param">LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le">
<code class="sig-name descname">le</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le_">
<code class="sig-name descname">le_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp">
<code class="sig-name descname">lerp</code><span class="sig-paren">(</span><em class="sig-param">end</em>, <em class="sig-param">weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp_">
<code class="sig-name descname">lerp_</code><span class="sig-paren">(</span><em class="sig-param">end</em>, <em class="sig-param">weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lgamma">
<code class="sig-name descname">lgamma</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lgamma" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lgamma_">
<code class="sig-name descname">lgamma_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lgamma_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lgamma" title="torch.Tensor.lgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log">
<code class="sig-name descname">log</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_">
<code class="sig-name descname">log_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logdet">
<code class="sig-name descname">logdet</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logdet" title="torch.logdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10">
<code class="sig-name descname">log10</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10_">
<code class="sig-name descname">log10_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p">
<code class="sig-name descname">log1p</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p_">
<code class="sig-name descname">log1p_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2">
<code class="sig-name descname">log2</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2_">
<code class="sig-name descname">log2_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_normal_">
<code class="sig-name descname">log_normal_</code><span class="sig-paren">(</span><em class="sig-param">mean=1</em>, <em class="sig-param">std=2</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.log_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution
parameterized by the given mean <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>

</span> and standard deviation
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>

</span>. Note that <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> are the mean and
standard deviation of the underlying normal distribution, and not of the
returned distribution:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>x</mi><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow></mfrac><mtext> </mtext><msup><mi>e</mi><mrow><mo>−</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>ln</mi><mo>⁡</mo><mi>x</mi><mo>−</mo><mi>μ</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25909em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.2027799999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.90722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span><span style="top:-2.86722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.13278em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"> </span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.32909em;"><span style="top:-3.4534200000000004em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.250957142857143em;"><span style="top:-2.5061857142857145em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9384399999999999em;"><span style="top:-2.93844em;margin-right:0.1em;"><span class="pstrut" style="height:2.64444em;"></span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.5020714285714285em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mop mtight"><span class="mtight">l</span><span class="mtight">n</span></span><span class="mspace mtight" style="margin-right:0.24533333333333335em;"></span><span class="mord mathdefault mtight">x</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">μ</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.04844em;"><span style="top:-3.04844em;margin-right:0.1em;"><span class="pstrut" style="height:2.64444em;"></span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.49381428571428565em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span></span></span>

</div></dd></dl>

<dl class="method">
<dt id="torch.Tensor.logsumexp">
<code class="sig-name descname">logsumexp</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logical_not">
<code class="sig-name descname">logical_not</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logical_not" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logical_not" title="torch.logical_not"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_not()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logical_not_">
<code class="sig-name descname">logical_not_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logical_not_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.logical_not" title="torch.Tensor.logical_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_not()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logical_xor">
<code class="sig-name descname">logical_xor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logical_xor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logical_xor" title="torch.logical_xor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_xor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logical_xor_">
<code class="sig-name descname">logical_xor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logical_xor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.logical_xor" title="torch.Tensor.logical_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_xor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.long">
<code class="sig-name descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.long" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lstsq">
<code class="sig-name descname">lstsq</code><span class="sig-paren">(</span><em class="sig-param">A) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.lstsq" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lstsq" title="torch.lstsq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lstsq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt">
<code class="sig-name descname">lt</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt_">
<code class="sig-name descname">lt_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lu">
<code class="sig-name descname">lu</code><span class="sig-paren">(</span><em class="sig-param">pivot=True</em>, <em class="sig-param">get_infos=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.lu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.lu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lu" title="torch.lu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lu_solve">
<code class="sig-name descname">lu_solve</code><span class="sig-paren">(</span><em class="sig-param">LU_data</em>, <em class="sig-param">LU_pivots</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lu_solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lu_solve" title="torch.lu_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu_solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.map_">
<code class="sig-name descname">map_</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">callable</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.map_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and
the given <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter_">
<code class="sig-name descname">masked_scatter_</code><span class="sig-paren">(</span><em class="sig-param">mask</em>, <em class="sig-param">source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True.
The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<a class="reference internal" href="#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>) – the boolean mask</p></li>
<li><p><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to copy from</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter">
<code class="sig-name descname">masked_scatter</code><span class="sig-paren">(</span><em class="sig-param">mask</em>, <em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill_">
<code class="sig-name descname">masked_fill_</code><span class="sig-paren">(</span><em class="sig-param">mask</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is
True. The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<a class="reference internal" href="#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>) – the boolean mask</p></li>
<li><p><strong>value</strong> (<em>python:float</em>) – the value to fill in with</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill">
<code class="sig-name descname">masked_fill</code><span class="sig-paren">(</span><em class="sig-param">mask</em>, <em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_select">
<code class="sig-name descname">masked_select</code><span class="sig-paren">(</span><em class="sig-param">mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matmul">
<code class="sig-name descname">matmul</code><span class="sig-paren">(</span><em class="sig-param">tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matrix_power">
<code class="sig-name descname">matrix_power</code><span class="sig-paren">(</span><em class="sig-param">n</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.matrix_power" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.matrix_power" title="torch.matrix_power"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_power()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.max">
<code class="sig-name descname">max</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; Tensor or (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mean">
<code class="sig-name descname">mean</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; Tensor or (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.median">
<code class="sig-name descname">median</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; (Tensor</em>, <em class="sig-param">LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.median" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.min">
<code class="sig-name descname">min</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; Tensor or (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.min" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mm">
<code class="sig-name descname">mm</code><span class="sig-paren">(</span><em class="sig-param">mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mode">
<code class="sig-name descname">mode</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False) -&gt; (Tensor</em>, <em class="sig-param">LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul">
<code class="sig-name descname">mul</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul_">
<code class="sig-name descname">mul_</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.multinomial">
<code class="sig-name descname">multinomial</code><span class="sig-paren">(</span><em class="sig-param">num_samples</em>, <em class="sig-param">replacement=False</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mv">
<code class="sig-name descname">mv</code><span class="sig-paren">(</span><em class="sig-param">vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mvlgamma">
<code class="sig-name descname">mvlgamma</code><span class="sig-paren">(</span><em class="sig-param">p</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mvlgamma" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mvlgamma_">
<code class="sig-name descname">mvlgamma_</code><span class="sig-paren">(</span><em class="sig-param">p</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mvlgamma_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow">
<code class="sig-name descname">narrow</code><span class="sig-paren">(</span><em class="sig-param">dimension</em>, <em class="sig-param">start</em>, <em class="sig-param">length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.narrow" title="torch.narrow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 2,  3],</span>
<span class="go">        [ 5,  6],</span>
<span class="go">        [ 8,  9]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow_copy">
<code class="sig-name descname">narrow_copy</code><span class="sig-paren">(</span><em class="sig-param">dimension</em>, <em class="sig-param">start</em>, <em class="sig-param">length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.narrow_copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <a class="reference internal" href="#torch.Tensor.narrow" title="torch.Tensor.narrow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.narrow()</span></code></a> except returning a copy rather
than shared storage.  This is primarily for sparse tensors, which
do not have a shared-storage narrow method.  Calling <code class="docutils literal notranslate"><span class="pre">`narrow_copy</span></code>
with <code class="docutils literal notranslate"><span class="pre">`dimemsion</span> <span class="pre">&gt;</span> <span class="pre">self.sparse_dim()`</span></code> will return a copy with the
relevant dense dimension narrowed, and <code class="docutils literal notranslate"><span class="pre">`self.shape`</span></code> updated accordingly.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ndimension">
<code class="sig-name descname">ndimension</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.ndimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne">
<code class="sig-name descname">ne</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne_">
<code class="sig-name descname">ne_</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg">
<code class="sig-name descname">neg</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg_">
<code class="sig-name descname">neg_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nelement">
<code class="sig-name descname">nelement</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.nelement" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nonzero">
<code class="sig-name descname">nonzero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.norm">
<code class="sig-name descname">norm</code><span class="sig-paren">(</span><em class="sig-param">p='fro'</em>, <em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.normal_">
<code class="sig-name descname">normal_</code><span class="sig-paren">(</span><em class="sig-param">mean=0</em>, <em class="sig-param">std=1</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numel">
<code class="sig-name descname">numel</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numpy">
<code class="sig-name descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#torch.Tensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>. This tensor and the
returned <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> share the same underlying storage. Changes to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will be reflected in the <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> and vice versa.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.orgqr">
<code class="sig-name descname">orgqr</code><span class="sig-paren">(</span><em class="sig-param">input2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.orgqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ormqr">
<code class="sig-name descname">ormqr</code><span class="sig-paren">(</span><em class="sig-param">input2</em>, <em class="sig-param">input3</em>, <em class="sig-param">left=True</em>, <em class="sig-param">transpose=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ormqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.permute">
<code class="sig-name descname">permute</code><span class="sig-paren">(</span><em class="sig-param">*dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*dims</strong> (<em>python:int...</em>) – The desired ordering of dimensions</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([5, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pin_memory">
<code class="sig-name descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the tensor to pinned memory, if it’s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pinverse">
<code class="sig-name descname">pinverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pinverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.polygamma">
<code class="sig-name descname">polygamma</code><span class="sig-paren">(</span><em class="sig-param">n</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.polygamma" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.polygamma" title="torch.polygamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.polygamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.polygamma_">
<code class="sig-name descname">polygamma_</code><span class="sig-paren">(</span><em class="sig-param">n</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.polygamma_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.polygamma" title="torch.Tensor.polygamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">polygamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow">
<code class="sig-name descname">pow</code><span class="sig-paren">(</span><em class="sig-param">exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow_">
<code class="sig-name descname">pow_</code><span class="sig-paren">(</span><em class="sig-param">exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.prod">
<code class="sig-name descname">prod</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.prod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.put_">
<code class="sig-name descname">put_</code><span class="sig-paren">(</span><em class="sig-param">indices</em>, <em class="sig-param">tensor</em>, <em class="sig-param">accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the positions specified by
indices. For the purpose of indexing, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is treated as if
it were a 1-D tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<em>LongTensor</em>) – the indices into self</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy from</p></li>
<li><p><strong>accumulate</strong> (<em>bool</em>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="go">                        [6, 7, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">tensor([[  4,   9,   5],</span>
<span class="go">        [ 10,   7,   8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qr">
<code class="sig-name descname">qr</code><span class="sig-paren">(</span><em class="sig-param">some=True) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qscheme">
<code class="sig-name descname">qscheme</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.qscheme<a class="headerlink" href="#torch.Tensor.qscheme" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the quantization scheme of a given QTensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.q_scale">
<code class="sig-name descname">q_scale</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.q_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the scale of the underlying quantizer().</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.q_zero_point">
<code class="sig-name descname">q_zero_point</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.q_zero_point" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the zero_point of the underlying quantizer().</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.q_per_channel_scales">
<code class="sig-name descname">q_per_channel_scales</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.q_per_channel_scales" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a Tensor of scales of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.q_per_channel_zero_points">
<code class="sig-name descname">q_per_channel_zero_points</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.q_per_channel_zero_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a tensor of zero_points of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.q_per_channel_axis">
<code class="sig-name descname">q_per_channel_axis</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.q_per_channel_axis" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns the index of dimension on which per-channel quantization is applied.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.random_">
<code class="sig-name descname">random_</code><span class="sig-paren">(</span><em class="sig-param">from=0</em>, <em class="sig-param">to=None</em>, <em class="sig-param">*</em>, <em class="sig-param">generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.random_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform
distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If not specified, the values are usually
only bounded by <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s data type. However, for floating point
types, if unspecified, range will be <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^mantissa]</span></code> to ensure that every
value is representable. For example, <cite>torch.tensor(1, dtype=torch.double).random_()</cite>
will be uniform in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^53]</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal">
<code class="sig-name descname">reciprocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal_">
<code class="sig-name descname">reciprocal_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.record_stream">
<code class="sig-name descname">record_stream</code><span class="sig-paren">(</span><em class="sig-param">stream</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.record_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensures that the tensor memory is not reused for another tensor until all
current work queued on <code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code> are complete.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The caching allocator is aware of only the stream where a tensor was
allocated. Due to the awareness, it already correctly manages the life
cycle of tensors on only one stream. But if a tensor is used on a stream
different from the stream of origin, the allocator might reuse the memory
unexpectedly. Calling this method lets the allocator know which streams
have used the tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt>
<code class="sig-name descname">register_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.register_hook"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
Tensor is computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <a class="reference internal" href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span>

<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 6</span>
<span class="go">[torch.FloatTensor of size (3,)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder">
<code class="sig-name descname">remainder</code><span class="sig-paren">(</span><em class="sig-param">divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder_">
<code class="sig-name descname">remainder_</code><span class="sig-paren">(</span><em class="sig-param">divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.real">
<code class="sig-name descname">real</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.real" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.real" title="torch.real"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.real()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm">
<code class="sig-name descname">renorm</code><span class="sig-paren">(</span><em class="sig-param">p</em>, <em class="sig-param">dim</em>, <em class="sig-param">maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm_">
<code class="sig-name descname">renorm_</code><span class="sig-paren">(</span><em class="sig-param">p</em>, <em class="sig-param">dim</em>, <em class="sig-param">maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat">
<code class="sig-name descname">repeat</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a>, this function copies the tensor’s data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat()</span></code> behaves differently from
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html">numpy.repeat</a>,
but is more similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html">numpy.tile</a>.
For the operator similar to <cite>numpy.repeat</cite>, see <a class="reference internal" href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>python:int...</em>) – The number of times to repeat this tensor along each
dimension</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat_interleave">
<code class="sig-name descname">repeat_interleave</code><span class="sig-paren">(</span><em class="sig-param">repeats</em>, <em class="sig-param">dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.repeat_interleave" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-name descname">requires_grad</code></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fact that gradients need to be computed for a Tensor do not mean that the <a class="reference internal" href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>
attribute will be populated, see <a class="reference internal" href="autograd.html#torch.Tensor.is_leaf" title="torch.Tensor.is_leaf"><code class="xref py py-attr docutils literal notranslate"><span class="pre">is_leaf</span></code></a> for more details.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.requires_grad_">
<code class="sig-name descname">requires_grad_</code><span class="sig-paren">(</span><em class="sig-param">requires_grad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on this tensor: sets this tensor’s
<a class="reference internal" href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute in-place. Returns this tensor.</p>
<p><a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a>’s main use case is to tell autograd to begin recording
operations on a Tensor <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. If <code class="docutils literal notranslate"><span class="pre">tensor</span></code> has <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>
(because it was obtained through a DataLoader, or required preprocessing or
initialization), <code class="docutils literal notranslate"><span class="pre">tensor.requires_grad_()</span></code> makes it so that autograd will
begin to record operations on <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – If autograd should record operations on this tensor.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Let&#39;s say we want to preprocess some saved weights and use</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the result as new weights.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span>  <span class="c1"># some function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span>
<span class="go">tensor([-0.5503,  0.4926, -2.1158, -0.8303])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now, start to record operations done to weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([-1.1007,  0.9853, -4.2316, -1.6606])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape">
<code class="sig-name descname">reshape</code><span class="sig-paren">(</span><em class="sig-param">*shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
but with the specified shape. This method returns a view if <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code> is
compatible with the current shape. See <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is
possible to return a view.</p>
<p>See <a class="reference internal" href="torch.html#torch.reshape" title="torch.reshape"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reshape()</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>tuple of python:ints</em><em> or </em><em>python:int...</em>) – the desired shape</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape_as">
<code class="sig-name descname">reshape_as</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.reshape_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.reshape(other.sizes())</span></code>.
This method returns a view if <code class="docutils literal notranslate"><span class="pre">other.sizes()</span></code> is compatible with the current
shape. See <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is possible to return a view.</p>
<p>Please see <a class="reference internal" href="torch.html#torch.reshape" title="torch.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same shape
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_">
<code class="sig-name descname">resize_</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is a low-level method. The storage is reinterpreted as C-contiguous,
ignoring the current strides (unless the target size equals the current
size, in which case the tensor is left unchanged). For most purposes, you
will instead want to use <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a>, which checks for
contiguity, or <a class="reference internal" href="#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which copies data if needed. To
change the size in-place with custom strides, see <a class="reference internal" href="#torch.Tensor.set_" title="torch.Tensor.set_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>python:int...</em>) – the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2],</span>
<span class="go">        [ 3,  4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_as_">
<code class="sig-name descname">resize_as_</code><span class="sig-paren">(</span><em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_as_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>. This is equivalent to <code class="docutils literal notranslate"><span class="pre">self.resize_(tensor.size())</span></code>.</p>
</dd></dl>

<dl class="method">
<dt>
<code class="sig-name descname">retain_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.retain_grad"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Enables .grad attribute for non-leaf Tensors.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rfft">
<code class="sig-name descname">rfft</code><span class="sig-paren">(</span><em class="sig-param">signal_ndim</em>, <em class="sig-param">normalized=False</em>, <em class="sig-param">onesided=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rfft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rfft()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.roll">
<code class="sig-name descname">roll</code><span class="sig-paren">(</span><em class="sig-param">shifts</em>, <em class="sig-param">dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.roll" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.roll" title="torch.roll"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rot90">
<code class="sig-name descname">rot90</code><span class="sig-paren">(</span><em class="sig-param">k</em>, <em class="sig-param">dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rot90" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.rot90" title="torch.rot90"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rot90()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round">
<code class="sig-name descname">round</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round_">
<code class="sig-name descname">round_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt">
<code class="sig-name descname">rsqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt_">
<code class="sig-name descname">rsqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter">
<code class="sig-name descname">scatter</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">source</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_">
<code class="sig-name descname">scatter_</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">src</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, its output
index is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by
the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>This is the reverse operation of the manner described in <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> (if it is a Tensor) should have same
number of dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code>
for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<p>Moreover, as for <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive, and all values in a row
along the specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> must be unique.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter,
can be either empty or the same size of src.
When empty, the operation returns identity</p></li>
<li><p><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source element(s) to scatter,
incase <cite>value</cite> is not specified</p></li>
<li><p><strong>value</strong> (<em>python:float</em>) – the source element(s) to scatter,
incase <cite>src</cite> is not specified</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],</span>
<span class="go">        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],</span>
<span class="go">        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],</span>
<span class="go">        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>
<span class="go">tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  1.2300]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_add_">
<code class="sig-name descname">scatter_add_</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as
<a class="reference internal" href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code></a>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, it is added to
an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>
for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> should have same number of
dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">other.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions
<code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and add,
can be either empty or the same size of src.
When empty, the operation returns identity.</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source elements to scatter and add</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],</span>
<span class="go">        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],</span>
<span class="go">        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],</span>
<span class="go">        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_add">
<code class="sig-name descname">scatter_add</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em>, <em class="sig-param">source</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor along the selected dimension at the given index.
This function returns a tensor with the given dimension removed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – the dimension to slice</p></li>
<li><p><strong>index</strong> (<em>python:int</em>) – the index to select with</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-meth docutils literal notranslate"><span class="pre">select()</span></code></a> is equivalent to slicing. For example,
<code class="docutils literal notranslate"><span class="pre">tensor.select(0,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[index]</span></code> and
<code class="docutils literal notranslate"><span class="pre">tensor.select(2,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[:,:,index]</span></code>.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.set_">
<code class="sig-name descname">set_</code><span class="sig-paren">(</span><em class="sig-param">source=None</em>, <em class="sig-param">storage_offset=0</em>, <em class="sig-param">size=None</em>, <em class="sig-param">stride=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.set_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a tensor,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will share the same storage and have the same size and
strides as <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code>. Changes to elements in one tensor will be reflected
in the other.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – the tensor or storage to use</p></li>
<li><p><strong>storage_offset</strong> (<em>python:int</em><em>, </em><em>optional</em>) – the offset in the storage</p></li>
<li><p><strong>size</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – the desired size. Defaults to the size of the source.</p></li>
<li><p><strong>stride</strong> (<em>tuple</em><em>, </em><em>optional</em>) – the desired stride. Defaults to C-contiguous strides.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.share_memory_">
<code class="sig-name descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.share_memory_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.short">
<code class="sig-name descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.short" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid">
<code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid_">
<code class="sig-name descname">sigmoid_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign">
<code class="sig-name descname">sign</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign_">
<code class="sig-name descname">sign_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin">
<code class="sig-name descname">sin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin_">
<code class="sig-name descname">sin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh">
<code class="sig-name descname">sinh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh_">
<code class="sig-name descname">sinh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.size">
<code class="sig-name descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Size<a class="headerlink" href="#torch.Tensor.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The returned value is a subclass of
<code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.slogdet">
<code class="sig-name descname">slogdet</code><span class="sig-paren">(</span><em class="sig-param">) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.slogdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.solve">
<code class="sig-name descname">solve</code><span class="sig-paren">(</span><em class="sig-param">A</em><span class="sig-paren">)</span> &#x2192; Tensor, Tensor<a class="headerlink" href="#torch.Tensor.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sort">
<code class="sig-name descname">sort</code><span class="sig-paren">(</span><em class="sig-param">dim=-1</em>, <em class="sig-param">descending=False) -&gt; (Tensor</em>, <em class="sig-param">LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.split">
<code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param">split_size</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.split" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sparse_mask">
<code class="sig-name descname">sparse_mask</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sparse_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SparseTensor with values from Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filtered
by indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> and values are ignored. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
must have the same shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an input Tensor</p></li>
<li><p><strong>mask</strong> (<em>SparseTensor</em>) – a SparseTensor which we filter <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> based on its indices</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nnz</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nnz</span><span class="p">,)),</span>
<span class="go">                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nnz</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 2],</span>
<span class="go">                       [0, 1, 4, 3]]),</span>
<span class="go">       values=tensor([[[ 1.6550,  0.2397],</span>
<span class="go">                       [-0.1611, -0.0779]],</span>

<span class="go">                      [[ 0.2326, -1.0558],</span>
<span class="go">                       [ 1.4711,  1.9678]],</span>

<span class="go">                      [[-0.5138, -0.0411],</span>
<span class="go">                       [ 1.9417,  0.5158]],</span>

<span class="go">                      [[ 0.0793,  0.0036],</span>
<span class="go">                       [-0.2569, -0.1055]]]),</span>
<span class="go">       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sparse_dim">
<code class="sig-name descname">sparse_dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.sparse_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse COO tensor (i.e., with <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> layout),
this returns the number of sparse dimensions. Otherwise, this throws an error.</p>
<p>See also <a class="reference internal" href="#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.dense_dim()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt">
<code class="sig-name descname">sqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt_">
<code class="sig-name descname">sqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze">
<code class="sig-name descname">squeeze</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze_">
<code class="sig-name descname">squeeze_</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.std">
<code class="sig-name descname">std</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">unbiased=True</em>, <em class="sig-param">keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.stft">
<code class="sig-name descname">stft</code><span class="sig-paren">(</span><em class="sig-param">n_fft</em>, <em class="sig-param">hop_length=None</em>, <em class="sig-param">win_length=None</em>, <em class="sig-param">window=None</em>, <em class="sig-param">center=True</em>, <em class="sig-param">pad_mode='reflect'</em>, <em class="sig-param">normalized=False</em>, <em class="sig-param">onesided=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.stft"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.stft" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.stft" title="torch.stft"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></a></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function changed signature at version 0.4.1. Calling with
the previous signature may cause error or return incorrect result.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage">
<code class="sig-name descname">storage</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Storage<a class="headerlink" href="#torch.Tensor.storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying storage.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_offset">
<code class="sig-name descname">storage_offset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.storage_offset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of
number of storage elements (not bytes).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_type">
<code class="sig-name descname">storage_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; type<a class="headerlink" href="#torch.Tensor.storage_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type of the underlying storage.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.stride">
<code class="sig-name descname">stride</code><span class="sig-paren">(</span><em class="sig-param">dim</em><span class="sig-paren">)</span> &#x2192; tuple or int<a class="headerlink" href="#torch.Tensor.stride" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<p>Stride is the jump necessary to go from one element to the next one in the
specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>python:int</em><em>, </em><em>optional</em>) – the desired dimension in which stride is required</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="go">&gt;&gt;&gt;x.stride(0)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub">
<code class="sig-name descname">sub</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts a scalar or tensor from <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If both <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub_">
<code class="sig-name descname">sub_</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum">
<code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum_to_size">
<code class="sig-name descname">sum_to_size</code><span class="sig-paren">(</span><em class="sig-param">*size</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sum_to_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Sum <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor to <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>.
<a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> must be broadcastable to <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.svd">
<code class="sig-name descname">svd</code><span class="sig-paren">(</span><em class="sig-param">some=True</em>, <em class="sig-param">compute_uv=True) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.svd" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.symeig">
<code class="sig-name descname">symeig</code><span class="sig-paren">(</span><em class="sig-param">eigenvectors=False</em>, <em class="sig-param">upper=True) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.symeig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t">
<code class="sig-name descname">t</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t_">
<code class="sig-name descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Tensor dtype and/or device conversion. A <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> are
inferred from the arguments of <code class="docutils literal notranslate"><span class="pre">self.to(*args,</span> <span class="pre">**kwargs)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">self</span></code> Tensor already
has the correct <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, then <code class="docutils literal notranslate"><span class="pre">self</span></code> is returned.
Otherwise, the returned tensor is a copy of <code class="docutils literal notranslate"><span class="pre">self</span></code> with the desired
<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>.</p>
</div>
<p>Here are the ways to call <code class="docutils literal notranslate"><span class="pre">to</span></code>:</p>
<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">dtype</em>, <em class="sig-param">non_blocking=False</em>, <em class="sig-param">copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></p>
</dd></dl>

<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device=None</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">non_blocking=False</em>, <em class="sig-param">copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <a class="reference internal" href="#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> and (optional)
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> it is inferred to be <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert asynchronously with respect to
the host if possible, e.g., converting a CPU Tensor with pinned memory to a
CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">other</em>, <em class="sig-param">non_blocking=False</em>, <em class="sig-param">copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as
the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert
asynchronously with respect to the host if possible, e.g., converting a CPU
Tensor with pinned memory to a CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Initially dtype=float32, device=cpu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">other</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to_mkldnn">
<code class="sig-name descname">to_mkldnn</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to_mkldnn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of the tensor in <code class="docutils literal notranslate"><span class="pre">torch.mkldnn</span></code> layout.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.take">
<code class="sig-name descname">take</code><span class="sig-paren">(</span><em class="sig-param">indices</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.take" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.take" title="torch.take"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan">
<code class="sig-name descname">tan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan_">
<code class="sig-name descname">tan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh">
<code class="sig-name descname">tanh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh_">
<code class="sig-name descname">tanh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tolist">
<code class="sig-name descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>”
tolist() -&gt; list or number</p>
<p>Returns the tensor as a (nested) list. For scalars, a standard
Python number is returned, just like with <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item()</span></code></a>.
Tensors are automatically moved to the CPU first if necessary.</p>
<p>This operation is not differentiable.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[0.012766935862600803, 0.5415473580360413],</span>
<span class="go"> [-0.08909505605697632, 0.7729271650314331]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">0.012766935862600803</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.topk">
<code class="sig-name descname">topk</code><span class="sig-paren">(</span><em class="sig-param">k</em>, <em class="sig-param">dim=None</em>, <em class="sig-param">largest=True</em>, <em class="sig-param">sorted=True) -&gt; (Tensor</em>, <em class="sig-param">LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to_sparse">
<code class="sig-name descname">to_sparse</code><span class="sig-paren">(</span><em class="sig-param">sparseDims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
<a class="reference internal" href="sparse.html#sparse-docs"><span class="std std-ref">coordinate format</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparseDims</strong> (<em>python:int</em><em>, </em><em>optional</em>) – the number of sparse dimensions to include in the new sparse tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1, 1],</span>
<span class="go">                       [0, 2]]),</span>
<span class="go">       values=tensor([ 9, 10]),</span>
<span class="go">       size=(3, 3), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([[ 9,  0, 10]]),</span>
<span class="go">       size=(3, 3), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trace">
<code class="sig-name descname">trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose">
<code class="sig-name descname">transpose</code><span class="sig-paren">(</span><em class="sig-param">dim0</em>, <em class="sig-param">dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose_">
<code class="sig-name descname">transpose_</code><span class="sig-paren">(</span><em class="sig-param">dim0</em>, <em class="sig-param">dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triangular_solve">
<code class="sig-name descname">triangular_solve</code><span class="sig-paren">(</span><em class="sig-param">A</em>, <em class="sig-param">upper=True</em>, <em class="sig-param">transpose=False</em>, <em class="sig-param">unitriangular=False) -&gt; (Tensor</em>, <em class="sig-param">Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.triangular_solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.triangular_solve" title="torch.triangular_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triangular_solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril">
<code class="sig-name descname">tril</code><span class="sig-paren">(</span><em class="sig-param">k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril_">
<code class="sig-name descname">tril_</code><span class="sig-paren">(</span><em class="sig-param">k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu">
<code class="sig-name descname">triu</code><span class="sig-paren">(</span><em class="sig-param">k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu_">
<code class="sig-name descname">triu_</code><span class="sig-paren">(</span><em class="sig-param">k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc">
<code class="sig-name descname">trunc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc_">
<code class="sig-name descname">trunc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type">
<code class="sig-name descname">type</code><span class="sig-paren">(</span><em class="sig-param">dtype=None</em>, <em class="sig-param">non_blocking=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; str or Tensor<a class="headerlink" href="#torch.Tensor.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<em>python:type</em><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type_as">
<code class="sig-name descname">type_as</code><span class="sig-paren">(</span><em class="sig-param">tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.type_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to <code class="docutils literal notranslate"><span class="pre">self.type(tensor.type())</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor which has the desired type</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unbind">
<code class="sig-name descname">unbind</code><span class="sig-paren">(</span><em class="sig-param">dim=0</em><span class="sig-paren">)</span> &#x2192; seq<a class="headerlink" href="#torch.Tensor.unbind" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.unbind" title="torch.unbind"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unbind()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unfold">
<code class="sig-name descname">unfold</code><span class="sig-paren">(</span><em class="sig-param">dimension</em>, <em class="sig-param">size</em>, <em class="sig-param">step</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor which contains all slices of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the size of dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> for <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, the size of
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> in the returned tensor will be
<cite>(sizedim - size) / step + 1</cite>.</p>
<p>An additional dimension of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> is appended in the returned tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dimension</strong> (<em>python:int</em>) – dimension in which unfolding happens</p></li>
<li><p><strong>size</strong> (<em>python:int</em>) – the size of each slice that is unfolded</p></li>
<li><p><strong>step</strong> (<em>python:int</em>) – the step between each slice</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  3.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 4.,  5.],</span>
<span class="go">        [ 5.,  6.],</span>
<span class="go">        [ 6.,  7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.uniform_">
<code class="sig-name descname">uniform_</code><span class="sig-paren">(</span><em class="sig-param">from=0</em>, <em class="sig-param">to=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform
distribution:</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mtext>to</mtext><mo>−</mo><mtext>from</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(x) = \dfrac{1}{\text{to} - \text{from}}

</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">to</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord text"><span class="mord">from</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

</div></dd></dl>

<dl class="method">
<dt id="torch.Tensor.unique">
<code class="sig-name descname">unique</code><span class="sig-paren">(</span><em class="sig-param">sorted=True</em>, <em class="sig-param">return_inverse=False</em>, <em class="sig-param">return_counts=False</em>, <em class="sig-param">dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.unique"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique elements of the input tensor.</p>
<p>See <a class="reference internal" href="torch.html#torch.unique" title="torch.unique"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unique_consecutive">
<code class="sig-name descname">unique_consecutive</code><span class="sig-paren">(</span><em class="sig-param">return_inverse=False</em>, <em class="sig-param">return_counts=False</em>, <em class="sig-param">dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.unique_consecutive"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.unique_consecutive" title="Permalink to this definition">¶</a></dt>
<dd><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p>
<p>See <a class="reference internal" href="torch.html#torch.unique_consecutive" title="torch.unique_consecutive"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique_consecutive()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze">
<code class="sig-name descname">unsqueeze</code><span class="sig-paren">(</span><em class="sig-param">dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze_">
<code class="sig-name descname">unsqueeze_</code><span class="sig-paren">(</span><em class="sig-param">dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.values">
<code class="sig-name descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.values" title="Permalink to this definition">¶</a></dt>
<dd><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse COO tensor (i.e., with <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> layout),
this returns a view of the contained values tensor. Otherwise, this throws an
error.</p>
<p>See also <a class="reference internal" href="#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.indices()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code> for details.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.var">
<code class="sig-name descname">var</code><span class="sig-paren">(</span><em class="sig-param">dim=None</em>, <em class="sig-param">unbiased=True</em>, <em class="sig-param">keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view">
<code class="sig-name descname">view</code><span class="sig-paren">(</span><em class="sig-param">*shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. For a tensor to be viewed, the new
view size must be compatible with its original size and stride, i.e., each new
view dimension must either be a subspace of an original dimension, or only span
across original dimensions <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo separator="true">,</mo><mi>d</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo>+</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">d, d+1, \dots, d+k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>

</span> that satisfy the following
contiguity-like condition that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∀</mi><mi>i</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\forall i = 0, \dots, k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">∀</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>

</span>,</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo><mo>×</mo><mtext>size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">stride</span></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">stride</span></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">size</span></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>

</div><p>Otherwise, <a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a> needs to be called before the tensor can be
viewed. See also: <a class="reference internal" href="torch.html#torch.reshape" title="torch.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which returns a view if the shapes are
compatible, and copies (equivalent to calling <a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>) otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>torch.Size</em><em> or </em><em>python:int...</em>) – the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps 2nd and 3rd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Does not change tensor layout in memory</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view_as">
<code class="sig-name descname">view_as</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view_as" title="Permalink to this definition">¶</a></dt>
<dd><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.view_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.view(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">view</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.where">
<code class="sig-name descname">where</code><span class="sig-paren">(</span><em class="sig-param">condition</em>, <em class="sig-param">y</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.where" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code>.
See <a class="reference internal" href="torch.html#torch.where" title="torch.where"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.where()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.zero_">
<code class="sig-name descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.zero_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.BoolTensor">
<em class="property">class </em><code class="sig-prename descclassname">torch.</code><code class="sig-name descname">BoolTensor</code><a class="headerlink" href="#torch.BoolTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>The following methods are unique to <a class="reference internal" href="#torch.BoolTensor" title="torch.BoolTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code></a>.</p>
<dl class="method">
<dt id="torch.BoolTensor.all">
<code class="sig-name descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.BoolTensor.all" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="sig-name descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if all elements in the tensor are True, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[False, True]], dtype=torch.bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">tensor(False, dtype=torch.bool)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="sig-name descname">all</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if all elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are True, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[True, True],</span>
<span class="go">        [True, False],</span>
<span class="go">        [True, True],</span>
<span class="go">        [True, True]], dtype=torch.bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ True, False,  True,  True], dtype=torch.bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ True, False], dtype=torch.bool)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.BoolTensor.any">
<code class="sig-name descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.BoolTensor.any" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="sig-name descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if any elements in the tensor are True, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[False, True]], dtype=torch.bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
<span class="go">tensor(True, dtype=torch.bool)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="sig-name descname">any</code><span class="sig-paren">(</span><em class="sig-param">dim</em>, <em class="sig-param">keepdim=False</em>, <em class="sig-param">out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if any elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are True, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ True,  True],</span>
<span class="go">        [False,  True],</span>
<span class="go">        [ True,  True],</span>
<span class="go">        [False, False]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ True,  True,  True, False])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([True, True])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensor_attributes.html" class="btn btn-neutral float-right" title="Tensor Attributes" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="nn.functional.html" class="btn btn-neutral" title="torch.nn.functional" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.Tensor</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
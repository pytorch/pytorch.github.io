


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.sparse &mdash; PyTorch 1.11.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/sparse.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.Tensor.coalesce" href="generated/torch.Tensor.coalesce.html" />
    <link rel="prev" title="torch.random" href="random.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.11.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.sparse</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/sparse.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torch-sparse">
<span id="sparse-docs"></span><h1>torch.sparse<a class="headerlink" href="#torch-sparse" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>PyTorch provides <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> to represent a
multi-dimensional array containing elements of a single data type. By
default, array elements are stored contiguously in memory leading to
efficient implementations of various array processing algorithms that
relay on the fast access to array elements.  However, there exists an
important class of multi-dimensional arrays, so-called sparse arrays,
where the contiguous memory storage of array elements turns out to be
suboptimal. Sparse arrays have a property of having a vast portion of
elements being equal to zero which means that a lot of memory as well
as processor resources can be spared if only the non-zero elements are
stored or/and processed. Various sparse storage formats (<a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_matrix">such as COO,
CSR/CSC, LIL, etc.</a>) have been developed that are optimized for a
particular structure of non-zero elements in sparse arrays as well as
for specific operations on the arrays.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When talking about storing only non-zero elements of a sparse
array, the usage of adjective “non-zero” is not strict: one is
allowed to store also zeros in the sparse array data
structure. Hence, in the following, we use “specified elements” for
those array elements that are actually stored. In addition, the
unspecified elements are typically assumed to have zero value, but
not only, hence we use the term “fill value” to denote such
elements.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using a sparse storage format for storing sparse arrays can be
advantageous only when the size and sparsity levels of arrays are
high. Otherwise, for small-sized or low-sparsity arrays using the
contiguous memory storage format is likely the most efficient
approach.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The PyTorch API of sparse tensors is in beta and may change in the near future.</p>
</div>
</div>
<div class="section" id="sparse-coo-tensors">
<span id="sparse-coo-docs"></span><h2>Sparse COO tensors<a class="headerlink" href="#sparse-coo-tensors" title="Permalink to this headline">¶</a></h2>
<p>PyTorch implements the so-called Coordinate format, or COO
format, as one of the storage formats for implementing sparse
tensors.  In COO format, the specified elements are stored as tuples
of element indices and the corresponding values. In particular,</p>
<blockquote>
<div><ul class="simple">
<li><p>the indices of specified elements are collected in <code class="docutils literal notranslate"><span class="pre">indices</span></code>
tensor of size <code class="docutils literal notranslate"><span class="pre">(ndim,</span> <span class="pre">nse)</span></code> and with element type
<code class="docutils literal notranslate"><span class="pre">torch.int64</span></code>,</p></li>
<li><p>the corresponding values are collected in <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor of
size <code class="docutils literal notranslate"><span class="pre">(nse,)</span></code> and with an arbitrary integer or floating point
number element type,</p></li>
</ul>
</div></blockquote>
<p>where <code class="docutils literal notranslate"><span class="pre">ndim</span></code> is the dimensionality of the tensor and <code class="docutils literal notranslate"><span class="pre">nse</span></code> is the
number of specified elements.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The memory consumption of a sparse COO tensor is at least <code class="docutils literal notranslate"><span class="pre">(ndim</span> <span class="pre">*</span>
<span class="pre">8</span> <span class="pre">+</span> <span class="pre">&lt;size</span> <span class="pre">of</span> <span class="pre">element</span> <span class="pre">type</span> <span class="pre">in</span> <span class="pre">bytes&gt;)</span> <span class="pre">*</span> <span class="pre">nse</span></code> bytes (plus a constant
overhead from storing other tensor data).</p>
<p>The memory consumption of a strided tensor is at least
<code class="docutils literal notranslate"><span class="pre">product(&lt;tensor</span> <span class="pre">shape&gt;)</span> <span class="pre">*</span> <span class="pre">&lt;size</span> <span class="pre">of</span> <span class="pre">element</span> <span class="pre">type</span> <span class="pre">in</span> <span class="pre">bytes&gt;</span></code>.</p>
<p>For example, the memory consumption of a 10 000 x 10 000 tensor
with 100 000 non-zero 32-bit floating point numbers is at least
<code class="docutils literal notranslate"><span class="pre">(2</span> <span class="pre">*</span> <span class="pre">8</span> <span class="pre">+</span> <span class="pre">4)</span> <span class="pre">*</span> <span class="pre">100</span> <span class="pre">000</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">000</span> <span class="pre">000</span></code> bytes when using COO tensor
layout and <code class="docutils literal notranslate"><span class="pre">10</span> <span class="pre">000</span> <span class="pre">*</span> <span class="pre">10</span> <span class="pre">000</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">=</span> <span class="pre">400</span> <span class="pre">000</span> <span class="pre">000</span></code> bytes when using
the default strided tensor layout. Notice the 200 fold memory
saving from using the COO storage format.</p>
</div>
<div class="section" id="construction">
<h3>Construction<a class="headerlink" href="#construction" title="Permalink to this headline">¶</a></h3>
<p>A sparse COO tensor can be constructed by providing the two tensors of
indices and values, as well as the size of the sparse tensor (when it
cannot be inferred from the indices and values tensors) to a function
<a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse_coo_tensor()</span></code></a>.</p>
<p>Suppose we want to define a sparse tensor with the entry 3 at location
(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).
Unspecified elements are assumed to have the same value, fill value,
which is zero by default. We would then write:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([3, 4, 5]),</span>
<span class="go">       size=(2, 3), nnz=3, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[0, 0, 3],</span>
<span class="go">        [4, 0, 5]])</span>
</pre></div>
</div>
<p>Note that the input <code class="docutils literal notranslate"><span class="pre">i</span></code> is NOT a list of index tuples.  If you want
to write your indices this way, you should transpose before passing them to
the sparse constructor:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span>      <span class="mi">4</span><span class="p">,</span>      <span class="mi">5</span>    <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)),</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Or another equivalent formulation to get s</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[0, 0, 3],</span>
<span class="go">        [4, 0, 5]])</span>
</pre></div>
</div>
<p>An empty sparse COO tensor can be constructed by specifying its size
only:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor(indices=tensor([], size=(2, 0)),</span>
<span class="go">       values=tensor([], size=(0,)),</span>
<span class="go">       size=(2, 3), nnz=0, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</div>
<div class="section" id="hybrid-sparse-coo-tensors">
<span id="sparse-hybrid-coo-docs"></span><h3>Hybrid sparse COO tensors<a class="headerlink" href="#hybrid-sparse-coo-tensors" title="Permalink to this headline">¶</a></h3>
<p>Pytorch implements an extension of sparse tensors with scalar values
to sparse tensors with (contiguous) tensor values. Such tensors are
called hybrid tensors.</p>
<p>PyTorch hybrid COO tensor extends the sparse COO tensor by allowing
the <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor to be a multi-dimensional tensor so that we
have:</p>
<blockquote>
<div><ul class="simple">
<li><p>the indices of specified elements are collected in <code class="docutils literal notranslate"><span class="pre">indices</span></code>
tensor of size <code class="docutils literal notranslate"><span class="pre">(sparse_dims,</span> <span class="pre">nse)</span></code> and with element type
<code class="docutils literal notranslate"><span class="pre">torch.int64</span></code>,</p></li>
<li><p>the corresponding (tensor) values are collected in <code class="docutils literal notranslate"><span class="pre">values</span></code>
tensor of size <code class="docutils literal notranslate"><span class="pre">(nse,</span> <span class="pre">dense_dims)</span></code> and with an arbitrary integer
or floating point number element type.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use (M + K)-dimensional tensor to denote a N-dimensional hybrid
sparse tensor, where M and K are the numbers of sparse and dense
dimensions, respectively, such that M + K == N holds.</p>
</div>
<p>Suppose we want to create a (2 + 1)-dimensional tensor with the entry
[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry
[7, 8] at location (1, 2). We would write</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([[3, 4],</span>
<span class="go">                      [5, 6],</span>
<span class="go">                      [7, 8]]),</span>
<span class="go">       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[[0, 0],</span>
<span class="go">         [0, 0],</span>
<span class="go">         [3, 4]],</span>
<span class="go">        [[5, 6],</span>
<span class="go">         [0, 0],</span>
<span class="go">         [7, 8]]])</span>
</pre></div>
</div>
<p>In general, if <code class="docutils literal notranslate"><span class="pre">s</span></code> is a sparse COO tensor and <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">=</span>
<span class="pre">s.sparse_dim()</span></code>, <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">=</span> <span class="pre">s.dense_dim()</span></code>, then we have the following
invariants:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">+</span> <span class="pre">K</span> <span class="pre">==</span> <span class="pre">len(s.shape)</span> <span class="pre">==</span> <span class="pre">s.ndim</span></code> - dimensionality of a tensor
is the sum of the number of sparse and dense dimensions,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">s.indices().shape</span> <span class="pre">==</span> <span class="pre">(M,</span> <span class="pre">nse)</span></code> - sparse indices are stored
explicitly,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">s.values().shape</span> <span class="pre">==</span> <span class="pre">(nse,)</span> <span class="pre">+</span> <span class="pre">s.shape[M</span> <span class="pre">:</span> <span class="pre">M</span> <span class="pre">+</span> <span class="pre">K]</span></code> - the values
of a hybrid tensor are K-dimensional tensors,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">s.values().layout</span> <span class="pre">==</span> <span class="pre">torch.strided</span></code> - values are stored as
strided tensors.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dense dimensions always follow sparse dimensions, that is, mixing
of dense and sparse dimensions is not supported.</p>
</div>
</div>
<div class="section" id="uncoalesced-sparse-coo-tensors">
<span id="sparse-uncoalesced-coo-docs"></span><h3>Uncoalesced sparse COO tensors<a class="headerlink" href="#uncoalesced-sparse-coo-tensors" title="Permalink to this headline">¶</a></h3>
<p>PyTorch sparse COO tensor format permits <em>uncoalesced</em> sparse tensors,
where there may be duplicate coordinates in the indices; in this case,
the interpretation is that the value at that index is the sum of all
duplicate value entries. For example, one can specify multiple values,
<code class="docutils literal notranslate"><span class="pre">3</span></code> and <code class="docutils literal notranslate"><span class="pre">4</span></code>, for the same index <code class="docutils literal notranslate"><span class="pre">1</span></code>, that leads to an 1-D
uncoalesced tensor:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[1, 1]]),</span>
<span class="go">       values=tensor(  [3, 4]),</span>
<span class="go">       size=(3,), nnz=2, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>while the coalescing process will accumulate the multi-valued elements
into a single value using summation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([7]),</span>
<span class="go">       size=(3,), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>In general, the output of <a class="reference internal" href="generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce" title="torch.Tensor.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.coalesce()</span></code></a> method is a
sparse tensor with the following properties:</p>
<ul class="simple">
<li><p>the indices of specified tensor elements are unique,</p></li>
<li><p>the indices are sorted in lexicographical order,</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced" title="torch.Tensor.is_coalesced"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.is_coalesced()</span></code></a> returns <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the most part, you shouldn’t have to care whether or not a
sparse tensor is coalesced or not, as most operations will work
identically given a coalesced or uncoalesced sparse tensor.</p>
<p>However, some operations can be implemented more efficiently on
uncoalesced tensors, and some on coalesced tensors.</p>
<p>For instance, addition of sparse COO tensors is implemented by
simply concatenating the indices and values tensors:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="go">tensor(indices=tensor([[0, 0, 1, 1]]),</span>
<span class="go">       values=tensor([7, 8, 5, 6]),</span>
<span class="go">       size=(2,), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>If you repeatedly perform an operation that can produce duplicate
entries (e.g., <a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.add()</span></code></a>), you should occasionally
coalesce your sparse tensors to prevent them from growing too large.</p>
<p>On the other hand, the lexicographical ordering of indices can be
advantageous for implementing algorithms that involve many element
selection operations, such as slicing or matrix products.</p>
</div>
</div>
<div class="section" id="working-with-sparse-coo-tensors">
<h3>Working with sparse COO tensors<a class="headerlink" href="#working-with-sparse-coo-tensors" title="Permalink to this headline">¶</a></h3>
<p>Let’s consider the following example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>As mentioned above, a sparse COO tensor is a <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>
instance and to distinguish it from the <cite>Tensor</cite> instances that use
some other layout, on can use <a class="reference internal" href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" title="torch.Tensor.is_sparse"><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor.is_sparse</span></code></a> or
<code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.Tensor.layout</span></code> properties:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">is_sparse</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The number of sparse and dense dimensions can be acquired using
methods <a class="reference internal" href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.sparse_dim()</span></code></a> and
<a class="reference internal" href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.dense_dim()</span></code></a>, respectively. For instance:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">(),</span> <span class="n">s</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span>
<span class="go">(2, 1)</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">s</span></code> is a sparse COO tensor then its COO format data can be
acquired using methods <a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.indices()</span></code></a> and
<a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.values()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, one can acquire the COO format data only when the tensor
instance is coalesced:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
<span class="go">RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first</span>
</pre></div>
</div>
<p>For acquiring the COO format data of an uncoalesced tensor, use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor._values()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor._indices()</span></code>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
<span class="go">tensor([[0, 1, 1],</span>
<span class="go">        [2, 0, 2]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor._values()</span></code> will return a <em>detached</em> tensor.
To track gradients, <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.coalesce().values()</span></code> must be
used instead.</p>
</div>
</div>
<p>Constructing a new sparse COO tensor results a tensor that is not
coalesced:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()</span>
<span class="go">False</span>
</pre></div>
</div>
<p>but one can construct a coalesced copy of a sparse COO tensor using
the <a class="reference internal" href="generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce" title="torch.Tensor.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.coalesce()</span></code></a> method:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
<span class="go">tensor([[0, 1, 1],</span>
<span class="go">       [2, 0, 2]])</span>
</pre></div>
</div>
<p>When working with uncoalesced sparse COO tensors, one must take into
an account the additive nature of uncoalesced data: the values of the
same indices are the terms of a sum that evaluation gives the value of
the corresponding tensor element. For example, the scalar
multiplication on an uncoalesced sparse tensor could be implemented by
multiplying all the uncoalesced values with the scalar because <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">*</span>
<span class="pre">(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">==</span> <span class="pre">c</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">c</span> <span class="pre">*</span> <span class="pre">b</span></code> holds. However, any nonlinear operation,
say, a square root, cannot be implemented by applying the operation to
uncoalesced data because <code class="docutils literal notranslate"><span class="pre">sqrt(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">==</span> <span class="pre">sqrt(a)</span> <span class="pre">+</span> <span class="pre">sqrt(b)</span></code> does not
hold in general.</p>
<p>Slicing (with positive step) of a sparse COO tensor is supported only
for dense dimensions. Indexing is supported for both sparse and dense
dimensions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor(indices=tensor([[0, 2]]),</span>
<span class="go">       values=tensor([[5, 6],</span>
<span class="go">                      [7, 8]]),</span>
<span class="go">       size=(3, 2), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="go">tensor([6])</span>
</pre></div>
</div>
<p>In PyTorch, the fill value of a sparse tensor cannot be specified
explicitly and is assumed to be zero in general. However, there exists
operations that may interpret the fill value differently. For
instance, <a class="reference internal" href="generated/torch.sparse.softmax.html#torch.sparse.softmax" title="torch.sparse.softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.softmax()</span></code></a> computes the softmax with the
assumption that the fill value is negative infinity.</p>
</div>
</div>
<div class="section" id="sparse-csr-tensor">
<span id="sparse-csr-docs"></span><h2>Sparse CSR Tensor<a class="headerlink" href="#sparse-csr-tensor" title="Permalink to this headline">¶</a></h2>
<p>The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format
for storage of 2 dimensional tensors. Although there is no support for N-dimensional
tensors, the primary advantage over the COO format is better use of storage and
much faster computation operations such as sparse matrix-vector multiplication
using MKL and MAGMA backends. CUDA support does not exist as of now.</p>
<p>A CSR sparse tensor consists of three 1-D tensors: <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code>, <code class="docutils literal notranslate"><span class="pre">col_indices</span></code>
and <code class="docutils literal notranslate"><span class="pre">values</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code> tensor consists of compressed row indices. This is a 1-D tensor
of size <code class="docutils literal notranslate"><span class="pre">size[0]</span> <span class="pre">+</span> <span class="pre">1</span></code>. The last element is the number of non-zeros. This tensor
encodes the index in <code class="docutils literal notranslate"><span class="pre">values</span></code> and <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> depending on where the given row
starts. Each successive number in the tensor subtracted by the number before it denotes
the number of elements in a given row.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> tensor contains the column indices of each value. This is a 1-D
tensor of size <code class="docutils literal notranslate"><span class="pre">nnz</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor  contains the values of the CSR tensor. This is a 1-D tensor
of size <code class="docutils literal notranslate"><span class="pre">nnz</span></code>.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The index tensors <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> should have element type either
<code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>. If you want to use MKL-enabled matrix
operations, use <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>. This is as a result of the default linking of pytorch
being with MKL LP64, which uses 32 bit integer indexing.</p>
</div>
<div class="section" id="construction-of-csr-tensors">
<h3>Construction of CSR tensors<a class="headerlink" href="#construction-of-csr-tensors" title="Permalink to this headline">¶</a></h3>
<p>Sparse CSR matrices can be directly constructed by using the <a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse_csr_tensor()</span></code></a>
method. The user must supply the row and column indices and values tensors separately.
The <code class="docutils literal notranslate"><span class="pre">size</span></code> argument is optional and will be deduced from the the <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code>
and <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> if it is not present.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crow_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">col_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr_tensor</span><span class="p">(</span><span class="n">crow_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 4]),</span>
<span class="go">      col_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,</span>
<span class="go">      dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[1., 2.],</span>
<span class="go">        [3., 4.]], dtype=torch.float64)</span>
</pre></div>
</div>
</div>
<div class="section" id="csr-tensor-operations">
<h3>CSR Tensor Operations<a class="headerlink" href="#csr-tensor-operations" title="Permalink to this headline">¶</a></h3>
<p>The simplest way of constructing a sparse CSR tensor from a strided or sparse COO
tensor is to use <code class="xref py py-meth docutils literal notranslate"><span class="pre">tensor.to_sparse_csr()</span></code>. Any zeros in the (strided) tensor will
be interpreted as missing values in the sparse tensor:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 3, 3]),</span>
<span class="go">      col_indices=tensor([2, 0, 1]),</span>
<span class="go">      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)</span>
</pre></div>
</div>
<p>The sparse matrix-vector multiplication can be performed with the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">tensor.matmul()</span></code> method. This is currently the only math operation
supported on CSR tensors.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="go">tensor([[0.9078],</span>
<span class="go">        [1.3180],</span>
<span class="go">        [0.0000]], dtype=torch.float64)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="supported-linear-algebra-operations">
<h2>Supported Linear Algebra operations<a class="headerlink" href="#supported-linear-algebra-operations" title="Permalink to this headline">¶</a></h2>
<p>The following table summarizes supported Linear Algebra operations on
sparse matrices where the operands layouts may vary. Here
<code class="docutils literal notranslate"><span class="pre">T[layout]</span></code> denotes a tensor with a given layout. Similarly,
<code class="docutils literal notranslate"><span class="pre">M[layout]</span></code> denotes a matrix (2-D PyTorch tensor), and <code class="docutils literal notranslate"><span class="pre">V[layout]</span></code>
denotes a vector (1-D PyTorch tensor). In addition, <code class="docutils literal notranslate"><span class="pre">f</span></code> denotes a
scalar (float or 0-D PyTorch tensor), <code class="docutils literal notranslate"><span class="pre">*</span></code> is element-wise
multiplication, and <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> is matrix multiplication.</p>
<table class="colwidths-given docutils colwidths-auto align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 6%" />
<col style="width: 71%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch operation</p></th>
<th class="head"><p>Sparse grad?</p></th>
<th class="head"><p>Layout signature</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">V[strided]</span> <span class="pre">-&gt;</span> <span class="pre">V[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_csr]</span> <span class="pre">&#64;</span> <span class="pre">V[strided]</span> <span class="pre">-&gt;</span> <span class="pre">V[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_csr]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.mm.html#torch.sparse.mm" title="torch.sparse.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.mm()</span></code></a></p></td>
<td><p>yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[sparse_coo]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.hspmm.html#torch.hspmm" title="torch.hspmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hspmm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[hybrid</span> <span class="pre">sparse_coo]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">T[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">T[strided]</span> <span class="pre">-&gt;</span> <span class="pre">T[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.addmm.html#torch.sparse.addmm" title="torch.sparse.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.addmm()</span></code></a></p></td>
<td><p>yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sspaddmm.html#torch.sspaddmm" title="torch.sspaddmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sspaddmm()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[sparse_coo]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">&#64;</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[sparse_coo]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lobpcg()</span></code></a></p></td>
<td><p>no</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GENEIG(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pca_lowrank()</span></code></a></p></td>
<td><p>yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">PCA(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd_lowrank()</span></code></a></p></td>
<td><p>yes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SVD(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
</tbody>
</table>
<p>where “Sparse grad?” column indicates if the PyTorch operation supports
backward with respect to sparse matrix argument. All PyTorch operations,
except <a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></a>, support backward with respect to strided
matrix arguments.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, PyTorch does not support matrix multiplication with the
layout signature <code class="docutils literal notranslate"><span class="pre">M[strided]</span> <span class="pre">&#64;</span> <span class="pre">M[sparse_coo]</span></code>. However,
applications can still compute this using the matrix relation <code class="docutils literal notranslate"><span class="pre">D</span> <span class="pre">&#64;</span>
<span class="pre">S</span> <span class="pre">==</span> <span class="pre">(S.t()</span> <span class="pre">&#64;</span> <span class="pre">D.t()).t()</span></code>.</p>
</div>
</div>
<div class="section" id="tensor-methods-and-sparse">
<h2>Tensor methods and sparse<a class="headerlink" href="#tensor-methods-and-sparse" title="Permalink to this headline">¶</a></h2>
<p>The following Tensor methods are related to sparse tensors:</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" title="torch.Tensor.is_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_sparse</span></code></a></p></td>
<td><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.dense_dim</span></code></a></p></td>
<td><p>Return the number of dense dimensions in a <a class="reference internal" href="#sparse-docs"><span class="std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_dim</span></code></a></p></td>
<td><p>Return the number of sparse dimensions in a <a class="reference internal" href="#sparse-docs"><span class="std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_mask</span></code></a></p></td>
<td><p>Returns a new <a class="reference internal" href="#sparse-docs"><span class="std std-ref">sparse tensor</span></a> with values from a strided tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> filtered by the indices of the sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" title="torch.Tensor.to_sparse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse</span></code></a></p></td>
<td><p>Returns a sparse copy of the tensor.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_coo</span></code></p></td>
<td><p>Convert a tensor to <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">coordinate format</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_sparse_csr</span></code></p></td>
<td><p>Convert a tensor to compressed row storage format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.indices</span></code></a></p></td>
<td><p>Return the indices tensor of a <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">sparse COO tensor</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.values</span></code></a></p></td>
<td><p>Return the values tensor of a <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">sparse COO tensor</span></a>.</p></td>
</tr>
</tbody>
</table>
<p>The following Tensor methods are specific to sparse COO tensors:</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce" title="torch.Tensor.coalesce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.coalesce</span></code></a></p></td>
<td><p>Returns a coalesced copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is an <a class="reference internal" href="#sparse-uncoalesced-coo-docs"><span class="std std-ref">uncoalesced tensor</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_" title="torch.Tensor.sparse_resize_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_resize_</span></code></a></p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> <a class="reference internal" href="#sparse-docs"><span class="std std-ref">sparse tensor</span></a> to the desired size and the number of sparse and dense dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_" title="torch.Tensor.sparse_resize_and_clear_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.sparse_resize_and_clear_</span></code></a></p></td>
<td><p>Removes all specified elements from a <a class="reference internal" href="#sparse-docs"><span class="std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> to the desired size and the number of sparse and dense dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced" title="torch.Tensor.is_coalesced"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.is_coalesced</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">sparse COO tensor</span></a> that is coalesced, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" title="torch.Tensor.to_dense"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.to_dense</span></code></a></p></td>
<td><p>Creates a strided copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>The following methods are specific to <a class="reference internal" href="#sparse-csr-docs"><span class="std std-ref">sparse CSR tensors</span></a>:</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.crow_indices</span></code></p></td>
<td><p>Returns the tensor containing the compressed row indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.col_indices</span></code></p></td>
<td><p>Returns the tensor containing the column indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>The following Tensor methods support sparse COO tensors:</p>
<p><a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.add_.html#torch.Tensor.add_" title="torch.Tensor.add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" title="torch.Tensor.addmm_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.any.html#torch.Tensor.any" title="torch.Tensor.any"><code class="xref py py-meth docutils literal notranslate"><span class="pre">any()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.asin.html#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.asin_.html#torch.Tensor.asin_" title="torch.Tensor.asin_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" title="torch.Tensor.arcsin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" title="torch.Tensor.arcsin_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.bmm.html#torch.Tensor.bmm" title="torch.Tensor.bmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clone()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" title="torch.Tensor.deg2rad"><code class="xref py py-meth docutils literal notranslate"><span class="pre">deg2rad()</span></code></a>
<code class="xref py py-meth docutils literal notranslate"><span class="pre">deg2rad_()</span></code>
<a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.detach_.html#torch.Tensor.detach_" title="torch.Tensor.detach_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.div.html#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.div_.html#torch.Tensor.div_" title="torch.Tensor.div_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" title="torch.Tensor.floor_divide"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" title="torch.Tensor.floor_divide_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device" title="torch.Tensor.get_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_device()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.index_select.html#torch.Tensor.index_select" title="torch.Tensor.index_select"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_select()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.isnan.html#torch.Tensor.isnan" title="torch.Tensor.isnan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">isnan()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" title="torch.Tensor.log1p_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mm.html#torch.Tensor.mm" title="torch.Tensor.mm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mul.html#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mul_.html#torch.Tensor.mul_" title="torch.Tensor.mul_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mv.html#torch.Tensor.mv" title="torch.Tensor.mv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mv()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" title="torch.Tensor.narrow_copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">narrow_copy()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.neg.html#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.neg_.html#torch.Tensor.neg_" title="torch.Tensor.neg_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.negative.html#torch.Tensor.negative" title="torch.Tensor.negative"><code class="xref py py-meth docutils literal notranslate"><span class="pre">negative()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.negative_.html#torch.Tensor.negative_" title="torch.Tensor.negative_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">negative_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.numel.html#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" title="torch.Tensor.rad2deg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rad2deg()</span></code></a>
<code class="xref py py-meth docutils literal notranslate"><span class="pre">rad2deg_()</span></code>
<a class="reference internal" href="generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" title="torch.Tensor.resize_as_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">resize_as_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.size.html#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-meth docutils literal notranslate"><span class="pre">size()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.pow.html#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.square.html#torch.Tensor.square" title="torch.Tensor.square"><code class="xref py py-meth docutils literal notranslate"><span class="pre">square()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.smm.html#torch.Tensor.smm" title="torch.Tensor.smm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">smm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" title="torch.Tensor.sspaddmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sspaddmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sub.html#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sub_.html#torch.Tensor.sub_" title="torch.Tensor.sub_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.t_.html#torch.Tensor.t_" title="torch.Tensor.t_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" title="torch.Tensor.transpose_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.zero_.html#torch.Tensor.zero_" title="torch.Tensor.zero_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">zero_()</span></code></a></p>
</div>
<div class="section" id="torch-functions-specific-to-sparse-tensors">
<h2>Torch functions specific to sparse Tensors<a class="headerlink" href="#torch-functions-specific-to-sparse-tensors" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.sparse_coo_tensor"/><a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_coo_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_csr_tensor"/><a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_csr_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="#sparse-csr-docs"><span class="std std-ref">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">col_indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.sum.html#torch.sparse.sum" title="torch.sparse.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.sum</span></code></a></p></td>
<td><p>Returns the sum of each row of the sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the given dimensions <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.addmm.html#torch.sparse.addmm" title="torch.sparse.addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.addmm</span></code></a></p></td>
<td><p>This function does exact same thing as <a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a> in the forward, except that it supports backward for sparse matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm" title="torch.sparse.sampled_addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.sampled_addmm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the dense matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> at the locations specified by the sparsity pattern of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.mm.html#torch.sparse.mm" title="torch.sparse.mm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.mm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the sparse matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and the (sparse or strided) matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sspaddmm"/><a class="reference internal" href="generated/torch.sspaddmm.html#torch.sspaddmm" title="torch.sspaddmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sspaddmm</span></code></a></p></td>
<td><p>Matrix multiplies a sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> with a dense tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>, then adds the sparse tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the result.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hspmm"/><a class="reference internal" href="generated/torch.hspmm.html#torch.hspmm" title="torch.hspmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hspmm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of a <a class="reference internal" href="#sparse-coo-docs"><span class="std std-ref">sparse COO matrix</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and a strided matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.smm"/><a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">smm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the sparse matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the dense matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.softmax.html#torch.sparse.softmax" title="torch.sparse.softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.softmax</span></code></a></p></td>
<td><p>Applies a softmax function.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax" title="torch.sparse.log_softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse.log_softmax</span></code></a></p></td>
<td><p>Applies a softmax function followed by logarithm.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="other-functions">
<h2>Other functions<a class="headerlink" href="#other-functions" title="Permalink to this headline">¶</a></h2>
<p>The following <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch</span></code> functions support sparse tensors:</p>
<p><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">cat()</span></code></a>
<a class="reference internal" href="generated/torch.dstack.html#torch.dstack" title="torch.dstack"><code class="xref py py-func docutils literal notranslate"><span class="pre">dstack()</span></code></a>
<a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal notranslate"><span class="pre">empty()</span></code></a>
<a class="reference internal" href="generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">empty_like()</span></code></a>
<a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-func docutils literal notranslate"><span class="pre">hstack()</span></code></a>
<a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">index_select()</span></code></a>
<a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_complex()</span></code></a>
<a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_floating_point()</span></code></a>
<a class="reference internal" href="generated/torch.is_nonzero.html#torch.is_nonzero" title="torch.is_nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_nonzero()</span></code></a>
<code class="xref py py-func docutils literal notranslate"><span class="pre">is_same_size()</span></code>
<code class="xref py py-func docutils literal notranslate"><span class="pre">is_signed()</span></code>
<a class="reference internal" href="generated/torch.is_tensor.html#torch.is_tensor" title="torch.is_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_tensor()</span></code></a>
<a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-func docutils literal notranslate"><span class="pre">lobpcg()</span></code></a>
<a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">mm()</span></code></a>
<code class="xref py py-func docutils literal notranslate"><span class="pre">native_norm()</span></code>
<a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-func docutils literal notranslate"><span class="pre">pca_lowrank()</span></code></a>
<a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">select()</span></code></a>
<a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-func docutils literal notranslate"><span class="pre">stack()</span></code></a>
<a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-func docutils literal notranslate"><span class="pre">svd_lowrank()</span></code></a>
<a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a>
<a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-func docutils literal notranslate"><span class="pre">vstack()</span></code></a>
<a class="reference internal" href="generated/torch.zeros.html#torch.zeros" title="torch.zeros"><code class="xref py py-func docutils literal notranslate"><span class="pre">zeros()</span></code></a>
<a class="reference internal" href="generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">zeros_like()</span></code></a></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torch.Tensor.coalesce.html" class="btn btn-neutral float-right" title="torch.Tensor.coalesce" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="random.html" class="btn btn-neutral" title="torch.random" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.sparse</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#sparse-coo-tensors">Sparse COO tensors</a><ul>
<li><a class="reference internal" href="#construction">Construction</a></li>
<li><a class="reference internal" href="#hybrid-sparse-coo-tensors">Hybrid sparse COO tensors</a></li>
<li><a class="reference internal" href="#uncoalesced-sparse-coo-tensors">Uncoalesced sparse COO tensors</a></li>
<li><a class="reference internal" href="#working-with-sparse-coo-tensors">Working with sparse COO tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-csr-tensor">Sparse CSR Tensor</a><ul>
<li><a class="reference internal" href="#construction-of-csr-tensors">Construction of CSR tensors</a></li>
<li><a class="reference internal" href="#csr-tensor-operations">CSR Tensor Operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#supported-linear-algebra-operations">Supported Linear Algebra operations</a></li>
<li><a class="reference internal" href="#tensor-methods-and-sparse">Tensor methods and sparse</a></li>
<li><a class="reference internal" href="#torch-functions-specific-to-sparse-tensors">Torch functions specific to sparse Tensors</a></li>
<li><a class="reference internal" href="#other-functions">Other functions</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
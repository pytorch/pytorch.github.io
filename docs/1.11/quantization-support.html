


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quantization API Reference &mdash; PyTorch 1.11.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/quantization-support.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="quantize" href="generated/torch.quantization.quantize.html" />
    <link rel="prev" title="Quantization" href="quantization.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.11.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="quantization.html">Quantization</a> &gt;</li>
        
      <li>Quantization API Reference</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/quantization-support.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="quantization-api-reference">
<h1>Quantization API Reference<a class="headerlink" href="#quantization-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="torch-quantization">
<h2>torch.quantization<a class="headerlink" href="#torch-quantization" title="Permalink to this headline">¶</a></h2>
<p>This module contains Eager mode quantization APIs.</p>
<div class="section" id="top-level-apis">
<h3>Top level APIs<a class="headerlink" href="#top-level-apis" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.quantize"/><a class="reference internal" href="generated/torch.quantization.quantize.html#torch.quantization.quantize" title="torch.quantization.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a></p></td>
<td><p>Quantize the input float model with post training static quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.quantize_dynamic"/><a class="reference internal" href="generated/torch.quantization.quantize_dynamic.html#torch.quantization.quantize_dynamic" title="torch.quantization.quantize_dynamic"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_dynamic</span></code></a></p></td>
<td><p>Converts a float model to dynamic (i.e.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.quantize_qat"/><a class="reference internal" href="generated/torch.quantization.quantize_qat.html#torch.quantization.quantize_qat" title="torch.quantization.quantize_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_qat</span></code></a></p></td>
<td><p>Do quantization aware training and output a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.prepare"/><a class="reference internal" href="generated/torch.quantization.prepare.html#torch.quantization.prepare" title="torch.quantization.prepare"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.prepare_qat"/><a class="reference internal" href="generated/torch.quantization.prepare_qat.html#torch.quantization.prepare_qat" title="torch.quantization.prepare_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.convert"/><a class="reference internal" href="generated/torch.quantization.convert.html#torch.quantization.convert" title="torch.quantization.convert"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert</span></code></a></p></td>
<td><p>Converts submodules in input module to a different module according to <cite>mapping</cite> by calling <cite>from_float</cite> method on the target module class.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="preparing-model-for-quantization">
<h3>Preparing model for quantization<a class="headerlink" href="#preparing-model-for-quantization" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.fuse_modules"/><a class="reference internal" href="generated/torch.quantization.fuse_modules.html#torch.quantization.fuse_modules" title="torch.quantization.fuse_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_modules</span></code></a></p></td>
<td><p>Fuses a list of modules into a single module</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.QuantStub"/><a class="reference internal" href="generated/torch.quantization.QuantStub.html#torch.quantization.QuantStub" title="torch.quantization.QuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantStub</span></code></a></p></td>
<td><p>Quantize stub module, before calibration, this is same as an observer, it will be swapped as <cite>nnq.Quantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.DeQuantStub"/><a class="reference internal" href="generated/torch.quantization.DeQuantStub.html#torch.quantization.DeQuantStub" title="torch.quantization.DeQuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DeQuantStub</span></code></a></p></td>
<td><p>Dequantize stub module, before calibration, this is same as identity, this will be swapped as <cite>nnq.DeQuantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.QuantWrapper"/><a class="reference internal" href="generated/torch.quantization.QuantWrapper.html#torch.quantization.QuantWrapper" title="torch.quantization.QuantWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantWrapper</span></code></a></p></td>
<td><p>A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.add_quant_dequant"/><a class="reference internal" href="generated/torch.quantization.add_quant_dequant.html#torch.quantization.add_quant_dequant" title="torch.quantization.add_quant_dequant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_quant_dequant</span></code></a></p></td>
<td><p>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="utility-functions">
<h3>Utility functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.add_observer_"/><a class="reference internal" href="generated/torch.quantization.add_observer_.html#torch.quantization.add_observer_" title="torch.quantization.add_observer_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_observer_</span></code></a></p></td>
<td><p>Add observer for the leaf child of the module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.swap_module"/><a class="reference internal" href="generated/torch.quantization.swap_module.html#torch.quantization.swap_module" title="torch.quantization.swap_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swap_module</span></code></a></p></td>
<td><p>Swaps the module if it has a quantized counterpart and it has an <cite>observer</cite> attached.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.propagate_qconfig_"/><a class="reference internal" href="generated/torch.quantization.propagate_qconfig_.html#torch.quantization.propagate_qconfig_" title="torch.quantization.propagate_qconfig_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">propagate_qconfig_</span></code></a></p></td>
<td><p>Propagate qconfig through the module hierarchy and assign <cite>qconfig</cite> attribute on each leaf module</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.default_eval_fn"/><a class="reference internal" href="generated/torch.quantization.default_eval_fn.html#torch.quantization.default_eval_fn" title="torch.quantization.default_eval_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_eval_fn</span></code></a></p></td>
<td><p>Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.get_observer_dict"/><a class="reference internal" href="generated/torch.quantization.get_observer_dict.html#torch.quantization.get_observer_dict" title="torch.quantization.get_observer_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_observer_dict</span></code></a></p></td>
<td><p>Traverse the modules and save all observers into dict.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="torch-quantization-quantize-fx">
<h2>torch.quantization.quantize_fx<a class="headerlink" href="#torch-quantization-quantize-fx" title="Permalink to this headline">¶</a></h2>
<p>This module contains FX graph mode quantization APIs (prototype).</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.quantize_fx.prepare_fx"/><a class="reference internal" href="generated/torch.quantization.quantize_fx.prepare_fx.html#torch.quantization.quantize_fx.prepare_fx" title="torch.quantization.quantize_fx.prepare_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_fx</span></code></a></p></td>
<td><p>Prepare a model for post training static quantization</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.quantize_fx.prepare_qat_fx"/><a class="reference internal" href="generated/torch.quantization.quantize_fx.prepare_qat_fx.html#torch.quantization.quantize_fx.prepare_qat_fx" title="torch.quantization.quantize_fx.prepare_qat_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat_fx</span></code></a></p></td>
<td><p>Prepare a model for quantization aware training</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.quantize_fx.convert_fx"/><a class="reference internal" href="generated/torch.quantization.quantize_fx.convert_fx.html#torch.quantization.quantize_fx.convert_fx" title="torch.quantization.quantize_fx.convert_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_fx</span></code></a></p></td>
<td><p>Convert a calibrated or trained model to a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.quantize_fx.fuse_fx"/><a class="reference internal" href="generated/torch.quantization.quantize_fx.fuse_fx.html#torch.quantization.quantize_fx.fuse_fx" title="torch.quantization.quantize_fx.fuse_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_fx</span></code></a></p></td>
<td><p>Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-quantization-related-functions">
<h2>torch (quantization related functions)<a class="headerlink" href="#torch-quantization-related-functions" title="Permalink to this headline">¶</a></h2>
<p>This describes the quantization related functions of the <cite>torch</cite> namespace.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantize_per_tensor"/><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantize_per_channel"/><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dequantize"/><a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-tensor-quantization-related-methods">
<h2>torch.Tensor (quantization related methods)<a class="headerlink" href="#torch-tensor-quantization-related-methods" title="Permalink to this headline">¶</a></h2>
<p>Quantized Tensors support a limited subset of data manipulation methods of the
regular full-precision tensor.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.Tensor.view"/><a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view</span></code></a></p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.as_strided"/><a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.expand"/><a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expand</span></code></a></p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.flatten"/><a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten" title="torch.Tensor.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.select"/><a class="reference internal" href="generated/torch.Tensor.select.html#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.ne"/><a class="reference internal" href="generated/torch.Tensor.ne.html#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.eq"/><a class="reference internal" href="generated/torch.Tensor.eq.html#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.ge"/><a class="reference internal" href="generated/torch.Tensor.ge.html#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.le"/><a class="reference internal" href="generated/torch.Tensor.le.html#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.gt"/><a class="reference internal" href="generated/torch.Tensor.gt.html#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.lt"/><a class="reference internal" href="generated/torch.Tensor.lt.html#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.copy_"/><a class="reference internal" href="generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy_</span></code></a></p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.clone"/><a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.dequantize"/><a class="reference internal" href="generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" title="torch.Tensor.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.equal"/><a class="reference internal" href="generated/torch.Tensor.equal.html#torch.Tensor.equal" title="torch.Tensor.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.int_repr"/><a class="reference internal" href="generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" title="torch.Tensor.int_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int_repr</span></code></a></p></td>
<td><p>Given a quantized Tensor, <code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.max"/><a class="reference internal" href="generated/torch.Tensor.max.html#torch.Tensor.max" title="torch.Tensor.max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.mean"/><a class="reference internal" href="generated/torch.Tensor.mean.html#torch.Tensor.mean" title="torch.Tensor.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.min"/><a class="reference internal" href="generated/torch.Tensor.min.html#torch.Tensor.min" title="torch.Tensor.min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_scale"/><a class="reference internal" href="generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" title="torch.Tensor.q_scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_scale</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.q_zero_point"/><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" title="torch.Tensor.q_zero_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_zero_point</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_per_channel_scales"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" title="torch.Tensor.q_per_channel_scales"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_scales</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.q_per_channel_zero_points"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" title="torch.Tensor.q_per_channel_zero_points"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_zero_points</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_per_channel_axis"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" title="torch.Tensor.q_per_channel_axis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_axis</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.resize_"/><a class="reference internal" href="generated/torch.Tensor.resize_.html#torch.Tensor.resize_" title="torch.Tensor.resize_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_</span></code></a></p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.sort"/><a class="reference internal" href="generated/torch.Tensor.sort.html#torch.Tensor.sort" title="torch.Tensor.sort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.topk"/><a class="reference internal" href="generated/torch.Tensor.topk.html#torch.Tensor.topk" title="torch.Tensor.topk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-quantization-observer">
<h2>torch.quantization.observer<a class="headerlink" href="#torch-quantization-observer" title="Permalink to this headline">¶</a></h2>
<p>This module contains observers which are used to collect statistics about
the values observed during calibration (PTQ) or training (QAT).</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.ObserverBase"/><a class="reference internal" href="generated/torch.quantization.observer.ObserverBase.html#torch.quantization.observer.ObserverBase" title="torch.quantization.observer.ObserverBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ObserverBase</span></code></a></p></td>
<td><p>Base observer Module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.MinMaxObserver"/><a class="reference internal" href="generated/torch.quantization.observer.MinMaxObserver.html#torch.quantization.observer.MinMaxObserver" title="torch.quantization.observer.MinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.MovingAverageMinMaxObserver"/><a class="reference internal" href="generated/torch.quantization.observer.MovingAverageMinMaxObserver.html#torch.quantization.observer.MovingAverageMinMaxObserver" title="torch.quantization.observer.MovingAverageMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAverageMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the moving average of the min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.PerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.quantization.observer.PerChannelMinMaxObserver.html#torch.quantization.observer.PerChannelMinMaxObserver" title="torch.quantization.observer.PerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.MovingAveragePerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.quantization.observer.MovingAveragePerChannelMinMaxObserver" title="torch.quantization.observer.MovingAveragePerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAveragePerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.HistogramObserver"/><a class="reference internal" href="generated/torch.quantization.observer.HistogramObserver.html#torch.quantization.observer.HistogramObserver" title="torch.quantization.observer.HistogramObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistogramObserver</span></code></a></p></td>
<td><p>The module records the running histogram of tensor values along with min/max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.PlaceholderObserver"/><a class="reference internal" href="generated/torch.quantization.observer.PlaceholderObserver.html#torch.quantization.observer.PlaceholderObserver" title="torch.quantization.observer.PlaceholderObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PlaceholderObserver</span></code></a></p></td>
<td><p>Observer that doesn’t do anything and just passes its configuration to the quantized module’s <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.RecordingObserver"/><a class="reference internal" href="generated/torch.quantization.observer.RecordingObserver.html#torch.quantization.observer.RecordingObserver" title="torch.quantization.observer.RecordingObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RecordingObserver</span></code></a></p></td>
<td><p>The module is mainly for debug and records the tensor values during runtime.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.NoopObserver"/><a class="reference internal" href="generated/torch.quantization.observer.NoopObserver.html#torch.quantization.observer.NoopObserver" title="torch.quantization.observer.NoopObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NoopObserver</span></code></a></p></td>
<td><p>Observer that doesn’t do anything and just passes its configuration to the quantized module’s <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.get_observer_state_dict"/><a class="reference internal" href="generated/torch.quantization.observer.get_observer_state_dict.html#torch.quantization.observer.get_observer_state_dict" title="torch.quantization.observer.get_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_observer_state_dict</span></code></a></p></td>
<td><p>Returns the state dict corresponding to the observer stats.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.load_observer_state_dict"/><a class="reference internal" href="generated/torch.quantization.observer.load_observer_state_dict.html#torch.quantization.observer.load_observer_state_dict" title="torch.quantization.observer.load_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_observer_state_dict</span></code></a></p></td>
<td><p>Given input model and a state_dict containing model observer stats, load the stats back into the model.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.default_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_observer.html#torch.quantization.observer.default_observer" title="torch.quantization.observer.default_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.default_placeholder_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_placeholder_observer.html#torch.quantization.observer.default_placeholder_observer" title="torch.quantization.observer.default_placeholder_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_placeholder_observer</span></code></a></p></td>
<td><p>Default placeholder observer, usually used for quantization to torch.float16.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.default_debug_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_debug_observer.html#torch.quantization.observer.default_debug_observer" title="torch.quantization.observer.default_debug_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_observer</span></code></a></p></td>
<td><p>Default debug-only observer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.default_weight_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_weight_observer.html#torch.quantization.observer.default_weight_observer" title="torch.quantization.observer.default_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.default_histogram_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_histogram_observer.html#torch.quantization.observer.default_histogram_observer" title="torch.quantization.observer.default_histogram_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.default_per_channel_weight_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_per_channel_weight_observer.html#torch.quantization.observer.default_per_channel_weight_observer" title="torch.quantization.observer.default_per_channel_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.observer.default_dynamic_quant_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_dynamic_quant_observer.html#torch.quantization.observer.default_dynamic_quant_observer" title="torch.quantization.observer.default_dynamic_quant_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_quant_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.observer.default_float_qparams_observer"/><a class="reference internal" href="generated/torch.quantization.observer.default_float_qparams_observer.html#torch.quantization.observer.default_float_qparams_observer" title="torch.quantization.observer.default_float_qparams_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_float_qparams_observer</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-quantization-fake-quantize">
<h2>torch.quantization.fake_quantize<a class="headerlink" href="#torch-quantization-fake-quantize" title="Permalink to this headline">¶</a></h2>
<p>This module implements modules which are used to perform fake quantization
during QAT.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.FakeQuantizeBase"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.FakeQuantizeBase.html#torch.quantization.fake_quantize.FakeQuantizeBase" title="torch.quantization.fake_quantize.FakeQuantizeBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantizeBase</span></code></a></p></td>
<td><p>Base fake quantize module Any fake quantize implementation should derive from this class.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.FakeQuantize"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.FakeQuantize.html#torch.quantization.fake_quantize.FakeQuantize" title="torch.quantization.fake_quantize.FakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantize</span></code></a></p></td>
<td><p>Simulate the quantize and dequantize operations in training time. The output of this module is given by::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.FixedQParamsFakeQuantize"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.quantization.fake_quantize.FixedQParamsFakeQuantize" title="torch.quantization.fake_quantize.FixedQParamsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FixedQParamsFakeQuantize</span></code></a></p></td>
<td><p>Simulate quantize and dequantize with fixed quantization parameters in training time.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize" title="torch.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FusedMovingAvgObsFakeQuantize</span></code></a></p></td>
<td><p>Fused module that is used to observe the input tensor (compute min/max), compute scale/zero_point and fake_quantize the tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.default_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_fake_quant.html#torch.quantization.fake_quantize.default_fake_quant" title="torch.quantization.fake_quantize.default_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.default_weight_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_weight_fake_quant.html#torch.quantization.fake_quantize.default_weight_fake_quant" title="torch.quantization.fake_quantize.default_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.default_per_channel_weight_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.quantization.fake_quantize.default_per_channel_weight_fake_quant" title="torch.quantization.fake_quantize.default_per_channel_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.default_histogram_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_histogram_fake_quant.html#torch.quantization.fake_quantize.default_histogram_fake_quant" title="torch.quantization.fake_quantize.default_histogram_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.default_fused_act_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.quantization.fake_quantize.default_fused_act_fake_quant" title="torch.quantization.fake_quantize.default_fused_act_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_act_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.default_fused_wt_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.quantization.fake_quantize.default_fused_wt_fake_quant" title="torch.quantization.fake_quantize.default_fused_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_wt_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant" title="torch.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_per_channel_wt_fake_quant</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.disable_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.disable_fake_quant.html#torch.quantization.fake_quantize.disable_fake_quant" title="torch.quantization.fake_quantize.disable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_fake_quant</span></code></a></p></td>
<td><p>Disable fake quantization for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.enable_fake_quant"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.enable_fake_quant.html#torch.quantization.fake_quantize.enable_fake_quant" title="torch.quantization.fake_quantize.enable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_fake_quant</span></code></a></p></td>
<td><p>Enable fake quantization for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.fake_quantize.disable_observer"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.disable_observer.html#torch.quantization.fake_quantize.disable_observer" title="torch.quantization.fake_quantize.disable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_observer</span></code></a></p></td>
<td><p>Disable observation for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.fake_quantize.enable_observer"/><a class="reference internal" href="generated/torch.quantization.fake_quantize.enable_observer.html#torch.quantization.fake_quantize.enable_observer" title="torch.quantization.fake_quantize.enable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_observer</span></code></a></p></td>
<td><p>Enable observation for this module, if applicable. Example usage::.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-quantization-qconfig">
<h2>torch.quantization.qconfig<a class="headerlink" href="#torch-quantization-qconfig" title="Permalink to this headline">¶</a></h2>
<p>This module defines <cite>QConfig</cite> objects which are used
to configure quantization settings for individual ops.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.QConfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.QConfig.html#torch.quantization.qconfig.QConfig" title="torch.quantization.qconfig.QConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QConfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.default_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_qconfig.html#torch.quantization.qconfig.default_qconfig" title="torch.quantization.qconfig.default_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.default_debug_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_debug_qconfig.html#torch.quantization.qconfig.default_debug_qconfig" title="torch.quantization.qconfig.default_debug_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.default_per_channel_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_per_channel_qconfig.html#torch.quantization.qconfig.default_per_channel_qconfig" title="torch.quantization.qconfig.default_per_channel_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.default_dynamic_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_dynamic_qconfig.html#torch.quantization.qconfig.default_dynamic_qconfig" title="torch.quantization.qconfig.default_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.float16_dynamic_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.float16_dynamic_qconfig.html#torch.quantization.qconfig.float16_dynamic_qconfig" title="torch.quantization.qconfig.float16_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_dynamic_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.float16_static_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.float16_static_qconfig.html#torch.quantization.qconfig.float16_static_qconfig" title="torch.quantization.qconfig.float16_static_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_static_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.per_channel_dynamic_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.quantization.qconfig.per_channel_dynamic_qconfig" title="torch.quantization.qconfig.per_channel_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">per_channel_dynamic_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.float_qparams_weight_only_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.quantization.qconfig.float_qparams_weight_only_qconfig" title="torch.quantization.qconfig.float_qparams_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_qparams_weight_only_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.default_qat_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_qat_qconfig.html#torch.quantization.qconfig.default_qat_qconfig" title="torch.quantization.qconfig.default_qat_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.default_weight_only_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_weight_only_qconfig.html#torch.quantization.qconfig.default_weight_only_qconfig" title="torch.quantization.qconfig.default_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_only_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantization.qconfig.default_activation_only_qconfig"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_activation_only_qconfig.html#torch.quantization.qconfig.default_activation_only_qconfig" title="torch.quantization.qconfig.default_activation_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_activation_only_qconfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantization.qconfig.default_qat_qconfig_v2"/><a class="reference internal" href="generated/torch.quantization.qconfig.default_qat_qconfig_v2.html#torch.quantization.qconfig.default_qat_qconfig_v2" title="torch.quantization.qconfig.default_qat_qconfig_v2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig_v2</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-intrinsic">
<h2>torch.nn.intrinsic<a class="headerlink" href="#torch-nn-intrinsic" title="Permalink to this headline">¶</a></h2>
<p>This module implements the combined (fused) modules conv + relu which can
then be quantized.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.ConvReLU1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvReLU1d.html#torch.nn.intrinsic.ConvReLU1d" title="torch.nn.intrinsic.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv1d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.ConvReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvReLU2d.html#torch.nn.intrinsic.ConvReLU2d" title="torch.nn.intrinsic.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv2d and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.ConvReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvReLU3d.html#torch.nn.intrinsic.ConvReLU3d" title="torch.nn.intrinsic.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv3d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.LinearReLU"/><a class="reference internal" href="generated/torch.nn.intrinsic.LinearReLU.html#torch.nn.intrinsic.LinearReLU" title="torch.nn.intrinsic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>This is a sequential container which calls the Linear and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.ConvBn1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBn1d.html#torch.nn.intrinsic.ConvBn1d" title="torch.nn.intrinsic.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.ConvBn2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBn2d.html#torch.nn.intrinsic.ConvBn2d" title="torch.nn.intrinsic.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.ConvBn3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBn3d.html#torch.nn.intrinsic.ConvBn3d" title="torch.nn.intrinsic.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBnReLU1d.html#torch.nn.intrinsic.ConvBnReLU1d" title="torch.nn.intrinsic.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBnReLU2d.html#torch.nn.intrinsic.ConvBnReLU2d" title="torch.nn.intrinsic.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.ConvBnReLU3d.html#torch.nn.intrinsic.ConvBnReLU3d" title="torch.nn.intrinsic.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.BNReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.BNReLU2d.html#torch.nn.intrinsic.BNReLU2d" title="torch.nn.intrinsic.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 2d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.BNReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.BNReLU3d.html#torch.nn.intrinsic.BNReLU3d" title="torch.nn.intrinsic.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 3d and ReLU modules.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-intrinsic-qat">
<h2>torch.nn.intrinsic.qat<a class="headerlink" href="#torch-nn-intrinsic-qat" title="Permalink to this headline">¶</a></h2>
<p>This module implements the versions of those fused operations needed for
quantization aware training.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.LinearReLU"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.LinearReLU.html#torch.nn.intrinsic.qat.LinearReLU" title="torch.nn.intrinsic.qat.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.qat.ConvBn1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBn1d.html#torch.nn.intrinsic.qat.ConvBn1d" title="torch.nn.intrinsic.qat.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBnReLU1d.html#torch.nn.intrinsic.qat.ConvBnReLU1d" title="torch.nn.intrinsic.qat.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.qat.ConvBn2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBn2d.html#torch.nn.intrinsic.qat.ConvBn2d" title="torch.nn.intrinsic.qat.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBnReLU2d.html#torch.nn.intrinsic.qat.ConvBnReLU2d" title="torch.nn.intrinsic.qat.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.qat.ConvReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvReLU2d.html#torch.nn.intrinsic.qat.ConvReLU2d" title="torch.nn.intrinsic.qat.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.ConvBn3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBn3d.html#torch.nn.intrinsic.qat.ConvBn3d" title="torch.nn.intrinsic.qat.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.qat.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvBnReLU3d.html#torch.nn.intrinsic.qat.ConvBnReLU3d" title="torch.nn.intrinsic.qat.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.ConvReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.ConvReLU3d.html#torch.nn.intrinsic.qat.ConvReLU3d" title="torch.nn.intrinsic.qat.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.qat.update_bn_stats"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.update_bn_stats.html#torch.nn.intrinsic.qat.update_bn_stats" title="torch.nn.intrinsic.qat.update_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.qat.freeze_bn_stats"/><a class="reference internal" href="generated/torch.nn.intrinsic.qat.freeze_bn_stats.html#torch.nn.intrinsic.qat.freeze_bn_stats" title="torch.nn.intrinsic.qat.freeze_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-intrinsic-quantized">
<h2>torch.nn.intrinsic.quantized<a class="headerlink" href="#torch-nn-intrinsic-quantized" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized implementations of fused operations
like conv + relu. No BatchNorm variants as it’s usually folded into convolution
for inference.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.quantized.BNReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.BNReLU2d.html#torch.nn.intrinsic.quantized.BNReLU2d" title="torch.nn.intrinsic.quantized.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>A BNReLU2d module is a fused module of BatchNorm2d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.quantized.BNReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.BNReLU3d.html#torch.nn.intrinsic.quantized.BNReLU3d" title="torch.nn.intrinsic.quantized.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>A BNReLU3d module is a fused module of BatchNorm3d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.quantized.ConvReLU1d"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.ConvReLU1d.html#torch.nn.intrinsic.quantized.ConvReLU1d" title="torch.nn.intrinsic.quantized.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>A ConvReLU1d module is a fused module of Conv1d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.quantized.ConvReLU2d"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.ConvReLU2d.html#torch.nn.intrinsic.quantized.ConvReLU2d" title="torch.nn.intrinsic.quantized.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.quantized.ConvReLU3d"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.ConvReLU3d.html#torch.nn.intrinsic.quantized.ConvReLU3d" title="torch.nn.intrinsic.quantized.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.intrinsic.quantized.LinearReLU"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.LinearReLU.html#torch.nn.intrinsic.quantized.LinearReLU" title="torch.nn.intrinsic.quantized.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-intrinsic-quantized-dynamic">
<h2>torch.nn.intrinsic.quantized.dynamic<a class="headerlink" href="#torch-nn-intrinsic-quantized-dynamic" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized dynamic implementations of fused operations
like linear + relu.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.intrinsic.quantized.dynamic.LinearReLU"/><a class="reference internal" href="generated/torch.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.nn.intrinsic.quantized.dynamic.LinearReLU" title="torch.nn.intrinsic.quantized.dynamic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-qat">
<h2>torch.nn.qat<a class="headerlink" href="#torch-nn-qat" title="Permalink to this headline">¶</a></h2>
<p>This module implements versions of the key nn modules <strong>Conv2d()</strong> and
<strong>Linear()</strong> which run in FP32 but with rounding applied to simulate the
effect of INT8 quantization.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.qat.Conv2d"/><a class="reference internal" href="generated/torch.nn.qat.Conv2d.html#torch.nn.qat.Conv2d" title="torch.nn.qat.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.qat.Conv3d"/><a class="reference internal" href="generated/torch.nn.qat.Conv3d.html#torch.nn.qat.Conv3d" title="torch.nn.qat.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.qat.Linear"/><a class="reference internal" href="generated/torch.nn.qat.Linear.html#torch.nn.qat.Linear" title="torch.nn.qat.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-qat-dynamic">
<h2>torch.nn.qat.dynamic<a class="headerlink" href="#torch-nn-qat-dynamic" title="Permalink to this headline">¶</a></h2>
<p>This module implements versions of the key nn modules such as <strong>Linear()</strong>
which run in FP32 but with rounding applied to simulate the effect of INT8
quantization and will be dynamically quantized during inference.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.qat.dynamic.Linear"/><a class="reference internal" href="generated/torch.nn.qat.dynamic.Linear.html#torch.nn.qat.dynamic.Linear" title="torch.nn.qat.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-quantized">
<h2>torch.nn.quantized<a class="headerlink" href="#torch-nn-quantized" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized versions of the nn layers such as
~`torch.nn.Conv2d` and <cite>torch.nn.ReLU</cite>.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.ReLU6"/><a class="reference internal" href="generated/torch.nn.quantized.ReLU6.html#torch.nn.quantized.ReLU6" title="torch.nn.quantized.ReLU6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ReLU6</span></code></a></p></td>
<td><p>Applies the element-wise function:</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.Hardswish"/><a class="reference internal" href="generated/torch.nn.quantized.Hardswish.html#torch.nn.quantized.Hardswish" title="torch.nn.quantized.Hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish" title="torch.nn.Hardswish"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardswish</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.ELU"/><a class="reference internal" href="generated/torch.nn.quantized.ELU.html#torch.nn.quantized.ELU" title="torch.nn.quantized.ELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ELU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.ELU.html#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.LeakyReLU"/><a class="reference internal" href="generated/torch.nn.quantized.LeakyReLU.html#torch.nn.quantized.LeakyReLU" title="torch.nn.quantized.LeakyReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.Sigmoid"/><a class="reference internal" href="generated/torch.nn.quantized.Sigmoid.html#torch.nn.quantized.Sigmoid" title="torch.nn.quantized.Sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Sigmoid</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.BatchNorm2d"/><a class="reference internal" href="generated/torch.nn.quantized.BatchNorm2d.html#torch.nn.quantized.BatchNorm2d" title="torch.nn.quantized.BatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.BatchNorm3d"/><a class="reference internal" href="generated/torch.nn.quantized.BatchNorm3d.html#torch.nn.quantized.BatchNorm3d" title="torch.nn.quantized.BatchNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.Conv1d"/><a class="reference internal" href="generated/torch.nn.quantized.Conv1d.html#torch.nn.quantized.Conv1d" title="torch.nn.quantized.Conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.Conv2d"/><a class="reference internal" href="generated/torch.nn.quantized.Conv2d.html#torch.nn.quantized.Conv2d" title="torch.nn.quantized.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.Conv3d"/><a class="reference internal" href="generated/torch.nn.quantized.Conv3d.html#torch.nn.quantized.Conv3d" title="torch.nn.quantized.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.ConvTranspose1d"/><a class="reference internal" href="generated/torch.nn.quantized.ConvTranspose1d.html#torch.nn.quantized.ConvTranspose1d" title="torch.nn.quantized.ConvTranspose1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a></p></td>
<td><p>Applies a 1D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.ConvTranspose2d"/><a class="reference internal" href="generated/torch.nn.quantized.ConvTranspose2d.html#torch.nn.quantized.ConvTranspose2d" title="torch.nn.quantized.ConvTranspose2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a></p></td>
<td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.ConvTranspose3d"/><a class="reference internal" href="generated/torch.nn.quantized.ConvTranspose3d.html#torch.nn.quantized.ConvTranspose3d" title="torch.nn.quantized.ConvTranspose3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a></p></td>
<td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.Embedding"/><a class="reference internal" href="generated/torch.nn.quantized.Embedding.html#torch.nn.quantized.Embedding" title="torch.nn.quantized.Embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Embedding</span></code></a></p></td>
<td><p>A quantized Embedding module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.EmbeddingBag"/><a class="reference internal" href="generated/torch.nn.quantized.EmbeddingBag.html#torch.nn.quantized.EmbeddingBag" title="torch.nn.quantized.EmbeddingBag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a></p></td>
<td><p>A quantized EmbeddingBag module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.FloatFunctional"/><a class="reference internal" href="generated/torch.nn.quantized.FloatFunctional.html#torch.nn.quantized.FloatFunctional" title="torch.nn.quantized.FloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FloatFunctional</span></code></a></p></td>
<td><p>State collector class for float operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.FXFloatFunctional"/><a class="reference internal" href="generated/torch.nn.quantized.FXFloatFunctional.html#torch.nn.quantized.FXFloatFunctional" title="torch.nn.quantized.FXFloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FXFloatFunctional</span></code></a></p></td>
<td><p>module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.QFunctional"/><a class="reference internal" href="generated/torch.nn.quantized.QFunctional.html#torch.nn.quantized.QFunctional" title="torch.nn.quantized.QFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QFunctional</span></code></a></p></td>
<td><p>Wrapper class for quantized operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.Linear"/><a class="reference internal" href="generated/torch.nn.quantized.Linear.html#torch.nn.quantized.Linear" title="torch.nn.quantized.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A quantized linear module with quantized tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.LayerNorm"/><a class="reference internal" href="generated/torch.nn.quantized.LayerNorm.html#torch.nn.quantized.LayerNorm" title="torch.nn.quantized.LayerNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.GroupNorm"/><a class="reference internal" href="generated/torch.nn.quantized.GroupNorm.html#torch.nn.quantized.GroupNorm" title="torch.nn.quantized.GroupNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm" title="torch.nn.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.InstanceNorm1d"/><a class="reference internal" href="generated/torch.nn.quantized.InstanceNorm1d.html#torch.nn.quantized.InstanceNorm1d" title="torch.nn.quantized.InstanceNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.InstanceNorm2d"/><a class="reference internal" href="generated/torch.nn.quantized.InstanceNorm2d.html#torch.nn.quantized.InstanceNorm2d" title="torch.nn.quantized.InstanceNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.InstanceNorm3d"/><a class="reference internal" href="generated/torch.nn.quantized.InstanceNorm3d.html#torch.nn.quantized.InstanceNorm3d" title="torch.nn.quantized.InstanceNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-quantized-functional">
<h2>torch.nn.quantized.functional<a class="headerlink" href="#torch-nn-quantized-functional" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized versions of the functional layers such as
~`torch.nn.functional.conv2d` and <cite>torch.nn.functional.relu</cite>. Note:
<a class="reference internal" href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">relu()</span></code></a> supports quantized inputs.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.avg_pool2d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.avg_pool2d.html#torch.nn.quantized.functional.avg_pool2d" title="torch.nn.quantized.functional.avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool2d</span></code></a></p></td>
<td><p>Applies 2D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.avg_pool3d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.avg_pool3d.html#torch.nn.quantized.functional.avg_pool3d" title="torch.nn.quantized.functional.avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool3d</span></code></a></p></td>
<td><p>Applies 3D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>D</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kD \ times kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace"> </span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">es</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>D</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sD \times sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">sD</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.adaptive_avg_pool2d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.adaptive_avg_pool2d.html#torch.nn.quantized.functional.adaptive_avg_pool2d" title="torch.nn.quantized.functional.adaptive_avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool2d</span></code></a></p></td>
<td><p>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.adaptive_avg_pool3d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.adaptive_avg_pool3d.html#torch.nn.quantized.functional.adaptive_avg_pool3d" title="torch.nn.quantized.functional.adaptive_avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool3d</span></code></a></p></td>
<td><p>Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.conv1d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.conv1d.html#torch.nn.quantized.functional.conv1d" title="torch.nn.quantized.functional.conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized 1D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.conv2d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.conv2d.html#torch.nn.quantized.functional.conv2d" title="torch.nn.quantized.functional.conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized 2D input composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.conv3d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.conv3d.html#torch.nn.quantized.functional.conv3d" title="torch.nn.quantized.functional.conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized 3D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.interpolate"/><a class="reference internal" href="generated/torch.nn.quantized.functional.interpolate.html#torch.nn.quantized.functional.interpolate" title="torch.nn.quantized.functional.interpolate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">interpolate</span></code></a></p></td>
<td><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.linear"/><a class="reference internal" href="generated/torch.nn.quantized.functional.linear.html#torch.nn.quantized.functional.linear" title="torch.nn.quantized.functional.linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linear</span></code></a></p></td>
<td><p>Applies a linear transformation to the incoming quantized data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9247em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.max_pool1d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.max_pool1d.html#torch.nn.quantized.functional.max_pool1d" title="torch.nn.quantized.functional.max_pool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.max_pool2d"/><a class="reference internal" href="generated/torch.nn.quantized.functional.max_pool2d.html#torch.nn.quantized.functional.max_pool2d" title="torch.nn.quantized.functional.max_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.celu"/><a class="reference internal" href="generated/torch.nn.quantized.functional.celu.html#torch.nn.quantized.functional.celu" title="torch.nn.quantized.functional.celu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">celu</span></code></a></p></td>
<td><p>Applies the quantized CELU function element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.leaky_relu"/><a class="reference internal" href="generated/torch.nn.quantized.functional.leaky_relu.html#torch.nn.quantized.functional.leaky_relu" title="torch.nn.quantized.functional.leaky_relu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">leaky_relu</span></code></a></p></td>
<td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LeakyReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">negative_slope</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.hardtanh"/><a class="reference internal" href="generated/torch.nn.quantized.functional.hardtanh.html#torch.nn.quantized.functional.hardtanh" title="torch.nn.quantized.functional.hardtanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardtanh</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.hardswish"/><a class="reference internal" href="generated/torch.nn.quantized.functional.hardswish.html#torch.nn.quantized.functional.hardswish" title="torch.nn.quantized.functional.hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardswish()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.threshold"/><a class="reference internal" href="generated/torch.nn.quantized.functional.threshold.html#torch.nn.quantized.functional.threshold" title="torch.nn.quantized.functional.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Applies the quantized version of the threshold function element-wise:</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.elu"/><a class="reference internal" href="generated/torch.nn.quantized.functional.elu.html#torch.nn.quantized.functional.elu" title="torch.nn.quantized.functional.elu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elu</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.hardsigmoid"/><a class="reference internal" href="generated/torch.nn.quantized.functional.hardsigmoid.html#torch.nn.quantized.functional.hardsigmoid" title="torch.nn.quantized.functional.hardsigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardsigmoid</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardsigmoid()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.clamp"/><a class="reference internal" href="generated/torch.nn.quantized.functional.clamp.html#torch.nn.quantized.functional.clamp" title="torch.nn.quantized.functional.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>float(input, min_, max_) -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.upsample"/><a class="reference internal" href="generated/torch.nn.quantized.functional.upsample.html#torch.nn.quantized.functional.upsample" title="torch.nn.quantized.functional.upsample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample</span></code></a></p></td>
<td><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.functional.upsample_bilinear"/><a class="reference internal" href="generated/torch.nn.quantized.functional.upsample_bilinear.html#torch.nn.quantized.functional.upsample_bilinear" title="torch.nn.quantized.functional.upsample_bilinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_bilinear</span></code></a></p></td>
<td><p>Upsamples the input, using bilinear upsampling.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.functional.upsample_nearest"/><a class="reference internal" href="generated/torch.nn.quantized.functional.upsample_nearest.html#torch.nn.quantized.functional.upsample_nearest" title="torch.nn.quantized.functional.upsample_nearest"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_nearest</span></code></a></p></td>
<td><p>Upsamples the input, using nearest neighbours’ pixel values.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-nn-quantized-dynamic">
<h2>torch.nn.quantized.dynamic<a class="headerlink" href="#torch-nn-quantized-dynamic" title="Permalink to this headline">¶</a></h2>
<p>Dynamically quantized <a class="reference internal" href="generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></a>, <a class="reference internal" href="generated/torch.nn.LSTM.html#torch.nn.LSTM" title="torch.nn.LSTM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code></a>,
<a class="reference internal" href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell" title="torch.nn.LSTMCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>, <a class="reference internal" href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell" title="torch.nn.GRUCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">GRUCell</span></code></a>, and
<a class="reference internal" href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell" title="torch.nn.RNNCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">RNNCell</span></code></a>.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.dynamic.Linear"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.Linear.html#torch.nn.quantized.dynamic.Linear" title="torch.nn.quantized.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A dynamic quantized linear module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.dynamic.LSTM"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.LSTM.html#torch.nn.quantized.dynamic.LSTM" title="torch.nn.quantized.dynamic.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A dynamic quantized LSTM module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.dynamic.GRU"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.GRU.html#torch.nn.quantized.dynamic.GRU" title="torch.nn.quantized.dynamic.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRU</span></code></a></p></td>
<td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.dynamic.RNNCell"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.RNNCell.html#torch.nn.quantized.dynamic.RNNCell" title="torch.nn.quantized.dynamic.RNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RNNCell</span></code></a></p></td>
<td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nn.quantized.dynamic.LSTMCell"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.LSTMCell.html#torch.nn.quantized.dynamic.LSTMCell" title="torch.nn.quantized.dynamic.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTMCell</span></code></a></p></td>
<td><p>A long short-term memory (LSTM) cell.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nn.quantized.dynamic.GRUCell"/><a class="reference internal" href="generated/torch.nn.quantized.dynamic.GRUCell.html#torch.nn.quantized.dynamic.GRUCell" title="torch.nn.quantized.dynamic.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRUCell</span></code></a></p></td>
<td><p>A gated recurrent unit (GRU) cell</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="quantized-dtypes-and-quantization-schemes">
<h2>Quantized dtypes and quantization schemes<a class="headerlink" href="#quantized-dtypes-and-quantization-schemes" title="Permalink to this headline">¶</a></h2>
<p>Note that operator implementations currently only
support per channel quantization for weights of the <strong>conv</strong> and <strong>linear</strong>
operators. Furthermore, the input data is
mapped linearly to the the quantized data and vice versa
as follows:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Quantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>out</mtext></msub><mo>=</mo><mtext>clamp</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>input</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo>+</mo><mi>z</mi><mo separator="true">,</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Dequantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>x</mi><mtext>out</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>Q</mi><mtext>input</mtext></msub><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>s</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{Quantization:}&amp;\\
    &amp;Q_\text{out} = \text{clamp}(x_\text{input}/s+z, Q_\text{min}, Q_\text{max})\\
    \text{Dequantization:}&amp;\\
    &amp;x_\text{out} = (Q_\text{input}-z)*s
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6em;vertical-align:-2.75em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Quantization:</span></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Dequantization:</span></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>clamp</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{clamp}(.)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord">.</span><span class="mclose">)</span></span></span></span></span> is the same as <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">clamp()</span></code></a> while the
scale <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and zero point <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> are then computed
as decribed in <code class="xref py py-class docutils literal notranslate"><span class="pre">MinMaxObserver</span></code>, specifically:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>if Symmetric:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mn>2</mn><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">∣</mi><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>if dtype is qint8</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>128</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Otherwise:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{if Symmetric:}&amp;\\
    &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) /
        \left( Q_\text{max} - Q_\text{min} \right) \\
    &amp;z = \begin{cases}
        0 &amp; \text{if dtype is qint8} \\
        128 &amp; \text{otherwise}
    \end{cases}\\
    \text{Otherwise:}&amp;\\
        &amp;s = \left( x_\text{max} - x_\text{min}  \right ) /
            \left( Q_\text{max} - Q_\text{min} \right ) \\
        &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:10.8em;vertical-align:-5.15em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">if Symmetric:</span></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">Otherwise:</span></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">128</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if dtype is qint8</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">otherwise</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_\text{min}, x_\text{max}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> denotes the range of the input data while
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>min</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are respectively the minimum and maximum values of the quantized dtype.</p>
<p>Note that the choice of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> implies that zero is represented with no quantization error whenever zero is within
the range of the input data or symmetric quantization is being used.</p>
<p>Additional data types and quantization schemes can be implemented through
the <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">custom operator mechanism</a>.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qscheme</span></code> — Type to describe the quantization scheme of a tensor.
Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code> — per tensor, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_affine</span></code> — per channel, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_symmetric</span></code> — per tensor, symmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_symmetric</span></code> — per channel, symmetric</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> — Type to describe the data. Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.quint8</span></code> — 8-bit unsigned integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint8</span></code> — 8-bit signed integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint32</span></code> — 32-bit signed integer</p></li>
</ul>
</li>
</ul>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torch.quantization.quantize.html" class="btn btn-neutral float-right" title="quantize" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="quantization.html" class="btn btn-neutral" title="Quantization" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Quantization API Reference</a><ul>
<li><a class="reference internal" href="#torch-quantization">torch.quantization</a><ul>
<li><a class="reference internal" href="#top-level-apis">Top level APIs</a></li>
<li><a class="reference internal" href="#preparing-model-for-quantization">Preparing model for quantization</a></li>
<li><a class="reference internal" href="#utility-functions">Utility functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch-quantization-quantize-fx">torch.quantization.quantize_fx</a></li>
<li><a class="reference internal" href="#torch-quantization-related-functions">torch (quantization related functions)</a></li>
<li><a class="reference internal" href="#torch-tensor-quantization-related-methods">torch.Tensor (quantization related methods)</a></li>
<li><a class="reference internal" href="#torch-quantization-observer">torch.quantization.observer</a></li>
<li><a class="reference internal" href="#torch-quantization-fake-quantize">torch.quantization.fake_quantize</a></li>
<li><a class="reference internal" href="#torch-quantization-qconfig">torch.quantization.qconfig</a></li>
<li><a class="reference internal" href="#torch-nn-intrinsic">torch.nn.intrinsic</a></li>
<li><a class="reference internal" href="#torch-nn-intrinsic-qat">torch.nn.intrinsic.qat</a></li>
<li><a class="reference internal" href="#torch-nn-intrinsic-quantized">torch.nn.intrinsic.quantized</a></li>
<li><a class="reference internal" href="#torch-nn-intrinsic-quantized-dynamic">torch.nn.intrinsic.quantized.dynamic</a></li>
<li><a class="reference internal" href="#torch-nn-qat">torch.nn.qat</a></li>
<li><a class="reference internal" href="#torch-nn-qat-dynamic">torch.nn.qat.dynamic</a></li>
<li><a class="reference internal" href="#torch-nn-quantized">torch.nn.quantized</a></li>
<li><a class="reference internal" href="#torch-nn-quantized-functional">torch.nn.quantized.functional</a></li>
<li><a class="reference internal" href="#torch-nn-quantized-dynamic">torch.nn.quantized.dynamic</a></li>
<li><a class="reference internal" href="#quantized-dtypes-and-quantization-schemes">Quantized dtypes and quantization schemes</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
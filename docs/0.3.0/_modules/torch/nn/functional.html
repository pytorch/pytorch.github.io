

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  
  
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="../../../_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="PyTorch master documentation" href="../../../index.html"/>
        <link rel="up" title="torch" href="../../torch.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.3.0 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id14"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id15"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id16"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id17"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id18"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id19"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id20"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id21"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id22"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id23"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id24"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id25"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id26"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id27"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id28"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id29"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id30"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id31"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id32"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id33"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id34"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id35"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id36"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id37">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id38">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id40"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#functions">Functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../torch.html">torch</a> &raquo;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">mul</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">reduce</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="k">import</span> <span class="n">_infer_size</span><span class="p">,</span> <span class="n">_add_docstr</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_functions</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="k">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">._functions.linear</span> <span class="k">import</span> <span class="n">Bilinear</span>
<span class="kn">from</span> <span class="nn">._functions.padding</span> <span class="k">import</span> <span class="n">ConstantPadNd</span>
<span class="kn">from</span> <span class="nn">._functions.vision</span> <span class="k">import</span> <span class="n">GridSampler</span><span class="p">,</span> <span class="n">AffineGridGenerator</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span>

<span class="c1"># Convolutions</span>
<span class="n">_ConvNd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">ConvNd</span>


<div class="viewcode-block" id="conv1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv1d">[docs]</a><span class="k">def</span> <span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iW)</span>
<span class="sd">        weight: filters of shape (out_channels x in_channels x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="sd">          a tuple (sW,). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padW,). Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dW,). Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups. Default: 1</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50))</span>
<span class="sd">        &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv2d">[docs]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D convolution over an input image composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters tensor (out_channels x in_channels/groups x kH x kW)</span>
<span class="sd">        bias: optional bias tensor (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="sd">          tuple (sH, sW). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padH, padW). Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dH, dW). Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by the</span>
<span class="sd">          number of groups. Default: 1</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(8,4,3,3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(1,4,5,5))</span>
<span class="sd">        &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 4D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv3d">[docs]</a><span class="k">def</span> <span class="nf">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D convolution over an input image composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters tensor of shape (out_channels x in_channels x kT x kH x kW)</span>
<span class="sd">        bias: optional bias tensor of shape (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="sd">          tuple (sT, sH, sW). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padT, padH, padW). Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dT, dH, dW). Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups. Default: 1</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50, 10, 20))</span>
<span class="sd">        &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 5D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose1d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input signal</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="sd">          tuple (sW,). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padW,). Default: 0</span>
<span class="sd">        output_padding: implicit zero-paddings of 0 &lt;= padding &lt; stride on both</span>
<span class="sd">          sides of the output. Can be a single number or a tuple (out_padW,).</span>
<span class="sd">          Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by the</span>
<span class="sd">          number of groups. Default: 1</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dW,). Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span>
                <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose2d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="sd">          tuple (sH, sW). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padH, padW). Default: 0</span>
<span class="sd">        output_padding: implicit zero-paddings of 0 &lt;= padding &lt; stride on both</span>
<span class="sd">          sides of the output. Can be a single number or a tuple</span>
<span class="sd">          (out_padH, out_padW). Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by the</span>
<span class="sd">          number of groups. Default: 1</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dH, dW). Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 4D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose3d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels). Default: None</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="sd">          tuple (sT, sH, sW). Default: 1</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padT, padH, padW). Default: 0</span>
<span class="sd">        output_padding: implicit zero-paddings of 0 &lt;= padding &lt; stride on both</span>
<span class="sd">          sides of the output. Can be a single number or a tuple</span>
<span class="sd">          (out_padT, out_padH, out_padW). Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by the</span>
<span class="sd">          number of groups. Default: 1</span>
<span class="sd">        dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="sd">          a tuple (dT, dH, dW). Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 5D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">_ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># Pooling</span>
<div class="viewcode-block" id="avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D average pooling over an input signal composed of several</span>
<span class="sd">    input planes.</span>

<span class="sd">    See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iW)</span>
<span class="sd">        kernel_size: the size of the window. Can be a single number or a</span>
<span class="sd">          tuple (kW,)</span>
<span class="sd">        stride: the stride of the window. Can be a single number or a tuple</span>
<span class="sd">          (sW,). Default: :attr:`kernel_size`</span>
<span class="sd">        padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="sd">          single number or a tuple (padW,). Default: 0</span>
<span class="sd">        ceil_mode: when True, will use `ceil` instead of `floor` to compute the</span>
<span class="sd">            output shape. Default: ``False``</span>
<span class="sd">        count_include_pad: when True, will include the zero-padding in the</span>
<span class="sd">            averaging calculation. Default: ``True``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.Tensor([[[1,2,3,4,5,6,7]]]))</span>
<span class="sd">        &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">          2  4  6</span>
<span class="sd">        [torch.FloatTensor of size 1x1x3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;expected 3D input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                      <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<span class="n">avg_pool2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Variable</span>

<span class="s2">Applies 2D average-pooling operation in kh x kw regions by step size</span>
<span class="s2">dh x dw steps. The number of output features is equal to the number of</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple (kH x kW)</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple (sH, sW). Default is equal to kernel size</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple (padH, padW). Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in th</span>
<span class="s2">        averaging calculation. Default: ``True``</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">avg_pool3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Variable</span>

<span class="s2">Applies 3D average-pooling operation in kt x kh x kw regions by step</span>
<span class="s2">size dt x dh x dw steps. The number of output features is equal to the</span>
<span class="s2">number of input planes / dt.</span>

<span class="s2">See :class:`~torch.nn.AvgPool3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor (minibatch x in_channels x iT x iH x iW)</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple (kT x kH x kW)</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple (sT, sH, sW). Default is equal to kernel size</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple (padT, padH, padW), Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in th</span>
<span class="s2">        averaging calculation</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="c1"># share the same interface</span>
<div class="viewcode-block" id="max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 1D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                          <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 3D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                          <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_size</span>

    <span class="n">output_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                         <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output_size</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool1d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool2d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool3d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.LPPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 1D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.LPPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="adaptive_max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="adaptive_avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>

<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">Dropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="alpha_dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.alpha_dropout">[docs]</a><span class="k">def</span> <span class="nf">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies alpha dropout to the input.</span>

<span class="sd">    See :class:`~torch.nn.AlphaDropout` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float, optional): the drop probability. Default: 0.5</span>
<span class="sd">        training (bool, optional): switch between training and evaluation mode. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.7580993408473766</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>
    <span class="c1"># TODO avoid casting to byte after resize</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">bernoulli_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">byte</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">keep_prob</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">keep_prob</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">keep_prob</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="n">threshold</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">threshold(input, threshold, value, inplace=False) -&gt; Variable</span>

<span class="s2">Thresholds each element of the input Tensor.</span>

<span class="s2">See :class:`~torch.nn.Threshold` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;relu(input, threshold, value, inplace=False) -&gt; Variable</span>

<span class="sd">    Applies the rectified linear unit function element-wise. See</span>
<span class="sd">    :class:`~torch.nn.ReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="n">glu</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">glu(input, dim=-1) -&gt; Variable</span>

<span class="s2">The gated linear unit. Computes:</span>

<span class="s2">.. math ::</span>

<span class="s2">    H = A \times \sigma(B)</span>

<span class="s2">where `input` is split in half along `dim` to form `A` and `B`.</span>

<span class="s2">See `Language Modeling with Gated Convolutional Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="s2">Args:</span>
<span class="s2">    input (Variable): input variable</span>
<span class="s2">    dim (int): dimension on which to split the input</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">hardtanh</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">hardtanh(input, min_val=-1., max_val=1., inplace=False) -&gt; Variable</span>

<span class="s2">Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more</span>
<span class="s2">details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu6(input, inplace=False) -&gt; Variable</span>

<span class="sd">    Applies the element-wise function :math:`{ReLU6}(x) = min(max(0,x), 6)`.</span>

<span class="sd">    See :class:`~torch.nn.ReLU6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="n">elu</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">elu(input, alpha=1., inplace=False) -&gt; Variable</span>

<span class="s2">Applies element-wise,</span>
<span class="s2">:math:`f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))`.</span>

<span class="s2">See :class:`~torch.nn.ELU` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;selu(input, inplace=False) -&gt; Variable</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`f(x) = scale * (\max(0,x) + \min(0, alpha * (\exp(x) - 1)))`,</span>
<span class="sd">    with ``alpha=1.6732632423543772848170429916717`` and</span>
<span class="sd">    ``scale=1.0507009873554804934193349852946``.</span>

<span class="sd">    See :class:`~torch.nn.SELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">SELU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">leaky_relu(input, negative_slope=0.01, inplace=False) -&gt; Variable</span>

<span class="s2">Applies element-wise,</span>
<span class="s2">:math:`f(x) = max(0, x) + {negative\_slope} * min(0, x)`</span>

<span class="s2">See :class:`~torch.nn.LeakyReLU` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="c1"># prelu = _add_docstr(torch._C._nn.prelu, r&quot;&quot;&quot;</span>
<span class="c1"># prelu(input, weight) -&gt; Variable</span>
<span class="c1"># &quot;&quot;&quot;)</span>
<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.prelu">[docs]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;prelu(input, weight) -&gt; Variable</span>

<span class="sd">    Applies element-wise the function</span>
<span class="sd">    :math:`PReLU(x) = max(0,x) + weight * min(0,x)` where weight is a</span>
<span class="sd">    learnable parameter.</span>

<span class="sd">    See :class:`~torch.nn.PReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">PReLU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<span class="n">rrelu</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">rrelu</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -&gt; Variable</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">logsigmoid(input) -&gt; Variable</span>

<span class="s2">Applies element-wise :math:`LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))`</span>

<span class="s2">See :class:`~torch.nn.LogSigmoid` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">hardshrink</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">hardshrink(input, lambd=0.5) -&gt; Variable</span>

<span class="s2">Applies the hard shrinkage function element-wise</span>

<span class="s2">See :class:`~torch.nn.Hardshrink` for more details.</span>


<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanhshrink(input) -&gt; Variable</span>

<span class="sd">    Applies element-wise, :math:`Tanhshrink(x) = x - Tanh(x)`</span>

<span class="sd">    See :class:`~torch.nn.Tanhshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;softsign(input) -&gt; Variable</span>

<span class="sd">    Applies element-wise, the function :math:`f(x) = x / (1 + |x|)`</span>

<span class="sd">    See :class:`~torch.nn.Softsign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="n">softplus</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softplus(input, beta=1, threshold=20) -&gt; Variable</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_softmax_dim</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">stacklevel</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Implicit dimension choice for &quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot; has been deprecated. &quot;</span>
                  <span class="s2">&quot;Change the call to include dim=X as an argument.&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmin function.</span>

<span class="sd">    Note that softmin(x) = softmax(-x). See softmax definition for mathematical formula.</span>

<span class="sd">    See :class:`~torch.nn.Softmin` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        dim (int): A dimension along which softmin will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmin&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax function.</span>

<span class="sd">    Softmax is defined as:</span>

<span class="sd">    :math:`softmax(x) = \frac{exp(x_i)}{\sum_j exp(x_j)}`</span>

<span class="sd">    It is applied to all slices along dim, and will rescale them so that the elements</span>
<span class="sd">    lie in the range `(0, 1)` and sum to 1.</span>

<span class="sd">    See :class:`~torch.nn.Softmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        dim (int): A dimension along which softmax will be computed.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This function doesn&#39;t work directly with NLLLoss,</span>
<span class="sd">        which expects the Log to be computed between the Softmax and itself.</span>
<span class="sd">        Use log_softmax instead (it&#39;s faster and has better numerical properties).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax followed by a logarithm.</span>

<span class="sd">    While mathematically equivalent to log(softmax(x)), doing these two</span>
<span class="sd">    operations separately is slower, and numerically unstable. This function</span>
<span class="sd">    uses an alternative formulation to compute the output and gradient correctly.</span>

<span class="sd">    See :class:`~torch.nn.LogSoftmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        dim (int): A dimension along which log_softmax will be computed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="softshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softshrink">[docs]</a><span class="k">def</span> <span class="nf">softshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;softshrink(input, lambd=0.5) -&gt; Variable</span>

<span class="sd">    Applies the soft shrinkage function elementwise</span>

<span class="sd">    See :class:`~torch.nn.Softshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softshrink</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanh(input) -&gt; Variable</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`</span>

<span class="sd">    See :class:`~torch.nn.Tanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;sigmoid(input) -&gt; Variable</span>

<span class="sd">    Applies the element-wise function :math:`f(x) = 1 / ( 1 + exp(-x))`</span>

<span class="sd">    See :class:`~torch.nn.Sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span></div>


<span class="c1"># etc.</span>

<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *, in\_features)` where `*` means any number of</span>
<span class="sd">          additional dimensions</span>
<span class="sd">        - Weight: :math:`(out\_features, in\_features)`</span>
<span class="sd">        - Bias: :math:`(out\_features)`</span>
<span class="sd">        - Output: :math:`(N, *, out\_features)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># fused op is marginally faster</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Bilinear</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Bilinear</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span>
              <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A simple lookup table that looks up embeddings in a fixed dictionary and size.</span>

<span class="sd">    This module is often used to retrieve word embeddings using indices.</span>
<span class="sd">    The input to the module is a list of indices, and the embedding matrix,</span>
<span class="sd">    and the output is the corresponding word embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: tensor, containing indices into the embedding matrix</span>
<span class="sd">        embedding_matrix:</span>
<span class="sd">                Number of rows should correspond to the maximum possible index + 1,</span>
<span class="sd">                number of columns is the embedding size</span>
<span class="sd">        max_norm (float, optional): If given, will renormalize the embeddings to always have a norm lesser than this</span>
<span class="sd">        norm_type (float, optional): The p of the p-norm to compute for the max_norm option</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the frequency of</span>
<span class="sd">                                                the words in the mini-batch.</span>
<span class="sd">        sparse (boolean, optional): if ``True``, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for</span>
<span class="sd">                                    more details regarding sparse gradients.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: LongTensor `(N, W)`, N = mini-batch, W = number of indices to extract per mini-batch</span>
<span class="sd">        - Embedding_matrix: FloatTensor `(V, embedding_dim)`, V = maximum index + 1, embedding_dim = embedding size</span>
<span class="sd">        - Output: `(N, W, embedding_dim)`</span>

<span class="sd">    Notes:</span>
<span class="sd">        It is advised to only use `sparse=True` if `embedding_matrix` is a leaf Variable,</span>
<span class="sd">        since some autograd functions may not propagate sparse gradients correctly.</span>
<span class="sd">        Additionally, keep in mind that only a limited number of optimizers support</span>
<span class="sd">        sparse gradients: currently it&#39;s `optim.SGD` (`cuda` and `cpu`), and `optim.Adagrad` (`cpu`)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))</span>
<span class="sd">        &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = Variable(torch.rand(10, 3))</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.functional.embedding(input, embedding_matrix)</span>

<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">         -1.0822  1.2522  0.2434</span>
<span class="sd">          0.8393 -0.6062 -0.3348</span>
<span class="sd">          0.6597  0.0350  0.0837</span>
<span class="sd">          0.5521  0.9447  0.0498</span>

<span class="sd">        (1 ,.,.) =</span>
<span class="sd">          0.6597  0.0350  0.0837</span>
<span class="sd">         -0.1527  0.0877  0.4260</span>
<span class="sd">          0.8393 -0.6062 -0.3348</span>
<span class="sd">         -0.8738 -0.9054  0.4281</span>
<span class="sd">        [torch.FloatTensor of size 2x4x3]</span>

<span class="sd">        &gt;&gt;&gt; # example with padding_idx</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; weights[0, :].zero_()</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = Variable(weights)</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.LongTensor([[0,2,0,5]]))</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.functional.embedding(input, embedding_matrix)</span>

<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">          0.0000  0.0000  0.0000</span>
<span class="sd">          0.3452  0.4937 -0.9361</span>
<span class="sd">          0.0000  0.0000  0.0000</span>
<span class="sd">          0.0706 -2.1962 -0.6276</span>
<span class="sd">        [torch.FloatTensor of size 1x4x3]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">embedding_bag</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes sums or means of &#39;bags&#39; of embeddings, without instantiating the</span>
<span class="sd">        intermediate embeddings.</span>

<span class="sd">        For bags of constant length,</span>
<span class="sd">            * embedding_bag with `mode=sum` is equivalent to nn.functional.embedding followed by `torch.sum(dim=1)`</span>
<span class="sd">            * with `mode=mean` is equivalent to nn.functional.embedding followed by `torch.mean(dim=1)`</span>

<span class="sd">        However, embedding_bag is much more time and memory efficient than using a chain of these</span>
<span class="sd">        operations.</span>

<span class="sd">        Args:</span>
<span class="sd">            embedding_matrix: FloatTensor, where number of rows should correspond to the maximum possible index + 1,</span>
<span class="sd">                              number of columns is the embedding size</span>
<span class="sd">            indices (N or BxN): LongTensor containing the indices of the embeddings to extract.</span>
<span class="sd">                                When `input` is 1D Tensor of shape `N`, an `offsets` Tensor is given, that contains the</span>
<span class="sd">                                starting position of each new sequence in the mini-batch.</span>
<span class="sd">            offsets (B or None): LongTensor containing the starting positions of each sample in a mini-batch of variable</span>
<span class="sd">                                 length sequences. If `input` is 2D (BxN), then offsets does not need to be given,</span>
<span class="sd">                                 as the `input` is treated as a mini-batch of fixed length sequences of length `N` each.</span>
<span class="sd">            max_norm (float, optional): If given, will renormalize the embeddings to always have a norm lesser than this</span>
<span class="sd">            norm_type (float, optional): The p of the p-norm to compute for the max_norm option</span>
<span class="sd">            scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the frequency of</span>
<span class="sd">                                                    the words in the dictionary.</span>
<span class="sd">            mode (string, optional): &#39;sum&#39; | &#39;mean&#39;. Specifies the way to reduce the bag. Default: &#39;mean&#39;</span>

<span class="sd">        Shape:</span>
<span class="sd">            - Embedding_matrix: FloatTensor `(V, embedding_dim)`,</span>
<span class="sd">                                V = number of embeddings, embedding_dim = embedding size</span>
<span class="sd">            - Input: LongTensor `N`, N = number of embeddings to extract</span>
<span class="sd">                     (or) LongTensor `BxN`, B = number of sequences in mini-batch,</span>
<span class="sd">                                            N = number of embeddings per sequence</span>
<span class="sd">            - Offsets: LongTensor `B`, B = number of bags. The values are the</span>
<span class="sd">                       offsets in `input` for each bag, i.e. the cumsum of lengths.</span>
<span class="sd">                       Offsets is not given if Input is 2D `BxN` Tensor,</span>
<span class="sd">                       the input is considered to be of fixed-length sequences</span>
<span class="sd">            - Output: `(B, embedding_dim)`</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3</span>
<span class="sd">            &gt;&gt;&gt; embedding_matrix = Variable(torch.rand(10, 3))</span>
<span class="sd">            &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">            &gt;&gt;&gt; input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))</span>
<span class="sd">            &gt;&gt;&gt; offsets = Variable(torch.LongTensor([0,4]))</span>
<span class="sd">            &gt;&gt;&gt; embedding_bag(embedding_matrix, input, offsets)</span>

<span class="sd">            Variable containing:</span>
<span class="sd">            -1.1840 -0.2547 -0.5860</span>
<span class="sd">            -0.7126  0.0002 -0.3411</span>
<span class="sd">            [torch.FloatTensor of size 2x3]</span>

<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;if input is 2D, then offsets has to be None&quot;</span>
                             <span class="s2">&quot;, as input is treated is a mini-batch of&quot;</span>
                             <span class="s2">&quot; fixed length sequences. However, found &quot;</span>
                             <span class="s2">&quot;offsets of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">offsets</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                               <span class="n">out</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">()</span><span class="o">.</span><span class="n">long</span><span class="p">()))</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input has to be 1D or 2D Tensor,&quot;</span>
                         <span class="s2">&quot; but got Tensor of dimension </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor but got None&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">mode</span>
    <span class="p">)</span>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">size</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected more than 1 value per channel when training, got input size </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># loss</span>

<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or `(N, C, H, W)`</span>
<span class="sd">            in case of 2D - Loss</span>
<span class="sd">        target: :math:`(N)` where each value is `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">            over observations for each minibatch. If size_average</span>
<span class="sd">            is False, the losses are summed for each minibatch. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When size_average is</span>
<span class="sd">            True, the loss is averaged over non-ignored targets. Default: -100</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5))</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = autograd.Variable(torch.LongTensor([1, 0, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or 4 dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span></div>


<div class="viewcode-block" id="poisson_nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.poisson_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Poisson negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.PoissonNLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: expectation of underlying Poisson distribution.</span>
<span class="sd">        target: random sample :math:`target \sim Pois(input)`.</span>
<span class="sd">        log_input: if ``True`` the loss is computed as</span>
<span class="sd">            `exp(input) - target * input`, if ``False`` then loss is</span>
<span class="sd">            `input - target * log(input+eps)`. Default: ``True``</span>
<span class="sd">        full: whether to compute full loss, i. e. to add the Stirling</span>
<span class="sd">            approximation term. Default: ``False``</span>
<span class="sd">            `target * log(target) - target + 0.5 * log(2 * pi * target)`.</span>
<span class="sd">        size_average: By default, the losses are averaged over observations for</span>
<span class="sd">            each minibatch. However, if the field sizeAverage is set to False,</span>
<span class="sd">            the losses are instead summed for each minibatch. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of log(0) when</span>
<span class="sd">            log_input=False. Default: 1e-8</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">log_input</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">full</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">loss</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">target</span><span class="p">))[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">size_average</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></div>


<span class="n">kl_div</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">kl_div</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">kl_div(input, target, size_average=True) -&gt; Variable</span>

<span class="s2">The `Kullback-Leibler divergence`_ Loss.</span>

<span class="s2">See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="s2">Args:</span>
<span class="s2">    input: Variable of arbitrary shape</span>
<span class="s2">    target: Variable of the same shape as input</span>
<span class="s2">    size_average: if ``True`` the output is divided by the number of elements</span>
<span class="s2">        in input tensor. Default: ``True``</span>
<span class="s2">    reduce (bool, optional): By default, the losses are averaged</span>
<span class="s2">        over observations for each minibatch, or summed, depending on</span>
<span class="s2">        size_average. When reduce is False, returns a loss per batch</span>
<span class="s2">        element instead and ignores size_average. Default: ``True``</span>

<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in a single</span>
<span class="sd">    function.</span>

<span class="sd">    See :class:`~torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable :math:`(N, C)` where `C = number of classes`</span>
<span class="sd">        target: Variable :math:`(N)` where each value is</span>
<span class="sd">            `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">                class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch. Ignored if reduce is False. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">                and does not contribute to the input gradient. When size_average is</span>
<span class="sd">                True, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): By default, the losses are averaged or summed over</span>
<span class="sd">                observations for each minibatch depending on size_average. When reduce</span>
<span class="sd">                is False, returns a loss per batch element instead and ignores</span>
<span class="sd">                size_average. Default: ``True``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = autograd.Variable(torch.LongTensor(3).random_(5))</span>
<span class="sd">        &gt;&gt;&gt; loss = F.cross_entropy(input, target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output.</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch. Default: ``True``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.randn(3), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = autograd.Variable(torch.LongTensor(3).random_(2))</span>
<span class="sd">        &gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">) is deprecated. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target and input must have the same number of elements. target nelement (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                         <span class="s2">&quot;!= input nelement (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">_infer_size</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures Binary Cross Entropy between target and output</span>
<span class="sd">    logits.</span>

<span class="sd">    See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch. Default: ``True``</span>

<span class="sd">    Examples::</span>

<span class="sd">         &gt;&gt;&gt; input = autograd.Variable(torch.randn(3), requires_grad=True)</span>
<span class="sd">         &gt;&gt;&gt; target = autograd.Variable(torch.FloatTensor(3).random_(2))</span>
<span class="sd">         &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)</span>
<span class="sd">         &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target size (</span><span class="si">{}</span><span class="s2">) must be the same as input size (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="n">max_val</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">max_val</span> <span class="o">+</span> <span class="p">((</span><span class="o">-</span><span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="n">size_average</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span></div>


<span class="n">smooth_l1_loss</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">smooth_l1_loss(input, target, size_average=True) -&gt; Variable</span>

<span class="s2">Function that uses a squared term if the absolute</span>
<span class="s2">element-wise error falls below 1 and an L1 term otherwise.</span>

<span class="s2">See :class:`~torch.nn.SmoothL1Loss` for details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">l1_loss</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">l1_loss(input, target, size_average=True, reduce=True) -&gt; Variable</span>

<span class="s2">Function that takes the mean element-wise absolute value difference.</span>

<span class="s2">See :class:`~torch.nn.L1Loss` for details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">mse_loss(input, target, size_average=True, reduce=True) -&gt; Variable</span>

<span class="s2">Measures the element-wise mean squared error.</span>

<span class="s2">See :class:`~torch.nn.MSELoss` for details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;margin_ranking_loss(input1, input2, target, margin=0, size_average=True) -&gt; Variable</span>

<span class="sd">    See :class:`~torch.nn.MarginRankingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;hinge_embedding_loss(input, target, margin=1.0, size_average=True) -&gt; Variable</span>

<span class="sd">    See :class:`~torch.nn.HingeEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">HingeEmbeddingLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<span class="n">multilabel_margin_loss</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">multilabel_margin_loss(input, target, size_average=True) -&gt; Variable</span>

<span class="s2">See :class:`~torch.nn.MultiLabelMarginLoss` for details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">soft_margin_loss</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">soft_margin_loss(input, target, size_average=True) -&gt; Variable</span>

<span class="s2">See :class:`~torch.nn.SoftMarginLoss` for details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;multilabel_soft_margin_loss(input, target, weight=None, size_average=True) -&gt; Variable</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cosine_embedding_loss(input1, input2, target, margin=0, size_average=True) -&gt; Variable</span>

<span class="sd">    See :class:`~torch.nn.CosineEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">CosineEmbeddingLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multi_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=True) -&gt; Variable</span>

<span class="sd">    See :class:`~torch.nn.MultiMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only p == 1 and p == 2 supported&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight must be one-dimensional&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rearranges elements in a tensor of shape ``[*, C*r^2, H, W]`` to a</span>
<span class="sd">    tensor of shape ``[C, H*r, W*r]``.</span>

<span class="sd">    See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): Input</span>
<span class="sd">        upscale_factor (int): factor to increase spatial resolution by</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; ps = nn.PixelShuffle(3)</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.Tensor(1, 9, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = ps(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output.size())</span>
<span class="sd">        torch.Size([1, 1, 12, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">channels</span> <span class="o">//=</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">out_height</span> <span class="o">=</span> <span class="n">in_height</span> <span class="o">*</span> <span class="n">upscale_factor</span>
    <span class="n">out_width</span> <span class="o">=</span> <span class="n">in_width</span> <span class="o">*</span> <span class="n">upscale_factor</span>

    <span class="n">input_view</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span>
        <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span><span class="p">)</span>

    <span class="n">shuffle_out</span> <span class="o">=</span> <span class="n">input_view</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">shuffle_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">out_height</span><span class="p">,</span> <span class="n">out_width</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    The algorithm used for upsampling is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric upsampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [depth] x [height] x width`</span>

<span class="sd">    The modes available for upsampling are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear` (4D-only), `trilinear` (5D-only)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;linear&#39; | &#39;bilinear&#39; | &#39;trilinear&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingLinear1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingTrilinear3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D, 4D and 5D input Tensors supported&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">D) for the modes: nearest | linear | bilinear | trilinear&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="upsample_nearest"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_nearest">[docs]</a><span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    **Note:: This function is deprecated. Use nn.functional.upsample instead**</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported (i.e. expected</span>
<span class="sd">    inputs are 4 or 5 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia</span>
<span class="sd">            size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_nearest is deprecated. Use nn.functional.upsample instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample_bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_bilinear">[docs]</a><span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upscales the input, using bilinear upsampling.</span>

<span class="sd">    **Note:: This function is deprecated. Use nn.functional.upsample instead**</span>

<span class="sd">    Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo</span>
<span class="sd">    volumetric (5 dimensional) inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int or Tuple[int, int]): multiplier for spatial size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given an :attr:`input` and a flow-field :attr:`grid`, computes the</span>
<span class="sd">    `output` using input pixel locations from the grid.</span>

<span class="sd">    Uses bilinear interpolation to sample the input pixels.</span>
<span class="sd">    Currently, only spatial (4 dimensional) inputs are supported.</span>

<span class="sd">    For each output location, :attr:`grid` has `x` and `y`</span>
<span class="sd">    input pixel locations which are used to compute output.</span>

<span class="sd">    :attr:`grid` has values in the range of `[-1, 1]`. This is because the</span>
<span class="sd">    pixel locations are normalized by the input height and width.</span>

<span class="sd">    For example, values: x: -1, y: -1 is the left-top pixel of the input</span>
<span class="sd">                 values: x: 1, y: 1 is the right-bottom pixel of the input</span>

<span class="sd">    If :attr:`grid` has values outside the range of `[-1, 1]`, those locations</span>
<span class="sd">    are handled as defined by `padding_mode`. Options are `zeros` or `border`,</span>
<span class="sd">    defining those locations to use 0 or image border values as contribution</span>
<span class="sd">    to the bilinear interpolation.</span>

<span class="sd">    .. Note:: This function is used in building Spatial Transformer Networks</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input batch of images (N x C x IH x IW)</span>
<span class="sd">        grid (Variable): flow-field of size (N x OH x OW x 2)</span>
<span class="sd">        padding_mode (str): padding mode for outside grid values</span>
<span class="sd">            &#39;zeros&#39; | &#39;border&#39;. Default: &#39;zeros&#39;</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Variable): output Tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">GridSampler</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.affine_grid">[docs]</a><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generates a 2d flow field, given a batch of affine matrices :attr:`theta`</span>
<span class="sd">    Generally used in conjunction with :func:`grid_sample` to</span>
<span class="sd">    implement Spatial Transformer Networks.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Variable): input batch of affine matrices (N x 2 x 3)</span>
<span class="sd">        size (torch.Size): the target output image size (N x C x H x W)</span>
<span class="sd">                           Example: torch.Size((32, 3, 24, 24))</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Variable): output Tensor of size (N x H x W x 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">AffineGridGenerator</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    Nd constant padding:  The number of dimensions to pad is</span>
<span class="sd">        len(padding) // 2 and the dimensions that gets padded begins with the</span>
<span class="sd">        last dimension and moves forward.  See below for examples.</span>

<span class="sd">    1D, 2D and 3D &quot;reflect&quot;/&quot;replicate&quot; padding:</span>
<span class="sd">        1D: 3D input with padding in form (pad_l, pad_r)</span>
<span class="sd">        2D: 4D input tensor pad should be in form</span>
<span class="sd">        (pad_l, pad_r, pad_t, pad_b ).</span>
<span class="sd">        3D: 5D pad (pleft, pright, ptop, pbottom, pfront, pback). No &quot;reflect&quot;</span>
<span class="sd">        implementation</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): Nd tensor</span>
<span class="sd">        pad (tuple): m-elem tuple, where m // 2 &lt;= input dimensions and m % 2 == 0</span>
<span class="sd">        mode: &#39;constant&#39;, &#39;reflect&#39; or &#39;replicate&#39;. Default: &#39;constant&#39;</span>
<span class="sd">        value: fill value for &#39;constant&#39; padding. Default: 0</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; t4d = torch.Tensor(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p1d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 4, 4])</span>
<span class="sd">        &gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p2d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 8, 4])</span>
<span class="sd">        &gt;&gt;&gt; t4d = torch.Tensor(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p3d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 9, 7, 3])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding length must be divisible by 2&#39;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="s1">&#39;Padding length too large&#39;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ConstantPadNd</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;3D tensors expect 2 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 3D, 4D, 5D padding with non-constant padding are supported for now&quot;</span><span class="p">)</span></div>


<span class="c1"># distance</span>

<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the batchwise pairwise distance between vectors v1,v2:</span>

<span class="sd">    .. math ::</span>
<span class="sd">        \Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}</span>

<span class="sd">    Args:</span>
<span class="sd">        x1: first input tensor</span>
<span class="sd">        x2: second input tensor</span>
<span class="sd">        p: the norm degree. Default: 2</span>
<span class="sd">        eps (float, optional): Small value to avoid division by zero. Default: 1e-6</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">        - Output: :math:`(N, 1)`</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.pairwise_distance(input1, input2, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input must be a 2D matrix.&quot;</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">diff</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_similarity"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_similarity">[docs]</a><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns cosine similarity between x1 and x2, computed along dim.</span>

<span class="sd">    .. math ::</span>
<span class="sd">        \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Variable): First input.</span>
<span class="sd">        x2 (Variable): Second input (of size matching x1).</span>
<span class="sd">        dim (int, optional): Dimension of vectors. Default: 1</span>
<span class="sd">        eps (float, optional): Small value to avoid division by zero.</span>
<span class="sd">            Default: 1e-8</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.</span>
<span class="sd">        - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.cosine_similarity(input1, input2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w12</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w12</span> <span class="o">/</span> <span class="p">(</span><span class="n">w1</span> <span class="o">*</span> <span class="n">w2</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span></div>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.triplet_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors x1, x2, x3 and a margin with a value greater than 0.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n`: anchor, positive examples and negative</span>
<span class="sd">    example respectively. The shape of all input variables should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\} \right)</span>

<span class="sd">    where :math:`d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p`.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor: anchor input tensor</span>
<span class="sd">        positive: positive input tensor</span>
<span class="sd">        negative: negative input tensor</span>
<span class="sd">        margin: the margin value. Default: 1</span>
<span class="sd">        p: the norm degree. Default: 2</span>
<span class="sd">        eps: small epsilon value to avoid numerical issues. Default: 1e-6</span>
<span class="sd">        swap: compute distance swap. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">        - Output: :math:`(N, 1)`</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input3 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.triplet_margin_loss(input1, input2, input3, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between anchor and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input must be a 2D matrix.&quot;</span>
    <span class="k">assert</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Margin should be positive value.&#39;</span>
    <span class="n">d_p</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">d_n</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">swap</span><span class="p">:</span>
        <span class="n">d_s</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">d_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d_n</span><span class="p">,</span> <span class="n">d_s</span><span class="p">)</span>

    <span class="n">dist_hinge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">margin</span> <span class="o">+</span> <span class="n">d_p</span> <span class="o">-</span> <span class="n">d_n</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist_hinge</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.normalize">[docs]</a><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs :math:`L_p` normalization of inputs over specified dimension.</span>

<span class="sd">    Does:</span>

<span class="sd">    .. math::</span>
<span class="sd">        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}</span>

<span class="sd">    for each subtensor v over dimension dim of input. Each subtensor is</span>
<span class="sd">    flattened into a vector, i.e. :math:`\lVert v \rVert_p` is not a matrix</span>
<span class="sd">    norm.</span>

<span class="sd">    With default arguments normalizes over the second dimension with Euclidean</span>
<span class="sd">    norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of any shape</span>
<span class="sd">        p (float): the exponent value in the norm formulation. Default: 2</span>
<span class="sd">        dim (int): the dimension to reduce. Default: 1</span>
<span class="sd">        eps (float): small value to avoid division by zero. Default: 1e-12</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Tensor &mdash; PyTorch master documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="PyTorch master documentation" href="index.html"/>
        <link rel="next" title="torch.sparse" href="sparse.html"/>
        <link rel="prev" title="torch" href="torch.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.3.0 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id14"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id15"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id16"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id17"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id18"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id19"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id20"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id21"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id22"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id23"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id24"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id25"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id26"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id27"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id28"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id29"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id30"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id31"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id32"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id33"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id34"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id35"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id36"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id37">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id38">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id40"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>torch.Tensor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/tensors.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torch-tensor">
<h1>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">Â¶</a></h1>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<p>Torch defines seven CPU tensor types and eight GPU tensor types:</p>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="33%" />
<col width="39%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Data type</th>
<th class="head">CPU tensor</th>
<th class="head">GPU tensor</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>32-bit floating point</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.FloatTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.FloatTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit floating point</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.DoubleTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.DoubleTensor</span></code></td>
</tr>
<tr class="row-even"><td>16-bit floating point</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.HalfTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.HalfTensor</span></code></td>
</tr>
<tr class="row-odd"><td>8-bit integer (unsigned)</td>
<td><a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal"><span class="pre">torch.ByteTensor</span></code></a></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.ByteTensor</span></code></td>
</tr>
<tr class="row-even"><td>8-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.CharTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.CharTensor</span></code></td>
</tr>
<tr class="row-odd"><td>16-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.ShortTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.ShortTensor</span></code></td>
</tr>
<tr class="row-even"><td>32-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.IntTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.IntTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.LongTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.LongTensor</span></code></td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> constructor is an alias for the default tensor type
(<code class="xref py py-class docutils literal"><span class="pre">torch.FloatTensor</span></code>).</p>
<p>A tensor can be constructed from a Python <code class="xref py py-class docutils literal"><span class="pre">list</span></code> or sequence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="go">1  2  3</span>
<span class="go">4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>An empty tensor can be constructed by specifying its size:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="go">0  0  0  0</span>
<span class="go">0  0  0  0</span>
<span class="go">[torch.IntTensor of size 2x4]</span>
</pre></div>
</div>
<p>The contents of a tensor can be accessed and modified using Python&#8217;s indexing
and slicing notation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">6.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go"> 1  8  3</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">Â¶</a></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>sequence</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>ndarray</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>storage</em><span class="sig-paren">)</span></dt>
<dd><p>Creates a new tensor from an optional size or data.</p>
<p>If no arguments are given, an empty zero-dimensional tensor is returned.
If a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>, <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a>, or <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code>
is given, a new tensor that shares the same data is returned. If a Python
sequence is given, a new tensor is created from a copy of the sequence.</p>
<dl class="method">
<dt id="torch.Tensor.abs">
<code class="descname">abs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs_">
<code class="descname">abs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos">
<code class="descname">acos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos_">
<code class="descname">acos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.add" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.add" title="torch.add"><code class="xref py py-func docutils literal"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add_">
<code class="descname">add_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.add_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm">
<code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm_">
<code class="descname">addbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv">
<code class="descname">addcdiv</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv_">
<code class="descname">addcdiv_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul">
<code class="descname">addcmul</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul_">
<code class="descname">addcmul_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm">
<code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm_">
<code class="descname">addmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv">
<code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv_">
<code class="descname">addmv_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr">
<code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr_">
<code class="descname">addr_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.apply_">
<code class="descname">apply_</code><span class="sig-paren">(</span><em>callable</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.apply_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin">
<code class="descname">asin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin_">
<code class="descname">asin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan">
<code class="descname">atan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2">
<code class="descname">atan2</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2_">
<code class="descname">atan2_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan_">
<code class="descname">atan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm">
<code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm_">
<code class="descname">baddbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli">
<code class="descname">bernoulli</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli_">
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-meth docutils literal"><span class="pre">bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bmm">
<code class="descname">bmm</code><span class="sig-paren">(</span><em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bmm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.byte" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to byte type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cauchy_">
<code class="descname">cauchy_</code><span class="sig-paren">(</span><em>median=0</em>, <em>sigma=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cauchy_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math">
\[P(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - median)^2 + \sigma^2}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil">
<code class="descname">ceil</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil_">
<code class="descname">ceil_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.char" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to char type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.chunk">
<code class="descname">chunk</code><span class="sig-paren">(</span><em>n_chunks</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.chunk" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Splits this tensor into a tuple of tensors.</p>
<p>See <a class="reference internal" href="torch.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal"><span class="pre">torch.chunk()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp">
<code class="descname">clamp</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp_">
<code class="descname">clamp_</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clone" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a copy of the tensor. The copy has the same size and data type as the
original tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.contiguous">
<code class="descname">contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.contiguous" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a contiguous Tensor containing the same data as this tensor. If this
tensor is contiguous, this function returns the original tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><em>src</em>, <em>async=False</em>, <em>broadcast=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.copy_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal"><span class="pre">src</span></code> into this tensor and returns this tensor.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">broadcast</span></code> is True, the source tensor must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with this tensor. Otherwise,
source tensor should have the same number of elements as this tensor.
It may be of a different data type or reside on a different device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Source tensor to copy</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code> and this copy is between CPU and GPU, then the copy
may occur asynchronously with respect to the host. For other
copies, this argument has no effect.</li>
<li><strong>broadcast</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">src</span></code> will be broadcast to the shape of
the underlying tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos">
<code class="descname">cos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos_">
<code class="descname">cos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh">
<code class="descname">cosh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh_">
<code class="descname">cosh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.cpu" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a CPU copy of this tensor if it&#8217;s not already on the CPU</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cross">
<code class="descname">cross</code><span class="sig-paren">(</span><em>other</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cross" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.cuda" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The destination GPU id. Defaults to the current device.</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code> and the source is in pinned memory, the copy will
be asynchronous with respect to the host. Otherwise, the
argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumprod">
<code class="descname">cumprod</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumprod" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumsum">
<code class="descname">cumsum</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumsum" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.data_ptr" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the address of the first element of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag">
<code class="descname">diag</code><span class="sig-paren">(</span><em>diagonal=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dim">
<code class="descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.dim" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the number of dimensions of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dist">
<code class="descname">dist</code><span class="sig-paren">(</span><em>other</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.dist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div">
<code class="descname">div</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.div" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div_">
<code class="descname">div_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.div_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dot">
<code class="descname">dot</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.dot" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.double" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to double type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eig">
<code class="descname">eig</code><span class="sig-paren">(</span><em>eigenvectors=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.eig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eig" title="torch.eig"><code class="xref py py-func docutils literal"><span class="pre">torch.eig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.element_size" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq">
<code class="descname">eq</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq_">
<code class="descname">eq_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.equal">
<code class="descname">equal</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.equal" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf">
<code class="descname">erf</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal"><span class="pre">torch.erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf_">
<code class="descname">erf_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erf_" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv">
<code class="descname">erfinv</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal"><span class="pre">torch.erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv_">
<code class="descname">erfinv_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erfinv_" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp">
<code class="descname">exp</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp_">
<code class="descname">exp_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a new view of the tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. (For the new dimensions, the
size cannot be set to -1.)</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The desired expanded size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go"> 1  1  1  1</span>
<span class="go"> 2  2  2  2</span>
<span class="go"> 3  3  3  3</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go"> 1  1  1  1</span>
<span class="go"> 2  2  2  2</span>
<span class="go"> 3  3  3  3</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand_as">
<code class="descname">expand_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.expand_as" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Expands this tensor to the size of the specified tensor.</p>
<p>This is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exponential_">
<code class="descname">exponential_</code><span class="sig-paren">(</span><em>lambd=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exponential_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with elements drawn from the exponential distribution:</p>
<div class="math">
\[P(x) = \lambda e^{-\lambda x}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fill_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with the specified value.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.float" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor">
<code class="descname">floor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor_">
<code class="descname">floor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod">
<code class="descname">fmod</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod_">
<code class="descname">fmod_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac">
<code class="descname">frac</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac_">
<code class="descname">frac_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gather">
<code class="descname">gather</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gather" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge">
<code class="descname">ge</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal"><span class="pre">torch.ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge_">
<code class="descname">ge_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal"><span class="pre">ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gels">
<code class="descname">gels</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gels" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal"><span class="pre">torch.gels()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geometric_">
<code class="descname">geometric_</code><span class="sig-paren">(</span><em>p</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.geometric_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with elements drawn from the geometric distribution:</p>
<div class="math">
\[P(X=k) = (1 - p)^{k - 1} p\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geqrf">
<code class="descname">geqrf</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.geqrf" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ger">
<code class="descname">ger</code><span class="sig-paren">(</span><em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ger" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gesv">
<code class="descname">gesv</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor, Tensor<a class="headerlink" href="#torch.Tensor.gesv" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gesv" title="torch.gesv"><code class="xref py py-func docutils literal"><span class="pre">torch.gesv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt">
<code class="descname">gt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal"><span class="pre">torch.gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt_">
<code class="descname">gt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal"><span class="pre">gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.half" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to half-precision float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.histc">
<code class="descname">histc</code><span class="sig-paren">(</span><em>bins=100</em>, <em>min=0</em>, <em>max=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.histc" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index">
<code class="descname">index</code><span class="sig-paren">(</span><em>m</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Selects elements from this tensor using a binary mask or along a given
dimension. The expression <code class="docutils literal"><span class="pre">tensor.index(m)</span></code> is equivalent to <code class="docutils literal"><span class="pre">tensor[m]</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>m</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#slice" title="(in Python v2.7)"><em>slice</em></a>) &#8211; The dimension or mask used to select elements</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add_">
<code class="descname">index_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Accumulate the elements of tensor into the original tensor by adding to the
indices in the order given in index. The shape of tensor must exactly match the
elements indexed or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices to select from tensor</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing values to add</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">  2   3   4</span>
<span class="go">  8   9  10</span>
<span class="go">  5   6   7</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy_">
<code class="descname">index_copy_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Copies the elements of tensor into the original tensor by selecting the
indices in the order given in index. The shape of tensor must exactly match the
elements indexed or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices to select from tensor</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing values to copy</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go"> 1  2  3</span>
<span class="go"> 7  8  9</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill_">
<code class="descname">index_fill_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>val</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills the elements of the original tensor with value <code class="xref py py-attr docutils literal"><span class="pre">val</span></code> by selecting
the indices in the order given in index.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices</li>
<li><strong>val</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Value to fill</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">-1  2 -1</span>
<span class="go">-1  5 -1</span>
<span class="go">-1  8 -1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_select">
<code class="descname">index_select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_select" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.int" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to int type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.inverse">
<code class="descname">inverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.inverse" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_contiguous">
<code class="descname">is_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_contiguous" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns True if this tensor is contiguous in memory in C order.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_cuda">
<code class="descname">is_cuda</code><a class="headerlink" href="#torch.Tensor.is_cuda" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_pinned" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns true if this tensor resides in pinned memory</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_set_to">
<code class="descname">is_set_to</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_set_to" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns True if this object refers to the same <code class="docutils literal"><span class="pre">THTensor</span></code> object from the
Torch C API as the given tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_signed">
<code class="descname">is_signed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_signed" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.kthvalue">
<code class="descname">kthvalue</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.kthvalue" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le">
<code class="descname">le</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal"><span class="pre">torch.le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le_">
<code class="descname">le_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal"><span class="pre">le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp">
<code class="descname">lerp</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.lerp" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp_">
<code class="descname">lerp_</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.lerp_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log">
<code class="descname">log</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p">
<code class="descname">log1p</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p_">
<code class="descname">log1p_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_">
<code class="descname">log_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_normal_">
<code class="descname">log_normal_</code><span class="sig-paren">(</span><em>mean=1</em>, <em>std=2</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.log_normal_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with numbers samples from the log-normal distribution
parameterized by the given mean (Âµ) and standard deviation (Ï).
Note that <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal"><span class="pre">mean</span></code></a> and <code class="xref py py-attr docutils literal"><span class="pre">stdv</span></code> are the mean and standard deviation of
the underlying normal distribution, and not of the returned distribution:</p>
<div class="math">
\[P(x) = \dfrac{1}{x \sigma \sqrt{2\pi}} e^{-\dfrac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.long" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to long type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt">
<code class="descname">lt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal"><span class="pre">torch.lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt_">
<code class="descname">lt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal"><span class="pre">lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.map_">
<code class="descname">map_</code><span class="sig-paren">(</span><em>tensor</em>, <em>callable</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.map_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> for each element in this tensor and the given tensor
and stores the results in this tensor.  This tensor and the given tensor must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>.</p>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter_">
<code class="descname">masked_scatter_</code><span class="sig-paren">(</span><em>mask</em>, <em>source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_scatter_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> into this tensor at positions where the
<code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> is one.
The shape of <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) &#8211; The binary mask</li>
<li><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The tensor to copy from</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal"><span class="pre">source</span></code> tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill_">
<code class="descname">masked_fill_</code><span class="sig-paren">(</span><em>mask</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_fill_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills elements of this tensor with <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> is one.
The shape of <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the shape of the underlying tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) &#8211; The binary mask</li>
<li><strong>value</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The value to fill</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_select">
<code class="descname">masked_select</code><span class="sig-paren">(</span><em>mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_select" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matmul">
<code class="descname">matmul</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.matmul" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Matrix product of two tensors.</p>
<p>See <a class="reference internal" href="torch.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal"><span class="pre">torch.matmul()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.max">
<code class="descname">max</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.max" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mean">
<code class="descname">mean</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mean" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.median">
<code class="descname">median</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.median" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.min">
<code class="descname">min</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.min" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mm">
<code class="descname">mm</code><span class="sig-paren">(</span><em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mode">
<code class="descname">mode</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mode" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul">
<code class="descname">mul</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mul" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal"><span class="pre">torch.mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul_">
<code class="descname">mul_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mul_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal"><span class="pre">mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.multinomial">
<code class="descname">multinomial</code><span class="sig-paren">(</span><em>num_samples</em>, <em>replacement=False</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.multinomial" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mv">
<code class="descname">mv</code><span class="sig-paren">(</span><em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mv" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow">
<code class="descname">narrow</code><span class="sig-paren">(</span><em>dimension</em>, <em>start</em>, <em>length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.narrow" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a new tensor that is a narrowed version of this tensor. The dimension
<a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></a> is narrowed from <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal"><span class="pre">start</span> <span class="pre">+</span> <span class="pre">length</span></code>. The
returned tensor and this tensor share the same underlying storage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The dimension along which to narrow</li>
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The starting dimension</li>
<li><strong>length</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 1  2  3</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 2  3</span>
<span class="go"> 5  6</span>
<span class="go"> 8  9</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ndimension">
<code class="descname">ndimension</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.ndimension" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne">
<code class="descname">ne</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal"><span class="pre">torch.ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne_">
<code class="descname">ne_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal"><span class="pre">ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg">
<code class="descname">neg</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg_">
<code class="descname">neg_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nelement">
<code class="descname">nelement</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.nelement" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new">
<code class="descname">new</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.new" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Constructs a new tensor of the same data type.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nonzero">
<code class="descname">nonzero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.nonzero" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.norm">
<code class="descname">norm</code><span class="sig-paren">(</span><em>p=2</em>, <em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.norm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.normal_">
<code class="descname">normal_</code><span class="sig-paren">(</span><em>mean=0</em>, <em>std=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.normal_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal"><span class="pre">mean</span></code></a> and <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numel">
<code class="descname">numel</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.numel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numpy">
<code class="descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; ndarray<a class="headerlink" href="#torch.Tensor.numpy" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns this tensor as a NumPy <code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code>. This tensor and the returned
<code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code> share the same underlying storage. Changes to this tensor will
be reflected in the <code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code> and vice versa.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.orgqr">
<code class="descname">orgqr</code><span class="sig-paren">(</span><em>input2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.orgqr" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ormqr">
<code class="descname">ormqr</code><span class="sig-paren">(</span><em>input2</em>, <em>input3</em>, <em>left=True</em>, <em>transpose=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ormqr" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.permute">
<code class="descname">permute</code><span class="sig-paren">(</span><em>*dims</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.permute" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Permute the dimensions of this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*dims</strong> (<em>int...</em>) &#8211; The desired ordering of dimensions</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([5, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pin_memory" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Copies the tensor to pinned memory, if it&#8217;s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrf">
<code class="descname">potrf</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potrf" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potrf" title="torch.potrf"><code class="xref py py-func docutils literal"><span class="pre">torch.potrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potri">
<code class="descname">potri</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potri" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potri" title="torch.potri"><code class="xref py py-func docutils literal"><span class="pre">torch.potri()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrs">
<code class="descname">potrs</code><span class="sig-paren">(</span><em>input2</em>, <em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potrs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potrs" title="torch.potrs"><code class="xref py py-func docutils literal"><span class="pre">torch.potrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow">
<code class="descname">pow</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pow" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow_">
<code class="descname">pow_</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pow_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.prod">
<code class="descname">prod</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.prod" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pstrf">
<code class="descname">pstrf</code><span class="sig-paren">(</span><em>upper=True</em>, <em>tol=-1) -&gt; (Tensor</em>, <em>IntTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pstrf" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pstrf" title="torch.pstrf"><code class="xref py py-func docutils literal"><span class="pre">torch.pstrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.put_">
<code class="descname">put_</code><span class="sig-paren">(</span><em>indices</em>, <em>tensor</em>, <em>accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.put_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code> into the positions specified by
indices. For the puropose of indexing, the <code class="docutils literal"><span class="pre">self</span></code> tensor is treated as if it
were a 1D tensor.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">accumulate</span></code> is <code class="docutils literal"><span class="pre">True</span></code>, the elements in <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code> are added to
<code class="xref py py-attr docutils literal"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal"><span class="pre">False</span></code>, the behavior is undefined if indices
contains duplicate elements.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>indices</strong> (<em>LongTensor</em>) &#8211; the indices into self</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing values to copy</li>
<li><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; True to accumulate into self</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">  4   9   5</span>
<span class="go"> 10   7   8</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qr">
<code class="descname">qr</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.qr" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.random_">
<code class="descname">random_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=None</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.random_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with numbers sampled from the discrete uniform distribution
over [from, to - 1]. If not specified, the values are usually only bounded by
this tensor&#8217;s data type. However, for floating point types, if unspecified,
range will be [0, 2^mantissa] to ensure that every value is representable.
For example, <cite>torch.DoubleTensor(1).random_()</cite> will be uniform in [0, 2^53].</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal">
<code class="descname">reciprocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal_">
<code class="descname">reciprocal_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder">
<code class="descname">remainder</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder_">
<code class="descname">remainder_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm">
<code class="descname">renorm</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm_">
<code class="descname">renorm_</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat">
<code class="descname">repeat</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.repeat" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal"><span class="pre">expand()</span></code></a>, this function copies the tensor&#8217;s data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The number of times to repeat this
tensor along each dimension</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go">[torch.FloatTensor of size 4x6]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.resize_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resizes this tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The desired size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go"> 1  2</span>
<span class="go"> 3  4</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_as_">
<code class="descname">resize_as_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.resize_as_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resizes the current tensor to be the same size as the specified tensor. This is
equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round">
<code class="descname">round</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round_">
<code class="descname">round_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt">
<code class="descname">rsqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt_">
<code class="descname">rsqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_">
<code class="descname">scatter_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>src</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Writes all values from the Tensor <code class="xref py py-attr docutils literal"><span class="pre">src</span></code> into self at the indices specified
in the <a class="reference internal" href="#torch.Tensor.index" title="torch.Tensor.index"><code class="xref py py-attr docutils literal"><span class="pre">index</span></code></a> Tensor. The indices are specified with respect to the
given dimension, dim, in the manner described in <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal"><span class="pre">gather()</span></code></a>.</p>
<p>Note that, as for gather, the values of index must be between <cite>0</cite> and
<cite>(self.size(dim) -1)</cite> inclusive and all values in a row along the specified
dimension must be unique.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The source tensor</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; The indices of elements to scatter</li>
<li><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The source element(s) to scatter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.4319  0.6500  0.4080  0.8760  0.2355</span>
<span class="go"> 0.2609  0.4711  0.8486  0.8573  0.1029</span>
<span class="go">[torch.FloatTensor of size 2x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>

<span class="go"> 0.4319  0.4711  0.8486  0.8760  0.2355</span>
<span class="go"> 0.0000  0.6500  0.0000  0.8573  0.0000</span>
<span class="go"> 0.2609  0.0000  0.4080  0.0000  0.1029</span>
<span class="go">[torch.FloatTensor of size 3x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>

<span class="go"> 0.0000  0.0000  1.2300  0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000  1.2300</span>
<span class="go">[torch.FloatTensor of size 2x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.select">
<code class="descname">select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor or number<a class="headerlink" href="#torch.Tensor.select" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Slices the tensor along the selected dimension at the given index. If this
tensor is one dimensional, this function returns a number. Otherwise, it
returns a tensor with the given dimension removed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension to slice</li>
<li><strong>index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Index to select</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-meth docutils literal"><span class="pre">select()</span></code></a> is equivalent to slicing. For example,
<code class="docutils literal"><span class="pre">tensor.select(0,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal"><span class="pre">tensor[index]</span></code> and
<code class="docutils literal"><span class="pre">tensor.select(2,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal"><span class="pre">tensor[:,:,index]</span></code>.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.set_">
<code class="descname">set_</code><span class="sig-paren">(</span><em>source=None</em>, <em>storage_offset=0</em>, <em>size=None</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.set_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> is a tensor,
this tensor will share the same storage and have the same size and strides
as the given tensor. Changes to elements in one tensor will be reflected in the
other.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) &#8211; The tensor or storage to use</li>
<li><strong>storage_offset</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The offset in the storage</li>
<li><strong>size</strong> (<em>torch.Size</em>) &#8211; The desired size. Defaults to the size of the source.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; The desired stride. Defaults to C-contiguous strides.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.share_memory_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.short" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Casts this tensor to short type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid">
<code class="descname">sigmoid</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid_">
<code class="descname">sigmoid_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign">
<code class="descname">sign</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign_">
<code class="descname">sign_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin">
<code class="descname">sin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin_">
<code class="descname">sin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh">
<code class="descname">sinh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh_">
<code class="descname">sinh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Size<a class="headerlink" href="#torch.Tensor.size" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the size of the tensor. The returned value is a subclass of
<code class="xref py py-class docutils literal"><span class="pre">tuple</span></code>.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sort">
<code class="descname">sort</code><span class="sig-paren">(</span><em>dim=None</em>, <em>descending=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.sort" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>split_size</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.split" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Splits this tensor into a tuple of tensors.</p>
<p>See <a class="reference internal" href="torch.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal"><span class="pre">torch.split()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt">
<code class="descname">sqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt_">
<code class="descname">sqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze">
<code class="descname">squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.squeeze" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze_">
<code class="descname">squeeze_</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.squeeze_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.std">
<code class="descname">std</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.std" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage">
<code class="descname">storage</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Storage<a class="headerlink" href="#torch.Tensor.storage" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the underlying storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_offset">
<code class="descname">storage_offset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.storage_offset" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns this tensor&#8217;s offset in the underlying storage in terms of number of
storage elements (not bytes).</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="torch.Tensor.storage_type">
<em class="property">classmethod </em><code class="descname">storage_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.storage_type" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.stride">
<code class="descname">stride</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; tuple or int<a class="headerlink" href="#torch.Tensor.stride" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the stride of the tensor.
Stride is the jump necessary to go from one element to the next one in the specified dimension dim.
Tuple is returned when no Argument is passed. So we get stride in all dimensions.
Integer value is returned when we desire stride in particular dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The desired dimension in which stride is required.</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="go">&gt;&gt;&gt;x.stride(0)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub">
<code class="descname">sub</code><span class="sig-paren">(</span><em>value</em>, <em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Subtracts a scalar or tensor from this tensor. If both <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and
<code class="xref py py-attr docutils literal"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal"><span class="pre">value</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub_">
<code class="descname">sub_</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum">
<code class="descname">sum</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.sum" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.svd">
<code class="descname">svd</code><span class="sig-paren">(</span><em>some=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.svd" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.symeig">
<code class="descname">symeig</code><span class="sig-paren">(</span><em>eigenvectors=False</em>, <em>upper=True) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.symeig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal"><span class="pre">torch.symeig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t">
<code class="descname">t</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t_">
<code class="descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.take">
<code class="descname">take</code><span class="sig-paren">(</span><em>indices</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.take" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.take" title="torch.take"><code class="xref py py-func docutils literal"><span class="pre">torch.take()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan">
<code class="descname">tan</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tan" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan_">
<code class="descname">tan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tan_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh">
<code class="descname">tanh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh_">
<code class="descname">tanh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tolist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a nested list represenation of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.topk">
<code class="descname">topk</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.topk" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trace">
<code class="descname">trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.trace" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose">
<code class="descname">transpose</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose_">
<code class="descname">transpose_</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril">
<code class="descname">tril</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril_">
<code class="descname">tril_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu">
<code class="descname">triu</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu_">
<code class="descname">triu_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trtrs">
<code class="descname">trtrs</code><span class="sig-paren">(</span><em>A</em>, <em>upper=True</em>, <em>transpose=False</em>, <em>unitriangular=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.trtrs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trtrs" title="torch.trtrs"><code class="xref py py-func docutils literal"><span class="pre">torch.trtrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc">
<code class="descname">trunc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc_">
<code class="descname">trunc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>new_type=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.type" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the type if <cite>new_type</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>new_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; The desired type</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, and the source is in pinned memory and
destination is on the GPU or vice versa, the copy is
performed asynchronously with respect to the host.
Otherwise, the argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type_as">
<code class="descname">type_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.type_as" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">())</span>
</pre></div>
</div>
<dl class="docutils">
<dt>Params:</dt>
<dd>tensor (Tensor): the tensor which has the desired type</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unfold">
<code class="descname">unfold</code><span class="sig-paren">(</span><em>dim</em>, <em>size</em>, <em>step</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unfold" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a tensor which contains all slices of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal"><span class="pre">size</span></code></a> in
the dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></a>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the original size of dimension dim, the size of dimension <cite>dim</cite>
in the returned tensor will be <cite>(sizedim - size) / step + 1</cite></p>
<p>An additional dimension of size size is appended in the returned tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension in which unfolding happens</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of each slice that is unfolded</li>
<li><strong>step</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the step between each slice</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go"> 5</span>
<span class="go"> 6</span>
<span class="go"> 7</span>
<span class="go">[torch.FloatTensor of size 7]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 1  2</span>
<span class="go"> 2  3</span>
<span class="go"> 3  4</span>
<span class="go"> 4  5</span>
<span class="go"> 5  6</span>
<span class="go"> 6  7</span>
<span class="go">[torch.FloatTensor of size 6x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="go"> 1  2</span>
<span class="go"> 3  4</span>
<span class="go"> 5  6</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.uniform_">
<code class="descname">uniform_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.uniform_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with numbers sampled from the uniform distribution:</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze">
<code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.unsqueeze" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze_">
<code class="descname">unsqueeze_</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.unsqueeze_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.var">
<code class="descname">var</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#torch.Tensor.var" title="Permalink to this definition">Â¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view">
<code class="descname">view</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns a new tensor with the same data but different size.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. A tensor must be
<a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-func docutils literal"><span class="pre">contiguous()</span></code></a> to be viewed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; Desired size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view_as">
<code class="descname">view_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.view_as" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns this tensor viewed as the size as the specified tensor.</p>
<p>This is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.zero_">
<code class="descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.zero_" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fills this tensor with zeros.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.ByteTensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">ByteTensor</code><a class="reference internal" href="_modules/torch.html#ByteTensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.ByteTensor" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The following methods are unique to <a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal"><span class="pre">torch.ByteTensor</span></code></a>.</p>
<dl class="method">
<dt id="torch.ByteTensor.all">
<code class="descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.ByteTensor.all" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns True if all elements in the tensor are non-zero, False otherwise.</p>
</dd></dl>

<dl class="method">
<dt id="torch.ByteTensor.any">
<code class="descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.ByteTensor.any" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns True if any elements in the tensor are non-zero, False otherwise.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="sparse.html" class="btn btn-neutral float-right" title="torch.sparse" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="torch.html" class="btn btn-neutral" title="torch" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
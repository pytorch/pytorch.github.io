


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.fsdp.fully_sharded_data_parallel &mdash; PyTorch 1.13 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.13 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.fsdp.fully_sharded_data_parallel</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.fsdp.fully_sharded_data_parallel</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Deque</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">NamedTuple</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span> <span class="k">as</span> <span class="nn">checkpoint_wrapper</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.distributed</span> <span class="kn">import</span> <span class="n">ProcessGroup</span>
<span class="kn">from</span> <span class="nn">torch.distributed._shard.sharded_tensor</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Shard</span><span class="p">,</span>
    <span class="n">ShardedTensor</span><span class="p">,</span>
    <span class="n">init_from_local_shards</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CHECKPOINT_PREFIX</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._comm_hooks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LOW_PRECISION_HOOKS</span><span class="p">,</span>
    <span class="n">default_hooks</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="kn">import</span> <span class="n">_get_default_group</span>
<span class="kn">from</span> <span class="nn">torch.distributed.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_replace_by_prefix</span><span class="p">,</span>
    <span class="n">_sync_params_and_buffers</span><span class="p">,</span>
    <span class="n">_to_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>

<span class="kn">from</span> <span class="nn">._optim_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_broadcast_pos_dim_tensor_states</span><span class="p">,</span>
    <span class="n">_broadcast_processed_optim_state_dict</span><span class="p">,</span>
    <span class="n">_flatten_optim_state_dict</span><span class="p">,</span>
    <span class="n">_get_param_id_to_param</span><span class="p">,</span>
    <span class="n">_get_param_id_to_param_from_optim_input</span><span class="p">,</span>
    <span class="n">_get_param_to_param_id</span><span class="p">,</span>
    <span class="n">_get_param_to_param_id_from_optim_input</span><span class="p">,</span>
    <span class="n">_optim_state_dict</span><span class="p">,</span>
    <span class="n">_process_pos_dim_tensor_state</span><span class="p">,</span>
    <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._fsdp_extensions</span> <span class="kn">import</span> <span class="n">_ext_chunk_tensor</span><span class="p">,</span> <span class="n">_ext_pre_load_state_dict_transform</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_apply_to_modules</span><span class="p">,</span>
    <span class="n">_apply_to_tensors</span><span class="p">,</span>
    <span class="n">_contains_batchnorm</span><span class="p">,</span>
    <span class="n">_free_storage</span><span class="p">,</span>
    <span class="n">_is_fsdp_flattened</span><span class="p">,</span>
    <span class="n">_override_batchnorm_mixed_precision</span><span class="p">,</span>
    <span class="n">p_assert</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.flat_param</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FlatParameter</span><span class="p">,</span>
    <span class="n">FlatParamHandle</span><span class="p">,</span>
    <span class="n">HandleConfig</span><span class="p">,</span>
    <span class="n">HandleShardingStrategy</span><span class="p">,</span>
    <span class="n">HandleTrainingState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.flatten_params_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FLAT_PARAM</span><span class="p">,</span>
    <span class="n">FPW_MODULE</span><span class="p">,</span>
    <span class="n">FlattenParamsWrapper</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.wrap</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ParamExecOrderWrapPolicy</span><span class="p">,</span>
    <span class="n">_or_policy</span><span class="p">,</span>
    <span class="n">_recursive_wrap</span><span class="p">,</span>
    <span class="n">_wrap_batchnorm_individually</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_TORCHDISTX_AVAIL</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchdistx</span> <span class="kn">import</span> <span class="n">deferred_init</span><span class="p">,</span> <span class="n">fake</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_TORCHDISTX_AVAIL</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">_TORCH_FX_AVAIL</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;fx&quot;</span><span class="p">):</span>
    <span class="n">_TORCH_FX_AVAIL</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">_TORCH_FX_AVAIL</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">._symbolic_trace</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">TracingConfig</span><span class="p">,</span>
        <span class="n">_init_execution_info</span><span class="p">,</span>
        <span class="n">_patch_tracer</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">,</span> <span class="s2">&quot;ShardingStrategy&quot;</span><span class="p">,</span> <span class="s2">&quot;MixedPrecision&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CPUOffload&quot;</span><span class="p">,</span> <span class="s2">&quot;BackwardPrefetch&quot;</span><span class="p">,</span> <span class="s2">&quot;StateDictType&quot;</span><span class="p">,</span> <span class="s2">&quot;StateDictConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FullStateDictConfig&quot;</span><span class="p">,</span> <span class="s2">&quot;LocalStateDictConfig&quot;</span><span class="p">,</span> <span class="s2">&quot;ShardedStateDictConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OptimStateKeyType&quot;</span><span class="p">,</span> <span class="s2">&quot;TrainingState_&quot;</span><span class="p">,</span> <span class="s2">&quot;clean_tensor_name&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">=</span> <span class="s2">&quot;_fsdp_wrapped_module&quot;</span>
<span class="n">FSDP_PREFIX</span> <span class="o">=</span> <span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">FPW_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>

<span class="n">_PARAM_BROADCAST_BUCKET_SIZE</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ShardingStrategy</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This specifies the sharding strategy to be used for distributed training by</span>
<span class="sd">    :class:`FullyShardedDataParallel`.</span>
<span class="sd">    FULL_SHARD: Parameters, gradients, and optimizer states are sharded. For</span>
<span class="sd">                the parameters, this algorithm all-gathers before the forward,</span>
<span class="sd">                reshards after the forward, all-gathers before the backward</span>
<span class="sd">                computation, and reshards after the backward computation. The</span>
<span class="sd">                gradients are synchronized and sharded via reduce-scatter after</span>
<span class="sd">                the backward computation. The sharded optimizer states are</span>
<span class="sd">                updated locally.</span>
<span class="sd">    SHARD_GRAD_OP: Gradients and optimizer states are sharded during</span>
<span class="sd">                   computation, and additionally parameters are sharded outside</span>
<span class="sd">                   computation. For the parameters, this algorithm all-gathers</span>
<span class="sd">                   before the forward, does not reshard after the forward, and</span>
<span class="sd">                   only reshards after the backward computation. The gradients</span>
<span class="sd">                   are synchronized and sharded via reduce-scatter after the</span>
<span class="sd">                   backward computation. The sharded optimizer states are</span>
<span class="sd">                   updated locally. Inside ``no_sync()``, the parameters are</span>
<span class="sd">                   not resharded after the backward computation.</span>
<span class="sd">    NO_SHARD: Parameters, gradients, and optimizer states are not sharded but</span>
<span class="sd">              instead replicated across ranks, similar to PyTorch&#39;s</span>
<span class="sd">              ``DistributedDataParallel`` API. The gradients are synchronized</span>
<span class="sd">              via all-reduce after the backward computation. The unsharded</span>
<span class="sd">              optimizer states are updated locally.</span>
<span class="sd">    HYBRID_SHARD(future support): Apply ``FULL_SHARD`` intra-node and</span>
<span class="sd">                                  ``NO_SHARD`` inter-node.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">FULL_SHARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SHARD_GRAD_OP</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">NO_SHARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="c1"># TODO</span>
    <span class="c1"># HYBRID_SHARD = auto()</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">MixedPrecision</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A config to enable mixed precision training with FullyShardedDataParallel.</span>
<span class="sd">    This class can be constructed with several flags:</span>
<span class="sd">        ``param_dtype`` controls the precision of model parameters, inputs, and</span>
<span class="sd">        therefore the precision under which computation happens. After forward</span>
<span class="sd">        and backward passes, FSDP parameters point to full precision shards</span>
<span class="sd">        that are kept in memory. Full precision parameters are always</span>
<span class="sd">        checkpointed.</span>
<span class="sd">        ``reduce_dtype`` controls the precision under which gradient reduction</span>
<span class="sd">        would occur, which can potentially be different than ``param_dtype``</span>
<span class="sd">        for use cases such as communication efficiency.</span>
<span class="sd">        ``buffer_dtype`` controls the precision that buffers are cast to. Note</span>
<span class="sd">        that buffers are unsharded and are cast in the first forward pass, and</span>
<span class="sd">        remain in their reduced precision state even after forward/backward</span>
<span class="sd">        passes. However, when taking checkpoints with ``state_dict``, buffers</span>
<span class="sd">        are checkpointed in their full precision (and then restored back to</span>
<span class="sd">        to their reduced precision) as expected. Note that this checkpoint</span>
<span class="sd">        support is currently limited to ``StateDictType.FULL_STATE_DICT``.</span>
<span class="sd">        ``keep_low_precision_grads``: Whether to upcast gradients back to the</span>
<span class="sd">        full parameter precision after backwards or not. This can be disabled</span>
<span class="sd">        to keep the gradients in the lower precision, which can potentially</span>
<span class="sd">        save memory if custom Optimizers are able to perform parameter updates</span>
<span class="sd">        effectively with lower precision grads.</span>

<span class="sd">    .. note:: In ``summon_full_params``, parameters are summoned in full</span>
<span class="sd">        precision but buffers are not.</span>

<span class="sd">    .. note:: Parameters and buffers are checkpointed in full precision. For</span>
<span class="sd">        buffers, this is only guaranteed to work for ``StateDictType.FULL_STATE_DICT``.</span>

<span class="sd">    .. note:: This API is experimental and subject to change.</span>

<span class="sd">    .. note:: Specification of reduced precision types must be explicit, in that</span>
<span class="sd">        if, for example, ``param_dtype`` is not specified, it will not be cast by</span>
<span class="sd">        FSDP. Thus, a config such as ``MixedPrecision(reduce_dtype=torch.float16)``</span>
<span class="sd">        will not cast buffers or parameters. Note that if a ``MixedPrecision``</span>
<span class="sd">        config is specified without a ``reduce_dtype``, gradient communication</span>
<span class="sd">        would occur in the `param_dtype` precision, if given, otherwise, in the</span>
<span class="sd">        original parameter precision.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># maintain a tensor of this dtype that the fp32 param shard will be cast to.</span>
    <span class="c1"># Will control the precision of model params, inputs, and thus compute as</span>
    <span class="c1"># well.</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Gradient communication precision.</span>
    <span class="n">reduce_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Buffer precision.</span>
    <span class="c1"># TODO: buffer + param are usually of the same type, if user specifies</span>
    <span class="c1"># param but not buffer, should we automatically make buffer be the same?</span>
    <span class="n">buffer_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">keep_low_precision_grads</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CPUOffload</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CPU offloading config. Currently, only parameter and gradient CPU</span>
<span class="sd">    offload are supported.</span>
<span class="sd">    offload_params: Offloading parameters to CPUs when these parameters are</span>
<span class="sd">                    not used for computation on GPUs. This implicitly enables</span>
<span class="sd">                    gradient offloading to CPUs in order for parameters and</span>
<span class="sd">                    gradients to be on the same device to work with optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">offload_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">class</span> <span class="nc">BackwardPrefetch</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specify where to prefetch next layer&#39;s full parameters</span>
<span class="sd">    during backward pass.</span>
<span class="sd">    BACKWARD_PRE: prefetch right before current layer&#39;s backward computation</span>
<span class="sd">                  starts, this approach will increase backward communication</span>
<span class="sd">                  and computation overalpping and potentialy improve training</span>
<span class="sd">                  performance, but it may increase the peak memory usage as</span>
<span class="sd">                  the prefetched full parameters will be kept in the GPU memory</span>
<span class="sd">                  until next layer&#39;s backward computation is done.</span>
<span class="sd">    BACKWARD_POST: prefetch right after current layer&#39;s backward computation finishes,</span>
<span class="sd">                   this approach will not increase peak memory as prefetching happens</span>
<span class="sd">                   after current layer&#39;s full parameters are freed.</span>
<span class="sd">                   It could potentially improve backward communication and computation</span>
<span class="sd">                   overlapping as it avoids all_gather and reduce_scatter are blocked</span>
<span class="sd">                   each other in the single NCCL stream. However, based on our experiments,</span>
<span class="sd">                   for some models, the backward post backward hook fire order is not always</span>
<span class="sd">                   the reversed forward computation order, so this</span>
<span class="sd">                   approach may prefetch full parameters for layers ahead of next layer,</span>
<span class="sd">                   this &#39;ahead&#39; all_gather could delay next layer&#39;s all_gather in the</span>
<span class="sd">                   single NCCL stream and cause the next layer&#39;s computation delay. So it may</span>
<span class="sd">                   cause some performance regession for some models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BACKWARD_PRE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_POST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="c1"># TODO, BACKWARD_PRE_CPU, prefetch full parameters and keep them in the CPU memory</span>


<span class="k">class</span> <span class="nc">TrainingState_</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple enum to indicate what state FSDP is in. Used for asserting</span>
<span class="sd">    to make sure APIs are called in the correct state.</span>
<span class="sd">    ..note::</span>
<span class="sd">        ``BACKWARD_PRE`` and ``BACKWARD_POST`` states are used to ensure we</span>
<span class="sd">        receives backward hooks in the correct order. It is used to catch</span>
<span class="sd">        unexpected order of hooks being called (likely due to our</span>
<span class="sd">        hook registration logic or autograd engine logic changes).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">IDLE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">FORWARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_PRE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_POST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SUMMON_FULL_PARAMS</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">StateDictType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This enum indicates that which type of ``state_dict`` the FSDP module is</span>
<span class="sd">    currently processing (returning or loading).</span>
<span class="sd">    The default value is FULL_STATE_DICT to comply the PyTorch convention.</span>
<span class="sd">    ..note::</span>
<span class="sd">        FSDP currently supports three types of ``state_dict``:</span>
<span class="sd">            1. ``state_dict/load_state_dict`: this pair of APIs return and load</span>
<span class="sd">               the non-sharded, unflattened parameters. The semantics is the</span>
<span class="sd">               same as using DDP.</span>
<span class="sd">            2. ``_local_state_dict/_load_local_state_dict``: this pair of APIs return</span>
<span class="sd">               and load local sharded, flattened parameters. The values returned</span>
<span class="sd">               by ``_local_state_dict`` can be directly used by FSDP and is only</span>
<span class="sd">               meaningful to FSDP (because parameters are flattened). Note that</span>
<span class="sd">               these APIs are meant for use via the :func:`state_dict_type`</span>
<span class="sd">               context manager as follows:</span>
<span class="sd">                   &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">                   &gt;&gt;&gt; with fsdp.state_dict_type(StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">                   ...     state = fsdp.state_dict()  # loads local state dict</span>
<span class="sd">            3. ``_sharded_state_dict/_load_sharded_state_dict``: this pair of APIs</span>
<span class="sd">               return and load sharded, unflattened parameters. The ``state_dict``</span>
<span class="sd">               return by ``sharded_state_dict`` can be used by all other parallel</span>
<span class="sd">               schemes (resharding may be required).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FULL_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">LOCAL_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SHARDED_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StateDictConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ``StateDictConfig`` is the base class for all state_dict configuration classes.</span>
<span class="sd">    Users should instantiate a child version (i.e. ``FullStateDictConfig``) in</span>
<span class="sd">    order to configure settings for the particular type of ``state_dict``</span>
<span class="sd">    implementation FSDP will use.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">FullStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ``FullStateDictConfig`` is a config class meant to be used with</span>
<span class="sd">    ``StateDictType.FULL_STATE_DICT``. Currently, it accepts two parameters,</span>
<span class="sd">    ``offload_to_cpu`` and ``rank0_only`` which can be configured to offload</span>
<span class="sd">    the full ``state_dict`` to CPU and to materialize the ``state_dict`` on</span>
<span class="sd">    rank 0 only. When used, it is recommended to enable both of these flags</span>
<span class="sd">    together to optimize memory savings when taking checkpoints. Note that</span>
<span class="sd">    this config class is meant for user via the :func:`state_dict_type`</span>
<span class="sd">    context manager as follows:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; fsdp = FSDP(model, auto_wrap_policy=...)</span>
<span class="sd">        &gt;&gt;&gt; cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</span>
<span class="sd">        &gt;&gt;&gt; with FullyShardedDataParallel.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):</span>
<span class="sd">        &gt;&gt;&gt;     state = fsdp.state_dict()</span>
<span class="sd">        &gt;&gt;&gt;     # state will be empty on non rank 0 and contain CPU tensors on rank 0.</span>
<span class="sd">        &gt;&gt;&gt; # To reload checkpoint for inference, finetuning, transfer learning, etc:</span>
<span class="sd">        &gt;&gt;&gt; model = model_fn() # Initialize model on CPU in preparation for wrapping with FSDP</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Load checkpoint only on rank 0 to avoid memory redundancy</span>
<span class="sd">        &gt;&gt;&gt;     state_dict = torch.load(&quot;my_checkpoint.pt&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     model.load_state_dict(state_dict)</span>
<span class="sd">        &gt;&gt;&gt; # All ranks initialize FSDP module as usual. ``sync_module_states`` argument</span>
<span class="sd">        &gt;&gt;&gt; # communicates loaded checkpoint states from rank 0 to rest of the world.</span>
<span class="sd">        &gt;&gt;&gt; fsdp = FSDP(model, device_id=torch.cuda.current_device(), auto_wrap_policy=..., sync_module_states=True)</span>
<span class="sd">        &gt;&gt;&gt; # After this point, all ranks have FSDP model with loaded checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LocalStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ShardedStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">_state_dict_type_to_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="n">FullStateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="n">LocalStateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="n">ShardedStateDictConfig</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">OptimStateKeyType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PARAM_NAME</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">PARAM_ID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="c1"># A handles key represents the group of `FlatParamHandle`s involved in a given</span>
<span class="c1"># module&#39;s forward. These will be all-gathered together in the pre-forward and</span>
<span class="c1"># pre-backward.</span>
<span class="n">_HandlesKey</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">_ExecOrderWarnStatus</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Used internally for execution order validation.&quot;&quot;&quot;</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>     <span class="c1"># no deviation yet</span>
    <span class="n">WARNING</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>  <span class="c1"># deviated this iteration; currently issuing warnings</span>
    <span class="n">WARNED</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>   <span class="c1"># deviated in a previous iteration</span>


<span class="k">class</span> <span class="nc">_ExecOrderData</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This contains the data structures to track the execution order. We track</span>
<span class="sd">    the pre-forward order on the *first* iteration for forward prefetching</span>
<span class="sd">    (which thus assumes static graph) and the post-forward order on *every*</span>
<span class="sd">    iteration for backward prefetching (which thus does not assume static</span>
<span class="sd">    graph but may be provide an incorrect order).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">debug_level</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">DebugLevel</span><span class="p">,</span>
        <span class="n">backward_prefetch_limit</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">forward_prefetch_limit</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Tracks the (static) pre-forward order for execution order validation</span>
        <span class="c1"># and forward prefetching</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Maps each handles key to its index in `handles_pre_forward_order`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_pre_forward_order_index</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Tracks the post-forward order for pre-backward prefetching</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_post_forward_order</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Maps each handles key to its index in `handles_post_forward_order`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_post_forward_order_index</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Gives the max number of backward/forward prefetched all-gathers by a</span>
        <span class="c1"># single module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_prefetch_limit</span> <span class="o">=</span> <span class="n">backward_prefetch_limit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_prefetch_limit</span> <span class="o">=</span> <span class="n">forward_prefetch_limit</span>

        <span class="c1"># Data structures for execution order validation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checking_order</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">debug_level</span> <span class="ow">in</span> <span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">DebugLevel</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">DebugLevel</span><span class="o">.</span><span class="n">DETAIL</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Maps each handle to its index in `all_handles`, which must be the</span>
        <span class="c1"># same across ranks for the execution order validation to work</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle_to_handle_index</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Names are prefixed from the root module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flat_param_to_prefixed_param_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Current index in the pre-forward execution order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_order_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">NONE</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">fsdp_root</span><span class="p">:</span> <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the data structures needed for checking the forward order.</span>
<span class="sd">        This should be called after a root FSDP instance has been set during</span>
<span class="sd">        lazy initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">process_group</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="n">process_group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="c1"># Fix an order over the handles, which should be the same across ranks</span>
        <span class="k">for</span> <span class="n">fsdp_module</span> <span class="ow">in</span> <span class="n">fsdp_root</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">fsdp_root</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_handles</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">all_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">handle_to_handle_index</span><span class="p">[</span><span class="n">handle</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flat_param_to_prefixed_param_names</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">Dict</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
            <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span><span class="n">fsdp_root</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># TODO (awgu): We can broadcast the metadata of rank 0&#39;s `all_handles`</span>
        <span class="c1"># to check that all ranks have the same handles in the same order.</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/79620</span>

    <span class="k">def</span> <span class="nf">get_handles_to_backward_prefetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">current_handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a :class:`list` of the handles keys of the handles to backward</span>
<span class="sd">        prefetch given the current handles key. If there are no valid handles</span>
<span class="sd">        keys to prefetch, then this returns an empty :class:`list`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">current_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_post_forward_order_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">current_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">target_index</span> <span class="o">=</span> <span class="n">current_index</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">target_handles_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_prefetch_limit</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">target_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">target_handles_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">handles_post_forward_order</span><span class="p">[</span><span class="n">target_index</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">target_index</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">target_handles_keys</span>

    <span class="k">def</span> <span class="nf">get_handles_to_forward_prefetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">current_handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a :class:`list` of the handles keys of the handles to forward</span>
<span class="sd">        prefetch given the current handles key. If there are no valid handles</span>
<span class="sd">        keys to prefetch, then this returns an empty :class:`list`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">current_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_pre_forward_order_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">current_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">target_index</span> <span class="o">=</span> <span class="n">current_index</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">target_handles_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_prefetch_limit</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">target_index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">):</span>
                <span class="k">break</span>
            <span class="n">target_handles_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">[</span><span class="n">target_index</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">target_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">target_handles_keys</span>

    <span class="k">def</span> <span class="nf">record_post_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Records ``handles`` in the post-forward order, where ``handles`` should</span>
<span class="sd">        be a group of handles used in the same module&#39;s forward. If ``handles``</span>
<span class="sd">        is empty, then it is omitted.</span>

<span class="sd">        Unlike :meth:`record_pre_forward`, this records the order *every*</span>
<span class="sd">        iteration with the expectation that the recorded order is reset in</span>
<span class="sd">        :meth:`next_iter`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">handles</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
        <span class="c1"># Only record the first usage of a handles key</span>
        <span class="k">if</span> <span class="n">handles_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_post_forward_order_index</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handles_post_forward_order</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_post_forward_order_index</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_post_forward_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">record_pre_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span> <span class="n">is_training</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Records ``handles`` in the pre-forward order on the first iteration,</span>
<span class="sd">        where ``handles`` should be a group of handles used in the same</span>
<span class="sd">        module&#39;s forward. If ``handles`` is empty, then it is omitted.</span>

<span class="sd">        On the first iteration, this checks the execution order across ranks.</span>
<span class="sd">        See :meth:`_check_order` for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">handles</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_order</span><span class="p">(</span><span class="n">handles_key</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
        <span class="c1"># Fix the order after the first iteration and only record the first</span>
        <span class="c1"># usage of a handles key</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span>
            <span class="ow">or</span> <span class="n">handles_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_pre_forward_order_index</span>
        <span class="p">):</span>
            <span class="k">return</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_pre_forward_order_index</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_order</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span> <span class="n">is_training</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks the forward execution order as long as ``is_training`` is</span>
<span class="sd">        ``True`` since checking in eval mode is not supported.</span>

<span class="sd">        - On the first iteration, this uses all-gathers to check that all ranks</span>
<span class="sd">        are all-gathering the same handles and hence ``FlatParameter`` s,</span>
<span class="sd">        raising an error if not.</span>
<span class="sd">        - On subsequent iterations, if the distributed debug level is at least</span>
<span class="sd">        INFO, then this checks that each rank is locally consistent with its</span>
<span class="sd">        own forward order from the first iteration, issuing a warning if not.</span>
<span class="sd">        This issues a warning on the first deviating iteration and stops</span>
<span class="sd">        warning thereafter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Do not check order in eval mode since the post-backward callback does</span>
        <span class="c1"># not run so it cannot be used to mark the end of an iteration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_training</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span><span class="p">:</span>
            <span class="n">msg_prefix</span> <span class="o">=</span> <span class="s2">&quot;Forward order differs across ranks:&quot;</span>
            <span class="n">local_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_handle_indices</span><span class="p">(</span>
                <span class="n">handles_key</span>
            <span class="p">)</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">handles_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>  <span class="c1"># guaranteed to be non-CPU</span>
            <span class="n">num_valid_indices</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">local_indices</span><span class="p">)</span>
            <span class="n">tensor_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>
            <span class="n">world_num_valid_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="o">**</span><span class="n">tensor_kwargs</span><span class="p">)</span>
            <span class="n">local_num_valid_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">num_valid_indices</span><span class="p">],</span> <span class="o">**</span><span class="n">tensor_kwargs</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span>
                <span class="n">world_num_valid_indices</span><span class="p">,</span>
                <span class="n">local_num_valid_indices</span><span class="p">,</span>
                <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Check that all ranks plan to all-gather the same number of</span>
            <span class="c1"># parameters</span>
            <span class="c1"># TODO (awgu): Since every module has at most one handle in the</span>
            <span class="c1"># current implementation, this should never raise the error.</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">n1</span><span class="p">),</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_num_valid_indices</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
                    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="mi">2</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">n1</span> <span class="o">!=</span> <span class="n">n2</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> rank </span><span class="si">{</span><span class="n">r1</span><span class="si">}</span><span class="s2"> is all-gathering </span><span class="si">{</span><span class="n">n1</span><span class="si">}</span><span class="s2"> parameters &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;while rank </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="s2"> is all-gathering </span><span class="si">{</span><span class="n">n2</span><span class="si">}</span><span class="s2"> parameters&quot;</span>
                    <span class="p">)</span>
            <span class="n">world_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">*</span> <span class="n">num_valid_indices</span><span class="p">,</span> <span class="o">**</span><span class="n">tensor_kwargs</span>
            <span class="p">)</span>
            <span class="n">local_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">local_indices</span><span class="p">,</span> <span class="o">**</span><span class="n">tensor_kwargs</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span>
                <span class="n">world_indices</span><span class="p">,</span> <span class="n">local_indices</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
            <span class="p">)</span>
            <span class="c1"># Check that all ranks plan to all-gather the same index parameters</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">i1</span><span class="p">),</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">i2</span><span class="p">)</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="p">(</span>
                        <span class="n">rank</span><span class="p">,</span>
                        <span class="n">world_indices</span><span class="p">[</span>
                            <span class="n">rank</span> <span class="o">*</span> <span class="n">num_valid_indices</span> <span class="p">:</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_valid_indices</span>
                        <span class="p">],</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="mi">2</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">i1</span> <span class="o">!=</span> <span class="n">i2</span><span class="p">:</span>
                    <span class="n">r1_param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_names_from_handle_indices</span><span class="p">(</span><span class="n">i1</span><span class="p">)</span>
                    <span class="n">r2_param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_names_from_handle_indices</span><span class="p">(</span><span class="n">i2</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> rank </span><span class="si">{</span><span class="n">r1</span><span class="si">}</span><span class="s2"> is all-gathering parameters &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;for </span><span class="si">{</span><span class="n">r1_param_names</span><span class="si">}</span><span class="s2"> while rank </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="s2"> is all-gathering &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;parameters for </span><span class="si">{</span><span class="n">r2_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checking_order</span><span class="p">:</span>
            <span class="c1"># Only issue warnings on the first deviating iteration and stop</span>
            <span class="c1"># checking thereafter to avoid flooding the console</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">==</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNED</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="n">msg_prefix</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># non-`None` means we should warn</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_order_index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">):</span>
                <span class="c1"># This iteration sees extra all-gather(s) compared to the first</span>
                <span class="n">msg_prefix</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;Expected to not all-gather any more parameters in the &quot;</span>
                    <span class="s2">&quot;forward but trying to all-gather parameters for &quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">expected_handles_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles_pre_forward_order</span><span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">current_order_index</span>
                <span class="p">]</span>
                <span class="k">if</span> <span class="n">expected_handles_key</span> <span class="o">!=</span> <span class="n">handles_key</span><span class="p">:</span>
                    <span class="n">expected_param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_names_from_handles</span><span class="p">(</span>
                        <span class="n">expected_handles_key</span>
                    <span class="p">)</span>
                    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Expected to all-gather for </span><span class="si">{</span><span class="n">expected_param_names</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="s2">&quot;but trying to all-gather parameters for &quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">msg_prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_names_from_handles</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>
                <span class="n">msg_suffix</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">if</span> <span class="n">param_names</span>
                    <span class="k">else</span> <span class="s2">&quot;a newly-added parameter since construction time&quot;</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Forward order differs from that of the first iteration &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">. Collectives are unchecked and may &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;give incorrect results or hang.</span><span class="se">\n</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}{</span><span class="n">msg_suffix</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNING</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_order_index</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_get_handle_indices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">...</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the handle indices (i.e. indices into ``self.all_handles``)</span>
<span class="sd">        corresponding to the handles in ``handles_key``. An entry in the</span>
<span class="sd">        returned tuple is ``None`` if the handle is invalid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles_key</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">handle</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">handle_to_handle_index</span><span class="p">:</span>
                <span class="n">indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle_to_handle_index</span><span class="p">[</span><span class="n">handle</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_names_from_handle_indices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handle_indices</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of prefixed parameter names for each handle in</span>
<span class="sd">        ``handle_indices``. If a handle index is invalid, then its prefixed</span>
<span class="sd">        parameter names are omitted from the returned list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prefixed_param_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">handle_indices</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_handles</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_handles</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">flat_param</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
            <span class="n">prefixed_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flat_param_to_prefixed_param_names</span><span class="p">[</span><span class="n">flat_param</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">prefixed_param_names</span>

    <span class="k">def</span> <span class="nf">_get_names_from_handles</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of prefixed parameter names for each handle in</span>
<span class="sd">        ``handles_key``. If a handle is invalid, then its prefixed parameter</span>
<span class="sd">        names are omitted from the returned list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prefixed_param_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles_key</span><span class="p">:</span>
            <span class="n">flat_param</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
            <span class="k">if</span> <span class="n">flat_param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">flat_param_to_prefixed_param_names</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">prefixed_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">flat_param_to_prefixed_param_names</span><span class="p">[</span><span class="n">flat_param</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">prefixed_param_names</span>

    <span class="k">def</span> <span class="nf">next_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Advances the internal data structures per iteration. This should be</span>
<span class="sd">        called in the post-backward callback since that marks the true end of</span>
<span class="sd">        an iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_to_post_forward_order_index</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handles_post_forward_order</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checking_order</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_order_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">==</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNING</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNED</span>


<span class="k">class</span> <span class="nc">_FreeEventQueue</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This tracks all pending frees corresponding to inflight all-gathers. The</span>
<span class="sd">    queueing pattern is iterative enqueues with a single dequeue per iteration</span>
<span class="sd">    once the limit ``_max_num_inflight_all_gathers`` is reached.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_queue</span><span class="p">:</span> <span class="n">Deque</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_num_inflight_all_gathers</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># empirically chosen</span>

    <span class="k">def</span> <span class="nf">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">free_event</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Enqueues a free event.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">free_event</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dequeue_if_needed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Dequeues a single event if the limit is reached.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_queue</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_num_inflight_all_gathers</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dequeue</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_dequeue</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Dequeues a free event if possible.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_queue</span><span class="p">:</span>
            <span class="n">event</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">event</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># TODO (awgu): Refactor this later</span>
<span class="n">sharding_strategy_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">:</span> <span class="n">HandleShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">:</span> <span class="n">HandleShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">SHARD_GRAD_OP</span><span class="p">:</span> <span class="n">HandleShardingStrategy</span><span class="o">.</span><span class="n">SHARD_GRAD_OP</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="FullyShardedDataParallel"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper for sharding Module parameters across data parallel workers. This</span>
<span class="sd">    is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.</span>
<span class="sd">    FullyShardedDataParallel is commonly shortened to FSDP.</span>

<span class="sd">    .. _`Xu et al.`: https://arxiv.org/abs/2004.13336</span>
<span class="sd">    .. _DeepSpeed: https://www.deepspeed.ai/</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">        &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">        &gt;&gt;&gt; optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span class="sd">        &gt;&gt;&gt; x = sharded_module(x, y=3, z=torch.Tensor([1]))</span>
<span class="sd">        &gt;&gt;&gt; loss = x.sum()</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; optim.step()</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The optimizer must be initialized *after* the module has been wrapped,</span>
<span class="sd">        since FSDP will shard parameters in-place and this will break any</span>
<span class="sd">        previously initialized optimizers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the destination CUDA device has ID ``dev_id``, either (1)</span>
<span class="sd">        ``module`` should already be placed on that device, (2) the device</span>
<span class="sd">        should be set using ``torch.cuda.set_device(dev_id)``, or (3)</span>
<span class="sd">        ``dev_id`` should be passed into the ``device_id`` constructor</span>
<span class="sd">        argument. This FSDP instance&#39;s compute device will be that destination</span>
<span class="sd">        device. For (1) and (3), the FSDP initialization always occurs on GPU.</span>
<span class="sd">        For (2), the FSDP initialization happens on ``module`` &#39;s current</span>
<span class="sd">        device, which may be CPU.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        FSDP currently does not support gradient accumulation outside</span>
<span class="sd">        ``no_sync()`` when using CPU offloading. Trying to do so yields</span>
<span class="sd">        incorrect results since FSDP will use the newly-reduced gradient</span>
<span class="sd">        instead of accumulating with any existing gradient.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Changing the original parameter variable names after construction will</span>
<span class="sd">        lead to undefined behavior.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Passing in `sync_module_states=True` flag requires module to be put</span>
<span class="sd">        on GPU, or to use ``device_id`` argument to specify a CUDA device that</span>
<span class="sd">        FSDP will move module to. This is because ``sync_module_states=True``</span>
<span class="sd">        requires GPU communication.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        As of PyTorch 1.12, FSDP only offers limited support for shared parameters</span>
<span class="sd">        (for example, setting one ``Linear`` layer&#39;s weight to another&#39;s). In</span>
<span class="sd">        particular, modules that share parameters must be wrapped as part of the</span>
<span class="sd">        same FSDP unit. If enhanced shared parameter support is needed for your</span>
<span class="sd">        use case, please ping https://github.com/pytorch/pytorch/issues/77724</span>

<span class="sd">    .. note::</span>
<span class="sd">        Inputs into FSDP ``forward`` function will be moved to compute device</span>
<span class="sd">        (same device FSDP module is on) before running ``forward``, so user does</span>
<span class="sd">        not have to manually move inputs from CPU -&gt; GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module):</span>
<span class="sd">            module to be wrapped with FSDP.</span>
<span class="sd">        process_group (Optional[ProcessGroup]):</span>
<span class="sd">            process group for sharding</span>
<span class="sd">        sharding_strategy (Optional[ShardingStrategy]):</span>
<span class="sd">            Config sharding algorithm, different sharding algorithm has trade</span>
<span class="sd">            off between memory saving and communication overhead. ``FULL_SHARD``</span>
<span class="sd">            will be chosen if sharding_strategy is not specified.</span>
<span class="sd">        cpu_offload (Optional[CPUOffload]):</span>
<span class="sd">            CPU offloading config. Currently, only parameter and gradient CPU</span>
<span class="sd">            offload is supported. It can be enabled via passing in</span>
<span class="sd">            ``cpu_offload=CPUOffload(offload_params=True)``. Note that this</span>
<span class="sd">            currently implicitly enables gradient offloading to CPU in order for</span>
<span class="sd">            params and grads to be on same device to work with optimizer. This</span>
<span class="sd">            API is subject to change. Default is ``None`` in which case there</span>
<span class="sd">            will be no offloading.</span>
<span class="sd">        auto_wrap_policy (Optional[Callable[[nn.Module, bool, int], bool]]):</span>
<span class="sd">            A callable specifying a policy to recursively wrap layers with FSDP.</span>
<span class="sd">            Note that this policy currently will only apply to child modules of</span>
<span class="sd">            the passed in module. The remainder modules are always wrapped in</span>
<span class="sd">            the returned FSDP root instance.</span>
<span class="sd">            ``size_based_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is</span>
<span class="sd">            an example of ``auto_wrap_policy`` callable, this policy wraps layers</span>
<span class="sd">            with the number of parameters larger than 100M. ``transformer_auto_wrap_policy``</span>
<span class="sd">            written in ``torch.distributed.fsdp.wrap`` is an example of ``auto_wrap_policy``</span>
<span class="sd">            callable for transformer-like model architectures. Users can supply the customized</span>
<span class="sd">            ``auto_wrap_policy`` callable that should accept following arguments:</span>
<span class="sd">            ``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``, and return</span>
<span class="sd">            a ``bool`` specifying whether the passed in ``module``` should be wrapped</span>
<span class="sd">            (if ``recurse=False``) or whether we should recurse down the subgraph of ``module``</span>
<span class="sd">            children (if ``recurse=True``). Extra customized arguments could be added to</span>
<span class="sd">            the customized ``auto_wrap_policy`` callable as well. It is a good practice to</span>
<span class="sd">            print out the sharded model and check whether the sharded model is what</span>
<span class="sd">            the application wants and then adjust accordingly.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; def custom_auto_wrap_policy(</span>
<span class="sd">                &gt;&gt;&gt;     module: nn.Module,</span>
<span class="sd">                &gt;&gt;&gt;     recurse: bool,</span>
<span class="sd">                &gt;&gt;&gt;     unwrapped_params: int,</span>
<span class="sd">                &gt;&gt;&gt;     # These are customizable for this policy function.</span>
<span class="sd">                &gt;&gt;&gt;     min_num_params: int = int(1e8),</span>
<span class="sd">                &gt;&gt;&gt; ) -&gt; bool:</span>
<span class="sd">                &gt;&gt;&gt;     return unwrapped_params &gt;= min_num_params</span>
<span class="sd">                &gt;&gt;&gt; # Configure a custom min_num_params</span>
<span class="sd">                &gt;&gt;&gt; my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=1e5)</span>

<span class="sd">        backward_prefetch (Optional[BackwardPrefetch]):</span>
<span class="sd">            This is an experimental feature that is subject to change in the</span>
<span class="sd">            the near future. It allows users to enable two different backward_prefetch</span>
<span class="sd">            algorithms to help backward communication and computation overlapping.</span>
<span class="sd">            Pros and cons of each algorithm is explained in the class ``BackwardPrefetch``.</span>
<span class="sd">        mixed_precision (Optional[MixedPrecision]): A ``MixedPrecision`` instance</span>
<span class="sd">            describing the mixed precision training config to be used. ``MixedPrecision``</span>
<span class="sd">            supports configuring parameter, buffer, and gradient communication dtype. Note</span>
<span class="sd">            that only floating point data is cast to the reduced precision. This allows</span>
<span class="sd">            users potential memory saving and training speedup while trading off</span>
<span class="sd">            accuracy during model training. If ``None``, no mixed precision is applied.</span>
<span class="sd">            Note that if ``mixed_precision`` is enabled for FSDP model that</span>
<span class="sd">            contains ``BatchNorm`` with ``auto_wrap_policy``, FSDP will take</span>
<span class="sd">            care to disable mixed precision for ``BatchNorm`` units by wrapping</span>
<span class="sd">            them separately in their own FSDP unit with ``mixed_precision=None``.</span>
<span class="sd">            This is done because several ``BatchNorm`` kernels do not implement</span>
<span class="sd">            reduced type support at the moment. If individually wrapping the model,</span>
<span class="sd">            users must take care to set ``mixed_precision=None`` for</span>
<span class="sd">            ``BatchNorm`` units.</span>
<span class="sd">            (Default: ``None``)</span>
<span class="sd">        ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose</span>
<span class="sd">            own parameters and child modules&#39; parameters and buffers are</span>
<span class="sd">            ignored by this instance. None of the modules directly in</span>
<span class="sd">            ``ignored_modules`` should be :class:`FullyShardedDataParallel`</span>
<span class="sd">            instances, and any child modules that are already-constructed</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances will not be ignored if</span>
<span class="sd">            they are nested under this instance. This argument may be used to</span>
<span class="sd">            avoid sharding specific parameters at module granularity when using an</span>
<span class="sd">            ``auto_wrap_policy`` or if parameters&#39; sharding is not managed by</span>
<span class="sd">            FSDP. (Default: ``None``)</span>
<span class="sd">        param_init_fn (Optional[Callable[[nn.Module], None]]):</span>
<span class="sd">            A ``Callable[torch.nn.Module] -&gt; None`` that</span>
<span class="sd">            specifies how modules that are currently on the meta device should be initialized</span>
<span class="sd">            onto an actual device. Note that as of v1.12, we detect modules on the meta</span>
<span class="sd">            device via ``is_meta`` check and apply a default initialization that calls</span>
<span class="sd">            ``reset_parameters`` method on the passed in ``nn.Module`` if ``param_init_fn``</span>
<span class="sd">            is not specified, otherwise we run ``param_init_fn`` to initialize the passed</span>
<span class="sd">            in ``nn.Module``. In particular, this means that if ``is_meta=True`` for any</span>
<span class="sd">            module parameters for modules that will be wrapped with FSDP and ``param_init_fn``</span>
<span class="sd">            is not specified, we assume your module properly implements a ``reset_paramters()``</span>
<span class="sd">            and will throw errors if not. Note that additionally, we offer support for modules</span>
<span class="sd">            initialized with torchdistX&#39;s (https://github.com/pytorch/torchdistX)</span>
<span class="sd">            ``deferred_init`` API. In this case, deferred modules would be initialized</span>
<span class="sd">            by a default initialization function that calls torchdistX&#39;s</span>
<span class="sd">            ``materialize_module``, or the passed in ``param_init_fn``, if it is not</span>
<span class="sd">            ``None``. The same ``Callable`` is applied to initialize all meta modules.</span>
<span class="sd">            Note that this initialization function is applied before doing any FSDP sharding</span>
<span class="sd">            logic.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">                &gt;&gt;&gt; module = MyModule(device=&quot;meta&quot;)</span>
<span class="sd">                &gt;&gt;&gt; def my_init_fn(module):</span>
<span class="sd">                &gt;&gt;&gt;     # responsible for initializing a module, such as with reset_parameters</span>
<span class="sd">                &gt;&gt;&gt;     ...</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)</span>
<span class="sd">                &gt;&gt;&gt; print(next(fsdp_model.parameters()).device) # current CUDA device</span>
<span class="sd">                &gt;&gt;&gt; # With torchdistX</span>
<span class="sd">                &gt;&gt;&gt; module = deferred_init.deferred_init(MyModule, device=&quot;cuda&quot;)</span>
<span class="sd">                &gt;&gt;&gt; # Will initialize via deferred_init.materialize_module().</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)</span>

<span class="sd">        device_id (Optional[Union[int, torch.device]]): An ``int`` or ``torch.device``</span>
<span class="sd">            describing the CUDA device the FSDP module should be moved to determining where</span>
<span class="sd">            initialization such as sharding takes place. If this argument is not specified</span>
<span class="sd">            and ``module`` is on CPU, we issue a warning mentioning that this argument can</span>
<span class="sd">            be specified for faster initialization. If specified, resulting FSDP instances</span>
<span class="sd">            will reside on this device, including moving ignored modules&#39; parameters if</span>
<span class="sd">            needed. Note that if ``device_id`` is specified but ``module`` is already on a</span>
<span class="sd">            different CUDA device, an error will be thrown. (Default: ``None``)</span>
<span class="sd">        sync_module_states (bool): If ``True``, each individually wrapped FSDP unit will broadcast</span>
<span class="sd">            module parameters from rank 0 to ensure they are the same across all ranks after</span>
<span class="sd">            initialization. This helps ensure model parameters are the same across ranks</span>
<span class="sd">            before starting training, but adds communication overhead to ``__init__``, as at least</span>
<span class="sd">            one broadcast is triggered per individually wrapped FSDP unit.</span>
<span class="sd">            This can also help load checkpoints taken by ``state_dict`` and to be loaded by</span>
<span class="sd">            ``load_state_dict`` in a memory efficient way. See documentation for</span>
<span class="sd">            :class:`FullStateDictConfig` for an example of this. (Default: ``False``)</span>
<span class="sd">        forward_prefetch (bool): If ``True``, then FSDP *explicitly* prefetches</span>
<span class="sd">            the next upcoming all-gather while executing in the forward pass.</span>
<span class="sd">            This may improve communication and computation overlap for CPU</span>
<span class="sd">            bound workloads. This should only be used for static graph models</span>
<span class="sd">            since the forward order is fixed based on the first iteration&#39;s</span>
<span class="sd">            execution. (Default: ``False``)</span>
<span class="sd">        limit_all_gathers (bool): If ``False``, then FSDP allows the CPU</span>
<span class="sd">            thread to schedule all-gathers without any extra synchronization.</span>
<span class="sd">            If ``True``, then FSDP explicitly synchronizes the CPU thread to</span>
<span class="sd">            prevent too many in-flight all-gathers. This ``bool`` only affects</span>
<span class="sd">            the sharded strategies that schedule all-gathers. Enabling this can</span>
<span class="sd">            help lower the number of CUDA malloc retries.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShardingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cpu_offload</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CPUOffload</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackwardPrefetch</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MixedPrecision</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignored_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">param_init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sync_module_states</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">forward_prefetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">limit_all_gathers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_wrap_policy</span><span class="p">,</span> <span class="n">ParamExecOrderWrapPolicy</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_param_exec_order_wrap_policy</span><span class="p">(</span>
                <span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
                <span class="n">process_group</span><span class="o">=</span><span class="n">process_group</span><span class="p">,</span>
                <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">sharding_strategy</span><span class="p">,</span>
                <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
                <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">auto_wrap_policy</span><span class="p">,</span>
                <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">backward_prefetch</span><span class="p">,</span>
                <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mixed_precision</span><span class="p">,</span>
                <span class="n">ignored_modules</span><span class="o">=</span><span class="n">ignored_modules</span><span class="p">,</span>
                <span class="n">param_init_fn</span><span class="o">=</span><span class="n">param_init_fn</span><span class="p">,</span>
                <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
                <span class="n">sync_module_states</span><span class="o">=</span><span class="n">sync_module_states</span><span class="p">,</span>
                <span class="n">forward_prefetch</span><span class="o">=</span><span class="n">forward_prefetch</span><span class="p">,</span>
                <span class="n">limit_all_gathers</span><span class="o">=</span><span class="n">limit_all_gathers</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.distributed.fsdp&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ignored_modules</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_modules</span><span class="p">)</span>
        <span class="n">ignored_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ignored_params</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_modules</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_buffer_names</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">auto_wrap_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">auto_wrap_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="n">module</span><span class="p">,</span>
                <span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">:</span> <span class="n">auto_wrap_policy</span><span class="p">,</span>
                <span class="s2">&quot;wrapper_cls&quot;</span><span class="p">:</span> <span class="n">FullyShardedDataParallel</span><span class="p">,</span>
                <span class="s2">&quot;ignored_modules&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_modules</span><span class="p">,</span>
                <span class="s2">&quot;ignored_params&quot;</span><span class="p">:</span> <span class="n">ignored_params</span><span class="p">,</span>
                <span class="s2">&quot;only_wrap_children&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># avoid double wrapping the root</span>
            <span class="p">}</span>
            <span class="n">fsdp_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;process_group&quot;</span><span class="p">:</span> <span class="n">process_group</span><span class="p">,</span>
                <span class="s2">&quot;sharding_strategy&quot;</span><span class="p">:</span> <span class="n">sharding_strategy</span><span class="p">,</span>
                <span class="s2">&quot;cpu_offload&quot;</span><span class="p">:</span> <span class="n">cpu_offload</span><span class="p">,</span>
                <span class="s2">&quot;backward_prefetch&quot;</span><span class="p">:</span> <span class="n">backward_prefetch</span><span class="p">,</span>
                <span class="s2">&quot;mixed_precision&quot;</span><span class="p">:</span> <span class="n">mixed_precision</span><span class="p">,</span>
                <span class="s2">&quot;param_init_fn&quot;</span><span class="p">:</span> <span class="n">param_init_fn</span><span class="p">,</span>
                <span class="s2">&quot;device_id&quot;</span><span class="p">:</span> <span class="n">device_id</span><span class="p">,</span>
                <span class="s2">&quot;sync_module_states&quot;</span><span class="p">:</span> <span class="n">sync_module_states</span><span class="p">,</span>
                <span class="s2">&quot;forward_prefetch&quot;</span><span class="p">:</span> <span class="n">forward_prefetch</span><span class="p">,</span>
                <span class="s2">&quot;limit_all_gathers&quot;</span><span class="p">:</span> <span class="n">limit_all_gathers</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_auto_wrap</span><span class="p">(</span><span class="n">auto_wrap_kwargs</span><span class="p">,</span> <span class="n">fsdp_kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">cpu_offload</span> <span class="ow">or</span> <span class="n">CPUOffload</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">=</span> <span class="n">backward_prefetch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward_prefetch</span> <span class="o">=</span> <span class="n">forward_prefetch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span> <span class="o">=</span> <span class="n">limit_all_gathers</span>
        <span class="n">backward_prefetch_limit</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">forward_prefetch_limit</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># We clamp the strategy to `NO_SHARD` for world size of 1 since they</span>
        <span class="c1"># are currently functionally equivalent. This may change if/when we</span>
        <span class="c1"># integrate FSDP with MoE.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sharding_strategy</span> <span class="o">=</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">=</span> <span class="n">sharding_strategy</span> <span class="ow">or</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">mixed_precision</span> <span class="ow">or</span> <span class="n">MixedPrecision</span><span class="p">()</span>
        <span class="c1"># Save a mapping from fully prefixed buffer name to its original dtype</span>
        <span class="c1"># since for mixed precision, buffers are restored to their original</span>
        <span class="c1"># dtype for model checkpointing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_name_to_orig_dtype</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_single_device_module</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">)</span>
        <span class="n">device_from_device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_device_from_device_id</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_materialize_module</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">param_init_fn</span><span class="p">,</span> <span class="n">device_from_device_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_move_module_to_device</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">,</span> <span class="n">device_from_device_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_compute_device</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">,</span> <span class="n">device_from_device_id</span><span class="p">)</span>
        <span class="n">params_to_flatten</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_orig_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">sync_module_states</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_module_states</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">params_to_flatten</span><span class="p">)</span>

        <span class="c1"># This FSDP instance&#39;s handles should inherit the same process group,</span>
        <span class="c1"># compute device, CPU offload, and mixed precision settings. However,</span>
        <span class="c1"># different sharding strategies are allowed.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">HandleConfig</span><span class="p">(</span>
            <span class="n">sharding_strategy_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">reduce_dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">keep_low_precision_grads</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span> <span class="o">=</span> <span class="n">FlattenParamsWrapper</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">params_to_flatten</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
            <span class="n">config</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_orig_params_flattened</span><span class="p">(</span><span class="n">ignored_params</span><span class="p">)</span>
        <span class="c1"># Invariant: `self.params` contains exactly the `FlatParameter`s of the</span>
        <span class="c1"># handles in `self._handles`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">has_params</span><span class="p">:</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">handle</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_register_param_handle</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span> <span class="ow">and</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">_flat_param_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_comm_hook</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_comm_hook_state</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hook_registered</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Used to prevent running the pre-backward hook multiple times</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_pre_backward_hook</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># `None` indicates not yet set</span>
        <span class="c1"># The following attributes are owned by the root FSDP instance and</span>
        <span class="c1"># shared with non-root FSDP instances</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_free_event_queue</span> <span class="o">=</span> <span class="n">_FreeEventQueue</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_debug_level</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_debug_level</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span> <span class="o">=</span> <span class="n">_ExecOrderData</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_level</span><span class="p">,</span>
            <span class="n">backward_prefetch_limit</span><span class="p">,</span>
            <span class="n">forward_prefetch_limit</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Used for guarding against mistargeted backward prefetches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Used for guarding against mistargeted forward prefetches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_forward_unshard</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># The data structures use tuples of handles to generalize over the case</span>
        <span class="c1"># where a module&#39;s forward involves multiple handles.</span>

        <span class="c1"># `_state_dict_type` controls the `state_dict()` behavior, which is</span>
        <span class="c1"># implemented using post-save and pre-load hooks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">FullStateDictConfig</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_post_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_post_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_post_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_load_state_dict_pre_hook</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook</span><span class="p">,</span> <span class="n">with_module</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_pre_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_pre_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_pre_load_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_load_state_dict_post_hook</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_post_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_post_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_post_load_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_get_ignored_modules</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">root_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">_ignored_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks that ``_ignored_modules`` is an iterable of ``nn.Module`` s</span>
<span class="sd">        without any FSDP instances, and returns the modules contained in their</span>
<span class="sd">        module subtrees as a :class:`set`. Nested FSDP instances are excluded,</span>
<span class="sd">        but their already-computed ignored modules are included.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_ignored_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">msg_prefix</span> <span class="o">=</span> <span class="s2">&quot;`ignored_modules` should be an iterable of `torch.nn.Module`s &quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ignored_root_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_ignored_modules</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg_prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">_ignored_modules</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">ignored_root_modules</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg_prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but got an iterable with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`ignored_modules` should not include FSDP modules&quot;</span><span class="p">)</span>
        <span class="c1"># Include child modules and exclude nested FSDP modules themselves</span>
        <span class="n">ignored_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">child</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">ignored_root_modules</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">FlattenParamsWrapper</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">root_module</span> <span class="ow">in</span> <span class="n">ignored_modules</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Trying to ignore the top-level module passed into the FSDP &quot;</span>
                <span class="s2">&quot;constructor itself will result in all parameters being &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;ignored and is not well-supported: </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Include nested FSDP modules&#39; ignored modules</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">root_module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="s2">&quot;_ignored_modules&quot;</span><span class="p">)</span>
                <span class="n">ignored_modules</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">submodule</span><span class="o">.</span><span class="n">_ignored_modules</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ignored_modules</span>

    <span class="k">def</span> <span class="nf">_get_ignored_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">root_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_modules</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the parameters of the modules in ``ignored_modules``,</span>
<span class="sd">        excluding any :class:`FlatParameter` s, and their fully prefixed names,</span>
<span class="sd">        both as :class:`set` s.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ignored_params</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">p</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">ignored_modules</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_fsdp_flattened</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Conservatively include all shared parameters&#39; names</span>
        <span class="n">param_to_unflat_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span>
            <span class="n">root_module</span><span class="p">,</span>
            <span class="n">dedup_shared_params</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ignored_param_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ignored_params</span><span class="p">:</span>
            <span class="n">unflat_param_names</span> <span class="o">=</span> <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
            <span class="n">clean_names</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unflat_param_names</span><span class="p">:</span>
                <span class="c1"># Clean any module wrapper prefixes in case of nested wrapping</span>
                <span class="n">clean_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
            <span class="n">ignored_param_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">clean_names</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ignored_params</span><span class="p">,</span> <span class="n">ignored_param_names</span>

    <span class="k">def</span> <span class="nf">_get_buffer_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the fully prefixed names of all buffers in the module hierarchy</span>
<span class="sd">        rooted at ``root_module`` as a class:`set`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">module_fn</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">buffer_names</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
            <span class="c1"># For FSDP modules, only add the entry when considering the</span>
            <span class="c1"># contained `FlattenParamsWrapper` to avoid duplication</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                    <span class="c1"># Clean module wrapper prefixes in case of nested wrapping</span>
                    <span class="n">prefixed_buffer_name</span> <span class="o">=</span> <span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">buffer_name</span><span class="p">)</span>
                    <span class="n">buffer_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">prefixed_buffer_name</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">return_fn</span><span class="p">(</span><span class="n">buffer_names</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">buffer_names</span>

        <span class="n">buffer_names</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">_apply_to_modules</span><span class="p">(</span>
            <span class="n">root_module</span><span class="p">,</span>
            <span class="n">module_fn</span><span class="p">,</span>
            <span class="n">return_fn</span><span class="p">,</span>
            <span class="n">buffer_names</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_auto_wrap</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">auto_wrap_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">fsdp_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively auto wraps the root module given by the key &quot;module&quot; in</span>
<span class="sd">        ``auto_wrap_kwargs`` with the arguments in ``auto_wrap_kwargs`` and</span>
<span class="sd">        ``fsdp_kwargs``.</span>

<span class="sd">        Precondition: ``auto_wrap_policy`` contains the arguments expected by</span>
<span class="sd">        ``_recursive_wrap()``, where ``auto_wrap_policy`` is not ``None``.</span>
<span class="sd">        ``fsdp_kwargs`` contains all FSDP arguments except ``module``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">auto_wrap_kwargs</span><span class="p">[</span><span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">]</span>
        <span class="n">root_module</span> <span class="o">=</span> <span class="n">auto_wrap_kwargs</span><span class="p">[</span><span class="s2">&quot;module&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">auto_wrap_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># For auto wrapping, submodules should not already be wrapped with FSDP</span>
        <span class="c1"># since double wrapping is not supported</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">root_module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2"> to NOT be FullyShardedDataParallel &quot;</span>
                    <span class="s2">&quot;if using an `auto_wrap_policy`&quot;</span>
                <span class="p">)</span>
        <span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">fsdp_kwargs</span><span class="p">[</span><span class="s2">&quot;mixed_precision&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">_contains_batchnorm</span><span class="p">(</span><span class="n">root_module</span><span class="p">):</span>
            <span class="n">_override_batchnorm_mixed_precision</span><span class="p">(</span><span class="n">root_module</span><span class="p">)</span>
            <span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                <span class="n">_or_policy</span><span class="p">,</span> <span class="n">policies</span><span class="o">=</span><span class="p">[</span><span class="n">_wrap_batchnorm_individually</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Both mixed precision and an `auto_wrap_policy` were specified &quot;</span>
                <span class="s2">&quot;for FSDP, where the wrapped module has batch norm submodules. &quot;</span>
                <span class="s2">&quot;The batch norm submodules will be wrapped as separate FSDP &quot;</span>
                <span class="s2">&quot;instances with mixed precision disabled since some batch norm &quot;</span>
                <span class="s2">&quot;kernels do not support low precision.&quot;</span>
            <span class="p">)</span>
            <span class="n">auto_wrap_kwargs</span><span class="p">[</span><span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto_wrap_policy</span>
        <span class="n">_recursive_wrap</span><span class="p">(</span><span class="o">**</span><span class="n">auto_wrap_kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">fsdp_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_single_device_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_params</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Raises an error if ``module`` has original parameters on multiple</span>
<span class="sd">        devices, ignoring the parameters in ``ignored_params``. Thus, after</span>
<span class="sd">        this method, the module must be either fully on the CPU or fully on a</span>
<span class="sd">        non-CPU device.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_orig_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FSDP only supports single device modules but got params on </span><span class="si">{</span><span class="n">devices</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_device_from_device_id</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">device_id</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_id</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FSDP got the argument `device_id` </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2"> on rank &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">, which does not have an explicit index. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;FSDP will use the current device </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;If this is incorrect, please explicitly call `torch.cuda.set_device()` &quot;</span>
                <span class="s2">&quot;before FSDP initialization or pass in the explicit device &quot;</span>
                <span class="s2">&quot;index as the `device_id` argument.&quot;</span>
            <span class="p">)</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">device</span>

    <span class="k">def</span> <span class="nf">_materialize_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">param_init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">]],</span>
        <span class="n">device_from_device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Materializes the wrapped module ``module`` in place if needed: either</span>
<span class="sd">        if the module has parameters that use meta device or are torchdistX</span>
<span class="sd">        fake tensors.</span>

<span class="sd">        This method uses ``param_init_fn`` to materialize the module if the</span>
<span class="sd">        function is not ``None`` and falls back to default behavior otherwise.</span>
<span class="sd">        For meta device, this moves the module to ``device_from_device_id`` if</span>
<span class="sd">        it is not ``None`` or the current device otherwise and calls</span>
<span class="sd">        ``reset_parameters()``, and for torchdistX fake tensors, this calls</span>
<span class="sd">        ``deferred_init.materialize_module()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">is_meta_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">is_meta</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">is_torchdistX_deferred_init</span> <span class="o">=</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">is_meta_module</span>
            <span class="ow">and</span> <span class="n">_TORCHDISTX_AVAIL</span>
            <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">is_fake</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">is_meta_module</span> <span class="ow">or</span> <span class="n">is_torchdistX_deferred_init</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="n">param_init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">param_init_fn</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">param_init_fn</span><span class="si">}</span><span class="s2"> to be callable but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">param_init_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">param_init_fn</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_meta_module</span><span class="p">:</span>
            <span class="c1"># Run default meta device initialization</span>
            <span class="n">materialization_device</span> <span class="o">=</span> <span class="n">device_from_device_id</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
            <span class="n">module</span><span class="o">.</span><span class="n">to_empty</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">materialization_device</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to call `reset_parameters()` for module on meta &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;device with error </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">. Please ensure your &quot;</span>
                    <span class="s2">&quot;module implements a `reset_parameters()` method.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>
        <span class="k">elif</span> <span class="n">is_torchdistX_deferred_init</span><span class="p">:</span>
            <span class="c1"># Run default torchdistX initialization</span>
            <span class="n">deferred_init</span><span class="o">.</span><span class="n">materialize_module</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span>
                <span class="n">check_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_move_module_to_device</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_params</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="n">device_from_device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Moves ``module`` depending on ``device_from_device_id`` and its current</span>
<span class="sd">        device. This includes moving ignored modules&#39; parameters.</span>

<span class="sd">        - If ``device_from_device_id`` is not ``None``, then this moves</span>
<span class="sd">        ``module`` to the device.</span>
<span class="sd">        - If ``device_from_device_id`` is ``None``, then this does not move</span>
<span class="sd">        ``module`` but warns the user if it is on CPU.</span>

<span class="sd">        Precondition: ``_check_single_device_module()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cpu_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">param</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_orig_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># no original parameters to manage</span>
        <span class="k">if</span> <span class="n">device_from_device_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">cpu_device</span><span class="p">:</span>
                <span class="c1"># NOTE: This includes moving ignored modules&#39; parameters.</span>
                <span class="n">module</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device_from_device_id</span><span class="p">)</span>
                <span class="c1"># TODO: This is a temporary fix to move already- constructed</span>
                <span class="c1"># `FlatParameter`s back to CPU if needed. This is needed to</span>
                <span class="c1"># make CPU offload work with `device_id`.</span>
                <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">submodule</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span>
                    <span class="p">):</span>
                        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                                <span class="n">handle</span><span class="o">.</span><span class="n">_flat_param_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">cpu_device</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Module is put on CPU and will thus have flattening and sharding&quot;</span>
                <span class="s2">&quot; run on CPU, which is less efficient than on GPU. We recommend passing in &quot;</span>
                <span class="s2">&quot;`device_id` argument which will enable FSDP to put module on GPU device,&quot;</span>
                <span class="s2">&quot; module must also be on GPU device to work with `sync_module_states=True` flag&quot;</span>
                <span class="s2">&quot; which requires GPU communication.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_compute_device</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_params</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="n">device_from_device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determines and returns this FSDP instance&#39;s compute device. If the</span>
<span class="sd">        module is already on a non-CPU device, then the compute device is that</span>
<span class="sd">        non-CPU device. If the module is on CPU, then the compute device is the</span>
<span class="sd">        current device.</span>

<span class="sd">        Since this method should be called after materializing the module, any</span>
<span class="sd">        non-CPU device should not be meta device. For now, the compute device</span>
<span class="sd">        is always a CUDA GPU device with its explicit index.</span>

<span class="sd">        Precondition: ``_check_single_device_module()`` and</span>
<span class="sd">        ``_move_module_to_device()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If the module is on GPU already, then that GPU device has priority</span>
        <span class="c1"># over the current device</span>
        <span class="n">param</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_orig_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="n">compute_device</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">compute_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">device_from_device_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">compute_device</span> <span class="o">!=</span> <span class="n">device_from_device_id</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Inconsistent compute device and `device_id` on rank &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">compute_device</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">device_from_device_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">compute_device</span>

    <span class="k">def</span> <span class="nf">_sync_module_states</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Synchronizes module states (i.e. parameters ``params`` and all</span>
<span class="sd">        not-yet-synced buffers) by broadcasting from rank 0 to all ranks.</span>

<span class="sd">        Precondition: ``sync_module_states == True`` and ``self.process_group``</span>
<span class="sd">        has been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Module has CPU parameters, but sync_module_states=True is specified.&quot;</span>
                <span class="s2">&quot;This only works for GPU module, please specify `device_id` argument or move&quot;</span>
                <span class="s2">&quot; module to GPU before init.&quot;</span>
            <span class="p">)</span>
        <span class="n">module_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO (awgu): When exposing the original parameters, we need to also</span>
        <span class="c1"># use this attribute to prevent re-synchronizing parameters.</span>
        <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
            <span class="c1"># Avoid re-synchronizing buffers in case of nested wrapping</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s2">&quot;_fsdp_synced&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">buffer</span><span class="o">.</span><span class="n">_fsdp_synced</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">module_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
        <span class="n">module_states</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">_sync_params_and_buffers</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">module_states</span><span class="p">,</span> <span class="n">_PARAM_BROADCAST_BUCKET_SIZE</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_orig_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_params</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns an iterator over the original parameters in ``module``,</span>
<span class="sd">        ignoring the parameters in ``ignored_params`` and any ``FlatParameter``</span>
<span class="sd">        s (which may be present due to nested FSDP wrapping).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param_gen</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">param</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">param_gen</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_fsdp_flattened</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">param</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_check_orig_params_flattened</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ignored_params</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks that all original parameters have been flattened and hence made</span>
<span class="sd">        invisible to ``named_parameters()``. This should be called as a sanity</span>
<span class="sd">        check after flattening the wrapped module&#39;s parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_fsdp_flattened</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Found an unflattened parameter: </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_register_param_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">:</span> <span class="n">FlatParamHandle</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Registers the parameter handle to this FSDP instance.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">handle</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_unshard</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unshards the handles in ``handles``. If the handles are in</span>
<span class="sd">        :meth:`summon_full_params` and are using mixed precision, then they are</span>
<span class="sd">        forced to full precision.</span>

<span class="sd">        Postcondition: Each handle&#39;s ``FlatParameter`` &#39;s data is the padded</span>
<span class="sd">        unsharded flattened parameter on the compute device.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">handles</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span><span class="p">:</span>
            <span class="n">event</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_free_event_queue</span><span class="o">.</span><span class="n">dequeue_if_needed</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">event</span><span class="p">:</span>
                <span class="n">event</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">any_ran_pre_unshard</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;pre_all_gather&quot;</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
                <span class="n">ran_pre_unshard</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">pre_unshard</span><span class="p">()</span>
                <span class="n">any_ran_pre_unshard</span> <span class="o">=</span> <span class="n">any_ran_pre_unshard</span> <span class="ow">or</span> <span class="n">ran_pre_unshard</span>
        <span class="k">if</span> <span class="n">any_ran_pre_unshard</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;pre_all_gather&quot;</span><span class="p">])</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
                <span class="n">handle</span><span class="o">.</span><span class="n">unshard</span><span class="p">()</span>
                <span class="n">handle</span><span class="o">.</span><span class="n">post_unshard</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_reshard</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>  <span class="c1"># unused</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
        <span class="n">free_unsharded_flat_params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reshards the handles in ``handles``. ``free_unsharded_flat_params``</span>
<span class="sd">        should have the same length as ``handles``, and each element should</span>
<span class="sd">        give whether the corresponding handle should free its padded unsharded</span>
<span class="sd">        flattened parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">handles</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">p_assert</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">free_unsharded_flat_params</span><span class="p">),</span>
            <span class="s2">&quot;Expects both lists to have equal length but got &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">free_unsharded_flat_params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">handle</span><span class="p">,</span> <span class="n">free_unsharded_flat_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">handles</span><span class="p">,</span>
            <span class="n">free_unsharded_flat_params</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">reshard</span><span class="p">(</span><span class="n">free_unsharded_flat_param</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span> <span class="ow">and</span> <span class="n">free_unsharded_flat_param</span><span class="p">:</span>
                <span class="n">free_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
                <span class="n">free_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_free_event_queue</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="n">free_event</span><span class="p">)</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">post_reshard</span><span class="p">()</span>
        <span class="c1"># Since we prefetch entire handles keys at a time, conservatively mark</span>
        <span class="c1"># the entire key as no longer prefetched once we free at least one</span>
        <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">free_unsharded_flat_params</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">handles_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the wrapped module (like :class:`DistributedDataParallel`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">FlattenParamsWrapper</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">module</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward missing attributes to wrapped module.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward indexing calls in case the module is a nn.Sequential.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

    <span class="k">def</span> <span class="nf">check_is_root</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>

<div class="viewcode-block" id="FullyShardedDataParallel.fsdp_modules"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">fsdp_modules</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">root_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all nested FSDP instances, possibly including ``module`` itself</span>
<span class="sd">        and only including FSDP root modules if ``root_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module, which may or may not be an</span>
<span class="sd">                ``FSDP`` module.</span>
<span class="sd">            root_only (bool): Whether to return only FSDP root modules.</span>
<span class="sd">                (Default: ``False``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[FullyShardedDataParallel]: FSDP modules that are nested in</span>
<span class="sd">            the input ``module``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">submodule</span> <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="ow">and</span>
            <span class="p">(</span><span class="ow">not</span> <span class="n">root_only</span> <span class="ow">or</span> <span class="n">submodule</span><span class="o">.</span><span class="n">check_is_root</span><span class="p">())</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.apply"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.apply">[docs]</a>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">        as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">        (see also :ref:`nn-init-doc`).</span>

<span class="sd">        Compared to ``torch.nn.Module.apply``, this version additionally gathers</span>
<span class="sd">        the full parameters before applying ``fn``. It should not be called from</span>
<span class="sd">        within another ``summon_full_params`` context.</span>

<span class="sd">        Args:</span>
<span class="sd">            fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>

<span class="sd">        Returns:</span>
<span class="sd">            Module: self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">uninitialized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="c1"># Reset lazy init that might be called by _summon_full_params, since</span>
        <span class="c1"># it could have set is_root incorrectly for non-root FSDP instances.</span>
        <span class="k">if</span> <span class="n">uninitialized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_reset_lazy_init</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ret</span></div>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        parameters or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        buffers or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">buffer_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        gradient reduction or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">reduce_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_keep_low_precision_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">keep_low_precision_grads</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_low_precision_hook_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wether a low precision hook is registered or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="ow">in</span> <span class="n">LOW_PRECISION_HOOKS</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_cast_fp_inputs_to_dtype</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Casts floating point tensors in ``args`` and ``kwargs`` to the</span>
<span class="sd">        precision given by ``dtype``, while respecting the existing</span>
<span class="sd">        ``requires_grad`` on the tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">cast_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="c1"># Explicitly copy over `requires_grad` since this runs inside</span>
            <span class="c1"># `torch.no_grad()`</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
                <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="k">return</span> <span class="n">y</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span>
                <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_cast_buffers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Move all buffers to the given *device* and *dtype*.</span>
<span class="sd">        If *device* is not given, then it will default to</span>
<span class="sd">        ``self.compute_device``, otherwise buffer will be moved to ``device``.</span>
<span class="sd">        In the case of nested FSDP instances, we will respect the child instance&#39;s</span>
<span class="sd">        ``compute_device`` configuration.</span>
<span class="sd">        If *dtype* is given, it must be a mapping of buffer name to buffer dtype,</span>
<span class="sd">            and this argument is currently only given to restore back to original</span>
<span class="sd">            buffer types during checkpoint. If *dtype* is not given, and we are</span>
<span class="sd">            in mixed precision training, the buffer will be cast to buffer_dtype,</span>
<span class="sd">            otherwise the buffer will not be cast.</span>
<span class="sd">        Args:</span>
<span class="sd">            device (torch.device, Optional):</span>
<span class="sd">                device to cast buffers to (defaults to compute_device)</span>
<span class="sd">            dtype: (Dict[str, torch.dtype], Optional):</span>
<span class="sd">                Mapping of buffer name to their dtype to cast to.</span>
<span class="sd">            memo (Set, Optional):</span>
<span class="sd">                set of modules that have already been processed</span>
<span class="sd">            recurse (bool, Optional):</span>
<span class="sd">                Whether to call _cast_buffers recursively on nested FSDP</span>
<span class="sd">                instances (default is True).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="ow">and</span> <span class="n">recurse</span><span class="p">:</span>
                <span class="c1"># Allow any child FSDP instances to handle their own buffers.</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">memo</span><span class="o">=</span><span class="n">memo</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">buf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_name_to_orig_dtype</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_name_to_orig_dtype</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="c1"># If given, cast buffer to the given dtype. This is used to</span>
                    <span class="c1"># suppport mixed precision for buffers</span>
                    <span class="c1"># (given by self.mixed_precision.buffer_dtype) and also used</span>
                    <span class="c1"># to restore the buffer dtype to the original precision for</span>
                    <span class="c1"># state_dict() calls.</span>
                    <span class="c1"># Note that non-floating point buffers are not casted.</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">buf</span><span class="p">):</span>
                        <span class="c1"># We are restoring the original buffer type in</span>
                        <span class="c1"># preparation for checkpoint.</span>
                        <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
                            <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
                        <span class="c1"># Note that we don&#39;t pass in self.mixed_precision.buffer_dtype</span>
                        <span class="c1"># recursively into _cast_buffers, as we want to respect</span>
                        <span class="c1"># mp config for child FSDP instances.</span>
                        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">():</span>
                            <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">buffer_dtype</span><span class="p">)</span>

                    <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reset_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reset instance so :func:`_lazy_init` will run on the next forward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_local_shard&quot;</span><span class="p">):</span>
                <span class="c1"># We only need to `del` `_local_shard` because</span>
                <span class="c1"># `_init_param_attributes()` gates the logic based on its</span>
                <span class="c1"># existence (and not any of the other attributes).</span>
                <span class="k">del</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs initialization lazily, typically right before the first</span>
<span class="sd">        forward pass. The laziness is needed to ensure that the parameter</span>
<span class="sd">        device/dtype and the FSDP hierarchy have finalized.</span>

<span class="sd">        This method&#39;s actual logic only runs on the root FSDP instance, which</span>
<span class="sd">        performs initialization for all non-root FSDP instances to avoid</span>
<span class="sd">        partial initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># no-op: already initialized</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="c1"># Allow the FSDP constructor to run even with CUDA but check this</span>
            <span class="c1"># once we start real execution</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;FSDP does not support CPU only execution&quot;</span><span class="p">)</span>
        <span class="c1"># The following logic is only run on the root FSDP instance since it</span>
        <span class="c1"># will set `_is_root=False` for the non-root instances</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_streams</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_param_attributes</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="c1"># Initialize non-root FSDP instances and share attributes from the root</span>
        <span class="c1"># to non-root instances</span>
        <span class="n">inconsistent_limit_all_gathers</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">fsdp_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fsdp_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
                <span class="c1"># Relax the assert for non-root FSDP instances in case the</span>
                <span class="c1"># nested initialized module is wrapped again in FSDP later (e.g.</span>
                <span class="c1"># after training to run inference)</span>
                <span class="k">assert</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="p">(</span>
                    <span class="s2">&quot;Non-root FSDP instance&#39;s `_is_root` should not have been &quot;</span>
                    <span class="s2">&quot;set yet or should have been set to `False`&quot;</span>
                <span class="p">)</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_is_root</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_streams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_exec_order_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span>
                <span class="k">if</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">limit_all_gathers</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span><span class="p">:</span>
                    <span class="c1"># Prefer the root&#39;s value</span>
                    <span class="n">inconsistent_limit_all_gathers</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">limit_all_gathers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_free_event_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_free_event_queue</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles_prefetched</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span>
                <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_init_param_attributes</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inconsistent_limit_all_gathers</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Found inconsistent `limit_all_gathers` values across FSDP &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;instances on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">. Using the root FSDP&#39;s value &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">limit_all_gathers</span><span class="si">}</span><span class="s2"> for all instances.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># TODO (awgu): Move this to the `FlatParamHandle` class later</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_init_param_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">:</span> <span class="n">FlatParamHandle</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        We manage several attributes on each Parameter instance.</span>
<span class="sd">        A few attributes are set here:</span>
<span class="sd">            ``_local_shard``: a single shard of the parameter. This is needed to</span>
<span class="sd">                recover the shard after rebuilding full parameter in forward</span>
<span class="sd">                and backward.</span>
<span class="sd">            ``_full_param_padded``: the full weight (padded to be evenly</span>
<span class="sd">                divisible by ``world_size``), used for computation in the</span>
<span class="sd">                forward and backward pass. It is initialized with the</span>
<span class="sd">                appropriate size and then has its storage freed. This will be</span>
<span class="sd">                resized in place and only materialized (via all-gather) as needed.</span>
<span class="sd">        Another attribute is set by :func:`_register_post_backward_hooks`:</span>
<span class="sd">            ``_post_backward_hook_state``: it holds the parameter&#39;s AccumulateGrad object</span>
<span class="sd">                and the registered post hook handle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="c1"># If _local_shard has been set in the first lazy init and</span>
        <span class="c1"># current parameter is pointed to _local_shard, no need to</span>
        <span class="c1"># set the _local_shard again.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_local_shard&quot;</span><span class="p">):</span>
            <span class="c1"># If CPU offloading, p._local_shard should have been placed on CPU</span>
            <span class="c1"># during its first lazy construction.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="s2">&quot;cpu&quot;</span>
                <span class="p">),</span> <span class="p">(</span>
                    <span class="s2">&quot;Expected p._local_shard to be on CPU, &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="sa">f</span><span class="s2">&quot;but it&#39;s on </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># A single shard of the parameters. Also makes p._local_shard to be on</span>
        <span class="c1"># CPU if we are CPU offloading, since p.data would be on CPU during</span>
        <span class="c1"># init.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Expected param to be on CPU when cpu_offloading is enabled. &quot;</span>
                <span class="s2">&quot;If CPU offloading is enabled correctly, you may be &quot;</span>
                <span class="s2">&quot;accidentally moving the model to CUDA after FSDP initialization.&quot;</span>
            <span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="c1"># If CPU offloading, pin the memory to enable faster CPU -&gt; GPU device</span>
        <span class="c1"># transfer.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="c1"># When offloading parameters, also move the grad shard to CPU during</span>
            <span class="c1"># backward pass. In this case, it&#39;s important to pre-allocate the</span>
            <span class="c1"># CPU grad shard in pinned memory so that we can do a non-blocking</span>
            <span class="c1"># transfer.</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">p</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>

        <span class="c1"># If mixed_precision, maintain reduced precision param shard on</span>
        <span class="c1"># compute_device for computation in fwd/bwd. We resize storage to 0 here</span>
        <span class="c1"># and rematerialize before building the full param when needed. After</span>
        <span class="c1"># fwd/bwd, it is freed and we only hold on to the full precision shard.</span>
        <span class="c1"># As a result, this reduced precision shard is not allocated if we are</span>
        <span class="c1"># not in the forward/backward pass.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="p">)</span>

        <span class="c1"># We also maintain a full-sized parameter of type self.compute_dtype.</span>
        <span class="c1"># We resize the storage to size 0 at init (here) and only materialize</span>
        <span class="c1"># as needed. The storage may contain padding elements so that it is</span>
        <span class="c1"># evenly divisible by world_size, although these padding elements will</span>
        <span class="c1"># be removed before the relevant computation.</span>
        <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="c1"># We set p._full_param_padded&#39;s dtype to the desired parameter dtype</span>
            <span class="c1"># in the case of mixed precision. This is so that when we all_gather</span>
            <span class="c1"># into full_param_padded it can occur without issues and result in</span>
            <span class="c1"># full_param_padded having the expected param_dtype.</span>
            <span class="n">full_param_dtype</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">full_param_dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_padded_unsharded_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">():</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_full_prec_full_param_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>  <span class="c1"># full precision</span>
                <span class="p">)</span>
                <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_full_prec_full_param_padded</span><span class="p">)</span>

        <span class="c1"># Track whether the `FlatParameter`&#39;s post-backward hook has been</span>
        <span class="c1"># called for validation in `_wait_for_post_backward()`</span>
        <span class="n">p</span><span class="o">.</span><span class="n">_post_backward_called</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_init_streams</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initializes CUDA streams for overlapping data transfer and</span>
<span class="sd">        computation. This should only be called on the root FSDP instance.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="c1"># Stream for all-gathering parameters.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
        <span class="c1"># Stream for overlapping grad reduction with the backward pass.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
        <span class="c1"># Stream for pre-all-gather copies (e.g. H2D or precision cast).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;pre_all_gather&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_wait_for_previous_optim_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The root :class:`FullyShardedDataParallel` instance needs to</span>
<span class="sd">        synchronize with the default stream to ensure that the previous</span>
<span class="sd">        optimizer step is done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">current_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
        <span class="c1"># Having the pre-all-gather stream wait for the current stream even if</span>
        <span class="c1"># we do not leverage the pre-all-gather stream is tolerable since this</span>
        <span class="c1"># only runs once per iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;pre_all_gather&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prefetch_handles</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">current_handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prefetches the next handles if needed (without synchronization). An</span>
<span class="sd">        empty handles key cannot prefetch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">current_handles_key</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">handles_to_prefetch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_handles_to_prefetch</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">handles_key</span> <span class="ow">in</span> <span class="n">handles_to_prefetch</span><span class="p">:</span>
            <span class="c1"># Prefetch the next set of handles without synchronizing to allow</span>
            <span class="c1"># the sync to happen as late as possible to maximize overlap</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_unshard</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_get_handles_to_prefetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">current_handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a :class:`list` of the handles keys to prefetch for the next</span>
<span class="sd">        module(s), where ``current_handles_key`` represents the current module.</span>

<span class="sd">        &quot;Prefetching&quot; refers to running the unshard logic early (without</span>
<span class="sd">        synchronization), and the &quot;next&quot; modules depend on the recorded</span>
<span class="sd">        execution order and the current training state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">training_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_training_state</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">)</span>
        <span class="n">valid_training_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
            <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">,</span>
            <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">FORWARD</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">p_assert</span><span class="p">(</span>
            <span class="n">training_state</span> <span class="ow">in</span> <span class="n">valid_training_states</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;Prefetching is only supported in </span><span class="si">{</span><span class="n">valid_training_states</span><span class="si">}</span><span class="s2"> but &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;currently in </span><span class="si">{</span><span class="n">training_state</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">eod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span>
        <span class="n">target_handles_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_HandlesKey</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="n">training_state</span> <span class="o">==</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">==</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>
            <span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="n">training_state</span> <span class="o">==</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">==</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">target_handles_keys</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">target_handles_key</span> <span class="k">for</span> <span class="n">target_handles_key</span> <span class="ow">in</span>
                <span class="n">eod</span><span class="o">.</span><span class="n">get_handles_to_backward_prefetch</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_handles_key</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_handles_key</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">training_state</span> <span class="o">==</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">FORWARD</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_prefetch</span>
        <span class="p">):</span>
            <span class="n">target_handles_keys</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">target_handles_key</span> <span class="k">for</span> <span class="n">target_handles_key</span> <span class="ow">in</span>
                <span class="n">eod</span><span class="o">.</span><span class="n">get_handles_to_forward_prefetch</span><span class="p">(</span><span class="n">current_handles_key</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_forward_unshard</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_handles_key</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_handles_key</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">return</span> <span class="n">target_handles_keys</span>

    <span class="k">def</span> <span class="nf">_get_training_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles_key</span><span class="p">:</span> <span class="n">_HandlesKey</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">HandleTrainingState</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the training state of the handles in ``handles_key``.&quot;&quot;&quot;</span>
        <span class="n">p_assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Expects a non-empty handles key&quot;</span><span class="p">)</span>
        <span class="n">training_states</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles_key</span><span class="p">)</span>
        <span class="n">p_assert</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">training_states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;Expects uniform training state but got </span><span class="si">{</span><span class="n">training_states</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">training_states</span><span class="p">))</span>

<div class="viewcode-block" id="FullyShardedDataParallel.state_dict_type"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">state_dict_type</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict_type</span><span class="p">:</span> <span class="n">StateDictType</span><span class="p">,</span>
        <span class="n">state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to set the ``state_dict_type`` of all the descendant</span>
<span class="sd">        FSDP modules of the target module. The target module does not have to</span>
<span class="sd">        be a FSDP module. If the target module is a FSDP module, its</span>
<span class="sd">        ``state_dict_type`` will also be changed.</span>

<span class="sd">        .. note:: This API should be called for only the top-level (root)</span>
<span class="sd">            module.</span>

<span class="sd">        .. note:: This API enables users to transparently use the conventional</span>
<span class="sd">            ``state_dict`` API to take model checkpoints in cases where the</span>
<span class="sd">            root FSDP module is wrapped by another ``nn.Module``. For example,</span>
<span class="sd">            the following will ensure ``state_dict``  is called on all non-FSDP</span>
<span class="sd">            instances, while dispatching into `local_state_dict` implementation</span>
<span class="sd">            for FSDP:</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = DDP(FSDP(...))</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(model, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">            &gt;&gt;&gt;     checkpoint = model.state_dict()</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module.</span>
<span class="sd">            state_dict_type (StateDictType): the desired ``state_dict_type`` to set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Use default config a state_dict config is not set.</span>
        <span class="k">if</span> <span class="n">state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict_config</span> <span class="o">=</span> <span class="n">_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]()</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">prev_state_dict_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span>
            <span class="k">if</span> <span class="n">prev_state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span>
            <span class="k">if</span> <span class="n">prev_state_dict_type</span> <span class="o">!=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All FSDP module should the same state_dict_type.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">prev_state_dict_config</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;All FSDP modules should have the same type of state_dict_config.&quot;</span>
                <span class="p">)</span>

            <span class="n">expected_state_dict_config_type</span> <span class="o">=</span> <span class="n">_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">expected_state_dict_config_type</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected state_dict_config of type </span><span class="si">{</span><span class="n">expected_state_dict_config_type</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">state_dict_type</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">state_dict_config</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">prev_state_dict_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># Avoid mypy warning</span>
            <span class="k">assert</span> <span class="n">prev_state_dict_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># Avoid mypy warning</span>
            <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">prev_state_dict_type</span>
                <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">prev_state_dict_config</span></div>

    <span class="k">def</span> <span class="nf">_convert_to_wrapped_module_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">module_name</span><span class="p">:</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="c1"># Activation checkpoint adds a prefix that has to be</span>
        <span class="c1"># removed as well.</span>
        <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">checkpoint_wrapper</span><span class="o">.</span><span class="n">_CHECKPOINT_PREFIX</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">module_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_param_fqns</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">parameter_module_names</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_to_wrapped_module_name</span><span class="p">(</span><span class="n">module_name</span><span class="p">)</span>
            <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">yield</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">module_name</span>

    <span class="k">def</span> <span class="nf">_full_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hook that runs after model.state_dict() is called before returning result to</span>
<span class="sd">        user. For FSDP, we may have to clone the tensors in state_dict as params go</span>
<span class="sd">        back to sharded version after _summon_full_params ends, and also remove</span>
<span class="sd">        &quot;_fsdp_wrapped_module&quot; prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">])</span>
        <span class="c1"># Return early for trivial cases</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state_dict</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">has_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="c1"># If the `FlatParameter` is registered, then this rank only needed to</span>
        <span class="c1"># participate in the all-gather but does not actually save the state</span>
        <span class="c1"># dict (e.g. when `rank0_only=True` and `self.rank != 0`)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="s2">&quot;flat_param&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="n">offload_to_cpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="o">.</span><span class="n">offload_to_cpu</span>
        <span class="n">cpu_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Loop only the parameters saved in self._fsdp_wrapped_module to avoid</span>
        <span class="c1"># processing buffers.</span>
        <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_fqns</span><span class="p">:</span>
            <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">clean_key</span> <span class="o">=</span> <span class="n">fqn</span>
            <span class="n">clean_prefix</span> <span class="o">=</span> <span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span>
            <span class="c1"># Strip prefix out of key if needed as buffer names and param names</span>
            <span class="c1"># do not have prefix considered as they are not computed in `state_dict`</span>
            <span class="c1"># call.</span>
            <span class="k">if</span> <span class="n">clean_key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">clean_prefix</span><span class="p">):</span>
                <span class="n">clean_key</span> <span class="o">=</span> <span class="n">clean_key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">clean_prefix</span><span class="p">):]</span>

            <span class="c1"># Clone non-ignored parameters before exiting the</span>
            <span class="c1"># `_summon_full_params()` context</span>
            <span class="k">assert</span> <span class="n">fqn</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FSDP assumes </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> is in the state_dict but the state_dict &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;only has </span><span class="si">{</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">. prefix=</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;module_name=</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2"> param_name=</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> rank=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">clean_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_param_names</span> <span class="ow">and</span> \
                    <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">],</span> <span class="s2">&quot;_has_been_cloned&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span><span class="o">.</span><span class="n">_has_been_cloned</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Failed to clone() tensor with name </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">. This may mean &quot;</span>
                        <span class="s2">&quot;that this state_dict entry could point to invalid memory &quot;</span>
                        <span class="s2">&quot;regions after returning from state_dict() call if this &quot;</span>
                        <span class="s2">&quot;parameter is managed by FSDP. Please check clone &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;implementation of </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

        <span class="c1"># Offload the buffer to CPU if needed -- we do not do this in</span>
        <span class="c1"># `_summon_full_params()` since without care, that would free</span>
        <span class="c1"># the original buffer&#39;s GPU memory and require reallocating</span>
        <span class="c1"># that memory later; this only affects the state dict&#39;s buffer</span>
        <span class="c1"># variable and leaves the original buffer&#39;s GPU memory intact</span>
        <span class="k">if</span> <span class="n">offload_to_cpu</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">clean_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_names</span><span class="p">:</span>
                <span class="c1"># This is a hack to support activation checkpoint.</span>
                <span class="n">clean_key</span> <span class="o">=</span> <span class="n">clean_key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">checkpoint_wrapper</span><span class="o">.</span><span class="n">_CHECKPOINT_PREFIX</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span>
                <span class="p">)</span>
                <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">clean_key</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">if</span> <span class="n">fqn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                    <span class="c1"># A buffer can be registered as non-persistent.</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">cpu_device</span><span class="p">:</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">_local_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This hook create a ShardedTensor from the local flat_param and replace</span>
<span class="sd">        the state_dict[f&quot;{prefix}{FLAT_PARAM}] with the ShardedTensor. No copy</span>
<span class="sd">        will happen. The underlying storage is the same.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">has_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="c1"># state_dict[f&quot;{prefix}{FLAT_PARAM}&quot;] exists and has the same tensor</span>
        <span class="c1"># value as the flat_param but it is a pure Tensor because</span>
        <span class="c1"># nn.Module.state_dict() will detach the parameter. Therefore, we need</span>
        <span class="c1"># to get flat_param from the FlattenParamsWrapper to get the metadata.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">FLAT_PARAM</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># Construct a ShardedTensor from the flat_param.</span>
        <span class="n">full_numel</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_unpadded_unsharded_size</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="n">shard_offset</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
        <span class="n">valid_data_size</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span>
        <span class="k">if</span> <span class="n">valid_data_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">flat_param</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">valid_data_size</span><span class="p">)</span>
        <span class="n">local_shards</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">Shard</span><span class="o">.</span><span class="n">from_tensor_and_offsets</span><span class="p">(</span><span class="n">flat_param</span><span class="p">,</span> <span class="p">[</span><span class="n">shard_offset</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FLAT_PARAM</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_from_local_shards</span><span class="p">(</span>
            <span class="n">local_shards</span><span class="p">,</span> <span class="n">full_numel</span><span class="p">,</span> <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
        <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_sharded_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The hook replaces the unflattened, unsharded parameter in the state_dict</span>
<span class="sd">        with a unflattened, sharded parameter (a ShardedTensor).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">has_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Inside _sharded_post_load_state_dict_hook, the training_state must &quot;</span>
            <span class="s2">&quot;not be SUMMON_FULL_PARAMS.&quot;</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">fqn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_fqns</span><span class="p">:</span>
                <span class="c1"># Create a ShardedTensor for the unflattened, non-sharded parameter.</span>
                <span class="n">param</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="nb">getattr</span><span class="p">,</span> <span class="n">fqn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">fqn</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_ext_chunk_tensor</span><span class="p">(</span>
                    <span class="n">tensor</span><span class="o">=</span><span class="n">param</span><span class="p">,</span>
                    <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
                    <span class="n">world_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                    <span class="n">num_devices_per_node</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span>
                    <span class="n">pg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
                <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FLAT_PARAM</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_post_state_dict_hook</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        _post_state_dict_hook() is called after the state_dict() of this</span>
<span class="sd">        FSDP module is executed. ``self._state_dict_type`` is used to decide</span>
<span class="sd">        what postprocessing will be done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="n">processed_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">](</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="c1"># Restore buffers, which currently are in their full precision type,</span>
        <span class="c1"># back to their mixed precision type. This is because buffers are cast</span>
        <span class="c1"># during lazy_init() and stay at their mixed precision type before/after</span>
        <span class="c1"># forward/backward. As a result state_dict() should maintain this.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">processed_state_dict</span>

<div class="viewcode-block" id="FullyShardedDataParallel.state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is the entry point of all three FSDP ``state_dict`` APIs: full,</span>
<span class="sd">        local, and sharded. For the full state dict</span>
<span class="sd">        (``StateDictType.FULL_STATE_DICT``), FSDP attempts to unshard the model</span>
<span class="sd">        on all ranks, which may result in an OOM error if the full model cannot</span>
<span class="sd">        fit on a single GPU. In that case, users may pass in a</span>
<span class="sd">        :class:`FullStateDictConfig` to only save the checkpoint on rank 0 and/</span>
<span class="sd">        or to offload it to CPU memory layer by layer, enabling much larger</span>
<span class="sd">        checkpoints. If the full model cannot fit in CPU memory, then users may</span>
<span class="sd">        instead take a local state dict (``StateDictType.LOCAL_STATE_DICT``)</span>
<span class="sd">        that only saves the local shard of the model. The sharded state dict</span>
<span class="sd">        (``StateDictType.SHARDED_STATE_DICT``) saves the model parameters as</span>
<span class="sd">        ``ShardedTensor`` s. The ``state_dict`` type can be configured using</span>
<span class="sd">        the :meth:`state_dict_type` context manager.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">            &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">            &gt;&gt;&gt; my_module = nn.Linear(...)</span>
<span class="sd">            &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">            &gt;&gt;&gt; full_state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.FULL_STATE_DICT, full_state_dict_config):</span>
<span class="sd">            &gt;&gt;&gt;     full_dict = sharded_module.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; full_dict.keys()</span>
<span class="sd">            &gt;&gt;&gt; odict_keys([&#39;weight&#39;, &#39;bias&#39;])</span>
<span class="sd">            &gt;&gt;&gt; # using local state dict</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">            &gt;&gt;&gt;     local_dict = sharded_module.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; local_dict.keys()</span>
<span class="sd">            &gt;&gt;&gt; odict_keys([&#39;flat_param&#39;, &#39;inner.flat_param&#39;])</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives may be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO (rohan-varma): separate these out once a state_dict pre-hook</span>
        <span class="c1"># is available.</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span>
            <span class="c1"># Get config args</span>
            <span class="n">full_state_dict_config</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">FullStateDictConfig</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">rank0_only</span> <span class="o">=</span> <span class="n">full_state_dict_config</span><span class="o">.</span><span class="n">rank0_only</span>
            <span class="n">offload_to_cpu</span> <span class="o">=</span> <span class="n">full_state_dict_config</span><span class="o">.</span><span class="n">offload_to_cpu</span>
            <span class="n">summon_ctx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                    <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span> <span class="k">else</span>
                <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">with</span> <span class="n">summon_ctx</span><span class="p">:</span>
                <span class="c1"># Since buffers are not sharded and stay casted, restore them to their</span>
                <span class="c1"># original user module specified types for checkpoint. We take care to</span>
                <span class="c1"># recast in post_state_dict_hook for consistency with the fact that</span>
                <span class="c1"># buffers stay casted after forward/backward. We must have the</span>
                <span class="c1"># call here instead of above because _summon_full_params itself</span>
                <span class="c1"># calls _lazy_init() which would cast the buffers.</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_buffer_name_to_orig_dtype</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span>
                    <span class="p">)</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># TODO: support offload to CPU in post state dict hook.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">rank0_only</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">state_dict</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{}</span>

        <span class="k">elif</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span> <span class="ow">or</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;sharded_state_dict/local_state_dict can only be called &quot;</span>
                    <span class="s2">&quot;when parameters are flatten and sharded.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown StateDictType </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_local_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the local state of the module. Parameters are flattened and</span>
<span class="sd">        sharded, so the resulting state_dict can only be loaded after the module</span>
<span class="sd">        has been wrapped with FSDP.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_full_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We should exit summon_full_params context.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">])</span>
        <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_full_param_ctx&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_sharded_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sharded states of the module. Parameters are unflattened and</span>
<span class="sd">        sharded, so the resulting state_dict can be used with any parallelism</span>
<span class="sd">        (e.g., DPP, model parallelism, and single trainer) after a valid</span>
<span class="sd">        resharding.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span><span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_full_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We do not expect to be calling pre-hooks twice without post-hook</span>
        <span class="c1"># call in between.</span>
        <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_full_param_ctx&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="c1"># Note that it needs writeback=True to persist.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
            <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_local_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_local_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This hook finds the local flat_param for this FSDP module from the</span>
<span class="sd">        state_dict. The flat_param should be a ShardedTensor. This hook converts</span>
<span class="sd">        the ShardedTensor to a tensor. No copy happen unless padding is required.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">FLAT_PARAM</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">fqn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">FLAT_PARAM</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;No flat parameter in state_dict but self._fsdp_wrapped_module.flat_param is not None&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">load_tensor</span><span class="p">,</span> <span class="n">ShardedTensor</span>
        <span class="p">),</span> <span class="s2">&quot;Tensors in local_state_dict should be ShardedTensor.&quot;</span>

        <span class="c1"># Convert the ShardedTensor to a Tensor.</span>
        <span class="n">shards</span> <span class="o">=</span> <span class="n">load_tensor</span><span class="o">.</span><span class="n">local_shards</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">),</span> <span class="s2">&quot;load_local_state_dict assume one shard per ShardedTensor.&quot;</span>
        <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Get the metada of the flat_param to decide whether to pad the loaded</span>
        <span class="c1"># tensor.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="k">assert</span> <span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()):</span>
            <span class="k">assert</span> <span class="n">load_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Local shard size = </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and the tensor in &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;the state_dict is </span><span class="si">{</span><span class="n">load_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
            <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">load_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_tensor</span>

    <span class="k">def</span> <span class="nf">_sharded_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_sharded_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The hook combines the unflattened, sharded parameters (ShardedTensor) to</span>
<span class="sd">        a new FlatParameter and shards the new FlatParameter to the local chunk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">has_params</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;load_sharded_state_dict can only be called when parameters &quot;</span>
                <span class="s2">&quot;are flatten and sharded.&quot;</span>
            <span class="p">)</span>

        <span class="n">nonsharded_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO: Reduce the communication by using only one _all_gather_base to</span>
        <span class="c1"># gather all the parameters in this layer. This can be achieved by</span>
        <span class="c1"># concatenated all the local shards and then append the padding.</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/77461</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">module_name</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_param_infos</span><span class="p">:</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_to_wrapped_module_name</span><span class="p">(</span><span class="n">module_name</span><span class="p">)</span>
            <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">param</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>

            <span class="c1"># All-gather the param (ShardedTensor)</span>
            <span class="n">param</span><span class="p">,</span> <span class="n">shards</span> <span class="o">=</span> <span class="n">_ext_pre_load_state_dict_transform</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expects 0 or 1 shard per rank but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">}</span><span class="s2"> shards on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">param_numel</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="n">dim_0_size</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">dim_0_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_numel</span> <span class="o">//</span> <span class="n">dim_0_size</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">shards</span><span class="p">:</span>
                <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">local_tensor</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                    <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">local_tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
                <span class="n">num_padding</span> <span class="o">=</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="n">local_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">num_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">local_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_padding</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">chunk_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">local_tensor</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">local_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">param_numel</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
            <span class="n">nonsharded_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Create a new flat_param from the loaded, non-sharded tensors.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="n">loaded_flat_param</span> <span class="o">=</span> <span class="n">FlatParamHandle</span><span class="o">.</span><span class="n">flatten_params</span><span class="p">(</span><span class="n">nonsharded_tensors</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Get the chunk from the loaded flat_param for the local rank.</span>
        <span class="n">loaded_flat_param</span><span class="p">,</span> <span class="n">num_to_pad</span> <span class="o">=</span> <span class="n">FlatParamHandle</span><span class="o">.</span><span class="n">_get_shard</span><span class="p">(</span>
            <span class="n">loaded_flat_param</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">loaded_flat_param</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">loaded_flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The loaded local chunk has different numel(</span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">) &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;from the local chunk </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span> <span class="o">==</span> <span class="n">num_to_pad</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The loaded local chunk has different padding(</span><span class="si">{</span><span class="n">num_to_pad</span><span class="si">}</span><span class="s2">) &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;from the local chunk </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_shard_numel_padded</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_fsdp_wrapped_module.flat_param&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loaded_flat_param</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ``_pre_state_dict_hook` is called before ``self._load_from_state_dict()``</span>
<span class="sd">        is called. ``self._state_dict_type`` is used to decide what preprocessing</span>
<span class="sd">        will be done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Code that is common for all state_dict impls</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="c1"># Dispatch into state_dict specific implementation of pre-hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">](</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_post_load_state_dict_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Code that is common for all state_dict impls</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="c1"># Dispatch into state_dict type specific implementation of post-hook for</span>
        <span class="c1"># loading state_dict.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">]()</span>

<div class="viewcode-block" id="FullyShardedDataParallel.load_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The entry point of all three FSDP ``load_state_dict`` APIs. By default,</span>
<span class="sd">        calling ``load_state_dict`` on an FSDP module will result in FSDP</span>
<span class="sd">        attempting to load a &quot;full&quot; state_dict, i.e. a state_dict consisting of</span>
<span class="sd">        full, unsharded, unflattened original module parameters. This requires</span>
<span class="sd">        FSDP to load the full parameter context on each rank which could result</span>
<span class="sd">        in GPU OOM. As a result, :func:`state_dict_type` API is available to</span>
<span class="sd">        configure between ``load_state_dict`` implementations. User can thus use</span>
<span class="sd">        ``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`` context</span>
<span class="sd">        manager to load a local state dict checkpoint that will restore only</span>
<span class="sd">        local shards of the module. Currently, the only supported</span>
<span class="sd">        implementations are ``StateDictType.LOCAL_STATE_DICT`` and</span>
<span class="sd">        ``StateDictType.FULL_STATE_DICT`` (default). Please see :func:`state_dict`</span>
<span class="sd">        for documentation around creating an FSDP checkpoint.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">            &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">            &gt;&gt;&gt; my_module = nn.Linear(...)</span>
<span class="sd">            &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">            &gt;&gt;&gt; checkpoint = torch.load(PATH)</span>
<span class="sd">            &gt;&gt;&gt; full_state_dict = checkpoint[&#39;full_state_dict&#39;]</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.FULL_STATE_DICT):</span>
<span class="sd">            &gt;&gt;&gt;     sharded_module.load_state_dict(full_state_dict)</span>
<span class="sd">            &gt;&gt;&gt; full_dict.keys()</span>
<span class="sd">            &gt;&gt;&gt; odict_keys([&#39;weight&#39;, &#39;bias&#39;])</span>
<span class="sd">            &gt;&gt;&gt; # using local state dict</span>
<span class="sd">            &gt;&gt;&gt; local_state_dict = checkpoint[&#39;local_state_dict&#39;]</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">            &gt;&gt;&gt;     sharded_module.load_state_dict(local_state_dict)</span>
<span class="sd">            &gt;&gt;&gt; local_dict.keys()</span>
<span class="sd">            &gt;&gt;&gt; odict_keys([&#39;flat_param&#39;, &#39;inner.flat_param&#39;])</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives may be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_load_local_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load states from a flattened, sharded state dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_sharded_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="s2">&quot;OrderedDict[str, torch.Tensor]&quot;</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load states from a unflattened, sharded state dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span><span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.forward"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the forward pass for the wrapped module, inserting FSDP-specific</span>
<span class="sd">        pre- and post-forward sharding logic.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;FullyShardedDataParallel.forward&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_root_pre_forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">unused</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">unshard_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pre_forward_unshard</span><span class="p">,</span> <span class="n">handles</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
            <span class="c1"># Do not free the root&#39;s parameters in the post-forward for</span>
            <span class="c1"># `FULL_SHARD` with the intention that they are immediately used</span>
            <span class="c1"># for backward computation (though this may not be true)</span>
            <span class="n">free_unsharded_flat_params</span> <span class="o">=</span> <span class="p">[</span>
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
                <span class="ow">and</span> <span class="n">handle</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">HandleShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span>
            <span class="p">]</span>
            <span class="n">reshard_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_reshard</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span>
                <span class="n">free_unsharded_flat_params</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">unshard_fn</span><span class="p">,</span> <span class="n">unused</span><span class="p">,</span> <span class="n">unused</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                <span class="n">p_assert</span><span class="p">(</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                    <span class="s2">&quot;Expected `FlatParameter` to be on the compute device &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">reshard_fn</span><span class="p">,</span> <span class="n">unused</span><span class="p">,</span> <span class="n">unused</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_pre_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
        <span class="n">unshard_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the pre-forward logic. This includes an opportunity to unshard</span>
<span class="sd">        currently sharded parameters such as those for the current forward and</span>
<span class="sd">        registering post-backward hooks for these current parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            handles (List[FlatParamHandle]): Handles giving the parameters</span>
<span class="sd">                used in the current forward.</span>
<span class="sd">            unshard_fn (Optional[Callable]): A callable to unshard any</span>
<span class="sd">                currently sharded parameters or ``None`` to not do any</span>
<span class="sd">                unsharding.</span>
<span class="sd">            module (nn.Module): Unused; expected by the hook signature.</span>
<span class="sd">            input (Any): Unused; expected by the hook signature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">FORWARD</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">record_pre_forward</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">FORWARD</span>
        <span class="k">if</span> <span class="n">unshard_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unshard_fn</span><span class="p">()</span>
        <span class="c1"># Register post-backward hooks to reshard the parameters and</span>
        <span class="c1"># reduce-scatter their gradients. They must be re-registered every</span>
        <span class="c1"># forward pass in case the `grad_fn` is mutated.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_post_backward_hooks</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pre_forward_unshard</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Unshards parameters in the pre-forward.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">handles</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_unshard</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
            <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_forward_unshard</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_handles</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_post_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
        <span class="n">reshard_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">output</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the post-forward logic. This includes an opportunity to reshard</span>
<span class="sd">        currently unsharded parameters such as those used in the current</span>
<span class="sd">        forward and registering pre-backward hooks on the forward outputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            handles (List[FlatParamHandle]): Handles giving the parameters</span>
<span class="sd">                used in the current forward.</span>
<span class="sd">            reshard_fn (Optional[Callable]): A callable to reshard any</span>
<span class="sd">                currently unsharded parameters (e.g. from the current forward)</span>
<span class="sd">                or ``None`` to not do any resharding.</span>
<span class="sd">            module (nn.Module): Unused; expected by the hook signature.</span>
<span class="sd">            input (Any): Unused; exepcted by the hook signature.</span>
<span class="sd">            output (Any): Forward pass output; pre-backward hooks are</span>
<span class="sd">                registered on the tensors that require gradients in this</span>
<span class="sd">                output.</span>

<span class="sd">        Postcondition: Each ``FlatParameter`` &#39;s data points to the sharded</span>
<span class="sd">        flattened parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">record_post_forward</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reshard_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reshard_fn</span><span class="p">()</span>
        <span class="c1"># Register pre-backward hooks to unshard the flattened parameters</span>
        <span class="c1"># for the gradient computation (if needed)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_register_pre_backward_hooks</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">handles</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">IDLE</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_cast_forward_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Moves the forward inputs to the compute device and casts them to the</span>
<span class="sd">        appropriate dtype if needed.&quot;&quot;&quot;</span>
        <span class="c1"># TODO: Do not use the side stream for tensor copies for now;</span>
        <span class="c1"># investigate the perf with/without it</span>
        <span class="c1"># TODO: For mixed precision, move the inputs to the compute device and</span>
        <span class="c1"># cast to reduced-precision in a single `to()` call</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_to_kwargs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">():</span>
            <span class="n">input_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cast_fp_inputs_to_dtype</span><span class="p">(</span>
                <span class="n">input_dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">_fsdp_root_pre_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs pre-forward logic specific to the root FSDP instance, which should</span>
<span class="sd">        run before any individual module&#39;s pre-forward. This includes</span>
<span class="sd">        synchronizing with the previous iteration and casting the forward</span>
<span class="sd">        inputs appropriately. If this is called on a non-root FSDP instance,</span>
<span class="sd">        then the forward inputs are returned directly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p_assert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expects a root FSDP to have been set&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_prefetch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">fsdp_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">handles_key</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_forward_unshard</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_for_previous_optim_step</span><span class="p">()</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cast_forward_inputs</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>

<div class="viewcode-block" id="FullyShardedDataParallel.summon_full_params"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">summon_full_params</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">writeback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; A context manager to expose full params for FSDP instances.</span>
<span class="sd">        Can be useful *after* forward/backward for a model to get</span>
<span class="sd">        the params for additional processing or checking. It can take a non-FSDP</span>
<span class="sd">        module and will summon full params for all contained FSDP modules as</span>
<span class="sd">        well as their children, depending on the ``recurse`` argument.</span>

<span class="sd">        .. note:: This can be used on inner FSDPs.</span>
<span class="sd">        .. note:: This can *not* be used within a forward or backward pass. Nor</span>
<span class="sd">            can forward and backward be started from within this context.</span>
<span class="sd">        .. note:: Parameters will revert to their local shards after the context</span>
<span class="sd">            manager exits, storage behavior is the same as forward.</span>
<span class="sd">        .. note:: The full parameters can be modified, but only the portion</span>
<span class="sd">            corresponding to the local param shard will persist after the</span>
<span class="sd">            context manager exits (unless ``writeback=False``, in which case</span>
<span class="sd">            changes will be discarded). In the case where FSDP does not shard</span>
<span class="sd">            the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``</span>
<span class="sd">            config, the modification is persisted regardless of ``writeback``.</span>
<span class="sd">        .. note:: This method works on modules which are not FSDP themselves but</span>
<span class="sd">            may contain multiple independent FSDP units. In that case, the given</span>
<span class="sd">            arguments will apply to all contained FSDP units.</span>

<span class="sd">        .. warning:: Note that ``rank0_only=True`` in conjunction with</span>
<span class="sd">            ``writeback=True`` is not currently supported and will raise an</span>
<span class="sd">            error. This is because model parameter shapes would be different</span>
<span class="sd">            across ranks within the context, and writing to them can lead to</span>
<span class="sd">            inconsistency across ranks when the context is exited.</span>

<span class="sd">        .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will</span>
<span class="sd">            result in full parameters being redundantly copied to CPU memory for</span>
<span class="sd">            GPUs that reside on the same machine, which may incur the risk of</span>
<span class="sd">            CPU OOM. It is recommended to use ``offload_to_cpu`` with</span>
<span class="sd">            ``rank0_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            recurse (bool, Optional): recursively summon all params for nested</span>
<span class="sd">                FSDP instances (default: True).</span>
<span class="sd">            writeback (bool, Optional): if ``False``, modifications to params are</span>
<span class="sd">                discarded after the context manager exits;</span>
<span class="sd">                disabling this can be slightly more efficient (default: True)</span>
<span class="sd">            rank0_only (bool, Optional): if ``True``, full parameters are</span>
<span class="sd">                materialized on only global rank 0. This means that within the</span>
<span class="sd">                context, only rank 0 will have full parameters and the other</span>
<span class="sd">                ranks will have sharded parameters. Note that setting</span>
<span class="sd">                ``rank0_only=True`` with ``writeback=True`` is not supported,</span>
<span class="sd">                as model parameter shapes will be different across ranks</span>
<span class="sd">                within the context, and writing to them can lead to</span>
<span class="sd">                inconsistency across ranks when the context is exited.</span>
<span class="sd">            offload_to_cpu (bool, Optional): If ``True``, full parameters are</span>
<span class="sd">                offloaded to CPU. Note that this offloading currently only</span>
<span class="sd">                occurs if the parameter is sharded (which is only not the case</span>
<span class="sd">                for world_size = 1 or ``NO_SHARD`` config). It is recommended</span>
<span class="sd">                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid</span>
<span class="sd">                redundant copies of model parameters being offloaded to the same CPU memory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note that we specify root_only as FSDP roots will handle summoning</span>
        <span class="c1"># child FSDP instances based on recurse argument.</span>
        <span class="n">root_fsdp_modules</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="n">root_only</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># Summon all params for all FSDP instances</span>
        <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">root_fsdp_modules</span><span class="p">:</span>
                <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                        <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">,</span>
                        <span class="n">writeback</span><span class="o">=</span><span class="n">writeback</span><span class="p">,</span>
                        <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
                        <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="c1"># Yield to the caller, with full params in all FSDP instances.</span>
            <span class="k">yield</span>
        <span class="c1"># Exiting from the ExitStack will reshard all params.</span>
        <span class="k">return</span></div>

    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">_summon_full_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">writeback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">writeback</span> <span class="ow">and</span> <span class="n">rank0_only</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;writeback=True and rank0_only=True is not supported, as model &quot;</span>
                <span class="s2">&quot;parameter shapes will be different across ranks, and writing &quot;</span>
                <span class="s2">&quot;to them can lead to inconsistencies across ranks when the &quot;</span>
                <span class="s2">&quot;context is exited.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">offload_to_cpu</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">rank0_only</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;offload_to_cpu and rank0_only=False will result in &quot;</span>
                <span class="s2">&quot;full parameters being redundantly copied to CPU memory for &quot;</span>
                <span class="s2">&quot;GPUs that reside on the same machine, which may incur the risk of &quot;</span>
                <span class="s2">&quot;CPU OOM. It is recommended to use ``offload_to_cpu`` with &quot;</span>
                <span class="s2">&quot;rank0_only=True.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">recurse</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                            <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">writeback</span><span class="o">=</span><span class="n">writeback</span><span class="p">,</span>
                            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
                            <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">yield</span>
            <span class="k">return</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">==</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">IDLE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>

        <span class="n">free_unsharded_flat_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">needs_unshard</span><span class="p">()</span> <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unshard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">rank0_only</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Free the unsharded flattened parameter early</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reshard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">free_unsharded_flat_params</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">yield</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">IDLE</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Unflatten the unsharded flattened parameters</span>
            <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
                <span class="c1"># Invariant: rank == 0 or !rank0_only</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">offload_to_cpu</span> <span class="ow">and</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">:</span>
                        <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">())</span>
                <span class="c1"># TODO (awgu): This FPW call assumes 1 `FlatParameter`</span>
                <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">unflatten_as_params</span><span class="p">())</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">yield</span>
                <span class="k">finally</span><span class="p">:</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">writeback</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_write_back_to_local_shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_reshard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">free_unsharded_flat_params</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
                    <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                        <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">IDLE</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_write_back_to_local_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For each handle, writes back the this rank&#39;s shard of the unsharded</span>
<span class="sd">        flattened parameter to the sharded flattened parameter.</span>

<span class="sd">        Precondition: Each handle&#39;s ``FlatParameter`` &#39;s data points to the</span>
<span class="sd">        padded unsharded flattened parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="c1"># For `NO_SHARD`, `_local_shard` is the unsharded flattened</span>
            <span class="c1"># parameter as well</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Expects `flat_param` to be flattened but got </span><span class="si">{</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="c1"># Get the unpadded shard instead of the padded shard to persist</span>
            <span class="c1"># user changes to the padding (though FSDP does not explicitly</span>
            <span class="c1"># support this)</span>
            <span class="n">shard</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">FlatParamHandle</span><span class="o">.</span><span class="n">_get_unpadded_shard</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="p">,</span> <span class="n">handle</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">handle</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_local_shard</span><span class="p">[:</span><span class="n">shard</span><span class="o">.</span><span class="n">numel</span><span class="p">()]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">shard</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.named_buffers"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers">[docs]</a>    <span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_buffers()` to intercept buffer names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened buffer prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">in_summon_full_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">in_summon_full_params</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">buffer_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.named_parameters"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_parameters()` to intercept parameter names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened parameter prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Determine which logic to use based on the context at call time</span>
        <span class="n">in_summon_full_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">in_summon_full_params</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_register_pre_backward_hooks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers pre-backward hooks on the tensors that require gradients in</span>
<span class="sd">        the forward pass outputs ``outputs``, which were computed using the</span>
<span class="sd">        ``FlatParameter`` s of ``handles``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Forward pass outputs with pre-backward hooks registered to tensors</span>
<span class="sd">            that require gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If there is no gradient computation, then there is no need for</span>
        <span class="c1"># pre-backward logic</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">outputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># only defined on the root</span>

        <span class="n">handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">handles</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">handles_key</span><span class="p">:</span>
            <span class="c1"># Since these handles&#39; `FlatParameter`s participated in a forward,</span>
            <span class="c1"># we conservatively assume that they will be used in the backward</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ran_pre_backward_hook</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">def</span> <span class="nf">_pre_backward_hook</span><span class="p">(</span><span class="n">_handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span> <span class="o">*</span><span class="n">unused</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="sd">&quot;&quot;&quot;Prepares ``_handles`` &#39;s ``FlatParameter`` s for gradient</span>
<span class="sd">            computation.&quot;&quot;&quot;</span>
            <span class="n">_handles_key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_handles</span><span class="p">)</span>  <span class="c1"># avoid shadowing `handles_key`</span>
            <span class="c1"># Only run the pre-backward hook once per group of handles involved</span>
            <span class="c1"># in the same module forward computation</span>
            <span class="k">if</span> <span class="n">_handles_key</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_pre_backward_hook</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">_handles_key</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">return</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span>
                <span class="s2">&quot;FullyShardedDataParallel._pre_backward_hook&quot;</span>
            <span class="p">):</span>
                <span class="c1"># Queue the post-backward callback once for the root FSDP</span>
                <span class="c1"># instance to attach it to the outermost backward graph task so</span>
                <span class="c1"># that it is called after all backward calls complete</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_queue_wait_for_post_backward</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">_handles_key</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>
                <span class="c1"># Queueing the post-backward callback is the only logic that is</span>
                <span class="c1"># not per-handle in the pre-backward hook, so we can return</span>
                <span class="c1"># early here if there are no handles.</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_handles_key</span><span class="p">:</span>
                    <span class="k">return</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">_handles</span><span class="p">:</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>

                <span class="c1"># If the handles have been prefetched, this `_unshard()` simply</span>
                <span class="c1"># switches to using the unsharded parameter</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_unshard</span><span class="p">(</span><span class="n">_handles</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

                <span class="c1"># Set this to `False` to ensure that a mistargeted prefetch</span>
                <span class="c1"># does not actually unshard these handles</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span><span class="p">[</span><span class="n">_handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_handles</span><span class="p">(</span><span class="n">_handles_key</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">_handles</span><span class="p">:</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">prepare_gradient</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ran_pre_backward_hook</span><span class="p">[</span><span class="n">_handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">def</span> <span class="nf">_register_hook</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">t</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_pre_backward_hook</span><span class="p">,</span> <span class="n">handles</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_needs_pre_backward_unshard</span><span class="p">[</span><span class="n">handles_key</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">t</span>

        <span class="k">return</span> <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">_register_hook</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_register_post_backward_hooks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers post-backward hooks on the ``FlatParameter`` s&#39;</span>
<span class="sd">        ``AccumulateGrad`` objects to reshard and to reduce-scatter gradients.</span>

<span class="sd">        The ``AccumulateGrad`` object represents the last function that</span>
<span class="sd">        finalizes the ``FlatParameter`` &#39;s gradient, so it only runs after its</span>
<span class="sd">        entire gradient computation has finished.</span>

<span class="sd">        We register the post-backward hook only once in the *first* forward</span>
<span class="sd">        that a ``FlatParameter`` participates in. This relies on the</span>
<span class="sd">        ``AccumulateGrad`` object being preserved through multiple forwards.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If there is no gradient computation, then there is no need for</span>
        <span class="c1"># post-backward logic</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">flat_param</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
            <span class="n">already_registered</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">flat_param</span><span class="p">,</span> <span class="s2">&quot;_post_backward_hook_state&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">already_registered</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># Get the `AccumulateGrad` object</span>
            <span class="n">temp_flat_param</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">flat_param</span><span class="p">)</span>
            <span class="n">p_assert</span><span class="p">(</span>
                <span class="n">temp_flat_param</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;The `grad_fn` is needed to access the `AccumulateGrad` and &quot;</span>
                <span class="s2">&quot;register the post-backward hook&quot;</span>
            <span class="p">)</span>
            <span class="n">acc_grad</span> <span class="o">=</span> <span class="n">temp_flat_param</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">hook_handle</span> <span class="o">=</span> <span class="n">acc_grad</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_hook</span><span class="p">,</span> <span class="n">handle</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">flat_param</span><span class="o">.</span><span class="n">_post_backward_hook_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">acc_grad</span><span class="p">,</span> <span class="n">hook_handle</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_post_backward_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">handle</span><span class="p">:</span> <span class="n">FlatParamHandle</span><span class="p">,</span>
        <span class="o">*</span><span class="n">unused</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reduce-scatters the gradient of ``handle`` &#39;s ``FlatParameter``.</span>

<span class="sd">        Precondition: The ``FlatParameter`` &#39;s ``.grad`` attribute contains the</span>
<span class="sd">        unsharded gradient for the local batch.</span>

<span class="sd">        Postcondition:</span>
<span class="sd">        - If using ``NO_SHARD``, then the ``.grad`` attribute is the reduced</span>
<span class="sd">        unsharded gradient.</span>
<span class="sd">        - Otherwise, the ``_saved_grad_shard`` attribute is the reduced sharded</span>
<span class="sd">        gradient (accumulating with any existing gradient).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="n">param</span><span class="o">.</span><span class="n">_post_backward_called</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span>
            <span class="s2">&quot;FullyShardedDataParallel._post_backward_hook&quot;</span>
        <span class="p">):</span>
            <span class="c1"># First hook callback will see PRE state. If we have multiple params,</span>
            <span class="c1"># then subsequent hook callbacks will see POST state.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">BACKWARD_POST</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_param_exec_order_policy</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span><span class="p">:</span>
                <span class="c1"># In self._fsdp_params_exec_order, the parameters are ordered based on</span>
                <span class="c1"># the execution order in the backward pass in the first iteration.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;FSDP only works with gradients that don&#39;t require gradients&quot;</span>
                <span class="p">)</span>

            <span class="n">free_unsharded_flat_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_free_unsharded_flat_param</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reshard</span><span class="p">([</span><span class="n">handle</span><span class="p">],</span> <span class="p">[</span><span class="n">free_unsharded_flat_param</span><span class="p">])</span>

            <span class="c1"># TODO (awgu): Post-backward prefetching does not support the</span>
            <span class="c1"># multiple handles per module case (which was why we keyed by</span>
            <span class="c1"># *tuple*). The post-backward hook runs per handle, not per group</span>
            <span class="c1"># of handles. To generalize this, we may need a 2-level mapping,</span>
            <span class="c1"># where we map each individual handle to its groups of handles and</span>
            <span class="c1"># then from the groups of handles to their indices in the order.</span>
            <span class="n">handles_key</span> <span class="o">=</span> <span class="p">(</span><span class="n">handle</span><span class="p">,)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_handles</span><span class="p">(</span><span class="n">handles_key</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="c1"># Wait for all ops in the current stream (e.g. gradient</span>
            <span class="c1"># computation) to finish before reduce-scattering the gradient</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]):</span>
                <span class="n">orig_grad_data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_reduce</span><span class="p">()</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_low_precision_hook_enabled</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="c1"># Cast gradient to precision in which it should be communicated.</span>
                    <span class="c1"># If a low precision hook is registered and reduce_dtype is specified</span>
                    <span class="c1"># in `MixedPrecision`, communication hook will take care of</span>
                    <span class="c1"># casting to lower precision and back.</span>
                    <span class="c1"># TODO: Make this a communication hook when communication hooks</span>
                    <span class="c1"># are implemented for FSDP. Note that this is a noop if the</span>
                    <span class="c1"># reduce_dtype matches the param dtype.</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">reduce_dtype</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">is_first_iter</span><span class="p">:</span>
                    <span class="c1"># For all sharding strategies communication is performed through `_communication_hook`:</span>
                    <span class="c1"># default comm hooks are: `reduce_scatter` for sharded strategies and</span>
                    <span class="c1"># `all_reduce` for non-sharded strategies. This checks asserts that `_communication_hook`</span>
                    <span class="c1"># and `_communication_hook_state`, required for communication not `None`.`</span>
                    <span class="n">p_assert</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="s2">&quot;Communication hook should not be None&quot;</span>
                    <span class="p">)</span>
                    <span class="n">p_assert</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="s2">&quot;Communication hook state should not be None&quot;</span>
                    <span class="p">)</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">:</span>
                    <span class="c1"># We clear `param.grad` to permit repeated gradient</span>
                    <span class="c1"># computations when this FSDP module is called multiple times.</span>
                    <span class="c1"># This is to avoid a race among multiple re-entrant backward</span>
                    <span class="c1"># passes. For example, the second backward pass computation</span>
                    <span class="c1"># precedes ahead of the first backward pass reduction, which is</span>
                    <span class="c1"># possible since the reduction is in a different stream and is</span>
                    <span class="c1"># async. Then, the first backward pass may be incorrectly</span>
                    <span class="c1"># reducing the second backward pass&#39;s `param.grad`.</span>
                    <span class="c1"># The reduced gradients are accumulated in</span>
                    <span class="c1"># `param._saved_grad_shard`, and the gradient reductions can</span>
                    <span class="c1"># happen in arbitrary order, though we tolerate this due to the</span>
                    <span class="c1"># (approximate) commutativity of floating-point addition.</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">grad_flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                    <span class="n">chunks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">grad_flatten</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">))</span>
                    <span class="n">num_pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">*</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="n">grad</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="n">input_flattened</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">grad_flatten</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_pad</span><span class="p">])</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook_state</span><span class="p">,</span> <span class="n">input_flattened</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">_cast_grad_to_param_dtype</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>

                    <span class="c1"># To support gradient accumulation outside `no_sync()`, we save</span>
                    <span class="c1"># the gradient data to `param._saved_grad_shard` before the</span>
                    <span class="c1"># backward pass, accumulate gradients into it here, and set</span>
                    <span class="c1"># `param.grad` with the accumulated value at the end of the</span>
                    <span class="c1"># backward pass in preparation for the optimizer step.</span>
                    <span class="n">accumulate_grad</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">accumulate_grad</span><span class="p">:</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="s2">&quot;Shape mismatch when accumulating gradients: &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;existing grad shape=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;new grad shape=</span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="p">)</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="s2">&quot;Device mismatch when accumulating gradients: &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;existing grad device=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;new grad device=</span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="p">)</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span> <span class="o">+=</span> <span class="n">output</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span> <span class="o">=</span> <span class="n">output</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook_state</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

                    <span class="c1"># For NO_SHARD keeping grads in the reduced precision, we</span>
                    <span class="c1"># can simply omit the cast as needed, we can&#39;t do this for</span>
                    <span class="c1"># other sharding strategies because grad field is assigned</span>
                    <span class="c1"># in _finalize_params. TODO (rvarm1) this divergence in</span>
                    <span class="c1"># logic is not ideal.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_keep_low_precision_grads</span><span class="p">():</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_cast_grad_to_param_dtype</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>

                <span class="c1"># Regardless of sharding or not, offload the grad to CPU if we are</span>
                <span class="c1"># offloading params. This is so param and grad reside on same device</span>
                <span class="c1"># which is needed for the optimizer step.</span>
                <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                    <span class="c1"># We specify non_blocking=True</span>
                    <span class="c1"># and ensure the appropriate synchronization is done by waiting</span>
                    <span class="c1"># streams in _wait_for_post_backward.</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">_cpu_grad</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">)</span>
                    <span class="c1"># Don&#39;t let this memory get reused until after the transfer.</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

                <span class="c1"># After _post_backward_hook returns, orig_grad_data will eventually</span>
                <span class="c1"># go out of scope, at which point it could otherwise be freed for</span>
                <span class="c1"># further reuse by the main stream while the div/reduce_scatter/copy</span>
                <span class="c1"># are underway in the post_backward stream. See:</span>
                <span class="c1"># github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py</span>
                <span class="n">orig_grad_data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_cast_grad_to_param_dtype</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">grad</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">param</span><span class="p">:</span> <span class="n">FlatParameter</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Casts gradient ``grad`` back to the full parameter dtype so that the</span>
<span class="sd">        optimizer step runs with that dtype. This performs an actual cast if</span>
<span class="sd">        1. parameters were in reduced precision during the forward since then</span>
<span class="sd">        gradients would be in that reduced precision, or</span>
<span class="sd">        2. parameters were not in reduced precision but gradients were in</span>
<span class="sd">        reduced precision for communication.</span>
<span class="sd">        However, if a low precision communication hook is registered, then this</span>
<span class="sd">        dtype cast happens in the hook instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_low_precision_hook_enabled</span><span class="p">()</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_reduce</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">low_prec_grad_data</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">data</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="c1"># Do not let the low precision gradient memory get reused until</span>
            <span class="c1"># the cast to full parameter precision completes</span>
            <span class="n">low_prec_grad_data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_should_free_unsharded_flat_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">:</span> <span class="n">FlatParamHandle</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="ow">and</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">handle</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">HandleShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_queue_wait_for_post_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Queues a post-backward callback from the root FSDP instance, which</span>
<span class="sd">        should happen at the beginning of its pre-backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p_assert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span>
            <span class="s2">&quot;`_queue_wait_for_post_backward()` should be called on the root FSDP instance&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wait_for_post_backward</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_wait_for_post_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Wait for post-backward to finish. Only called on root instance.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;_wait_for_post_backward can only be called on root.&quot;</span>
        <span class="c1"># Root&#39;s training state might be backward_pre or backward_post depending on</span>
        <span class="c1"># if root parameter&#39;s post backward hook was called. The post-backward hook</span>
        <span class="c1"># may not have been called if gradient was not computed for this param/FSDP</span>
        <span class="c1"># module.</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="c1"># We need to wait for the non-blocking GPU -&gt;</span>
                <span class="c1"># CPU grad transfers to finish. We need to do this for GPU -&gt; CPU</span>
                <span class="c1"># copies because when grad is on CPU, it won&#39;t wait for any CUDA</span>
                <span class="c1"># stream to finish GPU -&gt; CPU copies unless we explicitly block the</span>
                <span class="c1"># host-side with synchronize().</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">next_iter</span><span class="p">()</span>

        <span class="c1"># A backward pass is done, clean up below.</span>
        <span class="k">def</span> <span class="nf">_catch_all_reshard</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">:</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Reshards full parameters that may have not been resharded in</span>
<span class="sd">            post_backward_hook. This can happen when an FSDP module&#39;s output</span>
<span class="sd">            is used in forward so its pre-backward fires unsharding the param,</span>
<span class="sd">            but post-backward does not fire since the output was not ultimately</span>
<span class="sd">            used in loss computation so FSDP parameter did not get a gradient.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># Note that we wrap resharding logic in a try-catch as a defensive</span>
            <span class="c1"># approach, as if an error is thrown, we are in the backwards pass,</span>
            <span class="c1"># and autograd would not print out much useful info about the actual</span>
            <span class="c1"># error hit.</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">free_unsharded_flat_params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">handles_to_reshard</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParamHandle</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                    <span class="c1"># TODO: This already-resharded check is brittle:</span>
                    <span class="c1"># https://github.com/pytorch/pytorch/issues/83956</span>
                    <span class="n">already_resharded</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">already_resharded</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">free_unsharded_flat_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_should_free_unsharded_flat_param</span><span class="p">(</span><span class="n">handle</span><span class="p">))</span>
                    <span class="n">handles_to_reshard</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_reshard</span><span class="p">(</span><span class="n">handles_to_reshard</span><span class="p">,</span> <span class="n">free_unsharded_flat_params</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">p_assert</span><span class="p">(</span>
                    <span class="kc">False</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;Got exception while resharding module </span><span class="si">{</span><span class="n">fsdp_module</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">raise_assertion_error</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

        <span class="k">def</span> <span class="nf">_finalize_params</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">:</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="sd">&quot;&quot;&quot;Helper used below on all fsdp modules.&quot;&quot;&quot;</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_post_backward_hook_state&quot;</span><span class="p">):</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_post_backward_hook_state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="s2">&quot;p._post_backward_hook_state fields are not valid.&quot;</span>
                        <span class="p">)</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">_post_backward_hook_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="nb">delattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_post_backward_hook_state&quot;</span><span class="p">)</span>
                    <span class="c1"># Preserve the gradient accumulation state if not</span>
                    <span class="c1"># synchronizing: `p.grad` remains the unsharded gradient</span>
                    <span class="c1"># accumulated from prior `no_sync()` iterations, and</span>
                    <span class="c1"># `p._saved_grad_shard` remains the sharded gradient from</span>
                    <span class="c1"># the last synchronized iteration</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="c1"># Set `p.grad` as needed to ensure optimizer correctness</span>
                    <span class="c1"># since optimizers operate on the `grad` attribute</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_cpu_grad&quot;</span><span class="p">):</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                            <span class="sa">f</span><span class="s2">&quot;Device mismatch: p=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;p._cpu_grad=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">):</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;Device mismatch: p=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;p._saved_grad_shard=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="c1"># Check if post-backward was called for this param (FSDP unit).</span>
                        <span class="c1"># TODO: This logic will have to be revisited when non-recursive wrapping</span>
                        <span class="c1"># lands. If it was not called, there is no new gradient to accumulate</span>
                        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">_post_backward_called</span><span class="p">:</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span>
                            <span class="k">if</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">_mixed_precision_keep_low_precision_grads</span><span class="p">():</span>
                                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                                    <span class="n">fsdp_module</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
                                <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="ow">not</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_post_backward_called</span><span class="p">,</span>
                            <span class="s2">&quot;All sharded parameters that received a gradient &quot;</span>
                            <span class="s2">&quot;should use `_saved_grad_shard`&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">):</span>
                        <span class="nb">delattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">)</span>

                    <span class="n">p_assert</span><span class="p">(</span>
                        <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;_post_backward_called&#39;</span><span class="p">),</span>
                        <span class="s2">&quot;Expected flag _post_backward_called to be set on param.&quot;</span>
                    <span class="p">)</span>
                    <span class="c1"># Reset _post_backward_called in preparation for the next iteration.</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">_post_backward_called</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Update root and nested FSDP&#39;s hooks and flags.</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># includes self</span>
            <span class="n">_finalize_params</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
            <span class="n">_catch_all_reshard</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
            <span class="n">m</span><span class="o">.</span><span class="n">_ran_pre_backward_hook</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="n">m</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                <span class="n">handle</span><span class="o">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">HandleTrainingState</span><span class="o">.</span><span class="n">IDLE</span>
            <span class="n">m</span><span class="o">.</span><span class="n">_handles_prefetched</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
                <span class="c1"># reset this flag for cases like &quot;one forward pass + multiple backward passes&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_param_exec_order_policy</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_policy_second_iter_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_param_exec_order_policy_second_iter_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Let the parameters in self._fsdp_params_exec_order ordered based on</span>
        <span class="c1"># the execution order in the forward pass.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span>
                    <span class="n">m</span><span class="p">,</span> <span class="s2">&quot;_param_exec_order_policy&quot;</span>
                <span class="p">),</span> <span class="s2">&quot;Non-root FSDP modules should also have _param_exec_order_policy attribute&quot;</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span>
                    <span class="n">m</span><span class="p">,</span> <span class="s2">&quot;_param_exec_order_prep_stage&quot;</span>
                <span class="p">),</span> <span class="s2">&quot;Non-root FSDP modules should also have _param_exec_order_prep_stage attribute&quot;</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># TODO (linjianma): Construct a fsdp_wrap_map whose keys are all children modules with a FSDP wrap,</span>
        <span class="c1"># and values are its FSDP wraps. These children FSDP wraps will be detached from the root FSDP module</span>
        <span class="c1"># and will be used to schedule the parameters (rebuild_full_params and reshard).</span>
        <span class="c1"># TODO (linjianma): Remove all internal FSDP wraps from the root FSDP module.</span>
        <span class="c1"># TODO (linjianma): Based on self._fsdp_params_exec_order, get the information</span>
        <span class="c1"># needed to patch the forward() function of each key in the fsdp_wrap_map. The rules are as follows:</span>
        <span class="c1"># 1: Before each forward(), rebuild_full_params of all parameters that are currently sharded and</span>
        <span class="c1"># will be used in the forward, and reshard all parameters that are currently full and will not be</span>
        <span class="c1"># used in the next forward()</span>
        <span class="c1"># 2: After each forward(), reshard all parameters just used in the forward, and rebuild_full_params of</span>
        <span class="c1"># all parameters that will be used next.</span>
        <span class="c1"># TODO (linjianma): Patch the forward of each model in the keys</span>
        <span class="c1"># of fsdp_wrap_map based on the information above.</span>

    <span class="k">def</span> <span class="nf">_assert_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TrainingState_</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TrainingState_</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Assert we are in the given state.&quot;&quot;&quot;</span>
        <span class="c1"># Since assert can be turned off and this error checking</span>
        <span class="c1"># is really important, we use explicit error checking</span>
        <span class="c1"># and raise a ValueError if needed.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">TrainingState_</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;expected to be in states </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2"> but current state &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="c1"># In case we are failing in the context of autograd hook, asserting</span>
            <span class="c1"># may not generate useful msg. So, let&#39;s print it to be sure.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Asserting FSDP instance is: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">traceback</span><span class="o">.</span><span class="n">print_stack</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.no_sync"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">no_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to disable gradient synchronizations across FSDP</span>
<span class="sd">        instances. Within this context, gradients will be accumulated in module</span>
<span class="sd">        variables, which will later be synchronized in the first</span>
<span class="sd">        forward-backward pass after exiting the context. This should only be</span>
<span class="sd">        used on the root FSDP instance and will recursively apply to all</span>
<span class="sd">        children FSDP instances.</span>

<span class="sd">        .. note:: This likely results in higher memory usage because FSDP will</span>
<span class="sd">            accumulate the full model gradients (instead of gradient shards)</span>
<span class="sd">            until the eventual sync.</span>

<span class="sd">        .. note:: When used with CPU offloading, the gradients will not be</span>
<span class="sd">            offloaded to CPU when inside the context manager. Instead, they</span>
<span class="sd">            will only be offloaded right after the eventual sync.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;`no_sync()` on inner FSDP instances is not supported&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="n">old_flags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="n">old_flags</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">))</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">old_flag</span> <span class="ow">in</span> <span class="n">old_flags</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">,</span> <span class="p">(</span>
                    <span class="s2">&quot;`_sync_gradients` was incorrectly set to &quot;</span>
                    <span class="s2">&quot;`True` while in the `no_sync()` context manager&quot;</span>
                <span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="o">=</span> <span class="n">old_flag</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params_with_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively returns a list of all module parameters that have a gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

<div class="viewcode-block" id="FullyShardedDataParallel.clip_grad_norm_"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">clip_grad_norm_</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clip all gradients at this point in time. The norm is computed over all</span>
<span class="sd">        gradients together, as if they were concatenated into a single vector.</span>
<span class="sd">        Gradients are modified in-place.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_norm (float or int): max norm of the gradients</span>
<span class="sd">            norm_type (float or int): type of the used p-norm. Can be ``&#39;inf&#39;``</span>
<span class="sd">                for infinity norm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Total norm of the parameters (viewed as a single vector).</span>

<span class="sd">        .. note:: This is analogous to ``torch.nn.utils.clip_grad_norm_`` but</span>
<span class="sd">            handles the partitioning and multiple devices per rank under the</span>
<span class="sd">            hood. The default torch util is not applicable here, because each</span>
<span class="sd">            rank only has a partial view of all the grads in the model, so</span>
<span class="sd">            calling it for FSDP models would lead to different scaling being</span>
<span class="sd">            applied per subset of model parameters.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives will be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_for_previous_optim_step</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;clip_grad_norm should only be called on the root (parent) instance&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>

        <span class="n">max_norm</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_norm</span><span class="p">)</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
        <span class="c1"># Computes the max norm for this shard&#39;s gradients and sync&#39;s across workers</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">_calc_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_with_grad</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">local_norm</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">local_norm</span> <span class="o">**</span> <span class="n">norm_type</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">max_norm</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">total_norm</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">total_norm</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">clip_coef</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># multiply by clip_coef, aka, (max_norm/total_norm).</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_with_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">))</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `optim_input` argument is deprecated and will be removed after PyTorch 1.13. You may remove it &quot;</span>
                <span class="s2">&quot;from your code without changing its functionality.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_is_using_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">optim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use the default behavior of `optim_input``</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use the `optim_input` code path</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="c1"># Use the `optim` code path</span>
        <span class="k">return</span> <span class="kc">False</span>

<div class="viewcode-block" id="FullyShardedDataParallel.full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">full_optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Consolidates the full optimizer state on rank 0 and returns it</span>
<span class="sd">        as a :class:`dict` following the convention of</span>
<span class="sd">        :meth:`torch.optim.Optimizer.state_dict`, i.e. with keys ``&quot;state&quot;``</span>
<span class="sd">        and ``&quot;param_groups&quot;``. The flattened parameters in ``FSDP`` modules</span>
<span class="sd">        contained in ``model`` are mapped back to their unflattened parameters.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks since synchronization</span>
<span class="sd">            primitives are used. However, if ``rank0_only=True``, then the</span>
<span class="sd">            state dict is only populated on rank 0, and all other ranks return</span>
<span class="sd">            an empty :class:`dict`.</span>

<span class="sd">        .. warning:: Unlike ``torch.optim.Optimizer.state_dict()``, this method</span>
<span class="sd">            uses full parameter names as keys instead of parameter IDs.</span>

<span class="sd">        .. note:: Like in :meth:`torch.optim.Optimizer.state_dict`, the tensors</span>
<span class="sd">            contained in the optimizer state dict are not cloned, so there may</span>
<span class="sd">            be aliasing surprises. For best practices, consider saving the</span>
<span class="sd">            returned optimizer state dict immediately, e.g. using</span>
<span class="sd">            ``torch.save()``.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                were passed into the optimizer ``optim``.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer ``optim`` representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            rank0_only (bool): If ``True``, saves the populated :class:`dict`</span>
<span class="sd">                only on rank 0; if ``False``, saves it on all ranks. (Default:</span>
<span class="sd">                ``True``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group or ``None`` if using</span>
<span class="sd">                the default process group. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: A :class:`dict` containing the optimizer state for</span>
<span class="sd">            ``model`` &#39;s original unflattened parameters and including keys</span>
<span class="sd">            &quot;state&quot; and &quot;param_groups&quot; following the convention of</span>
<span class="sd">            :meth:`torch.optim.Optimizer.state_dict`. If ``rank0_only=True``,</span>
<span class="sd">            then nonzero ranks return an empty :class:`dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_optim_state_dict</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
            <span class="n">shard_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
            <span class="n">using_optim_input</span><span class="o">=</span><span class="n">using_optim_input</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.sharded_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sharded_optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The API is similar to :meth:`full_optim_state_dict` but this API chunks</span>
<span class="sd">        all non-zero-dimension states to :class:`ShardedTensor` to save memory.</span>
<span class="sd">        This API should only be used when the model ``state_dict`` is derived</span>
<span class="sd">        with the context manager ``with state_dict_type(SHARDED_STATE_DICT):``.</span>

<span class="sd">        For the detailed usage, refer to :meth:`full_optim_state_dict`.</span>

<span class="sd">        .. warning:: The returned state dict contains ``ShardedTensor`` and</span>
<span class="sd">            cannot be directly used by the regular ``optim.load_state_dict``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># TODO: The ultimate goal of the optimizer state APIs should be the same</span>
        <span class="c1"># as state_dict/load_state_dict -- using one API to get optimizer states</span>
        <span class="c1"># and one API to load optimizer states. ``state_dict_type`` will be used</span>
        <span class="c1"># to decide which optimizer states should be returned.</span>
        <span class="c1"># There are currently two APIs to load a full optimizer state. So the</span>
        <span class="c1"># first step of the unification is to merge the two full optimizer state</span>
        <span class="c1"># loading APIs.</span>
        <span class="c1"># Task: https://github.com/pytorch/pytorch/issues/82232</span>
        <span class="k">return</span> <span class="n">_optim_state_dict</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">shard_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
            <span class="n">using_optim_input</span><span class="o">=</span><span class="n">using_optim_input</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.shard_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">shard_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shards the full optimizer state dict ``full_optim_state_dict`` by</span>
<span class="sd">        remapping the state to flattened parameters instead of unflattened</span>
<span class="sd">        parameters and restricting to only this rank&#39;s part of the optimizer</span>
<span class="sd">        state. The first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)</span>
<span class="sd">            &gt;&gt;&gt; torch.save(full_osd, PATH)</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = torch.load(PATH)</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Dict[str, Any]): Optimizer state dict</span>
<span class="sd">                corresponding to the unflattened parameters and holding the</span>
<span class="sd">                full non-sharded optimizer state.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            optim (Optional[torch.optim.Optimizer]): Optimizer that will load</span>
<span class="sd">                the state dict returned by this method. This is the preferred</span>
<span class="sd">                argument to use over ``optim_input``. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">_flatten_optim_state_dict</span><span class="p">(</span>
            <span class="n">full_optim_state_dict</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">(</span>
            <span class="n">sharded_osd</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">,</span> <span class="n">using_optim_input</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.flatten_sharded_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">flatten_sharded_optim_state_dict</span><span class="p">(</span>
        <span class="n">sharded_optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The API is similar to :meth:`shard_full_optim_state_dict`. The only</span>
<span class="sd">        difference is that the input ``sharded_optim_state_dict`` should be</span>
<span class="sd">        returned from :meth:`sharded_optim_state_dict`. Therefore, there will</span>
<span class="sd">        be all-gather calls on each rank to gather ``ShardedTensor`` s.</span>

<span class="sd">        Args:</span>
<span class="sd">            sharded_optim_state_dict (Dict[str, Any]): Optimizer state dict</span>
<span class="sd">                corresponding to the unflattened parameters and holding the</span>
<span class="sd">                sharded optimizer state.</span>
<span class="sd">            model (torch.nn.Module):</span>
<span class="sd">                Refer to :meth:``shard_full_optim_state_dict``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Refer to :meth:`shard_full_optim_state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># TODO: The implementation is the same as ``shard_full_optim_state_dict``.</span>
        <span class="c1"># See the TODO in ``shard_full_optim_state_dict`` for the future</span>
        <span class="c1"># unification plan.</span>
        <span class="n">flattened_osd</span> <span class="o">=</span> <span class="n">_flatten_optim_state_dict</span><span class="p">(</span>
            <span class="n">sharded_optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">shard_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">(</span>
            <span class="n">flattened_osd</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">,</span> <span class="n">using_optim_input</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.scatter_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">scatter_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Scatters the full optimizer state dict from rank 0 to all other ranks,</span>
<span class="sd">        returning the sharded optimizer state dict on each rank. The return</span>
<span class="sd">        value is the same as :meth:`shard_full_optim_state_dict`, and on rank</span>
<span class="sd">        0, the first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim, new_group = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Optional[Dict[str, Any]]): Optimizer state</span>
<span class="sd">                dict corresponding to the unflattened parameters and holding</span>
<span class="sd">                the full non-sharded optimizer state if on rank 0; the argument</span>
<span class="sd">                is ignored on nonzero ranks.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            optim (Optional[torch.optim.Optimizer]): Optimizer that will load</span>
<span class="sd">                the state dict returned by this method. This is the preferred</span>
<span class="sd">                argument to use over ``optim_input``. (Default: ``None``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group or ``None`` if</span>
<span class="sd">                using the default process group. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Try to use the passed-in process group, the model&#39;s process group,</span>
        <span class="c1"># or the default process group (i.e. `None`) in that priority order</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;process_group&quot;</span><span class="p">):</span>
            <span class="n">group</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">process_group</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="c1"># Check for a valid broadcast device, preferring GPU when available</span>
        <span class="n">using_nccl</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_check_for_nccl_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">broadcast_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> \
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">using_nccl</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;NCCL requires a GPU for collectives&quot;</span><span class="p">)</span>
        <span class="c1"># Flatten the optimizer state dict and construct a copy with the</span>
        <span class="c1"># positive-dimension tensors&#39; shapes in place of the tensors themselves</span>
        <span class="c1"># since those tensors will be broadcast separately to avoid copying</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">full_optim_state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rank 0 must pass in the full optimizer state dict&quot;</span><span class="p">)</span>
            <span class="n">flat_osd</span> <span class="o">=</span> <span class="n">_flatten_optim_state_dict</span><span class="p">(</span>
                <span class="n">full_optim_state_dict</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">shard_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">processed_osd</span> <span class="o">=</span> <span class="n">_process_pos_dim_tensor_state</span><span class="p">(</span><span class="n">flat_osd</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
        <span class="c1"># Broadcast the optim state dict without positive-dimension tensor</span>
        <span class="c1"># state and the FSDP parameter IDs from rank 0 to all ranks</span>
        <span class="n">processed_osd</span> <span class="o">=</span> <span class="n">_broadcast_processed_optim_state_dict</span><span class="p">(</span>
            <span class="n">processed_osd</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Broadcast positive-dimension tensor state (both sharded tensors for</span>
        <span class="c1"># FSDP parameters and unsharded tensors for non-FSDP parameters)</span>
        <span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">_broadcast_pos_dim_tensor_states</span><span class="p">(</span>
            <span class="n">processed_osd</span><span class="p">,</span> <span class="n">flat_osd</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span> <span class="n">broadcast_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Rekey the optimizer state dict to use parameter IDs according to this</span>
        <span class="c1"># rank&#39;s `optim`</span>
        <span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">(</span>
            <span class="n">sharded_osd</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">,</span> <span class="n">using_optim_input</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sharded_osd</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.rekey_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">rekey_optim_state_dict</span><span class="p">(</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">optim_state_key_type</span><span class="p">:</span> <span class="n">OptimStateKeyType</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Re-keys the optimizer state dict ``optim_state_dict`` to use the key</span>
<span class="sd">        type ``optim_state_key_type``. This can be used to achieve</span>
<span class="sd">        compatibility between optimizer state dicts from models with FSDP</span>
<span class="sd">        instances and ones without.</span>

<span class="sd">        To re-key an FSDP full optimizer state dict (i.e. from</span>
<span class="sd">        :meth:`full_optim_state_dict`) to use parameter IDs and be loadable to</span>
<span class="sd">        a non-wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_optim.load_state_dict(rekeyed_osd)</span>

<span class="sd">        To re-key a normal optimizer state dict from a non-wrapped model to be</span>
<span class="sd">        loadable to a wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; osd = nonwrapped_optim.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The optimizer state dict re-keyed using the</span>
<span class="sd">            parameter keys specified by ``optim_state_key_type``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">optim_state_key_type</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">,</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">osd</span> <span class="o">=</span> <span class="n">optim_state_dict</span>  <span class="c1"># alias</span>
        <span class="c1"># Validate that the existing parameter keys are uniformly typed</span>
        <span class="n">uses_param_name_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="n">uses_param_id_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">))</span>
            <span class="ow">or</span> <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Invalid parameter keys: </span><span class="si">{</span><span class="n">osd</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>
        <span class="c1"># Return directly if the existing key type matches the target key type</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span> <span class="ow">and</span>
            <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">))</span> <span class="ow">or</span> \
            <span class="p">(</span><span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span> <span class="ow">and</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">osd</span>
        <span class="c1"># Otherwise, actually perform the re-keying</span>
        <span class="n">new_osd</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">:</span>  <span class="c1"># ID -&gt; name</span>
            <span class="n">param_id_to_param</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_param_id_to_param_from_optim_input</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">using_optim_input</span>
                <span class="k">else</span> <span class="n">_get_param_id_to_param</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_param_name</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_id_to_param_name</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">param_to_param_name</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_id_to_param</span>
            <span class="p">]</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span>
                    <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">param_id</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                <span class="p">])</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">elif</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">:</span>  <span class="c1"># name -&gt; ID</span>
            <span class="n">param_name_to_param</span> <span class="o">=</span> <span class="n">_get_param_name_to_param</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_to_param_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_param_to_param_id_from_optim_input</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">using_optim_input</span>
                <span class="k">else</span> <span class="n">_get_param_to_param_id</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Because not all model parameters may be passed as the optimizer</span>
            <span class="c1"># input, we may need to drop some parameters from this mapping</span>
            <span class="n">param_name_to_param_id</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name</span><span class="p">:</span> <span class="n">param_to_param_id</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_name_to_param</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_param_id</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span>
                    <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                <span class="p">])</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">return</span> <span class="n">new_osd</span>  <span class="c1"># should never reach here</span></div>

    <span class="k">def</span> <span class="nf">_get_default_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a default communication hook based on a sharding strategy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">!=</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_hooks</span><span class="o">.</span><span class="n">reduce_scatter_hook</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_hooks</span><span class="o">.</span><span class="n">allreduce_hook</span>

    <span class="k">def</span> <span class="nf">_get_default_comm_hook_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a default communication hook state based on a sharding strategy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">default_hooks</span><span class="o">.</span><span class="n">DefaultState</span><span class="p">(</span><span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.register_comm_hook"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">callable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a communication hook which is an enhancement that provides a</span>
<span class="sd">        flexible hook to users where they can specify how FSDP aggregates gradients</span>
<span class="sd">        across multiple workers.</span>
<span class="sd">        This hook can be used to implement several algorithms like</span>
<span class="sd">        `GossipGrad &lt;https://arxiv.org/abs/1803.05880&gt;`_ and gradient compression</span>
<span class="sd">        which involve different communication strategies for</span>
<span class="sd">        parameter syncs while training with :class:`FullyShardedDataParallel`.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            FSDP communication hook should be registered before running an initial forward pass</span>
<span class="sd">            and only once.</span>

<span class="sd">        Args:</span>
<span class="sd">            state (object): Passed to the hook to maintain any state information during the training process.</span>
<span class="sd">                            Examples include error feedback in gradient compression,</span>
<span class="sd">                            peers to communicate with next in `GossipGrad &lt;https://arxiv.org/abs/1803.05880&gt;`_, etc.</span>
<span class="sd">                            It is locally stored by each worker</span>
<span class="sd">                            and shared by all the gradient tensors on the worker.</span>
<span class="sd">            hook (Callable): Callable, which has one of the following signatures:</span>
<span class="sd">                            1) ``hook: Callable[torch.Tensor] -&gt; None``:</span>
<span class="sd">                            This function takes in a Python tensor, which represents</span>
<span class="sd">                            the full, flattened, unsharded gradient with respect to all variables</span>
<span class="sd">                            corresponding to the model this FSDP unit is wrapping</span>
<span class="sd">                            (that are not wrapped by other FSDP sub-units).</span>
<span class="sd">                            It then performs all necessary processing and returns ``None``;</span>
<span class="sd">                            2) ``hook: Callable[torch.Tensor, torch.Tensor] -&gt; None``:</span>
<span class="sd">                            This function takes in two Python tensors, the first one represents</span>
<span class="sd">                            the full, flattened, unsharded gradient with respect to all variables</span>
<span class="sd">                            corresponding to the model this FSDP unit is wrapping</span>
<span class="sd">                            (that are not wrapped by other FSDP sub-units). The latter</span>
<span class="sd">                            represents a pre-sized tensor to store a chunk of a sharded gradient after</span>
<span class="sd">                            reduction.</span>
<span class="sd">                            In both cases, callable performs all necessary processing and returns ``None``.</span>
<span class="sd">                            Callables with signature 1 are expected to handle gradient communication for a `NO_SHARD` case.</span>
<span class="sd">                            Callables with signature 2 are expected to handle gradient communication for sharded cases.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_is_root</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;register_comm_hook can only be called on a root instance.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_hook_registered</span><span class="p">,</span> <span class="s2">&quot;communication hook can be only registered once&quot;</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_hook_registered</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">assert</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_comm_hook</span><span class="p">(),</span>\
                <span class="sa">f</span><span class="s2">&quot;communication hook should be default, but it is </span><span class="si">{</span><span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook_state</span> <span class="o">=</span> <span class="n">state</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="o">=</span> <span class="n">hook</span></div>


    <span class="k">def</span> <span class="nf">_init_param_exec_order_wrap_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">]</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;module&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">auto_wrap_policy</span><span class="p">,</span> <span class="s2">&quot;tracing_config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_TORCH_FX_AVAIL</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;tracing_config should be None when torch.fx is not enabled&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span><span class="p">,</span>
            <span class="n">TracingConfig</span>
        <span class="p">):</span>
            <span class="n">tracer</span> <span class="o">=</span> <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span><span class="o">.</span><span class="n">tracer</span>
            <span class="n">execution_info</span> <span class="o">=</span> <span class="n">_init_execution_info</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span>
                <span class="p">),</span> <span class="s2">&quot;The input module of _patch_tracer should not contain FSDP modules&quot;</span>

            <span class="k">with</span> <span class="n">_patch_tracer</span><span class="p">(</span>
                <span class="n">tracer</span><span class="o">=</span><span class="n">tracer</span><span class="p">,</span>
                <span class="n">root_module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
                <span class="n">execution_info</span><span class="o">=</span><span class="n">execution_info</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span><span class="o">.</span><span class="n">concrete_args</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;tracer.trace failed inside _init_param_exec_order_wrap_policy&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; with the error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;tracing_config should either be an instance of TracingConfig or be None&quot;</span>
        <span class="c1"># The initial FSDP wrapping is done with auto_wrap_policy.init_policy</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">init_policy</span>
        <span class="bp">self</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_policy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># self._param_exec_order_prep_stage is set to True before we get the execution order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># A list that stores the flatten parameters and its name based on the parameter execution order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">_TORCH_FX_AVAIL</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">auto_wrap_policy</span><span class="o">.</span><span class="n">tracing_config</span><span class="p">,</span>
            <span class="n">TracingConfig</span>
        <span class="p">):</span>
            <span class="c1"># Initialize a dict that maps each module to its parent FSDP wrap</span>
            <span class="n">module_to_fsdp</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">wrap</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">module_to_fsdp</span><span class="p">[</span><span class="n">wrap</span><span class="o">.</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrap</span>
            <span class="c1"># Set self._fsdp_params_exec_order based on execution_info.module_forward_order.</span>
            <span class="c1"># TODO (linjianma): self._fsdp_params_exec_order will be set based on</span>
            <span class="c1"># the parameter execution order rather than module_forward_order,</span>
            <span class="c1"># once the non-recursive wrapping policy is fully implemented.</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">execution_info</span><span class="o">.</span><span class="n">module_forward_order</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module_to_fsdp</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">flat_param</span> <span class="ow">in</span> <span class="n">module_to_fsdp</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flat_param</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="c1"># Assignment by reference, so each children FSDP wrap has access to</span>
                <span class="c1"># the _fsdp_params_exec_order of the root module</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_params_exec_order</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_param_exec_order_policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_policy</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span>

    <span class="k">def</span> <span class="nf">_use_param_exec_order_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_param_exec_order_policy&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_policy</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_param_exec_order_prep_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">is_prep_stage</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_param_exec_order_prep_stage&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_exec_order_prep_stage</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_prep_stage</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_params_exec_order_hook_handle&quot;</span><span class="p">)</span>
                <span class="p">),</span> <span class="s2">&quot;When not in execution order prep stage, all _params_exec_order_hook_handle should be removed.&quot;</span>
        <span class="k">return</span> <span class="n">is_prep_stage</span></div>


<span class="k">def</span> <span class="nf">_calc_grad_norm</span><span class="p">(</span><span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate gradient norm of an iterable of parameters.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Total norm of the parameters (viewed as a single vector).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">par</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute the norm in full precision no matter what</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">par</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">parameters</span>
                <span class="p">]</span>
            <span class="p">),</span>
            <span class="n">p</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">local_norm</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">local_norm</span>


<span class="k">def</span> <span class="nf">_get_param_to_unflat_param_names</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">dedup_shared_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a mapping from flattened parameter (including non-FSDP-module</span>
<span class="sd">    parameters) to its unflattened parameter names. For non-FSDP-module</span>
<span class="sd">    parameters, these mapped-to lists always contain a single element. The</span>
<span class="sd">    unflattened parameter names should match the keys of the model state dict.</span>

<span class="sd">    For shared parameters, only the first parameter name is included (following</span>
<span class="sd">    the ``torch.nn.Module.parameters()`` order).</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">            :class:`FullyShardedDataParallel` instance).</span>
<span class="sd">        dedup_shared_params (bool): If ``True``, only includes the first</span>
<span class="sd">            list of unflattened parameter names corresponding to a parameter</span>
<span class="sd">            in the module walk order; if ``False``, then includes all of the</span>
<span class="sd">            unflattened parameter names.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">module_fn</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">param_to_unflat_param_names</span><span class="p">):</span>
        <span class="c1"># For FSDP modules, only add the entry when considering the contained</span>
        <span class="c1"># `FlattenParamsWrapper` to avoid duplication</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">module_prefixed_param_names</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">_prefixed_param_names</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlatParameter</span>
                    <span class="k">else</span> <span class="p">[</span><span class="n">param_name</span><span class="p">]</span>
                <span class="p">)</span>  <span class="c1"># prefixed from `module`</span>
                <span class="n">fully_prefixed_param_names</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_prefixed_param_names</span>
                <span class="p">]</span>  <span class="c1"># fully prefixed from the top level including `prefix`</span>
                <span class="c1"># If this parameter has already been visited, then it is a</span>
                <span class="c1"># shared parameter; then, only take the first parameter name</span>
                <span class="n">is_shared_param</span> <span class="o">=</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_unflat_param_names</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shared_param</span><span class="p">:</span>
                    <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">fully_prefixed_param_names</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">dedup_shared_params</span><span class="p">:</span>
                    <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">fully_prefixed_param_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">return_fn</span><span class="p">(</span><span class="n">param_to_unflat_param_names</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">param_to_unflat_param_names</span>

    <span class="n">param_to_unflat_param_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">return</span> <span class="n">_apply_to_modules</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">module_fn</span><span class="p">,</span> <span class="n">return_fn</span><span class="p">,</span> <span class="n">param_to_unflat_param_names</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_param_to_param_name</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a mapping from parameters to their parameter names. ``model``</span>
<span class="sd">    should not contain any :class:`FullyShardedDataParallel` instances, which</span>
<span class="sd">    means that none of the parameters should be ``FlatParameter`` s. As a</span>
<span class="sd">    result, compared to :meth:`_get_param_to_unflat_param_names`, the mapped</span>
<span class="sd">    values may be flattened from singleton :class:`list` s to the contained</span>
<span class="sd">    names themselves.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): Root module, which should not contain any</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">param_to_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`_get_param_to_unflat_param_names()` &quot;</span> \
            <span class="s2">&quot;should not construct empty lists&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Each parameter should only map to one parameter name but got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">param</span><span class="p">:</span> <span class="n">param_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">param_to_param_name</span>


<span class="k">def</span> <span class="nf">_get_param_name_to_param</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Constructs the inverse mapping of :meth:`_get_param_to_param_name`.&quot;&quot;&quot;</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_param_name</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">param_to_param_name</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">param_to_param_name</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">clean_tensor_name</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Cleans the parameter or buffer name by removing any module wrapper</span>
<span class="sd">    prefixes.&quot;&quot;&quot;</span>
    <span class="c1"># Call `replace()` twice separately since the name may not have both</span>
    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">tensor_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">tensor_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FPW_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="c1"># TODO: Explicitly replacing checkpoint_wrapper prefix is not ideal,</span>
    <span class="c1"># as it increases coupling between CheckpointWrapper and FSDP. This is also not</span>
    <span class="c1"># scalable for additional wrapped modules, we should come up with a general solution</span>
    <span class="c1"># for this issue.</span>
    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">tensor_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">_CHECKPOINT_PREFIX</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_name</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
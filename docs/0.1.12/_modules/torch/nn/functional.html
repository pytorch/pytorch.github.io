

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch 0.1.12_2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="../../../_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="PyTorch 0.1.12_2 documentation" href="../../../index.html"/>
        <link rel="up" title="torch" href="../../torch.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
<a href="https://pytorch.org/docs/versions.html">0.1.12_2 &#x25BC</a>                
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#random-sampling">Random sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">Sparse tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#linear"><span class="hidden-section">Linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#multi-gpu-layers">Multi-GPU layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id13"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id14"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id15"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id16"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id17"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id18"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id19"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id20"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id21"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id22"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id23"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id24"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id25"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id26"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id27"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id28"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id29"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id30"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id31"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id32"><span class="hidden-section">dropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id33">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id34">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad"><span class="hidden-section">pad</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#algorithms">Algorithms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#function"><span class="hidden-section">Function</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#streams-and-events">Streams and events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/torchvision.html">torchvision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#captions">Captions:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#detection">Detection:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#stl10">STL10</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/models.html">torchvision.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL.Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/utils.html">torchvision.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../torch.html">torch</a> &raquo;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_functions</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="k">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">._functions.padding</span> <span class="k">import</span> <span class="n">ConstantPad2d</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span>

<span class="c1"># Convolutions</span>
<span class="n">ConvNd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">ConvNd</span>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv2d">[docs]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D convolution over an input image composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters tensor (out_channels, in_channels/groups, kH, kW)</span>
<span class="sd">        bias: optional bias tensor (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="sd">          a tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(8,4,3,3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(1,4,5,5))</span>
<span class="sd">        &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv1d">[docs]</a><span class="k">def</span> <span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iW)</span>
<span class="sd">        weight: filters of shape (out_channels, in_channels, kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, default 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50))</span>
<span class="sd">        &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv3d">[docs]</a><span class="k">def</span> <span class="nf">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 3D convolution over an input image composed of several input</span>
<span class="sd">        planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters tensor of shape (out_channels, in_channels, kT, kH, kW)</span>
<span class="sd">        bias: optional bias tensor of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="sd">          a tuple (st x sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50, 10, 20))</span>
<span class="sd">        &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose1d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span>
               <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose2d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or a</span>
<span class="sd">          tuple (padh x padw). Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>
<span class="sd">        output_padding: A zero-padding of 0 &lt;= padding &lt; stride that should be</span>
<span class="sd">          added to the output. Can be a single number or a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose3d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or a</span>
<span class="sd">          tuple (padh x padw). Default: 0</span>
<span class="sd">        output_padding: A zero-padding of 0 &lt;= padding &lt; stride that should be</span>
<span class="sd">          added to the output. Can be a single number or a tuple. Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># Pooling</span>
<div class="viewcode-block" id="avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D average pooling over an input signal composed of several</span>
<span class="sd">    input planes.</span>

<span class="sd">    See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window</span>
<span class="sd">        stride: the stride of the window. Default value is :attr:`kernel_size`</span>
<span class="sd">        padding: implicit zero padding to be added on both sides</span>
<span class="sd">        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape</span>
<span class="sd">        count_include_pad: when True, will include the zero-padding in the averaging calculation</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.Tensor([[[1,2,3,4,5,6,7]]]))</span>
<span class="sd">        &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">          2  4  6</span>
<span class="sd">        [torch.FloatTensor of size 1x1x3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;expected 3D input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                  <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies 2D average-pooling operation in kh x kw regions by step size</span>
<span class="sd">    dh x dw steps. The number of output features is equal to the number of</span>
<span class="sd">    input planes.</span>

<span class="sd">    See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="sd">        kernel_size: size of the pooling region, a single number or a</span>
<span class="sd">          tuple (kh x kw)</span>
<span class="sd">        stride: stride of the pooling operation, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default is equal to kernel size</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or</span>
<span class="sd">          a tuple (padh x padw), Default: 0</span>
<span class="sd">        ceil_mode: operation that defines spatial output shape</span>
<span class="sd">        count_include_pad: divide by the number of elements inside the</span>
<span class="sd">          original non-padded image or kh * kw</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                     <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="avg_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies 3D average-pooling operation in kt x kh x kw regions by step</span>
<span class="sd">    size kt x dh x dw steps. The number of output features is equal to the</span>
<span class="sd">    number of input planes / dt.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="c1"># share the same interface</span>
<div class="viewcode-block" id="max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                     <span class="n">return_indices</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                     <span class="n">return_indices</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                     <span class="n">return_indices</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_size</span>

    <span class="n">output_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                         <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output_size</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="n">output_size</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>

<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="threshold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.threshold">[docs]</a><span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardtanh">[docs]</a><span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.leaky_relu">[docs]</a><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.prelu">[docs]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.rrelu">[docs]</a><span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">RReLU</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logsigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.logsigmoid">[docs]</a><span class="k">def</span> <span class="nf">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">(</span><span class="n">lambd</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softplus"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softplus">[docs]</a><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softplus</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softshrink">[docs]</a><span class="k">def</span> <span class="nf">softshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">(</span><span class="n">lambd</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="c1"># etc.</span>

<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">state</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">state</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Bilinear</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">state</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">state</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># loss</span>

<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or `(N, C, H, W)` in case of 2D - Loss</span>
<span class="sd">        target: :math:`(N)` where each value is `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight given to each</span>
<span class="sd">                class. If given, has to be a Variable of size &quot;nclasses&quot;</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight: the class-weights given as input to the constructor</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # input is of size nBatch x nClasses = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5))</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="sd">        &gt;&gt;&gt; target = autograd.Variable(torch.LongTensor([1, 0, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or 4 dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The `Kullback-Leibler divergence`_ Loss.</span>

<span class="sd">    See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        size_average: if True the output is divided by the number of elements</span>
<span class="sd">          in input tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">size_average</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in one single class.</span>

<span class="sd">    See :class:`torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable :math:`(N, C)` where `C = number of classes`</span>
<span class="sd">        target: Variable :math:`(N)` where each value is `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">                class. If given, has to be a Tensor of size &quot;nclasses&quot;</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output:</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">size_average</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rearranges elements in a tensor of shape ``[*, C*r^2, H, W]`` to a</span>
<span class="sd">    tensor of shape ``[C, H*r, W*r]``.</span>

<span class="sd">    See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): Input</span>
<span class="sd">        upscale_factor (int): factor to increase spatial resolution by</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; ps = nn.PixelShuffle(3)</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.Tensor(1, 9, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = ps(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output.size())</span>
<span class="sd">        torch.Size([1, 1, 12, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">channels</span> <span class="o">//=</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">out_height</span> <span class="o">=</span> <span class="n">in_height</span> <span class="o">*</span> <span class="n">upscale_factor</span>
    <span class="n">out_width</span> <span class="o">=</span> <span class="n">in_width</span> <span class="o">*</span> <span class="n">upscale_factor</span>

    <span class="n">input_view</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span>
        <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span><span class="p">)</span>

    <span class="n">shuffle_out</span> <span class="o">=</span> <span class="n">input_view</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">shuffle_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">out_height</span><span class="p">,</span> <span class="n">out_width</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    Currently only spatial upsampling is supported (i.e. expected inputs</span>
<span class="sd">    are 4 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Upscales the input, using the bilinear upsampling.</span>

<span class="sd">    Currently only spatial upsampling is supported (i.e. expected inputs</span>
<span class="sd">    are 4 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    Currently only 2D and 3D padding supported.</span>
<span class="sd">    In case of 4D input tensor pad should be in form (pad_l, pad_r, pad_t, pad_b )</span>
<span class="sd">    In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): 4D or 5D tensor</span>
<span class="sd">        pad (tuple): 4-elem or 6-elem tuple</span>
<span class="sd">        mode: &#39;constant&#39;, &#39;reflect&#39; or &#39;replicate&#39;</span>
<span class="sd">        value: fill value for &#39;constant&#39; padding</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ConstantPad2d</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">(</span><span class="o">*</span><span class="n">pad</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">(</span><span class="o">*</span><span class="n">pad</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">(</span><span class="o">*</span><span class="n">pad</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 4D and 5D padding is supported for now&quot;</span><span class="p">)</span></div>


<span class="c1"># distance</span>

<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the batchwise pairwise distance between vectors v1,v2:</span>

<span class="sd">        .. math ::</span>
<span class="sd">            \Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}</span>

<span class="sd">        Args:</span>
<span class="sd">            x1: first input tensor</span>
<span class="sd">            x2: second input tensor</span>
<span class="sd">            p: the norm degree. Default: 2</span>

<span class="sd">        Shape:</span>
<span class="sd">            - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">            - Output: :math:`(N, 1)`</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.pairwise_distance(input1, input2, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input must be a 2D matrix.&quot;</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">diff</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3</span>
<span class="sd">    and a margin with a value greater than 0.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet is composed by</span>
<span class="sd">    `a`, `p` and `n`: anchor, positive examples and negative example respectively.</span>
<span class="sd">    The shape of all input variables should be :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow convolutional feature descriptors with</span>
<span class="sd">    triplet losses`_ by V. Balntas, E. Riba et al.</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\} \right)</span>

<span class="sd">    where :math: `d(x_i, y_i) = \| {\bf x}_i - {\bf y}_i \|_2^2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor: anchor input tensor</span>
<span class="sd">        positive: positive input tensor</span>
<span class="sd">        negative: negative input tensor</span>
<span class="sd">        p: the norm degree. Default: 2</span>
<span class="sd">        eps: small epsilon value to avoid numerical issues</span>
<span class="sd">        swap: compute distance swap</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">        - Output: :math:`(N, 1)`</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input3 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.triplet_margin_loss(input1, input2, input3, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between anchor and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Inputd must be a 2D matrix.&quot;</span>
    <span class="k">assert</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Margin should be positive value.&#39;</span>
    <span class="n">d_p</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">d_n</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">swap</span><span class="p">:</span>
        <span class="n">d_s</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">d_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d_n</span><span class="p">,</span> <span class="n">d_s</span><span class="p">)</span>

    <span class="n">dist_hinge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">margin</span> <span class="o">+</span> <span class="n">d_p</span> <span class="o">-</span> <span class="n">d_n</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist_hinge</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.1.12_2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Index &mdash; PyTorch master documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="#" />
    <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.4.0 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-with-autograd">In-place operations with autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory">My model reports &#8220;cuda runtime error(2): out of memory&#8221;</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-gpu-memory-isn-t-freed-properly">My GPU memory isn&#8217;t freed properly</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-data-loader-workers-return-identical-random-numbers">My data loader workers return identical random numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism">My recurrent network doesn&#8217;t work with data parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#building-from-source">Building from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#include-optional-components">Include optional components</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#speeding-cuda-build-for-windows">Speeding CUDA build for Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#one-key-install-script">One key install script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#extension">Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cffi-extension">CFFI Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cpp-extension">Cpp Extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#package-not-found-in-win-32-channel">Package not found in win-32 channel.</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#why-are-there-no-python-2-packages-for-windows">Why are there no Python 2 packages for Windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#import-error">Import error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#usage-multiprocessing">Usage (multiprocessing)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-without-if-clause-protection">Multiprocessing error without if-clause protection</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-broken-pipe">Multiprocessing error &#8220;Broken pipe&#8221;</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-driver-shut-down">Multiprocessing error &#8220;driver shut down&#8221;</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cuda-ipc-operations">CUDA IPC operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-dtype">torch.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-device">torch.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-layout">torch.layout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#padding-layers">Padding layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations-weighted-sum-nonlinearity">Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations-other">Non-linear activations (other)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id20"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id21"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id22"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id23"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id24"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id25"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id26"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id27"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id28"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id29"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id30"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id31"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id32"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id33"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id34"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id35"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id36"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id37"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id38"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id39"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id40"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id41"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id42"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id43"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id44"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id45">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id46">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id47"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#in-place-operations-on-tensors">In-place operations on Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable-deprecated">Variable (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#tensor-autograd-functions">Tensor autograd functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#score-function">Score function</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pathwise-derivative">Pathwise derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#independent"><span class="hidden-section">Independent</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#lognormal"><span class="hidden-section">LogNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multinomial"><span class="hidden-section">Multinomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#poisson"><span class="hidden-section">Poisson</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.transforms"><cite>Transforms</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraints"><cite>Constraints</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#multi-gpu-collective-functions">Multi-GPU collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#launch-utility">Launch utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Index</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            

<h1 id="index">Index</h1>

<div class="genindex-jumpbox">
 <a href="#_"><strong>_</strong></a>
 | <a href="#A"><strong>A</strong></a>
 | <a href="#B"><strong>B</strong></a>
 | <a href="#C"><strong>C</strong></a>
 | <a href="#D"><strong>D</strong></a>
 | <a href="#E"><strong>E</strong></a>
 | <a href="#F"><strong>F</strong></a>
 | <a href="#G"><strong>G</strong></a>
 | <a href="#H"><strong>H</strong></a>
 | <a href="#I"><strong>I</strong></a>
 | <a href="#K"><strong>K</strong></a>
 | <a href="#L"><strong>L</strong></a>
 | <a href="#M"><strong>M</strong></a>
 | <a href="#N"><strong>N</strong></a>
 | <a href="#O"><strong>O</strong></a>
 | <a href="#P"><strong>P</strong></a>
 | <a href="#Q"><strong>Q</strong></a>
 | <a href="#R"><strong>R</strong></a>
 | <a href="#S"><strong>S</strong></a>
 | <a href="#T"><strong>T</strong></a>
 | <a href="#U"><strong>U</strong></a>
 | <a href="#V"><strong>V</strong></a>
 | <a href="#W"><strong>W</strong></a>
 | <a href="#X"><strong>X</strong></a>
 | <a href="#Z"><strong>Z</strong></a>
 
</div>
<h2 id="_">_</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Normalize.__call__">__call__() (torchvision.transforms.Normalize method)</a>

      <ul>
        <li><a href="torchvision/transforms.html#torchvision.transforms.ToPILImage.__call__">(torchvision.transforms.ToPILImage method)</a>
</li>
        <li><a href="torchvision/transforms.html#torchvision.transforms.ToTensor.__call__">(torchvision.transforms.ToTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR10.__getitem__">__getitem__() (torchvision.datasets.CIFAR10 method)</a>

      <ul>
        <li><a href="torchvision/datasets.html#torchvision.datasets.CocoCaptions.__getitem__">(torchvision.datasets.CocoCaptions method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.CocoDetection.__getitem__">(torchvision.datasets.CocoDetection method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.DatasetFolder.__getitem__">(torchvision.datasets.DatasetFolder method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.ImageFolder.__getitem__">(torchvision.datasets.ImageFolder method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.LSUN.__getitem__">(torchvision.datasets.LSUN method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.PhotoTour.__getitem__">(torchvision.datasets.PhotoTour method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.STL10.__getitem__">(torchvision.datasets.STL10 method)</a>
</li>
        <li><a href="torchvision/datasets.html#torchvision.datasets.SVHN.__getitem__">(torchvision.datasets.SVHN method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="sparse.html#torch.sparse.FloatTensor._indices">_indices() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor._nnz">_nnz() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor._values">_values() (torch.sparse.FloatTensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="A">A</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.abs">abs() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.abs">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.abs_">abs_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.AbsTransform">AbsTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="torch.html#torch.acos">acos() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.acos">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.acos_">acos_() (torch.Tensor method)</a>
</li>
      <li><a href="optim.html#torch.optim.Adadelta">Adadelta (class in torch.optim)</a>
</li>
      <li><a href="optim.html#torch.optim.Adagrad">Adagrad (class in torch.optim)</a>
</li>
      <li><a href="optim.html#torch.optim.Adam">Adam (class in torch.optim)</a>
</li>
      <li><a href="optim.html#torch.optim.Adamax">Adamax (class in torch.optim)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool1d">adaptive_avg_pool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool2d">adaptive_avg_pool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_avg_pool3d">adaptive_avg_pool3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool1d">adaptive_max_pool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool2d">adaptive_max_pool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.adaptive_max_pool3d">adaptive_max_pool3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool1d">AdaptiveAvgPool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool2d">AdaptiveAvgPool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveAvgPool3d">AdaptiveAvgPool3d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool1d">AdaptiveMaxPool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool2d">AdaptiveMaxPool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AdaptiveMaxPool3d">AdaptiveMaxPool3d (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.add">add() (in module torch)</a>, <a href="torch.html#torch.add">[1]</a>, <a href="torch.html#torch.add">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.add">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.add">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.add_">add_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.add_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.add_module">add_module() (torch.nn.Module method)</a>
</li>
      <li><a href="optim.html#torch.optim.Optimizer.add_param_group">add_param_group() (torch.optim.Optimizer method)</a>
</li>
      <li><a href="torch.html#torch.addbmm">addbmm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addbmm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addbmm_">addbmm_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.addcdiv">addcdiv() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addcdiv">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addcdiv_">addcdiv_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.addcmul">addcmul() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addcmul">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addcmul_">addcmul_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.addmm">addmm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addmm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addmm_">addmm_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.addmv">addmv() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addmv">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addmv_">addmv_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.addr">addr() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.addr">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.addr_">addr_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.affine_grid">affine_grid() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.AffineTransform">AffineTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.alexnet">alexnet() (in module torchvision.models)</a>
</li>
      <li><a href="tensors.html#torch.ByteTensor.all">all() (torch.ByteTensor method)</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_gather">all_gather() (in module torch.distributed)</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_gather_multigpu">all_gather_multigpu() (in module torch.distributed)</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_reduce">all_reduce() (in module torch.distributed)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.html#torch.distributed.all_reduce_multigpu">all_reduce_multigpu() (in module torch.distributed)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.alpha_dropout">alpha_dropout() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.AlphaDropout">AlphaDropout (class in torch.nn)</a>
</li>
      <li><a href="tensors.html#torch.ByteTensor.any">any() (torch.ByteTensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList.append">append() (torch.nn.ModuleList method)</a>

      <ul>
        <li><a href="nn.html#torch.nn.ParameterList.append">(torch.nn.ParameterList method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.apply">apply() (torch.nn.Module method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.apply_">apply_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.arange">arange() (in module torch)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.arg_constraints">arg_constraints (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.arg_constraints">(torch.distributions.beta.Beta attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.arg_constraints">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.arg_constraints">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.arg_constraints">(torch.distributions.cauchy.Cauchy attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.chi2.Chi2.arg_constraints">(torch.distributions.chi2.Chi2 attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.arg_constraints">(torch.distributions.dirichlet.Dirichlet attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.arg_constraints">(torch.distributions.distribution.Distribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.arg_constraints">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints">(torch.distributions.fishersnedecor.FisherSnedecor attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.arg_constraints">(torch.distributions.gamma.Gamma attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.arg_constraints">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.arg_constraints">(torch.distributions.gumbel.Gumbel attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.arg_constraints">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.arg_constraints">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.arg_constraints">(torch.distributions.log_normal.LogNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.arg_constraints">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints">(torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.arg_constraints">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.arg_constraints">(torch.distributions.pareto.Pareto attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.arg_constraints">(torch.distributions.poisson.Poisson attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.arg_constraints">(torch.distributions.studentT.StudentT attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints">(torch.distributions.transformed_distribution.TransformedDistribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.arg_constraints">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.argmax">argmax() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.argmax">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.argmin">argmin() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.argmin">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.ASGD">ASGD (class in torch.optim)</a>
</li>
      <li><a href="torch.html#torch.asin">asin() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.asin">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.asin_">asin_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.atan">atan() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.atan">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.atan2">atan2() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.atan2">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.atan2_">atan2_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.atan_">atan_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool1d">avg_pool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool2d">avg_pool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.avg_pool3d">avg_pool3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool1d">AvgPool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool2d">AvgPool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.AvgPool3d">AvgPool3d (class in torch.nn)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="B">B</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="autograd.html#torch.autograd.backward">backward() (in module torch.autograd)</a>

      <ul>
        <li><a href="autograd.html#torch.Tensor.backward">(torch.Tensor method)</a>
</li>
        <li><a href="autograd.html#torch.autograd.Function.backward">(torch.autograd.Function static method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.baddbmm">baddbmm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.baddbmm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.baddbmm_">baddbmm_() (torch.Tensor method)</a>
</li>
      <li><a href="distributed.html#torch.distributed.barrier">barrier() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.bartlett_window">bartlett_window() (in module torch)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.batch_norm">batch_norm() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.batch_shape">batch_shape (torch.distributions.distribution.Distribution attribute)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm1d">BatchNorm1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm2d">BatchNorm2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.BatchNorm3d">BatchNorm3d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.BCELoss">BCELoss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.BCEWithLogitsLoss">BCEWithLogitsLoss (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli">Bernoulli (class in torch.distributions.bernoulli)</a>
</li>
      <li><a href="torch.html#torch.bernoulli">bernoulli() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.bernoulli">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.bernoulli_">bernoulli_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta">Beta (class in torch.distributions.beta)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.Bilinear">Bilinear (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.binary_cross_entropy">binary_cross_entropy() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.binary_cross_entropy_with_logits">binary_cross_entropy_with_logits() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.binomial.Binomial">Binomial (class in torch.distributions.binomial)</a>
</li>
      <li><a href="torch.html#torch.bmm">bmm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.bmm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.comm.broadcast">broadcast() (in module torch.cuda.comm)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.broadcast">(in module torch.distributed)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.comm.broadcast_coalesced">broadcast_coalesced() (in module torch.cuda.comm)</a>
</li>
      <li><a href="distributed.html#torch.distributed.broadcast_multigpu">broadcast_multigpu() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.btrifact">btrifact() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.btrifact">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.btrifact_with_info">btrifact_with_info() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.btrifact_with_info">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.btrisolve">btrisolve() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.btrisolve">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.btriunpack">btriunpack() (in module torch)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.BuildExtension">BuildExtension() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.byte">byte() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.byte">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.ByteTensor">ByteTensor (class in torch)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="C">C</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.calculate_gain">calculate_gain() (in module torch.nn.init)</a>
</li>
      <li><a href="torch.html#torch.cat">cat() (in module torch)</a>
</li>
      <li><a href="distributions.html#torch.distributions.categorical.Categorical">Categorical (class in torch.distributions.categorical)</a>
</li>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy">Cauchy (class in torch.distributions.cauchy)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.cauchy_">cauchy_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.cdf">cdf() (torch.distributions.cauchy.Cauchy method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.cdf">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.cdf">(torch.distributions.exponential.Exponential method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.cdf">(torch.distributions.laplace.Laplace method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.cdf">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.cdf">(torch.distributions.transformed_distribution.TransformedDistribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.cdf">(torch.distributions.uniform.Uniform method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.ceil">ceil() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.ceil">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ceil_">ceil_() (torch.Tensor method)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.CenterCrop">CenterCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.char">char() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.char">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.constraints.Constraint.check">check() (torch.distributions.constraints.Constraint method)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.check_compiler_abi_compatibility">check_compiler_abi_compatibility() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.checkpoint">checkpoint() (in module torch.utils.checkpoint)</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.checkpoint_sequential">checkpoint_sequential() (in module torch.utils.checkpoint)</a>
</li>
      <li><a href="distributions.html#torch.distributions.chi2.Chi2">Chi2 (class in torch.distributions.chi2)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.children">children() (torch.nn.Module method)</a>
</li>
      <li><a href="torch.html#torch.chunk">chunk() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.chunk">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR10">CIFAR10 (class in torchvision.datasets)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CIFAR100">CIFAR100 (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.clamp">clamp() (in module torch)</a>, <a href="torch.html#torch.clamp">[1]</a>, <a href="torch.html#torch.clamp">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.clamp">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.clamp_">clamp_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.clip_grad_norm_">clip_grad_norm_() (in module torch.nn.utils)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.clip_grad_value_">clip_grad_value_() (in module torch.nn.utils)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.clone">clone() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.clone">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.clone">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.coalesce">coalesce() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CocoCaptions">CocoCaptions (class in torchvision.datasets)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.CocoDetection">CocoDetection (class in torchvision.datasets)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ColorJitter">ColorJitter (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Compose">Compose (class in torchvision.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.ComposeTransform">ComposeTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="data.html#torch.utils.data.ConcatDataset">ConcatDataset (class in torch.utils.data)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.concentration0">concentration0 (torch.distributions.beta.Beta attribute)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.concentration1">concentration1 (torch.distributions.beta.Beta attribute)</a>
</li>
      <li><a href="nn.html#torch.nn.init.constant_">constant_() (in module torch.nn.init)</a>
</li>
      <li><a href="nn.html#torch.nn.ConstantPad1d">ConstantPad1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ConstantPad2d">ConstantPad2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ConstantPad3d">ConstantPad3d (class in torch.nn)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.constraints.Constraint">Constraint (class in torch.distributions.constraints)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraint_registry.ConstraintRegistry">ConstraintRegistry (class in torch.distributions.constraint_registry)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.contiguous">contiguous() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv1d">Conv1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv1d">conv1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv2d">Conv2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv2d">conv2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Conv3d">Conv3d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv3d">conv3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose1d">conv_transpose1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose2d">conv_transpose2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.conv_transpose3d">conv_transpose3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose1d">ConvTranspose1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose2d">ConvTranspose2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ConvTranspose3d">ConvTranspose3d (class in torch.nn)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.copy_">copy_() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.copy_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.cos">cos() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cos">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cos_">cos_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.cosh">cosh() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cosh">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.cosh_">cosh_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.cosine_embedding_loss">cosine_embedding_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.cosine_similarity">cosine_similarity() (in module torch.nn.functional)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.CosineAnnealingLR">CosineAnnealingLR (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="nn.html#torch.nn.CosineEmbeddingLoss">CosineEmbeddingLoss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.CosineSimilarity">CosineSimilarity (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix">covariance_matrix (torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.CppExtension">CppExtension() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.cpu">cpu() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cpu">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.cpu">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="ffi.html#torch.utils.ffi.create_extension">create_extension() (in module torch.utils.ffi)</a>
</li>
      <li><a href="torch.html#torch.cross">cross() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cross">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.cross_entropy">cross_entropy() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.CrossEntropyLoss">CrossEntropyLoss (class in torch.nn)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.cuda">cuda() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cuda">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.cuda">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.CUDAExtension">CUDAExtension() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="torch.html#torch.cumprod">cumprod() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cumprod">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.cumsum">cumsum() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.cumsum">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.current_blas_handle">current_blas_handle() (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.current_device">current_device() (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.current_stream">current_stream() (in module torch.cuda)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="D">D</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.parallel.data_parallel">data_parallel() (in module torch.nn.parallel)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.data_ptr">data_ptr() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.data_ptr">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.DataLoader">DataLoader (class in torch.utils.data)</a>
</li>
      <li><a href="nn.html#torch.nn.DataParallel">DataParallel (class in torch.nn)</a>
</li>
      <li><a href="data.html#torch.utils.data.Dataset">Dataset (class in torch.utils.data)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.DatasetFolder">DatasetFolder (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.default_generator">default_generator (in module torch)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet121">densenet121() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet161">densenet161() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet169">densenet169() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.densenet201">densenet201() (in module torchvision.models)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.dependent_property">dependent_property (in module torch.distributions.constraints)</a>
</li>
      <li><a href="torch.html#torch.det">det() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.det">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.Tensor.detach">detach() (torch.Tensor method)</a>
</li>
      <li><a href="autograd.html#torch.Tensor.detach_">detach_() (torch.Tensor method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device">device (class in torch.cuda)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.device">(torch.Tensor attribute)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.device_count">device_count() (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device_ctx_manager">device_ctx_manager (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.device_of">device_of (class in torch.cuda)</a>
</li>
      <li><a href="distributions.html#torch.distributions.chi2.Chi2.df">df (torch.distributions.chi2.Chi2 attribute)</a>
</li>
      <li><a href="torch.html#torch.diag">diag() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.diag">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.diagflat">diagflat() (in module torch)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.diagonal">diagonal() (in module torch)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.dim">dim() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.dim">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.init.dirac_">dirac_() (in module torch.nn.init)</a>
</li>
      <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet">Dirichlet (class in torch.distributions.dirichlet)</a>
</li>
      <li><a href="torch.html#torch.dist">dist() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.dist">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel (class in torch.nn.parallel)</a>
</li>
      <li><a href="data.html#torch.utils.data.distributed.DistributedSampler">DistributedSampler (class in torch.utils.data.distributed)</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution">Distribution (class in torch.distributions.distribution)</a>
</li>
      <li><a href="torch.html#torch.div">div() (in module torch)</a>, <a href="torch.html#torch.div">[1]</a>, <a href="torch.html#torch.div">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.div">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.div">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.div_">div_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.div_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.dot">dot() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.dot">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.double">double() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.double">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.double">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Dropout">Dropout (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout">dropout() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Dropout2d">Dropout2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout2d">dropout2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Dropout3d">Dropout3d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.dropout3d">dropout3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.dump_patches">dump_patches (torch.nn.Module attribute)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="E">E</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.eig">eig() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.eig">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.einsum">einsum() (in module torch)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Event.elapsed_time">elapsed_time() (torch.cuda.Event method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.element_size">element_size() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.element_size">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.ELU">ELU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.elu">elu() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.elu_">elu_() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Embedding">Embedding (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.EmbeddingBag">EmbeddingBag (class in torch.nn)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.emit_nvtx">emit_nvtx (class in torch.autograd.profiler)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.EMNIST">EMNIST (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.empty">empty() (in module torch)</a>
</li>
      <li><a href="cuda.html#torch.cuda.empty_cache">empty_cache() (in module torch.cuda)</a>, <a href="cuda.html#torch.cuda.empty_cache">[1]</a>
</li>
      <li><a href="torch.html#torch.empty_like">empty_like() (in module torch)</a>
</li>
      <li><a href="autograd.html#torch.autograd.enable_grad">enable_grad (class in torch.autograd)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.entropy">entropy() (torch.distributions.bernoulli.Bernoulli method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.entropy">(torch.distributions.beta.Beta method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.entropy">(torch.distributions.categorical.Categorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.entropy">(torch.distributions.cauchy.Cauchy method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.entropy">(torch.distributions.dirichlet.Dirichlet method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.entropy">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exp_family.ExponentialFamily.entropy">(torch.distributions.exp_family.ExponentialFamily method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.entropy">(torch.distributions.exponential.Exponential method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.entropy">(torch.distributions.gamma.Gamma method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.entropy">(torch.distributions.geometric.Geometric method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.entropy">(torch.distributions.gumbel.Gumbel method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.entropy">(torch.distributions.independent.Independent method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.entropy">(torch.distributions.laplace.Laplace method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.entropy">(torch.distributions.log_normal.LogNormal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.entropy">(torch.distributions.multivariate_normal.MultivariateNormal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.entropy">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.entropy">(torch.distributions.one_hot_categorical.OneHotCategorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.entropy">(torch.distributions.pareto.Pareto method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.entropy">(torch.distributions.studentT.StudentT method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.entropy">(torch.distributions.uniform.Uniform method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.enumerate_support">enumerate_support() (torch.distributions.bernoulli.Bernoulli method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.enumerate_support">(torch.distributions.binomial.Binomial method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.enumerate_support">(torch.distributions.categorical.Categorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.enumerate_support">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.enumerate_support">(torch.distributions.independent.Independent method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support">(torch.distributions.one_hot_categorical.OneHotCategorical method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.eq">eq() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.eq">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.eq_">eq_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.equal">equal() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.equal">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.erf">erf() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.erf">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.erf_">erf_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.erfinv">erfinv() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.erfinv">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.erfinv_">erfinv_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.eval">eval() (torch.nn.Module method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Event">Event (class in torch.cuda)</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.event_shape">event_shape (torch.distributions.distribution.Distribution attribute)</a>
</li>
      <li><a href="torch.html#torch.exp">exp() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.exp">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.exp_">exp_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.expand">expand() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.expand_as">expand_as() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.expm1">expm1() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.expm1">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.expm1_">expm1_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.exponential.Exponential">Exponential (class in torch.distributions.exponential)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.exponential_">exponential_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.exp_family.ExponentialFamily">ExponentialFamily (class in torch.distributions.exp_family)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.ExponentialLR">ExponentialLR (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="onnx.html#torch.onnx.export">export() (in module torch.onnx)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.export_chrome_trace">export_chrome_trace() (torch.autograd.profiler.profile method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.ExpTransform">ExpTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList.extend">extend() (torch.nn.ModuleList method)</a>

      <ul>
        <li><a href="nn.html#torch.nn.ParameterList.extend">(torch.nn.ParameterList method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.extra_repr">extra_repr() (torch.nn.Module method)</a>
</li>
      <li><a href="torch.html#torch.eye">eye() (in module torch)</a>
</li>
      <li><a href="nn.html#torch.nn.init.eye_">eye_() (in module torch.nn.init)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="F">F</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/datasets.html#torchvision.datasets.FashionMNIST">FashionMNIST (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.fft">fft() (in module torch)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.fill_">fill_() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.fill_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor">FisherSnedecor (class in torch.distributions.fishersnedecor)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.FiveCrop">FiveCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.float">float() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.float">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.float">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage">FloatStorage (class in torch)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor">FloatTensor (class in torch.sparse)</a>
</li>
      <li><a href="torch.html#torch.floor">floor() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.floor">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.floor_">floor_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.fmod">fmod() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.fmod">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.fmod_">fmod_() (torch.Tensor method)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Function.forward">forward() (torch.autograd.Function static method)</a>

      <ul>
        <li><a href="nn.html#torch.nn.Module.forward">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.frac">frac() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.frac">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.frac_">frac_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.FractionalMaxPool2d">FractionalMaxPool2d (class in torch.nn)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.from_buffer">from_buffer() (torch.FloatStorage method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.from_file">from_file() (torch.FloatStorage method)</a>
</li>
      <li><a href="torch.html#torch.from_numpy">from_numpy() (in module torch)</a>
</li>
      <li><a href="nn.html#torch.nn.Embedding.from_pretrained">from_pretrained() (torch.nn.Embedding class method)</a>
</li>
      <li><a href="torch.html#torch.full">full() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.full_like">full_like() (in module torch)</a>
</li>
      <li><a href="autograd.html#torch.autograd.Function">Function (class in torch.autograd)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="G">G</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.gamma.Gamma">Gamma (class in torch.distributions.gamma)</a>
</li>
      <li><a href="torch.html#torch.gather">gather() (in module torch)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.comm.gather">(in module torch.cuda.comm)</a>
</li>
        <li><a href="distributed.html#torch.distributed.gather">(in module torch.distributed)</a>
</li>
        <li><a href="tensors.html#torch.Tensor.gather">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.ge">ge() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.ge">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ge_">ge_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.gels">gels() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.gels">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.geometric.Geometric">Geometric (class in torch.distributions.geometric)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.geometric_">geometric_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.geqrf">geqrf() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.geqrf">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.ger">ger() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.ger">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.gesv">gesv() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.gesv">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_all_sharing_strategies">get_all_sharing_strategies() (in module torch.multiprocessing)</a>
</li>
      <li><a href="torch.html#torch.get_default_dtype">get_default_dtype() (in module torch)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="sparse.html#torch.sparse.FloatTensor.get_device">get_device() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.get_device_capability">get_device_capability() (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.get_device_name">get_device_name() (in module torch.cuda)</a>
</li>
      <li><a href="torchvision/index.html#torchvision.get_image_backend">get_image_backend() (in module torchvision)</a>
</li>
      <li><a href="torch.html#torch.get_num_threads">get_num_threads() (in module torch)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_rank">get_rank() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.get_rng_state">get_rng_state() (in module torch)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.get_rng_state">(in module torch.cuda)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_sharing_strategy">get_sharing_strategy() (in module torch.multiprocessing)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_world_size">get_world_size() (in module torch.distributed)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.glu">glu() (in module torch.nn.functional)</a>
</li>
      <li><a href="autograd.html#torch.autograd.grad">grad() (in module torch.autograd)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Grayscale">Grayscale (class in torchvision.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.greater_than">greater_than (in module torch.distributions.constraints)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.grid_sample">grid_sample() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.GRU">GRU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.GRUCell">GRUCell (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.gt">gt() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.gt">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.gt_">gt_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.gumbel.Gumbel">Gumbel (class in torch.distributions.gumbel)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="H">H</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="storage.html#torch.FloatStorage.half">half() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.half">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.half">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.hamming_window">hamming_window() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.hann_window">hann_window() (in module torch)</a>
</li>
      <li><a href="nn.html#torch.nn.Hardshrink">Hardshrink (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.hardshrink">hardshrink() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Hardtanh">Hardtanh (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.hardtanh">hardtanh() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.hardtanh_">hardtanh_() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.has_enumerate_support">has_enumerate_support (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.has_enumerate_support">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.has_enumerate_support">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.has_enumerate_support">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.has_rsample">has_rsample (torch.distributions.beta.Beta attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.has_rsample">(torch.distributions.cauchy.Cauchy attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.has_rsample">(torch.distributions.dirichlet.Dirichlet attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.has_rsample">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample">(torch.distributions.fishersnedecor.FisherSnedecor attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.has_rsample">(torch.distributions.gamma.Gamma attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.has_rsample">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.has_rsample">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.has_rsample">(torch.distributions.log_normal.LogNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample">(torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.has_rsample">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.has_rsample">(torch.distributions.studentT.StudentT attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample">(torch.distributions.transformed_distribution.TransformedDistribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.has_rsample">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.hinge_embedding_loss">hinge_embedding_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.HingeEmbeddingLoss">HingeEmbeddingLoss (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.histc">histc() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.histc">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.hspmm">hspmm() (torch.sparse.FloatTensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="I">I</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.icdf">icdf() (torch.distributions.cauchy.Cauchy method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.icdf">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.icdf">(torch.distributions.exponential.Exponential method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.icdf">(torch.distributions.laplace.Laplace method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.icdf">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.icdf">(torch.distributions.transformed_distribution.TransformedDistribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.icdf">(torch.distributions.uniform.Uniform method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.ifft">ifft() (in module torch)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.ImageFolder">ImageFolder (class in torchvision.datasets)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.inception_v3">inception_v3() (in module torchvision.models)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.include_paths">include_paths() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="distributions.html#torch.distributions.independent.Independent">Independent (class in torch.distributions.independent)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index">index() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_add_">index_add_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_copy_">index_copy_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_fill_">index_fill_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.index_put_">index_put_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.index_select">index_select() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.index_select">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.init">init() (in module torch.cuda)</a>
</li>
      <li><a href="distributed.html#torch.distributed.init_process_group">init_process_group() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.initial_seed">initial_seed() (in module torch)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.initial_seed">(in module torch.cuda)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.instance_norm">instance_norm() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.InstanceNorm1d">InstanceNorm1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.InstanceNorm2d">InstanceNorm2d (class in torch.nn)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.InstanceNorm3d">InstanceNorm3d (class in torch.nn)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.int">int() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.int">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.constraints.integer_interval">integer_interval (in module torch.distributions.constraints)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.interval">interval (in module torch.distributions.constraints)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.inv">inv (torch.distributions.transforms.Transform attribute)</a>
</li>
      <li><a href="torch.html#torch.inverse">inverse() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.inverse">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.Event.ipc_handle">ipc_handle() (torch.cuda.Event method)</a>
</li>
      <li><a href="distributed.html#torch.distributed.irecv">irecv() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.irfft">irfft() (in module torch)</a>
</li>
      <li><a href="cuda.html#torch.cuda.is_available">is_available() (in module torch.cuda)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.is_coalesced">is_coalesced() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.is_contiguous">is_contiguous() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_cuda">is_cuda (torch.FloatStorage attribute)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.is_cuda">(torch.Tensor attribute)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.is_pinned">is_pinned() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.is_pinned">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.is_set_to">is_set_to() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_shared">is_shared() (torch.FloatStorage method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.is_signed">is_signed() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.is_sparse">is_sparse (torch.FloatStorage attribute)</a>
</li>
      <li><a href="torch.html#torch.is_storage">is_storage() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.is_tensor">is_tensor() (in module torch)</a>
</li>
      <li><a href="distributed.html#torch.distributed.isend">isend() (in module torch.distributed)</a>
</li>
      <li><a href="torch.html#torch.isnan">isnan() (in module torch)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.item">item() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="K">K</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.kaiming_normal_">kaiming_normal_() (in module torch.nn.init)</a>
</li>
      <li><a href="nn.html#torch.nn.init.kaiming_uniform_">kaiming_uniform_() (in module torch.nn.init)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.key_averages">key_averages() (torch.autograd.profiler.profile method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.kl_div">kl_div() (in module torch.nn.functional)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.kl.kl_divergence">kl_divergence() (in module torch.distributions.kl)</a>
</li>
      <li><a href="nn.html#torch.nn.KLDivLoss">KLDivLoss (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.kthvalue">kthvalue() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.kthvalue">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="L">L</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.l1_loss">l1_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.L1Loss">L1Loss (class in torch.nn)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Lambda">Lambda (class in torchvision.transforms)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.LambdaLR">LambdaLR (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="distributions.html#torch.distributions.laplace.Laplace">Laplace (class in torch.distributions.laplace)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.layer_norm">layer_norm() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.LayerNorm">LayerNorm (class in torch.nn)</a>
</li>
      <li><a href="optim.html#torch.optim.LBFGS">LBFGS (class in torch.optim)</a>
</li>
      <li><a href="torch.html#torch.le">le() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.le">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.le_">le_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.leaky_relu">leaky_relu() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.leaky_relu_">leaky_relu_() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.LeakyReLU">LeakyReLU (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.lerp">lerp() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.lerp">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.lerp_">lerp_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.less_than">less_than (in module torch.distributions.constraints)</a>
</li>
      <li><a href="nn.html#torch.nn.Linear">Linear (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.linear">linear() (in module torch.nn.functional)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.LinearTransformation">LinearTransformation (class in torchvision.transforms)</a>
</li>
      <li><a href="torch.html#torch.linspace">linspace() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.load">load() (in module torch)</a>

      <ul>
        <li><a href="cpp_extension.html#torch.utils.cpp_extension.load">(in module torch.utils.cpp_extension)</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.load_nvprof">load_nvprof() (in module torch.autograd.profiler)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.load_state_dict">load_state_dict() (torch.nn.Module method)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.load_state_dict">(torch.optim.Optimizer method)</a>
</li>
      </ul></li>
      <li><a href="model_zoo.html#torch.utils.model_zoo.load_url">load_url() (in module torch.utils.model_zoo)</a>
</li>
      <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.loc">loc (torch.distributions.log_normal.LogNormal attribute)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.local_response_norm">local_response_norm() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.LocalResponseNorm">LocalResponseNorm (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.log">log() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.log">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.log10">log10() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.log10">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.log10_">log10_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.log1p">log1p() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.log1p">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.log1p_">log1p_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.log2">log2() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.log2">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.log2_">log2_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.log_">log_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.log_abs_det_jacobian">log_abs_det_jacobian() (torch.distributions.transforms.Transform method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.log_normal_">log_normal_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.log_prob">log_prob() (torch.distributions.bernoulli.Bernoulli method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.log_prob">(torch.distributions.beta.Beta method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.log_prob">(torch.distributions.binomial.Binomial method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.log_prob">(torch.distributions.categorical.Categorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.log_prob">(torch.distributions.cauchy.Cauchy method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.log_prob">(torch.distributions.dirichlet.Dirichlet method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.log_prob">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.log_prob">(torch.distributions.exponential.Exponential method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.log_prob">(torch.distributions.fishersnedecor.FisherSnedecor method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.log_prob">(torch.distributions.gamma.Gamma method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.log_prob">(torch.distributions.geometric.Geometric method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.log_prob">(torch.distributions.independent.Independent method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.log_prob">(torch.distributions.laplace.Laplace method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.log_prob">(torch.distributions.multinomial.Multinomial method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.log_prob">(torch.distributions.multivariate_normal.MultivariateNormal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.log_prob">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob">(torch.distributions.one_hot_categorical.OneHotCategorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.log_prob">(torch.distributions.poisson.Poisson method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.log_prob">(torch.distributions.studentT.StudentT method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.log_prob">(torch.distributions.transformed_distribution.TransformedDistribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.log_prob">(torch.distributions.uniform.Uniform method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.functional.log_softmax">log_softmax() (in module torch.nn.functional)</a>
</li>
      <li><a href="torch.html#torch.logdet">logdet() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.logdet">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.logits">logits (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.logits">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.logits">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.logits">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.logits">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.logits">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.log_normal.LogNormal">LogNormal (class in torch.distributions.log_normal)</a>
</li>
      <li><a href="nn.html#torch.nn.LogSigmoid">LogSigmoid (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.logsigmoid">logsigmoid() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.LogSoftmax">LogSoftmax (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.logspace">logspace() (in module torch)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.long">long() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.long">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.transforms.LowerCholeskyTransform">LowerCholeskyTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.lp_pool1d">lp_pool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.lp_pool2d">lp_pool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.LPPool1d">LPPool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.LPPool2d">LPPool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.LSTM">LSTM (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.LSTMCell">LSTMCell (class in torch.nn)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.LSUN">LSUN (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.lt">lt() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.lt">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.lt_">lt_() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="M">M</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torchvision/utils.html#torchvision.utils.make_grid">make_grid() (in module torchvision.utils)</a>
</li>
      <li><a href="torch.html#torch.manual_seed">manual_seed() (in module torch)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.manual_seed">(in module torch.cuda)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.manual_seed_all">manual_seed_all() (in module torch.cuda)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.map_">map_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.margin_ranking_loss">margin_ranking_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.MarginRankingLoss">MarginRankingLoss (class in torch.nn)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.mark">mark() (in module torch.cuda.nvtx)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.masked_fill_">masked_fill_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.masked_scatter_">masked_scatter_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.masked_select">masked_select() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.masked_select">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.matmul">matmul() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.matmul">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.max">max() (in module torch)</a>, <a href="torch.html#torch.max">[1]</a>, <a href="torch.html#torch.max">[2]</a>, <a href="torch.html#torch.max">[3]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.max">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.max_memory_allocated">max_memory_allocated() (in module torch.cuda)</a>, <a href="cuda.html#torch.cuda.max_memory_allocated">[1]</a>
</li>
      <li><a href="cuda.html#torch.cuda.max_memory_cached">max_memory_cached() (in module torch.cuda)</a>, <a href="cuda.html#torch.cuda.max_memory_cached">[1]</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_pool1d">max_pool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_pool2d">max_pool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_pool3d">max_pool3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool1d">max_unpool1d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool2d">max_unpool2d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.max_unpool3d">max_unpool3d() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool1d">MaxPool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool2d">MaxPool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxPool3d">MaxPool3d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool1d">MaxUnpool1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool2d">MaxUnpool2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MaxUnpool3d">MaxUnpool3d (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.mean">mean (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.mean">(torch.distributions.beta.Beta attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.mean">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.mean">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.mean">(torch.distributions.cauchy.Cauchy attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.mean">(torch.distributions.dirichlet.Dirichlet attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.mean">(torch.distributions.distribution.Distribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.mean">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.mean">(torch.distributions.fishersnedecor.FisherSnedecor attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.mean">(torch.distributions.gamma.Gamma attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.mean">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.mean">(torch.distributions.gumbel.Gumbel attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.mean">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.mean">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.mean">(torch.distributions.log_normal.LogNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.mean">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.mean">(torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.mean">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.mean">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.mean">(torch.distributions.pareto.Pareto attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.mean">(torch.distributions.poisson.Poisson attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.mean">(torch.distributions.studentT.StudentT attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.mean">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.mean">mean() (in module torch)</a>, <a href="torch.html#torch.mean">[1]</a>, <a href="torch.html#torch.mean">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mean">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.median">median() (in module torch)</a>, <a href="torch.html#torch.median">[1]</a>, <a href="torch.html#torch.median">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.median">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.memory_allocated">memory_allocated() (in module torch.cuda)</a>, <a href="cuda.html#torch.cuda.memory_allocated">[1]</a>
</li>
      <li><a href="cuda.html#torch.cuda.memory_cached">memory_cached() (in module torch.cuda)</a>, <a href="cuda.html#torch.cuda.memory_cached">[1]</a>
</li>
      <li><a href="torch.html#torch.min">min() (in module torch)</a>, <a href="torch.html#torch.min">[1]</a>, <a href="torch.html#torch.min">[2]</a>, <a href="torch.html#torch.min">[3]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.min">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.mm">mm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mm">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.mm">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.MNIST">MNIST (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.mode">mode() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mode">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module">Module (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.to">Module.to() (in module torch.nn)</a>, <a href="nn.html#torch.nn.Module.to">[1]</a>, <a href="nn.html#torch.nn.Module.to">[2]</a>
</li>
      <li><a href="nn.html#torch.nn.ModuleList">ModuleList (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.modules">modules() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.mse_loss">mse_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.MSELoss">MSELoss (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.mul">mul() (in module torch)</a>, <a href="torch.html#torch.mul">[1]</a>, <a href="torch.html#torch.mul">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mul">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.mul">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.mul_">mul_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mul_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.multi_margin_loss">multi_margin_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.multilabel_margin_loss">multilabel_margin_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.multilabel_soft_margin_loss">multilabel_soft_margin_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiLabelMarginLoss">MultiLabelMarginLoss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiLabelSoftMarginLoss">MultiLabelSoftMarginLoss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.MultiMarginLoss">MultiMarginLoss (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multinomial.Multinomial">Multinomial (class in torch.distributions.multinomial)</a>
</li>
      <li><a href="torch.html#torch.multinomial">multinomial() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.multinomial">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.lr_scheduler.MultiStepLR">MultiStepLR (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal">MultivariateNormal (class in torch.distributions.multivariate_normal)</a>
</li>
      <li><a href="torch.html#torch.mv">mv() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.mv">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="N">N</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.Module.named_children">named_children() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.named_modules">named_modules() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.named_parameters">named_parameters() (torch.nn.Module method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.narrow">narrow() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.ndimension">ndimension() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.ne">ne() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.ne">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.ne_">ne_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.neg">neg() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.neg">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.neg_">neg_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.nelement">nelement() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.new">new() (torch.FloatStorage method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.new_empty">new_empty() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.new_full">new_full() (torch.Tensor method)</a>
</li>
      <li><a href="distributed.html#torch.distributed.new_group">new_group() (in module torch.distributed)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.new_ones">new_ones() (torch.Tensor method)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.new_tensor">new_tensor() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.new_zeros">new_zeros() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.nll_loss">nll_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.NLLLoss">NLLLoss (class in torch.nn)</a>
</li>
      <li><a href="autograd.html#torch.autograd.no_grad">no_grad (class in torch.autograd)</a>
</li>
      <li><a href="torch.html#torch.nonzero">nonzero() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.nonzero">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.norm">norm() (in module torch)</a>, <a href="torch.html#torch.norm">[1]</a>, <a href="torch.html#torch.norm">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.norm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.normal.Normal">Normal (class in torch.distributions.normal)</a>
</li>
      <li><a href="torch.html#torch.normal">normal() (in module torch)</a>, <a href="torch.html#torch.normal">[1]</a>, <a href="torch.html#torch.normal">[2]</a>, <a href="torch.html#torch.normal">[3]</a>
</li>
      <li><a href="nn.html#torch.nn.init.normal_">normal_() (in module torch.nn.init)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.normal_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Normalize">Normalize (class in torchvision.transforms)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.normalize">normalize() (in module torch.nn.functional)</a>
</li>
      <li><a href="torch.html#torch.numel">numel() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.numel">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.numpy">numpy() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="O">O</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical">OneHotCategorical (class in torch.distributions.one_hot_categorical)</a>
</li>
      <li><a href="torch.html#torch.ones">ones() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.ones_like">ones_like() (in module torch)</a>
</li>
      <li><a href="optim.html#torch.optim.Optimizer">Optimizer (class in torch.optim)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.orgqr">orgqr() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.orgqr">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.ormqr">ormqr() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.ormqr">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.init.orthogonal_">orthogonal_() (in module torch.nn.init)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="P">P</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.utils.rnn.pack_padded_sequence">pack_padded_sequence() (in module torch.nn.utils.rnn)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.pack_sequence">pack_sequence() (in module torch.nn.utils.rnn)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.PackedSequence">PackedSequence() (in module torch.nn.utils.rnn)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Pad">Pad (class in torchvision.transforms)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.pad">pad() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.pad_packed_sequence">pad_packed_sequence() (in module torch.nn.utils.rnn)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.rnn.pad_sequence">pad_sequence() (in module torch.nn.utils.rnn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.pairwise_distance">pairwise_distance() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.PairwiseDistance">PairwiseDistance (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.param_shape">param_shape (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.param_shape">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.param_shape">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.param_shape">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Parameter">Parameter (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ParameterList">ParameterList (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.parameters">parameters() (torch.nn.Module method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.pareto.Pareto">Pareto (class in torch.distributions.pareto)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.permute">permute() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.perplexity">perplexity() (torch.distributions.distribution.Distribution method)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.PhotoTour">PhotoTour (class in torchvision.datasets)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.pin_memory">pin_memory() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.pin_memory">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.pixel_shuffle">pixel_shuffle() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.PixelShuffle">PixelShuffle (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.poisson.Poisson">Poisson (class in torch.distributions.poisson)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.poisson_nll_loss">poisson_nll_loss() (in module torch.nn.functional)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.PoissonNLLLoss">PoissonNLLLoss (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.potrf">potrf() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.potrf">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.potri">potri() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.potri">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.potrs">potrs() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.potrs">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.pow">pow() (in module torch)</a>, <a href="torch.html#torch.pow">[1]</a>, <a href="torch.html#torch.pow">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.pow">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.pow_">pow_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.PowerTransform">PowerTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix">precision_matrix (torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
      <li><a href="nn.html#torch.nn.PReLU">PReLU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.prelu">prelu() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.probs">probs (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.probs">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.probs">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.probs">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.probs">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.probs">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.prod">prod() (in module torch)</a>, <a href="torch.html#torch.prod">[1]</a>, <a href="torch.html#torch.prod">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.prod">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.profile">profile (class in torch.autograd.profiler)</a>
</li>
      <li><a href="torch.html#torch.pstrf">pstrf() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.pstrf">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.put_">put_() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="Q">Q</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.qr">qr() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.qr">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Event.query">query() (torch.cuda.Event method)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.Stream.query">(torch.cuda.Stream method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="R">R</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.rand">rand() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.rand_like">rand_like() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.randint">randint() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.randint_like">randint_like() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.randn">randn() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.randn_like">randn_like() (in module torch)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.random_">random_() (torch.Tensor method)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomAffine">RandomAffine (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomApply">RandomApply (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomChoice">RandomChoice (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomCrop">RandomCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomGrayscale">RandomGrayscale (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip">RandomHorizontalFlip (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomOrder">RandomOrder (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomResizedCrop">RandomResizedCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomRotation">RandomRotation (class in torchvision.transforms)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.RandomSampler">RandomSampler (class in torch.utils.data.sampler)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomSizedCrop">RandomSizedCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.RandomVerticalFlip">RandomVerticalFlip (class in torchvision.transforms)</a>
</li>
      <li><a href="torch.html#torch.randperm">randperm() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.range">range() (in module torch)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.range_pop">range_pop() (in module torch.cuda.nvtx)</a>
</li>
      <li><a href="cuda.html#torch.cuda.nvtx.range_push">range_push() (in module torch.cuda.nvtx)</a>
</li>
      <li><a href="torch.html#torch.reciprocal">reciprocal() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.reciprocal">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.reciprocal_">reciprocal_() (torch.Tensor method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Event.record">record() (torch.cuda.Event method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream.record_event">record_event() (torch.cuda.Stream method)</a>
</li>
      <li><a href="distributed.html#torch.distributed.recv">recv() (in module torch.distributed)</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce">reduce() (in module torch.distributed)</a>
</li>
      <li><a href="cuda.html#torch.cuda.comm.reduce_add">reduce_add() (in module torch.cuda.comm)</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce_multigpu">reduce_multigpu() (in module torch.distributed)</a>
</li>
      <li><a href="optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau">ReduceLROnPlateau (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="nn.html#torch.nn.ReflectionPad1d">ReflectionPad1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ReflectionPad2d">ReflectionPad2d (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraint_registry.ConstraintRegistry.register">register() (torch.distributions.constraint_registry.ConstraintRegistry method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_backward_hook">register_backward_hook() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_buffer">register_buffer() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_forward_hook">register_forward_hook() (torch.nn.Module method)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_forward_pre_hook">register_forward_pre_hook() (torch.nn.Module method)</a>
</li>
      <li><a href="autograd.html#torch.Tensor.register_hook">register_hook() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.kl.register_kl">register_kl() (in module torch.distributions.kl)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.register_parameter">register_parameter() (torch.nn.Module method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli">RelaxedBernoulli (class in torch.distributions.relaxed_bernoulli)</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical">RelaxedOneHotCategorical (class in torch.distributions.relaxed_categorical)</a>
</li>
      <li><a href="nn.html#torch.nn.ReLU">ReLU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.relu">relu() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.ReLU6">ReLU6 (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.relu6">relu6() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.relu_">relu_() (in module torch.nn.functional)</a>
</li>
      <li><a href="torch.html#torch.remainder">remainder() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.remainder">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.remainder_">remainder_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.utils.remove_weight_norm">remove_weight_norm() (in module torch.nn.utils)</a>
</li>
      <li><a href="torch.html#torch.renorm">renorm() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.renorm">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.renorm_">renorm_() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.repeat">repeat() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.ReplicationPad1d">ReplicationPad1d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ReplicationPad2d">ReplicationPad2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.ReplicationPad3d">ReplicationPad3d (class in torch.nn)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.requires_grad_">requires_grad_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.reshape">reshape() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.reshape">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Resize">Resize (class in torchvision.transforms)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.resize_">resize_() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.resize_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.resize_as_">resize_as_() (torch.Tensor method)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.resizeAs_">resizeAs_() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet101">resnet101() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet152">resnet152() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet18">resnet18() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet34">resnet34() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.resnet50">resnet50() (in module torchvision.models)</a>
</li>
      <li><a href="autograd.html#torch.Tensor.retain_grad">retain_grad() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.rfft">rfft() (in module torch)</a>
</li>
      <li><a href="optim.html#torch.optim.RMSprop">RMSprop (class in torch.optim)</a>
</li>
      <li><a href="nn.html#torch.nn.RNN">RNN (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.RNNCell">RNNCell (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.round">round() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.round">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.round_">round_() (torch.Tensor method)</a>
</li>
      <li><a href="optim.html#torch.optim.Rprop">Rprop (class in torch.optim)</a>
</li>
      <li><a href="nn.html#torch.nn.RReLU">RReLU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.rrelu">rrelu() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.rrelu_">rrelu_() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.rsample">rsample() (torch.distributions.beta.Beta method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.rsample">(torch.distributions.cauchy.Cauchy method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.rsample">(torch.distributions.dirichlet.Dirichlet method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.rsample">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.rsample">(torch.distributions.exponential.Exponential method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.rsample">(torch.distributions.fishersnedecor.FisherSnedecor method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.rsample">(torch.distributions.gamma.Gamma method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.rsample">(torch.distributions.independent.Independent method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.rsample">(torch.distributions.laplace.Laplace method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.rsample">(torch.distributions.multivariate_normal.MultivariateNormal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.rsample">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.rsample">(torch.distributions.studentT.StudentT method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.rsample">(torch.distributions.transformed_distribution.TransformedDistribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.rsample">(torch.distributions.uniform.Uniform method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.rsqrt">rsqrt() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.rsqrt">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.rsqrt_">rsqrt_() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="S">S</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.sample">sample() (torch.distributions.bernoulli.Bernoulli method)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.sample">(torch.distributions.binomial.Binomial method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.sample">(torch.distributions.categorical.Categorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.sample">(torch.distributions.distribution.Distribution method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.sample">(torch.distributions.geometric.Geometric method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.sample">(torch.distributions.independent.Independent method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.sample">(torch.distributions.multinomial.Multinomial method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.sample">(torch.distributions.normal.Normal method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.sample">(torch.distributions.one_hot_categorical.OneHotCategorical method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.sample">(torch.distributions.poisson.Poisson method)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.sample">(torch.distributions.transformed_distribution.TransformedDistribution method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.sample_n">sample_n() (torch.distributions.distribution.Distribution method)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.Sampler">Sampler (class in torch.utils.data.sampler)</a>
</li>
      <li><a href="torch.html#torch.save">save() (in module torch)</a>
</li>
      <li><a href="torchvision/utils.html#torchvision.utils.save_image">save_image() (in module torchvision.utils)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.Scale">Scale (class in torchvision.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.scale">scale (torch.distributions.log_normal.LogNormal attribute)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril">scale_tril (torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
      <li><a href="cuda.html#torch.cuda.comm.scatter">scatter() (in module torch.cuda.comm)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.scatter">(in module torch.distributed)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.scatter_">scatter_() (torch.Tensor method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.seed">seed() (in module torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.seed_all">seed_all() (in module torch.cuda)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.select">select() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.SELU">SELU (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.selu">selu() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributed.html#torch.distributed.send">send() (in module torch.distributed)</a>
</li>
      <li><a href="nn.html#torch.nn.Sequential">Sequential (class in torch.nn)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.SequentialSampler">SequentialSampler (class in torch.utils.data.sampler)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.set_">set_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.set_default_dtype">set_default_dtype() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.set_default_tensor_type">set_default_tensor_type() (in module torch)</a>
</li>
      <li><a href="cuda.html#torch.cuda.set_device">set_device() (in module torch.cuda)</a>
</li>
      <li><a href="torch.html#torch.set_flush_denormal">set_flush_denormal() (in module torch)</a>
</li>
      <li><a href="autograd.html#torch.autograd.set_grad_enabled">set_grad_enabled (class in torch.autograd)</a>
</li>
      <li><a href="torchvision/index.html#torchvision.set_image_backend">set_image_backend() (in module torchvision)</a>
</li>
      <li><a href="torch.html#torch.set_num_threads">set_num_threads() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.set_printoptions">set_printoptions() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.set_rng_state">set_rng_state() (in module torch)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.set_rng_state">(in module torch.cuda)</a>
</li>
      </ul></li>
      <li><a href="multiprocessing.html#torch.multiprocessing.set_sharing_strategy">set_sharing_strategy() (in module torch.multiprocessing)</a>
</li>
      <li><a href="optim.html#torch.optim.SGD">SGD (class in torch.optim)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.share_memory_">share_memory_() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.share_memory_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.FloatStorage.short">short() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.short">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Sigmoid">Sigmoid (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.sigmoid">sigmoid() (in module torch)</a>

      <ul>
        <li><a href="nn.html#torch.nn.functional.sigmoid">(in module torch.nn.functional)</a>
</li>
        <li><a href="tensors.html#torch.Tensor.sigmoid">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sigmoid_">sigmoid_() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.SigmoidTransform">SigmoidTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.sign">sign (torch.distributions.transforms.Transform attribute)</a>
</li>
      <li><a href="torch.html#torch.sign">sign() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sign">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sign_">sign_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.sin">sin() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sin">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sin_">sin_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.sinh">sinh() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sinh">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sinh_">sinh_() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.size">size() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.size">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.size">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.slogdet">slogdet() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.slogdet">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.functional.smooth_l1_loss">smooth_l1_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.SmoothL1Loss">SmoothL1Loss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.soft_margin_loss">soft_margin_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.SoftMarginLoss">SoftMarginLoss (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmax">Softmax (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softmax">softmax() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmax2d">Softmax2d (class in torch.nn)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.SoftmaxTransform">SoftmaxTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="nn.html#torch.nn.Softmin">Softmin (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softmin">softmin() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Softplus">Softplus (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softplus">softplus() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Softshrink">Softshrink (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softshrink">softshrink() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Softsign">Softsign (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.softsign">softsign() (in module torch.nn.functional)</a>
</li>
      <li><a href="torch.html#torch.sort">sort() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sort">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.spadd">spadd() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.init.sparse_">sparse_() (in module torch.nn.init)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="optim.html#torch.optim.SparseAdam">SparseAdam (class in torch.optim)</a>
</li>
      <li><a href="torch.html#torch.split">split() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.split">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.spmm">spmm() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="torch.html#torch.sqrt">sqrt() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sqrt">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.sqrt_">sqrt_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.squeeze">squeeze() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.squeeze">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.squeeze_">squeeze_() (torch.Tensor method)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.squeezenet1_0">squeezenet1_0() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.squeezenet1_1">squeezenet1_1() (in module torchvision.models)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sspaddmm">sspaddmm() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sspmm">sspmm() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="torch.html#torch.stack">stack() (in module torch)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.state_dict">state_dict() (torch.nn.Module method)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.state_dict">(torch.optim.Optimizer method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.std">std() (in module torch)</a>, <a href="torch.html#torch.std">[1]</a>, <a href="torch.html#torch.std">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.std">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.stddev">stddev (torch.distributions.distribution.Distribution attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.stddev">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.stddev">(torch.distributions.gumbel.Gumbel attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.stddev">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.stddev">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.stddev">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.Adadelta.step">step() (torch.optim.Adadelta method)</a>

      <ul>
        <li><a href="optim.html#torch.optim.ASGD.step">(torch.optim.ASGD method)</a>
</li>
        <li><a href="optim.html#torch.optim.Adagrad.step">(torch.optim.Adagrad method)</a>
</li>
        <li><a href="optim.html#torch.optim.Adam.step">(torch.optim.Adam method)</a>
</li>
        <li><a href="optim.html#torch.optim.Adamax.step">(torch.optim.Adamax method)</a>
</li>
        <li><a href="optim.html#torch.optim.LBFGS.step">(torch.optim.LBFGS method)</a>
</li>
        <li><a href="optim.html#torch.optim.Optimizer.step">(torch.optim.Optimizer method)</a>
</li>
        <li><a href="optim.html#torch.optim.RMSprop.step">(torch.optim.RMSprop method)</a>
</li>
        <li><a href="optim.html#torch.optim.Rprop.step">(torch.optim.Rprop method)</a>
</li>
        <li><a href="optim.html#torch.optim.SGD.step">(torch.optim.SGD method)</a>
</li>
        <li><a href="optim.html#torch.optim.SparseAdam.step">(torch.optim.SparseAdam method)</a>
</li>
      </ul></li>
      <li><a href="optim.html#torch.optim.lr_scheduler.StepLR">StepLR (class in torch.optim.lr_scheduler)</a>
</li>
      <li><a href="torch.html#torch.stft">stft() (in module torch)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.StickBreakingTransform">StickBreakingTransform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.STL10">STL10 (class in torchvision.datasets)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage">storage() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage_offset">storage_offset() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.storage_type">storage_type() (torch.Tensor method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream">Stream (class in torch.cuda)</a>
</li>
      <li><a href="cuda.html#torch.cuda.stream">stream() (in module torch.cuda)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.stride">stride() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.studentT.StudentT">StudentT (class in torch.distributions.studentT)</a>
</li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sub">sub() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sub">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.sub_">sub_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sub_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.sampler.SubsetRandomSampler">SubsetRandomSampler (class in torch.utils.data.sampler)</a>
</li>
      <li><a href="torch.html#torch.sum">sum() (in module torch)</a>, <a href="torch.html#torch.sum">[1]</a>, <a href="torch.html#torch.sum">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.sum">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.support">support (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.support">(torch.distributions.beta.Beta attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.support">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.support">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.support">(torch.distributions.cauchy.Cauchy attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.support">(torch.distributions.dirichlet.Dirichlet attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.support">(torch.distributions.distribution.Distribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.support">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.support">(torch.distributions.fishersnedecor.FisherSnedecor attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.support">(torch.distributions.gamma.Gamma attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.support">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.support">(torch.distributions.gumbel.Gumbel attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.support">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.support">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.support">(torch.distributions.log_normal.LogNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.support">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.support">(torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.support">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.support">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.support">(torch.distributions.pareto.Pareto attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.support">(torch.distributions.poisson.Poisson attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.support">(torch.distributions.studentT.StudentT attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.support">(torch.distributions.transformed_distribution.TransformedDistribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.support">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.svd">svd() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.svd">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/datasets.html#torchvision.datasets.SVHN">SVHN (class in torchvision.datasets)</a>
</li>
      <li><a href="torch.html#torch.symeig">symeig() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.symeig">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="cuda.html#torch.cuda.synchronize">synchronize() (in module torch.cuda)</a>

      <ul>
        <li><a href="cuda.html#torch.cuda.Event.synchronize">(torch.cuda.Event method)</a>
</li>
        <li><a href="cuda.html#torch.cuda.Stream.synchronize">(torch.cuda.Stream method)</a>
</li>
      </ul></li>
  </ul></td>
</tr></table>

<h2 id="T">T</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.t">t() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.t">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.t_">t_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.t_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.table">table() (torch.autograd.profiler.profile method)</a>
</li>
      <li><a href="torch.html#torch.take">take() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.take">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.tan">tan() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.tan">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tan_">tan_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.Tanh">Tanh (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.tanh">tanh() (in module torch)</a>

      <ul>
        <li><a href="nn.html#torch.nn.functional.tanh">(in module torch.nn.functional)</a>
</li>
        <li><a href="tensors.html#torch.Tensor.tanh">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tanh_">tanh_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.Tanhshrink">Tanhshrink (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.tanhshrink">tanhshrink() (in module torch.nn.functional)</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature">temperature (torch.distributions.relaxed_bernoulli.RelaxedBernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical attribute)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.TenCrop">TenCrop (class in torchvision.transforms)</a>
</li>
      <li><a href="autograd.html#torch.Tensor">Tensor (class in torch)</a>, <a href="tensors.html#torch.Tensor">[1]</a>
</li>
      <li><a href="torch.html#torch.tensor">tensor() (in module torch)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.to">Tensor.to() (in module torch)</a>, <a href="tensors.html#torch.Tensor.to">[1]</a>, <a href="tensors.html#torch.Tensor.to">[2]</a>
</li>
      <li><a href="data.html#torch.utils.data.TensorDataset">TensorDataset (class in torch.utils.data)</a>
</li>
      <li><a href="nn.html#torch.nn.Threshold">Threshold (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.threshold">threshold() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.threshold_">threshold_() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.Module.to">to() (torch.nn.Module method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.to">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.toDense">toDense() (torch.sparse.FloatTensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.tolist">tolist() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.tolist">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ToPILImage">ToPILImage (class in torchvision.transforms)</a>
</li>
      <li><a href="torch.html#torch.topk">topk() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.topk">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#module-torch">torch (module)</a>
</li>
      <li><a href="autograd.html#module-torch.autograd">torch.autograd (module)</a>
</li>
      <li><a href="cuda.html#module-torch.cuda">torch.cuda (module)</a>
</li>
      <li><a href="tensor_attributes.html#torch.torch.device">torch.device (class in torch)</a>
</li>
      <li><a href="distributed.html#module-torch.distributed">torch.distributed (module)</a>
</li>
      <li><a href="distributed.html#module-torch.distributed.launch">torch.distributed.launch (module)</a>
</li>
      <li><a href="distributions.html#module-torch.distributions">torch.distributions (module)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#module-torch.distributions.constraint_registry">torch.distributions.constraint_registry (module)</a>
</li>
      <li><a href="distributions.html#module-torch.distributions.constraints">torch.distributions.constraints (module)</a>
</li>
      <li><a href="distributions.html#module-torch.distributions.kl">torch.distributions.kl (module)</a>
</li>
      <li><a href="distributions.html#module-torch.distributions.transforms">torch.distributions.transforms (module)</a>
</li>
      <li><a href="tensor_attributes.html#torch.torch.dtype">torch.dtype (class in torch)</a>
</li>
      <li><a href="tensor_attributes.html#torch.torch.layout">torch.layout (class in torch)</a>
</li>
      <li><a href="legacy.html#module-torch.legacy">torch.legacy (module)</a>
</li>
      <li><a href="multiprocessing.html#module-torch.multiprocessing">torch.multiprocessing (module)</a>
</li>
      <li><a href="nn.html#module-torch.nn">torch.nn (module)</a>
</li>
      <li><a href="onnx.html#module-torch.onnx">torch.onnx (module)</a>
</li>
      <li><a href="optim.html#module-torch.optim">torch.optim (module)</a>
</li>
      <li><a href="data.html#module-torch.utils.data">torch.utils.data (module)</a>
</li>
      <li><a href="model_zoo.html#module-torch.utils.model_zoo">torch.utils.model_zoo (module)</a>
</li>
      <li><a href="torchvision/index.html#module-torchvision">torchvision (module)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile.total_average">total_average() (torch.autograd.profiler.profile method)</a>
</li>
      <li><a href="torchvision/transforms.html#torchvision.transforms.ToTensor">ToTensor (class in torchvision.transforms)</a>
</li>
      <li><a href="torch.html#torch.trace">trace() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.trace">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.train">train() (torch.nn.Module method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform">Transform (class in torch.distributions.transforms)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution">TransformedDistribution (class in torch.distributions.transformed_distribution)</a>
</li>
      <li><a href="torch.html#torch.transpose">transpose() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.transpose">(torch.Tensor method)</a>
</li>
        <li><a href="sparse.html#torch.sparse.FloatTensor.transpose">(torch.sparse.FloatTensor method)</a>
</li>
      </ul></li>
      <li><a href="sparse.html#torch.sparse.FloatTensor.transpose_">transpose_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.transpose_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.tril">tril() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.tril">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.tril_">tril_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.triplet_margin_loss">triplet_margin_loss() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.TripletMarginLoss">TripletMarginLoss (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.triu">triu() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.triu">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.triu_">triu_() (torch.Tensor method)</a>
</li>
      <li><a href="torch.html#torch.trtrs">trtrs() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.trtrs">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.trunc">trunc() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.trunc">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.trunc_">trunc_() (torch.Tensor method)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage.type">type() (torch.FloatStorage method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.type">(torch.Tensor method)</a>
</li>
        <li><a href="nn.html#torch.nn.Module.type">(torch.nn.Module method)</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.type_as">type_as() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="U">U</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.unbind">unbind() (in module torch)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.unfold">unfold() (torch.Tensor method)</a>
</li>
      <li><a href="distributions.html#torch.distributions.uniform.Uniform">Uniform (class in torch.distributions.uniform)</a>
</li>
      <li><a href="nn.html#torch.nn.init.uniform_">uniform_() (in module torch.nn.init)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.uniform_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.unique">unique() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.unique">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.unsqueeze">unsqueeze() (in module torch)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.unsqueeze">(torch.Tensor method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.unsqueeze_">unsqueeze_() (torch.Tensor method)</a>
</li>
      <li><a href="nn.html#torch.nn.Upsample">Upsample (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample">upsample() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample_bilinear">upsample_bilinear() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.functional.upsample_nearest">upsample_nearest() (in module torch.nn.functional)</a>
</li>
      <li><a href="nn.html#torch.nn.UpsamplingBilinear2d">UpsamplingBilinear2d (class in torch.nn)</a>
</li>
      <li><a href="nn.html#torch.nn.UpsamplingNearest2d">UpsamplingNearest2d (class in torch.nn)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="V">V</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.html#torch.var">var() (in module torch)</a>, <a href="torch.html#torch.var">[1]</a>, <a href="torch.html#torch.var">[2]</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.var">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.variance">variance (torch.distributions.bernoulli.Bernoulli attribute)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.variance">(torch.distributions.beta.Beta attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.variance">(torch.distributions.binomial.Binomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.variance">(torch.distributions.categorical.Categorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.variance">(torch.distributions.cauchy.Cauchy attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.variance">(torch.distributions.dirichlet.Dirichlet attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.variance">(torch.distributions.distribution.Distribution attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.variance">(torch.distributions.exponential.Exponential attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.variance">(torch.distributions.fishersnedecor.FisherSnedecor attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.variance">(torch.distributions.gamma.Gamma attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.variance">(torch.distributions.geometric.Geometric attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.variance">(torch.distributions.gumbel.Gumbel attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.variance">(torch.distributions.independent.Independent attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.variance">(torch.distributions.laplace.Laplace attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.variance">(torch.distributions.log_normal.LogNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.variance">(torch.distributions.multinomial.Multinomial attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.variance">(torch.distributions.multivariate_normal.MultivariateNormal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.variance">(torch.distributions.normal.Normal attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.variance">(torch.distributions.one_hot_categorical.OneHotCategorical attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.variance">(torch.distributions.pareto.Pareto attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.variance">(torch.distributions.poisson.Poisson attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.variance">(torch.distributions.studentT.StudentT attribute)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.variance">(torch.distributions.uniform.Uniform attribute)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability">verify_ninja_availability() (in module torch.utils.cpp_extension)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg11">vgg11() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg11_bn">vgg11_bn() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg13">vgg13() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg13_bn">vgg13_bn() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg16">vgg16() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg16_bn">vgg16_bn() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg19">vgg19() (in module torchvision.models)</a>
</li>
      <li><a href="torchvision/models.html#torchvision.models.vgg19_bn">vgg19_bn() (in module torchvision.models)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.view">view() (torch.Tensor method)</a>
</li>
      <li><a href="tensors.html#torch.Tensor.view_as">view_as() (torch.Tensor method)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="W">W</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cuda.html#torch.cuda.Event.wait">wait() (torch.cuda.Event method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream.wait_event">wait_event() (torch.cuda.Stream method)</a>
</li>
      <li><a href="cuda.html#torch.cuda.Stream.wait_stream">wait_stream() (torch.cuda.Stream method)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.utils.weight_norm">weight_norm() (in module torch.nn.utils)</a>
</li>
      <li><a href="data.html#torch.utils.data.sampler.WeightedRandomSampler">WeightedRandomSampler (class in torch.utils.data.sampler)</a>
</li>
      <li><a href="torch.html#torch.where">where() (in module torch)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="X">X</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.xavier_normal_">xavier_normal_() (in module torch.nn.init)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.init.xavier_uniform_">xavier_uniform_() (in module torch.nn.init)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="Z">Z</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="sparse.html#torch.sparse.FloatTensor.zero_">zero_() (torch.sparse.FloatTensor method)</a>

      <ul>
        <li><a href="tensors.html#torch.Tensor.zero_">(torch.Tensor method)</a>
</li>
      </ul></li>
      <li><a href="nn.html#torch.nn.Module.zero_grad">zero_grad() (torch.nn.Module method)</a>

      <ul>
        <li><a href="optim.html#torch.optim.Optimizer.zero_grad">(torch.optim.Optimizer method)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.html#torch.nn.ZeroPad2d">ZeroPad2d (class in torch.nn)</a>
</li>
      <li><a href="torch.html#torch.zeros">zeros() (in module torch)</a>
</li>
      <li><a href="torch.html#torch.zeros_like">zeros_like() (in module torch)</a>
</li>
  </ul></td>
</tr></table>



           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
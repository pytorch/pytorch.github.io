


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/cuda.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="StreamContext" href="generated/torch.cuda.StreamContext.html" />
    <link rel="prev" title="torch.library" href="library.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.cuda</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/cuda.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="module-torch.cuda">
<span id="torch-cuda"></span><h1>torch.cuda<a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">¶</a></h1>
<p>This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.</p>
<p>It is lazily initialized, so you can always import it, and use
<a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_available()</span></code></a> to determine if your system supports CUDA.</p>
<p><a class="reference internal" href="notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cuda.StreamContext"/><a class="reference internal" href="generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext" title="torch.cuda.StreamContext"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StreamContext</span></code></a></p></td>
<td><p>Context-manager that selects a given stream.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.can_device_access_peer"/><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer" title="torch.cuda.can_device_access_peer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">can_device_access_peer</span></code></a></p></td>
<td><p>Checks if peer access between two devices is possible.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.current_blas_handle"/><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle" title="torch.cuda.current_blas_handle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_blas_handle</span></code></a></p></td>
<td><p>Returns cublasHandle_t pointer to current cuBLAS handle</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.current_device"/><a class="reference internal" href="generated/torch.cuda.current_device.html#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_device</span></code></a></p></td>
<td><p>Returns the index of a currently selected device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.current_stream"/><a class="reference internal" href="generated/torch.cuda.current_stream.html#torch.cuda.current_stream" title="torch.cuda.current_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_stream</span></code></a></p></td>
<td><p>Returns the currently selected <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.default_stream"/><a class="reference internal" href="generated/torch.cuda.default_stream.html#torch.cuda.default_stream" title="torch.cuda.default_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_stream</span></code></a></p></td>
<td><p>Returns the default <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.device"/><a class="reference internal" href="generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">device</span></code></a></p></td>
<td><p>Context-manager that changes the selected device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.device_count"/><a class="reference internal" href="generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count"><code class="xref py py-obj docutils literal notranslate"><span class="pre">device_count</span></code></a></p></td>
<td><p>Returns the number of GPUs available.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.device_of"/><a class="reference internal" href="generated/torch.cuda.device_of.html#torch.cuda.device_of" title="torch.cuda.device_of"><code class="xref py py-obj docutils literal notranslate"><span class="pre">device_of</span></code></a></p></td>
<td><p>Context-manager that changes the current device to that of given object.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.get_arch_list"/><a class="reference internal" href="generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list" title="torch.cuda.get_arch_list"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_arch_list</span></code></a></p></td>
<td><p>Returns list CUDA architectures this library was compiled for.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.get_device_capability"/><a class="reference internal" href="generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_device_capability</span></code></a></p></td>
<td><p>Gets the cuda capability of a device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.get_device_name"/><a class="reference internal" href="generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_device_name</span></code></a></p></td>
<td><p>Gets the name of a device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.get_device_properties"/><a class="reference internal" href="generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties" title="torch.cuda.get_device_properties"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_device_properties</span></code></a></p></td>
<td><p>Gets the properties of a device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.get_gencode_flags"/><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags" title="torch.cuda.get_gencode_flags"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_gencode_flags</span></code></a></p></td>
<td><p>Returns NVCC gencode flags this library was compiled with.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.get_sync_debug_mode"/><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode" title="torch.cuda.get_sync_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_sync_debug_mode</span></code></a></p></td>
<td><p>Returns current value of debug mode for cuda synchronizing operations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.init"/><a class="reference internal" href="generated/torch.cuda.init.html#torch.cuda.init" title="torch.cuda.init"><code class="xref py py-obj docutils literal notranslate"><span class="pre">init</span></code></a></p></td>
<td><p>Initialize PyTorch’s CUDA state.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.ipc_collect"/><a class="reference internal" href="generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect" title="torch.cuda.ipc_collect"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ipc_collect</span></code></a></p></td>
<td><p>Force collects GPU memory after it has been released by CUDA IPC.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.is_available"/><a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_available</span></code></a></p></td>
<td><p>Returns a bool indicating if CUDA is currently available.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.is_initialized"/><a class="reference internal" href="generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized" title="torch.cuda.is_initialized"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_initialized</span></code></a></p></td>
<td><p>Returns whether PyTorch’s CUDA state has been initialized.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.memory_usage"/><a class="reference internal" href="generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage" title="torch.cuda.memory_usage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_usage</span></code></a></p></td>
<td><p>Returns the percent of time over the past sample period during which global (device) memory was being read or written.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.set_device"/><a class="reference internal" href="generated/torch.cuda.set_device.html#torch.cuda.set_device" title="torch.cuda.set_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_device</span></code></a></p></td>
<td><p>Sets the current device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.set_stream"/><a class="reference internal" href="generated/torch.cuda.set_stream.html#torch.cuda.set_stream" title="torch.cuda.set_stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_stream</span></code></a></p></td>
<td><p>Sets the current stream.This is a wrapper API to set the stream.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.set_sync_debug_mode"/><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode" title="torch.cuda.set_sync_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_sync_debug_mode</span></code></a></p></td>
<td><p>Sets the debug mode for cuda synchronizing operations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.stream"/><a class="reference internal" href="generated/torch.cuda.stream.html#torch.cuda.stream" title="torch.cuda.stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stream</span></code></a></p></td>
<td><p>Wrapper around the Context-manager StreamContext that selects a given stream.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.synchronize"/><a class="reference internal" href="generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">synchronize</span></code></a></p></td>
<td><p>Waits for all kernels in all streams on a CUDA device to complete.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.utilization"/><a class="reference internal" href="generated/torch.cuda.utilization.html#torch.cuda.utilization" title="torch.cuda.utilization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">utilization</span></code></a></p></td>
<td><p>Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by <cite>nvidia-smi</cite>.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="random-number-generator">
<h2>Random Number Generator<a class="headerlink" href="#random-number-generator" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cuda.get_rng_state"/><a class="reference internal" href="generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state" title="torch.cuda.get_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_rng_state</span></code></a></p></td>
<td><p>Returns the random number generator state of the specified GPU as a ByteTensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.get_rng_state_all"/><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all" title="torch.cuda.get_rng_state_all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_rng_state_all</span></code></a></p></td>
<td><p>Returns a list of ByteTensor representing the random number states of all devices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.set_rng_state"/><a class="reference internal" href="generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state" title="torch.cuda.set_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_rng_state</span></code></a></p></td>
<td><p>Sets the random number generator state of the specified GPU.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.set_rng_state_all"/><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all" title="torch.cuda.set_rng_state_all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_rng_state_all</span></code></a></p></td>
<td><p>Sets the random number generator state of all devices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.manual_seed"/><a class="reference internal" href="generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed" title="torch.cuda.manual_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers for the current GPU.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.manual_seed_all"/><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_seed_all</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers on all GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.seed"/><a class="reference internal" href="generated/torch.cuda.seed.html#torch.cuda.seed" title="torch.cuda.seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers to a random number for the current GPU.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.seed_all"/><a class="reference internal" href="generated/torch.cuda.seed_all.html#torch.cuda.seed_all" title="torch.cuda.seed_all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed_all</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers to a random number on all GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.initial_seed"/><a class="reference internal" href="generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed" title="torch.cuda.initial_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">initial_seed</span></code></a></p></td>
<td><p>Returns the current random seed of the current GPU.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="communication-collectives">
<h2>Communication collectives<a class="headerlink" href="#communication-collectives" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast" title="torch.cuda.comm.broadcast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">comm.broadcast</span></code></a></p></td>
<td><p>Broadcasts a tensor to specified GPU devices.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced" title="torch.cuda.comm.broadcast_coalesced"><code class="xref py py-obj docutils literal notranslate"><span class="pre">comm.broadcast_coalesced</span></code></a></p></td>
<td><p>Broadcasts a sequence tensors to the specified GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add" title="torch.cuda.comm.reduce_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">comm.reduce_add</span></code></a></p></td>
<td><p>Sums tensors from multiple GPUs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter" title="torch.cuda.comm.scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">comm.scatter</span></code></a></p></td>
<td><p>Scatters tensor across multiple GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather" title="torch.cuda.comm.gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">comm.gather</span></code></a></p></td>
<td><p>Gathers tensors from multiple GPU devices.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="streams-and-events">
<h2>Streams and events<a class="headerlink" href="#streams-and-events" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cuda.Stream"/><a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Stream</span></code></a></p></td>
<td><p>Wrapper around a CUDA stream.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.ExternalStream"/><a class="reference internal" href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream" title="torch.cuda.ExternalStream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExternalStream</span></code></a></p></td>
<td><p>Wrapper around an externally allocated CUDA stream.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.Event"/><a class="reference internal" href="generated/torch.cuda.Event.html#torch.cuda.Event" title="torch.cuda.Event"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Event</span></code></a></p></td>
<td><p>Wrapper around a CUDA event.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="graphs-beta">
<h2>Graphs (beta)<a class="headerlink" href="#graphs-beta" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cuda.is_current_stream_capturing"/><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing" title="torch.cuda.is_current_stream_capturing"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_current_stream_capturing</span></code></a></p></td>
<td><p>Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.graph_pool_handle"/><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle" title="torch.cuda.graph_pool_handle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">graph_pool_handle</span></code></a></p></td>
<td><p>Returns an opaque token representing the id of a graph memory pool.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.CUDAGraph"/><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CUDAGraph</span></code></a></p></td>
<td><p>Wrapper around a CUDA graph.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.graph"/><a class="reference internal" href="generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">graph</span></code></a></p></td>
<td><p>Context-manager that captures CUDA work into a <a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.CUDAGraph</span></code></a> object for later replay.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.make_graphed_callables"/><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_graphed_callables</span></code></a></p></td>
<td><p>Accepts callables (functions or <a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Module</span></code></a>s) and returns graphed versions.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="memory-management">
<h2>Memory management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.cuda.empty_cache"/><a class="reference internal" href="generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty_cache</span></code></a></p></td>
<td><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in <cite>nvidia-smi</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.list_gpu_processes"/><a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes" title="torch.cuda.list_gpu_processes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list_gpu_processes</span></code></a></p></td>
<td><p>Returns a human-readable printout of the running processes and their GPU memory use for a given device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.mem_get_info"/><a class="reference internal" href="generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info" title="torch.cuda.mem_get_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_get_info</span></code></a></p></td>
<td><p>Returns the global free and total GPU memory occupied for a given device using cudaMemGetInfo.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.memory_stats"/><a class="reference internal" href="generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats" title="torch.cuda.memory_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_stats</span></code></a></p></td>
<td><p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.memory_summary"/><a class="reference internal" href="generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary" title="torch.cuda.memory_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_summary</span></code></a></p></td>
<td><p>Returns a human-readable printout of the current memory allocator statistics for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.memory_snapshot"/><a class="reference internal" href="generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot" title="torch.cuda.memory_snapshot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_snapshot</span></code></a></p></td>
<td><p>Returns a snapshot of the CUDA memory allocator state across all devices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.memory_allocated"/><a class="reference internal" href="generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated" title="torch.cuda.memory_allocated"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_allocated</span></code></a></p></td>
<td><p>Returns the current GPU memory occupied by tensors in bytes for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.max_memory_allocated"/><a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_memory_allocated</span></code></a></p></td>
<td><p>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.reset_max_memory_allocated"/><a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_max_memory_allocated</span></code></a></p></td>
<td><p>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.memory_reserved"/><a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_reserved</span></code></a></p></td>
<td><p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.max_memory_reserved"/><a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_memory_reserved</span></code></a></p></td>
<td><p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.set_per_process_memory_fraction"/><a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction" title="torch.cuda.set_per_process_memory_fraction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_per_process_memory_fraction</span></code></a></p></td>
<td><p>Set memory fraction for a process.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.memory_cached"/><a class="reference internal" href="generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached" title="torch.cuda.memory_cached"><code class="xref py py-obj docutils literal notranslate"><span class="pre">memory_cached</span></code></a></p></td>
<td><p>Deprecated; see <a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code class="xref py py-func docutils literal notranslate"><span class="pre">memory_reserved()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.max_memory_cached"/><a class="reference internal" href="generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_memory_cached</span></code></a></p></td>
<td><p>Deprecated; see <a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_reserved()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.reset_max_memory_cached"/><a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_max_memory_cached</span></code></a></p></td>
<td><p>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.reset_peak_memory_stats"/><a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats" title="torch.cuda.reset_peak_memory_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_peak_memory_stats</span></code></a></p></td>
<td><p>Resets the “peak” stats tracked by the CUDA memory allocator.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cuda.caching_allocator_alloc"/><a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc" title="torch.cuda.caching_allocator_alloc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">caching_allocator_alloc</span></code></a></p></td>
<td><p>Performs a memory allocation using the CUDA memory allocator.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cuda.caching_allocator_delete"/><a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete" title="torch.cuda.caching_allocator_delete"><code class="xref py py-obj docutils literal notranslate"><span class="pre">caching_allocator_delete</span></code></a></p></td>
<td><p>Deletes memory allocated using the CUDA memory allocator.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="nvidia-tools-extension-nvtx">
<h2>NVIDIA Tools Extension (NVTX)<a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark" title="torch.cuda.nvtx.mark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nvtx.mark</span></code></a></p></td>
<td><p>Describe an instantaneous event that occurred at some point.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push" title="torch.cuda.nvtx.range_push"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nvtx.range_push</span></code></a></p></td>
<td><p>Pushes a range onto a stack of nested range span.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop" title="torch.cuda.nvtx.range_pop"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nvtx.range_pop</span></code></a></p></td>
<td><p>Pops a range off of a stack of nested range spans.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="jiterator-beta">
<h2>Jiterator (beta)<a class="headerlink" href="#jiterator-beta" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn" title="torch.cuda.jiterator._create_jit_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jiterator._create_jit_fn</span></code></a></p></td>
<td><p>Create a jiterator-generated cuda kernel for an elementwise op.</p></td>
</tr>
</tbody>
</table>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torch.cuda.StreamContext.html" class="btn btn-neutral float-right" title="StreamContext" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="library.html" class="btn btn-neutral" title="torch.library" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.cuda</a><ul>
<li><a class="reference internal" href="#random-number-generator">Random Number Generator</a></li>
<li><a class="reference internal" href="#communication-collectives">Communication collectives</a></li>
<li><a class="reference internal" href="#streams-and-events">Streams and events</a></li>
<li><a class="reference internal" href="#graphs-beta">Graphs (beta)</a></li>
<li><a class="reference internal" href="#memory-management">Memory management</a></li>
<li><a class="reference internal" href="#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
<li><a class="reference internal" href="#jiterator-beta">Jiterator (beta)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>


 <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        <hr size="20" color="white" />
         <div class="privacy-policy">
            <p class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a>&nbsp;&nbsp; | &nbsp;&nbsp; <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></p>
        </div>
        <hr size="20" color="white" />
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/" style="color:#ee4c2c">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/" style="color:#ee4c2c">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>
  </footer>
  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
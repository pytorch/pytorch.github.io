


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.qconfig &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/qconfig.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.ao.quantization.qconfig</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.qconfig</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.fake_quantize</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeQuantize</span><span class="p">,</span>
    <span class="n">FakeQuantizeBase</span><span class="p">,</span>
    <span class="n">default_fake_quant</span><span class="p">,</span>
    <span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_act_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_wt_fake_quant</span><span class="p">,</span>
    <span class="n">FusedMovingAvgObsFakeQuantize</span><span class="p">,</span>
    <span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant_4bit</span><span class="p">,</span>
    <span class="n">fused_wt_fake_quant_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">fused_per_channel_wt_fake_quant_range_neg_127_to_127</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">.observer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HistogramObserver</span><span class="p">,</span>
    <span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
    <span class="n">NoopObserver</span><span class="p">,</span>
    <span class="n">PlaceholderObserver</span><span class="p">,</span>
    <span class="n">ReuseInputObserver</span><span class="p">,</span>
    <span class="n">default_debug_observer</span><span class="p">,</span>
    <span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer_4bit</span><span class="p">,</span>
    <span class="n">default_observer</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_observer</span><span class="p">,</span>
    <span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">default_weight_observer</span><span class="p">,</span>
    <span class="n">weight_observer_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">per_channel_weight_observer_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">default_reuse_input_observer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>


<span class="k">class</span> <span class="nc">QConfig</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfig&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for activations and weights respectively.</span>


<span class="sd">    Note that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization preparation function will instantiate observers multiple times for each of the layers.</span>


<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfig(</span>
<span class="sd">          activation=MinMaxObserver.with_args(dtype=torch.qint8),</span>
<span class="sd">          weight=default_observer.with_args(dtype=torch.qint8))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfig received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfig</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">QConfigDynamic</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfigDynamic&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to dynamically quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for weights.</span>

<span class="sd">    It&#39;s like QConfig, but for dynamic quantization.</span>

<span class="sd">    Note that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization function will instantiate observers multiple times for each of the layers.</span>

<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfigDynamic</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                          <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_debug_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">default_debug_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for debugging.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_per_channel_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for per channel weight quantization.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default dynamic qconfig.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_static_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
                                 <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with both activations and weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">per_channel_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized per channel.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized with a floating point zero_point.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer_4bit</span><span class="p">)</span>

<span class="n">default_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for dynamic QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing weights only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_activation_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                                          <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing activations only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># QAT config that uses a fused observer + fake quant modules for optimized training performance.</span>
<span class="c1"># to modify the activation/weight observers, the default entries in fake_quantize.py can be modified.</span>
<span class="n">default_qat_qconfig_v2</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fused_act_fake_quant</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Fused version of `default_qat_config`, has performance benefits.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_reuse_input_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_reuse_input_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">NoopObserver</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for operators that reuse the observers from input Tensor, e.g. reshape</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default PTQ qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend`: a string representing the target backend. Currently supports `fbgemm`,</span>
<span class="sd">        `qnnpack` and `onednn`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qconfig</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Version number: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span> <span class="o">+</span>
                             <span class="s2">&quot; in get_default_qconfig is not supported. Version number must be 0&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qconfig</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default, symmetric PTQ qconfig for the specified backend. And a per_channel</span>
<span class="sd">variant of the same.</span>

<span class="sd">Symmetric here applies to signed weights with zero point = 0, and additional</span>
<span class="sd">value restrictions. The activations are also signed 8-bit integers with this</span>
<span class="sd">qconfig.</span>

<span class="sd">    * Once this change is merged [as of 3/17/22], with backend or qengine =</span>
<span class="sd">    &#39;qnnpack&#39;, some quantized operators with this symmetric qconfig may use</span>
<span class="sd">    operators from xnnpack library.</span>

<span class="sd">        ** Support to use xnnpack ops with `qnnpack` backed for asymmetric</span>
<span class="sd">        qconfig (returned by get_default_qconfig()) is not available yet.</span>

<span class="sd">    * This qconfig uses signed activations and weights. Weights have added</span>
<span class="sd">    restrictions such as zero point is forced to be 0, making the weights</span>
<span class="sd">    symmetric, hence the name. And the 8-bit quantized values are</span>
<span class="sd">    restricting to to [-127, +127], excluding -128.</span>

<span class="sd">    * xnnpack has a requantization scale value restriction, 0x1p-32 &lt;=</span>
<span class="sd">    requantization_scale &lt; 256.0 where, `requantization_scale = (input_scale</span>
<span class="sd">    * kernel_scale) / (output_scale)`. Using this eps (w/ assumed max value</span>
<span class="sd">    of 256) is to prevent requantization_scale to go below xnnpack lower</span>
<span class="sd">    threshold.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">default_symmetric_qnnpack_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                                                   <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                                   <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
                                            <span class="n">weight</span><span class="o">=</span><span class="n">weight_observer_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_per_channel_symmetric_qnnpack_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                                                               <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                                               <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
                                                        <span class="n">weight</span><span class="o">=</span><span class="n">per_channel_weight_observer_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_embedding_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                        <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant</span><span class="p">)</span>

<span class="n">default_embedding_qat_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                             <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant_4bit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_default_qat_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default QAT qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend`: a string representing the target backend. Currently supports `fbgemm`,</span>
<span class="sd">        `qnnpack` and `onednn`.</span>
<span class="sd">      * `version`: version, for backwards compatibility. Can be `None` or `1`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Histogram observer is too slow for quantization aware training</span>
    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig</span>
    <span class="c1"># Use the fused observe + fake_quant modules for doing QAT.</span>
    <span class="k">elif</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig_v2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Version number: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span> <span class="o">+</span>
                             <span class="s2">&quot;in get_default_qat_qconfig is not supported. Version number must be 0 or 1&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qconfig</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default symmetric QAT qconfig for qnnpack. And its per channel weight variant.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">default_symmetric_qnnpack_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                       <span class="n">quant_min</span><span class="o">=-</span><span class="mi">128</span><span class="p">,</span>
                                                       <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span>
                                                       <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                       <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">fused_wt_fake_quant_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_per_channel_symmetric_qnnpack_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                       <span class="n">quant_min</span><span class="o">=-</span><span class="mi">128</span><span class="p">,</span>
                                                       <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span>
                                                       <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                       <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">fused_per_channel_wt_fake_quant_range_neg_127_to_127</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_default_qconfig_dict_helper</span><span class="p">(</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">,</span>
        <span class="s2">&quot;object_type&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="s2">&quot;reshape&quot;</span><span class="p">,</span> <span class="n">default_reuse_input_qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv1d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv3d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
                        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">)]}</span>

<span class="k">def</span> <span class="nf">get_default_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span>
    <span class="n">qconfig_transpose</span> <span class="o">=</span> <span class="n">qconfig</span>
    <span class="c1"># default_per_channel_weight_observer is not currently compatible with fbgemm backend</span>
    <span class="c1"># so we have to modify the weight observer to default_weight_observer or another</span>
    <span class="c1"># per tensor supported observer.</span>
    <span class="c1"># see https://github.com/pytorch/pytorch/issues/47535</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;fbgemm&quot;</span><span class="p">:</span>
        <span class="n">qconfig_transpose</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_default_qconfig_dict_helper</span><span class="p">(</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_default_qat_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span>
    <span class="n">qconfig_transpose</span> <span class="o">=</span> <span class="n">qconfig</span>
    <span class="c1"># default_per_channel_weight_observer is not currently compatible with fbgemm backend</span>
    <span class="c1"># so we have to modify the weight observer to default_weight_observer or another</span>
    <span class="c1"># per tensor supported observer</span>
    <span class="c1"># see https://github.com/pytorch/pytorch/issues/47535</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;fbgemm&quot;</span><span class="p">:</span>
        <span class="n">qconfig_transpose</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_default_qconfig_dict_helper</span><span class="p">(</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">qconfig_transpose</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">assert_valid_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">],</span>
                         <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that this `qconfig` is valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">is_conv_transpose_mod</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">is_conv_transpose_mod</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># for now, we assume that any qconfig for ConvTranspose without a weight is valid</span>
            <span class="k">return</span>
        <span class="n">example_observer</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
        <span class="n">is_per_channel</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_observer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">PerChannelMinMaxObserver</span><span class="p">)</span> <span class="ow">or</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_observer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">is_per_channel</span><span class="p">,</span> \
            <span class="s1">&#39;Per channel weight observer is not supported yet for ConvTranspose</span><span class="si">{n}</span><span class="s1">d.&#39;</span>

<span class="c1"># TODO: remove QConfigAny and replace it with Optional[QConfig]</span>
<span class="n">QConfigAny</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">add_module_to_qconfig_obs_ctr</span><span class="p">(</span>
        <span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This is a helper function for use in quantization prepare that updates a qconfig so that</span>
<span class="sd">    the constructors stored in the qconfig will create observers on the same device that</span>
<span class="sd">    &#39;module&#39; is on. This is intended to be used when the qconfigs are propagated to each</span>
<span class="sd">    module in order to avoid potential device alignment issues.</span>

<span class="sd">    Args:</span>
<span class="sd">        qconfig: QConfig with obs constructors stored in activation and weight</span>
<span class="sd">        module: module which the qconfig is related to</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig: configured so that obs constructors set to construct on the same device as module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">_fields</span> <span class="o">!=</span> <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">qconfig</span>

    <span class="k">def</span> <span class="nf">get_factory_kwargs_based_on_module_device</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> \
            <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()}</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">original_constructor</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># check if constructor can accept factory_kwargs</span>
            <span class="n">check</span> <span class="o">=</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">check</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_callable_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="n">get_factory_kwargs_based_on_module_device</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>  <span class="c1"># qconfig doesn&#39;t have activation or weight</span>
            <span class="k">return</span> <span class="n">original_constructor</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>  <span class="c1"># the class doesn&#39;t accept factory_kwargs argument</span>
            <span class="k">return</span> <span class="n">original_constructor</span>

    <span class="n">activation</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">qconfig_equals</span><span class="p">(</span><span class="n">q1</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span> <span class="n">q2</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns `True` if `q1` equals `q2`, and `False` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># functools.partial has no __eq__ operator defined so &#39;==&#39; defaults to &#39;is&#39;</span>
    <span class="k">def</span> <span class="nf">partial_equals</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
        <span class="n">same</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">func</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">func</span>
        <span class="n">same</span> <span class="o">=</span> <span class="n">same</span> <span class="ow">and</span> <span class="n">p1</span><span class="o">.</span><span class="n">args</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">args</span>
        <span class="k">return</span> <span class="n">same</span> <span class="ow">and</span> <span class="n">p1</span><span class="o">.</span><span class="n">keywords</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">keywords</span>

    <span class="k">if</span> <span class="n">q1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">q2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">q1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">q2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Qconfig weight and activation can be either a partial wrapper,</span>
            <span class="c1"># or an observer class. Special handling is required (above) for</span>
            <span class="c1"># comparing partial wrappers.</span>
            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">_PartialWrapper</span><span class="p">)):</span>
                <span class="n">activation_same</span> <span class="o">=</span> <span class="n">partial_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">activation_same</span> <span class="o">=</span> <span class="n">q1</span><span class="o">.</span><span class="n">activation</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">activation</span>
            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">_PartialWrapper</span><span class="p">)):</span>
                <span class="n">weight_same</span> <span class="o">=</span> <span class="n">partial_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weight_same</span> <span class="o">=</span> <span class="n">q1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="n">q2</span><span class="o">.</span><span class="n">weight</span>

            <span class="k">return</span> <span class="n">activation_same</span> <span class="ow">and</span> <span class="n">weight_same</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>

<span class="k">def</span> <span class="nf">activation_is_memoryless</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return whether the observer for activations defined in the given QConfig is memoryless.</span>
<span class="sd">    This means a MovingAverage observer with averaging constant equal to 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_is_memoryless</span><span class="p">(</span><span class="n">observer</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">observer</span><span class="p">,</span> <span class="s2">&quot;averaging_constant&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">observer</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">FakeQuantizeBase</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="s2">&quot;activation_post_process&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">is_reuse_input_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">(),</span> <span class="n">ReuseInputObserver</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">(),</span> <span class="n">NoopObserver</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.quantize_fx &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/quantize_fx.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.ao.quantization.quantize_fx</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.quantize_fx</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">GraphModule</span>
<span class="kn">from</span> <span class="nn">torch.fx._symbolic_trace</span> <span class="kn">import</span> <span class="n">Tracer</span>
<span class="kn">from</span> <span class="nn">torch.fx.node</span> <span class="kn">import</span> <span class="n">Target</span><span class="p">,</span> <span class="n">Node</span><span class="p">,</span> <span class="n">Argument</span>
<span class="kn">from</span> <span class="nn">torch.nn.intrinsic</span> <span class="kn">import</span> <span class="n">_FusedModule</span>
<span class="kn">from</span> <span class="nn">.fx</span> <span class="kn">import</span> <span class="n">fuse</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx</span> <span class="kn">import</span> <span class="n">prepare</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.convert</span> <span class="kn">import</span> <span class="n">convert</span>
<span class="kn">from</span> <span class="nn">.backend_config</span> <span class="kn">import</span> <span class="n">get_tensorrt_backend_config_dict</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.graph_module</span> <span class="kn">import</span> <span class="n">ObservedGraphModule</span>
<span class="kn">from</span> <span class="nn">.fx.qconfig_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_is_valid_convert_custom_config_dict</span><span class="p">,</span>
    <span class="n">check_is_valid_fuse_custom_config_dict</span><span class="p">,</span>
    <span class="n">check_is_valid_prepare_custom_config_dict</span><span class="p">,</span>
    <span class="n">check_is_valid_qconfig_dict</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.fx.utils</span> <span class="kn">import</span> <span class="n">graph_pretty_str</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.utils</span> <span class="kn">import</span> <span class="n">get_custom_module_class_keys</span>  <span class="c1"># noqa: F401</span>


<span class="k">def</span> <span class="nf">_check_is_graph_module</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">GraphModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;input model must be a GraphModule, &quot;</span>
            <span class="o">+</span> <span class="s2">&quot;Got type:&quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
            <span class="o">+</span> <span class="s2">&quot; Please make &quot;</span>
            <span class="o">+</span> <span class="s2">&quot;sure to follow the tutorials.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Swap FloatFunctional with FXFloatFunctional</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">modules_to_swap</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">FloatFunctional</span><span class="p">):</span>
            <span class="n">modules_to_swap</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">modules_to_swap</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">FXFloatFunctional</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_fuse_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fuse_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Internal helper function to fuse modules in preparation for quantization</span>

<span class="sd">    Args:</span>
<span class="sd">        graph_module: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_graph_module</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fuse</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span> <span class="n">is_qat</span><span class="p">,</span> <span class="n">fuse_custom_config_dict</span><span class="p">,</span> <span class="n">backend_config_dict</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>


<span class="k">class</span> <span class="nc">Scope</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Scope object that records the module path and the module type</span>
<span class="sd">    of a module. Scope is used to track the information of the module</span>
<span class="sd">    that contains a Node in a Graph of GraphModule. For example::</span>

<span class="sd">        class Sub(torch.nn.Module):</span>
<span class="sd">            def forward(self, x):</span>
<span class="sd">                # This will be a call_method Node in GraphModule,</span>
<span class="sd">                # scope for this would be (module_path=&quot;sub&quot;, module_type=Sub)</span>
<span class="sd">                return x.transpose(1, 2)</span>

<span class="sd">        class M(torch.nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                self.sub = Sub()</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                # This will be a call_method Node as well,</span>
<span class="sd">                # scope for this would be (module_path=&quot;&quot;, None)</span>
<span class="sd">                x = x.transpose(1, 2)</span>
<span class="sd">                x = self.sub(x)</span>
<span class="sd">                return x</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module_type</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_path</span> <span class="o">=</span> <span class="n">module_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_type</span> <span class="o">=</span> <span class="n">module_type</span>


<span class="k">class</span> <span class="nc">ScopeContextManager</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; A context manager to track the Scope of Node during symbolic tracing.</span>
<span class="sd">    When entering a forward function of a Module, we&#39;ll update the scope information of</span>
<span class="sd">    the current module, and when we exit, we&#39;ll restore the previous scope information.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">:</span> <span class="n">Scope</span><span class="p">,</span> <span class="n">current_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">current_module_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_module_type</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">module_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev_module_path</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">module_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="n">scope</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_path</span> <span class="o">=</span> <span class="n">current_module_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_module_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_module_type</span>
        <span class="k">return</span>


<span class="k">class</span> <span class="nc">QuantizationTracer</span><span class="p">(</span><span class="n">Tracer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">skipped_module_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">skipped_module_classes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skipped_module_names</span> <span class="o">=</span> <span class="n">skipped_module_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skipped_module_classes</span> <span class="o">=</span> <span class="n">skipped_module_classes</span>
        <span class="c1"># NB: initialized the module_type of top level module to None</span>
        <span class="c1"># we are assuming people won&#39;t configure the model with the type of top level</span>
        <span class="c1"># module here, since people can use &quot;&quot; for global config</span>
        <span class="c1"># We can change this if there is a use case that configures</span>
        <span class="c1"># qconfig using top level module type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="n">Scope</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node_name_to_scope</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">type</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_stack_traces</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">is_leaf_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">module_qualified_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="n">m</span><span class="o">.</span><span class="vm">__module__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;torch.nn&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="ow">or</span> <span class="n">module_qualified_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skipped_module_names</span>
            <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skipped_module_classes</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">_FusedModule</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">m</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">forward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">module_qualified_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_of_module</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
        <span class="c1"># Creating scope with information of current module</span>
        <span class="c1"># scope will be restored automatically upon exit</span>
        <span class="k">with</span> <span class="n">ScopeContextManager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">module_qualified_name</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call_module</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_node</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kind</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">target</span><span class="p">:</span> <span class="n">Target</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Argument</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Argument</span><span class="p">],</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">type_expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Node</span><span class="p">:</span>
        <span class="n">node</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">type_expr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node_name_to_scope</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_path</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">module_type</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">node</span>


<span class="k">def</span> <span class="nf">_prepare_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">prepare_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">equalization_qconfig_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_standalone_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ObservedGraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Internal helper function for prepare_fx</span>
<span class="sd">    Args:</span>
<span class="sd">      `model`, `qconfig_dict`, `prepare_custom_config_dict`, `equalization_qonfig_dict`:</span>
<span class="sd">      see docs for :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      `is_standalone_module`: a boolean flag indicates whether we are</span>
<span class="sd">      quantizing a standalone module or not, a standalone module</span>
<span class="sd">      is a submodule of the parent module that is not inlined in the</span>
<span class="sd">forward graph of the parent module,</span>
<span class="sd">      the way we quantize standalone module is described in:</span>
<span class="sd">      :func:`~torch.ao.quantization._prepare_standalone_module_fx`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prepare_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">equalization_qconfig_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">equalization_qconfig_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">check_is_valid_qconfig_dict</span><span class="p">(</span><span class="n">qconfig_dict</span><span class="p">)</span>
    <span class="n">check_is_valid_prepare_custom_config_dict</span><span class="p">(</span><span class="n">prepare_custom_config_dict</span><span class="p">)</span>
    <span class="n">check_is_valid_qconfig_dict</span><span class="p">(</span><span class="n">equalization_qconfig_dict</span><span class="p">)</span>

    <span class="n">skipped_module_names</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;non_traceable_module_name&quot;</span><span class="p">,</span> <span class="p">[]</span>
    <span class="p">)</span>
    <span class="n">skipped_module_classes</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;non_traceable_module_class&quot;</span><span class="p">,</span> <span class="p">[]</span>
    <span class="p">)</span>

    <span class="c1"># swap FloatFunctional with FXFloatFunctional</span>
    <span class="n">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># symbolically trace the model</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_standalone_module</span><span class="p">:</span>
        <span class="c1"># standalone module and custom module config are applied in top level module</span>
        <span class="n">standalone_module_name_configs</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;standalone_module_name&quot;</span><span class="p">,</span> <span class="p">[]</span>
        <span class="p">)</span>
        <span class="n">skipped_module_names</span> <span class="o">+=</span> <span class="p">[</span><span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">standalone_module_name_configs</span><span class="p">]</span>

        <span class="n">standalone_module_class_configs</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;standalone_module_class&quot;</span><span class="p">,</span> <span class="p">[]</span>
        <span class="p">)</span>
        <span class="n">skipped_module_classes</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">standalone_module_class_configs</span>
        <span class="p">]</span>
        <span class="n">float_custom_module_classes</span> <span class="o">=</span> <span class="n">get_custom_module_class_keys</span><span class="p">(</span>
            <span class="n">prepare_custom_config_dict</span><span class="p">,</span> <span class="s2">&quot;float_to_observed_custom_module_class&quot;</span>
        <span class="p">)</span>
        <span class="n">skipped_module_classes</span> <span class="o">+=</span> <span class="n">float_custom_module_classes</span>

    <span class="n">preserved_attributes</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;preserved_attributes&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">tracer</span> <span class="o">=</span> <span class="n">QuantizationTracer</span><span class="p">(</span><span class="n">skipped_module_names</span><span class="p">,</span> <span class="n">skipped_module_classes</span><span class="p">)</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">GraphModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">preserved_attributes</span><span class="p">:</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">))</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">_fuse_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">prepare_custom_config_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="p">)</span>
    <span class="n">prepared</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">tracer</span><span class="o">.</span><span class="n">node_name_to_scope</span><span class="p">,</span>
        <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="n">prepare_custom_config_dict</span><span class="p">,</span>
        <span class="n">equalization_qconfig_dict</span><span class="o">=</span><span class="n">equalization_qconfig_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="o">=</span><span class="n">backend_config_dict</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="n">is_standalone_module</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">preserved_attributes</span><span class="p">:</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">prepared</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">prepared</span>


<span class="k">def</span> <span class="nf">_prepare_standalone_module_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">prepare_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; [Internal use only] Prepare a standalone module, so that it can be used when quantizing the</span>
<span class="sd">    parent module.</span>
<span class="sd">    standalone_module means it a submodule that is not inlined in parent module,</span>
<span class="sd">    and will be quantized separately as one unit.</span>

<span class="sd">    How the standalone module is observed is specified by `input_quantized_idxs` and</span>
<span class="sd">    `output_quantized_idxs` in the prepare_custom_config for the standalone module</span>

<span class="sd">    Returns:</span>

<span class="sd">        * model(GraphModule): prepared standalone module. It has these attributes:</span>

<span class="sd">            * `_standalone_module_input_quantized_idxs(List[Int])`: a list of</span>
<span class="sd">              indexes for the graph input that is expected to be quantized,</span>
<span class="sd">              same as input_quantized_idxs configuration provided</span>
<span class="sd">              for the standalone module</span>
<span class="sd">            * `_standalone_module_output_quantized_idxs(List[Int])`: a list of</span>
<span class="sd">              indexs for the graph output that is quantized</span>
<span class="sd">              same as input_quantized_idxs configuration provided</span>
<span class="sd">              for the standalone module</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">prepare_custom_config_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="o">=</span><span class="n">backend_config_dict</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="fuse_fx"><a class="viewcode-back" href="../../../../generated/torch.quantization.quantize_fx.fuse_fx.html#torch.quantization.quantize_fx.fuse_fx">[docs]</a><span class="k">def</span> <span class="nf">fuse_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">fuse_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.</span>
<span class="sd">    Fusion rules are defined in torch.quantization.fx.fusion_pattern.py</span>

<span class="sd">    Args:</span>

<span class="sd">        * `model`: a torch.nn.Module model</span>
<span class="sd">        * `fuse_custom_config_dict`: Dictionary for custom configurations for fuse_fx, e.g.::</span>

<span class="sd">            fuse_custom_config_dict = {</span>
<span class="sd">              # Attributes that are not used in forward function will</span>
<span class="sd">              # be removed when constructing GraphModule, this is a list of attributes</span>
<span class="sd">              # to preserve as an attribute of the GraphModule even when they are</span>
<span class="sd">              # not used in the code, these attributes will also persist through deepcopy</span>
<span class="sd">              &quot;preserved_attributes&quot;: [&quot;preserved_attr&quot;],</span>
<span class="sd">            }</span>

<span class="sd">    Example::</span>

<span class="sd">        from torch.ao.quantization import fuse_fx</span>
<span class="sd">        m = Model().eval()</span>
<span class="sd">        m = fuse_fx(m)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.fuse_fx&quot;</span><span class="p">)</span>
    <span class="n">check_is_valid_fuse_custom_config_dict</span><span class="p">(</span><span class="n">fuse_custom_config_dict</span><span class="p">)</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">preserved_attributes</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">fuse_custom_config_dict</span><span class="p">:</span>
        <span class="n">preserved_attributes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">fuse_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;preserved_attributes&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">preserved_attributes</span><span class="p">:</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">_fuse_fx</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">fuse_custom_config_dict</span><span class="p">,</span> <span class="n">backend_config_dict</span><span class="p">)</span></div>


<div class="viewcode-block" id="prepare_fx"><a class="viewcode-back" href="../../../../generated/torch.quantization.quantize_fx.prepare_fx.html#torch.quantization.quantize_fx.prepare_fx">[docs]</a><span class="k">def</span> <span class="nf">prepare_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">prepare_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">equalization_qconfig_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ObservedGraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Prepare a model for post training static quantization</span>

<span class="sd">    Args:</span>
<span class="sd">      * `model`: torch.nn.Module model, must be in eval mode</span>

<span class="sd">      * `qconfig_dict`: qconfig_dict is a dictionary with the following configurations::</span>

<span class="sd">          qconfig_dict = {</span>
<span class="sd">            # optional, global config</span>
<span class="sd">            &quot;&quot;: qconfig?,</span>

<span class="sd">            # optional, used for module and function types</span>
<span class="sd">            # could also be split into module_types and function_types if we prefer</span>
<span class="sd">            &quot;object_type&quot;: [</span>
<span class="sd">              (torch.nn.Conv2d, qconfig?),</span>
<span class="sd">              (torch.nn.functional.add, qconfig?),</span>
<span class="sd">              ...,</span>
<span class="sd">             ],</span>

<span class="sd">            # optional, used for module names</span>
<span class="sd">            &quot;module_name&quot;: [</span>
<span class="sd">              (&quot;foo.bar&quot;, qconfig?)</span>
<span class="sd">              ...,</span>
<span class="sd">            ],</span>

<span class="sd">            # optional, matched in order, first match takes precedence</span>
<span class="sd">            &quot;module_name_regex&quot;: [</span>
<span class="sd">              (&quot;foo.*bar.*conv[0-9]+&quot;, qconfig?)</span>
<span class="sd">              ...,</span>
<span class="sd">            ],</span>

<span class="sd">            # optional, used for matching object type invocations in a submodule by</span>
<span class="sd">            # order</span>
<span class="sd">            # TODO(future PR): potentially support multiple indices (&#39;0,1&#39;) and/or</span>
<span class="sd">            #   ranges (&#39;0:3&#39;).</span>
<span class="sd">            &quot;module_name_object_type_order&quot;: [</span>
<span class="sd">              # fully_qualified_name, object_type, index, qconfig</span>
<span class="sd">              (&quot;foo.bar&quot;, torch.nn.functional.linear, 0, qconfig?),</span>
<span class="sd">            ],</span>

<span class="sd">            # priority (in increasing order):</span>
<span class="sd">            #   global, object_type, module_name_regex, module_name,</span>
<span class="sd">            #   module_name_object_type_order</span>
<span class="sd">            # qconfig == None means fusion and quantization should be skipped for anything</span>
<span class="sd">            # matching the rule</span>
<span class="sd">          }</span>

<span class="sd">      * `prepare_custom_config_dict`: customization configuration dictionary for quantization tool::</span>

<span class="sd">          prepare_custom_config_dict = {</span>
<span class="sd">            # optional: specify the path for standalone modules</span>
<span class="sd">            # These modules are symbolically traced and quantized as one unit</span>
<span class="sd">            &quot;standalone_module_name&quot;: [</span>
<span class="sd">               # module_name, qconfig_dict, prepare_custom_config_dict</span>
<span class="sd">               (&quot;submodule.standalone&quot;,</span>
<span class="sd">                None,  # qconfig_dict for the prepare function called in the submodule,</span>
<span class="sd">                       # None means use qconfig from parent qconfig_dict</span>
<span class="sd">                {&quot;input_quantized_idxs&quot;: [], &quot;output_quantized_idxs&quot;: []}),  # prepare_custom_config_dict</span>
<span class="sd">                {}  # backend_config_dict, TODO: point to README doc when it&#39;s ready</span>
<span class="sd">            ],</span>

<span class="sd">            &quot;standalone_module_class&quot;: [</span>
<span class="sd">                # module_class, qconfig_dict, prepare_custom_config_dict</span>
<span class="sd">                (StandaloneModule,</span>
<span class="sd">                 None,  # qconfig_dict for the prepare function called in the submodule,</span>
<span class="sd">                        # None means use qconfig from parent qconfig_dict</span>
<span class="sd">                {&quot;input_quantized_idxs&quot;: [0], &quot;output_quantized_idxs&quot;: [0]},  # prepare_custom_config_dict</span>
<span class="sd">                {})  # backend_config_dict, TODO: point to README doc when it&#39;s ready</span>
<span class="sd">            ],</span>

<span class="sd">            # user will manually define the corresponding observed</span>
<span class="sd">            # module class which has a from_float class method that converts</span>
<span class="sd">            # float custom module to observed custom module</span>
<span class="sd">            # (only needed for static quantization)</span>
<span class="sd">            &quot;float_to_observed_custom_module_class&quot;: {</span>
<span class="sd">               &quot;static&quot;: {</span>
<span class="sd">                   CustomModule: ObservedCustomModule</span>
<span class="sd">               }</span>
<span class="sd">            },</span>

<span class="sd">            # the qualified names for the submodule that are not symbolically traceable</span>
<span class="sd">            &quot;non_traceable_module_name&quot;: [</span>
<span class="sd">               &quot;non_traceable_module&quot;</span>
<span class="sd">            ],</span>

<span class="sd">            # the module classes that are not symbolically traceable</span>
<span class="sd">            # we&#39;ll also put dynamic/weight_only custom module here</span>
<span class="sd">            &quot;non_traceable_module_class&quot;: [</span>
<span class="sd">               NonTraceableModule</span>
<span class="sd">            ],</span>

<span class="sd">            # By default, inputs and outputs of the graph are assumed to be in</span>
<span class="sd">            # fp32. Providing `input_quantized_idxs` will set the inputs with the</span>
<span class="sd">            # corresponding indices to be quantized. Providing</span>
<span class="sd">            # `output_quantized_idxs` will set the outputs with the corresponding</span>
<span class="sd">            # indices to be quantized.</span>
<span class="sd">            &quot;input_quantized_idxs&quot;: [0],</span>
<span class="sd">            &quot;output_quantized_idxs&quot;: [0],</span>

<span class="sd">            # Attributes that are not used in forward function will</span>
<span class="sd">            # be removed when constructing GraphModule, this is a list of attributes</span>
<span class="sd">            # to preserve as an attribute of the GraphModule even when they are</span>
<span class="sd">            # not used in the code, these attributes will also persist through deepcopy</span>
<span class="sd">            &quot;preserved_attributes&quot;: [&quot;preserved_attr&quot;],</span>
<span class="sd">          }</span>

<span class="sd">      * `equalization_qconfig_dict`: equalization_qconfig_dict is a dictionary</span>
<span class="sd">        with a similar structure as qconfig_dict except it will contain</span>
<span class="sd">        configurations specific to equalization techniques such as input-weight</span>
<span class="sd">        equalization.</span>

<span class="sd">      * `backend_config_dict`: a dictionary that specifies how operators are quantized</span>
<span class="sd">         in a backend, this includes how the operaetors are observed,</span>
<span class="sd">         supported fusion patterns, how quantize/dequantize ops are</span>
<span class="sd">         inserted, supported dtypes etc. The structure of the dictionary is still WIP</span>
<span class="sd">         and will change in the future, please don&#39;t use right now.</span>


<span class="sd">    Return:</span>
<span class="sd">      A GraphModule with observer (configured by qconfig_dict), ready for calibration</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        from torch.ao.quantization import get_default_qconfig</span>
<span class="sd">        from torch.ao.quantization import prepare_fx</span>

<span class="sd">        float_model.eval()</span>
<span class="sd">        qconfig = get_default_qconfig(&#39;fbgemm&#39;)</span>
<span class="sd">        def calibrate(model, data_loader):</span>
<span class="sd">            model.eval()</span>
<span class="sd">            with torch.no_grad():</span>
<span class="sd">                for image, target in data_loader:</span>
<span class="sd">                    model(image)</span>

<span class="sd">        qconfig_dict = {&quot;&quot;: qconfig}</span>
<span class="sd">        prepared_model = prepare_fx(float_model, qconfig_dict)</span>
<span class="sd">        # Run calibration</span>
<span class="sd">        calibrate(prepared_model, sample_inference_data)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.prepare_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>  <span class="c1"># is_qat</span>
        <span class="n">prepare_custom_config_dict</span><span class="p">,</span>
        <span class="n">equalization_qconfig_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="prepare_qat_fx"><a class="viewcode-back" href="../../../../generated/torch.quantization.quantize_fx.prepare_qat_fx.html#torch.quantization.quantize_fx.prepare_qat_fx">[docs]</a><span class="k">def</span> <span class="nf">prepare_qat_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">prepare_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ObservedGraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Prepare a model for quantization aware training</span>

<span class="sd">    Args:</span>
<span class="sd">      * `model`: torch.nn.Module model, must be in train mode</span>
<span class="sd">      * `qconfig_dict`: see :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      * `prepare_custom_config_dict`: see :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      * `backend_config_dict`: see :func:`~torch.ao.quantization.prepare_fx`</span>

<span class="sd">    Return:</span>
<span class="sd">      A GraphModule with fake quant modules (configured by qconfig_dict), ready for</span>
<span class="sd">      quantization aware training</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        from torch.ao.quantization import get_default_qat_qconfig</span>
<span class="sd">        from torch.ao.quantization import prepare_fx</span>

<span class="sd">        qconfig = get_default_qat_qconfig(&#39;fbgemm&#39;)</span>
<span class="sd">        def train_loop(model, train_data):</span>
<span class="sd">            model.train()</span>
<span class="sd">            for image, target in data_loader:</span>
<span class="sd">                ...</span>

<span class="sd">        float_model.train()</span>
<span class="sd">        qconfig_dict = {&quot;&quot;: qconfig}</span>
<span class="sd">        prepared_model = prepare_fx(float_model, qconfig_dict)</span>
<span class="sd">        # Run calibration</span>
<span class="sd">        train_loop(prepared_model, train_loop)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.prepare_qat_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="kc">True</span><span class="p">,</span>  <span class="c1"># is_qat</span>
        <span class="n">prepare_custom_config_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="o">=</span><span class="n">backend_config_dict</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_convert_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_reference</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">convert_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_standalone_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">convert_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">convert_custom_config_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">_check_is_graph_module</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
    <span class="n">check_is_valid_convert_custom_config_dict</span><span class="p">(</span><span class="n">convert_custom_config_dict</span><span class="p">)</span>

    <span class="n">quantized</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config_dict</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="p">,</span>
        <span class="n">_remove_qconfig_flag</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">convert_qconfig_dict</span><span class="o">=</span><span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="o">=</span><span class="n">backend_config_dict</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">preserved_attributes</span> <span class="o">=</span> <span class="n">convert_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;preserved_attributes&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">preserved_attributes</span><span class="p">:</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">quantized</span>


<div class="viewcode-block" id="convert_fx"><a class="viewcode-back" href="../../../../generated/torch.quantization.quantize_fx.convert_fx.html#torch.quantization.quantize_fx.convert_fx">[docs]</a><span class="k">def</span> <span class="nf">convert_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_reference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Convert a calibrated or trained model to a quantized model</span>

<span class="sd">    Args:</span>
<span class="sd">        * `graph_module`: A prepared and calibrated/trained model (GraphModule)</span>
<span class="sd">        * `is_reference`: flag for whether to produce a reference quantized model,</span>
<span class="sd">          which will be a common interface between pytorch quantization with</span>
<span class="sd">          other backends like accelerators</span>
<span class="sd">        * `convert_custom_config_dict`: dictionary for custom configurations for convert function::</span>

<span class="sd">            convert_custom_config_dict = {</span>
<span class="sd">              # user will manually define the corresponding quantized</span>
<span class="sd">              # module class which has a from_observed class method that converts</span>
<span class="sd">              # observed custom module to quantized custom module</span>
<span class="sd">              &quot;observed_to_quantized_custom_module_class&quot;: {</span>
<span class="sd">                 &quot;static&quot;: {</span>
<span class="sd">                     ObservedCustomModule: QuantizedCustomModule</span>
<span class="sd">                 },</span>
<span class="sd">                 &quot;dynamic&quot;: {</span>
<span class="sd">                     ObservedCustomModule: QuantizedCustomModule</span>
<span class="sd">                 },</span>
<span class="sd">                 &quot;weight_only&quot;: {</span>
<span class="sd">                     ObservedCustomModule: QuantizedCustomModule</span>
<span class="sd">                 }</span>
<span class="sd">              },</span>

<span class="sd">              # Attributes that are not used in forward function will</span>
<span class="sd">              # be removed when constructing GraphModule, this is a list of attributes</span>
<span class="sd">              # to preserve as an attribute of the GraphModule even when they are</span>
<span class="sd">              # not used in the code</span>
<span class="sd">              &quot;preserved_attributes&quot;: [&quot;preserved_attr&quot;],</span>
<span class="sd">            }</span>

<span class="sd">        * `_remove_qconfig`: Option to remove the qconfig attributes in the model after convert.</span>

<span class="sd">        * `qconfig_dict`: qconfig_dict with either same keys as what is passed to</span>
<span class="sd">          the qconfig_dict in `prepare_fx` API, with same values or `None`, or</span>
<span class="sd">          additional keys with values set to `None`</span>

<span class="sd">          For each entry whose value is set to None, we skip quantizing that entry in the model::</span>

<span class="sd">            qconfig_dict = {</span>
<span class="sd">              # used for object_type, skip quantizing torch.nn.functional.add</span>
<span class="sd">              &quot;object_type&quot;: [</span>
<span class="sd">                (torch.nn.functional.add, None),</span>
<span class="sd">                (torch.nn.functional.linear, qconfig_from_prepare)</span>
<span class="sd">                ...,</span>
<span class="sd">              ],</span>

<span class="sd">              # sed for module names, skip quantizing &quot;foo.bar&quot;</span>
<span class="sd">              &quot;module_name&quot;: [</span>
<span class="sd">                (&quot;foo.bar&quot;, None)</span>
<span class="sd">                ...,</span>
<span class="sd">              ],</span>
<span class="sd">            }</span>

<span class="sd">         * `backend_config_dict`: A configuration for the backend which describes how</span>
<span class="sd">            operators should be quantized in the backend, this includes quantization</span>
<span class="sd">            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),</span>
<span class="sd">            observer placement for each operators and fused operators. Detailed</span>
<span class="sd">            documentation can be found in torch/ao/quantization/backend_config/README.md</span>

<span class="sd">    Return:</span>
<span class="sd">        A quantized model (GraphModule)</span>

<span class="sd">    Example::</span>

<span class="sd">        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training</span>
<span class="sd">        quantized_model = convert_fx(prepared_model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.convert_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config_dict</span><span class="p">,</span>
        <span class="n">_remove_qconfig</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">qconfig_dict</span><span class="o">=</span><span class="n">qconfig_dict</span><span class="p">,</span>
        <span class="n">backend_config_dict</span><span class="o">=</span><span class="n">backend_config_dict</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_convert_standalone_module_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_reference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_custom_config_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`</span>
<span class="sd">    and convert it to a quantized model</span>

<span class="sd">    Returns a quantized standalone module, whether input/output is quantized is</span>
<span class="sd">    specified by prepare_custom_config_dict, with</span>
<span class="sd">    input_quantized_idxs, output_quantized_idxs, please</span>
<span class="sd">    see docs for prepare_fx for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config_dict</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
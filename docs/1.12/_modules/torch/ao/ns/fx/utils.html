


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.ns.fx.utils &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/ns/fx/utils.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.ao.ns.fx.utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.ns.fx.utils</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">operator</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic.quantized</span> <span class="k">as</span> <span class="nn">nniq</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantized</span> <span class="k">as</span> <span class="nn">nnq</span>

<span class="n">toq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">GraphModule</span>
<span class="kn">from</span> <span class="nn">torch.fx.graph</span> <span class="kn">import</span> <span class="n">Node</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ObserverBase</span><span class="p">,</span>
    <span class="n">FakeQuantizeBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.utils</span> <span class="kn">import</span> <span class="n">getattr_from_fqn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize</span> <span class="kn">import</span> <span class="n">is_activation_post_process</span>

<span class="kn">from</span> <span class="nn">.ns_types</span> <span class="kn">import</span> <span class="n">NSNodeTargetType</span><span class="p">,</span> <span class="n">NSResultsType</span>

<span class="c1"># TODO(future PR): consider deleting this enum and using the torch types</span>
<span class="c1"># directly.  This might be tricky because it is not a one to one mapping.</span>
<span class="k">class</span> <span class="nc">NodeInputOrOutputType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">FP32</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>  <span class="c1"># torch.float</span>
    <span class="n">INT8</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>  <span class="c1"># torch.qint8 or torch.quint8</span>
    <span class="n">FP16</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>  <span class="c1"># torch.float16</span>
    <span class="n">UNKNOWN</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>  <span class="c1"># we cannot determine input/output dtype</span>
    <span class="c1"># TODO(future PR): while these functions can support multiple dtypes,</span>
    <span class="c1">#   for the purposes of numerical debugging we want to get the actual</span>
    <span class="c1">#   dtype used in the model. We will likely need some kind of dtype</span>
    <span class="c1">#   propagation to estimate this.</span>
    <span class="n">FP32_OR_INT8</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>  <span class="c1"># either torch.float or torch.quint8 or torch.qint8</span>
    <span class="c1"># TODO(future PRs): dynamic quant, fake quant, etc</span>


<span class="k">def</span> <span class="nf">get_node_first_input_and_output_type</span><span class="p">(</span>
    <span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">,</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">logger_cls</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">node_type_to_io_type_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">NSNodeTargetType</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">NodeInputOrOutputType</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="p">]:</span>

    <span class="c1"># TODO(future PR): clean this up</span>
    <span class="n">FUNS_IO_TYPE_FP32</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;funs_io_type_fp32&quot;</span><span class="p">]</span>
    <span class="n">FUNS_IO_TYPE_FP16</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;funs_io_type_fp16&quot;</span><span class="p">]</span>
    <span class="n">FUNS_IO_TYPE_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;funs_io_type_int8&quot;</span><span class="p">]</span>
    <span class="n">FUNS_IO_TYPE_FP32_OR_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;funs_io_type_fp32_or_int8&quot;</span><span class="p">]</span>
    <span class="n">MODS_IO_TYPE_FP32</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;mods_io_type_fp32&quot;</span><span class="p">]</span>
    <span class="n">MODS_IO_TYPE_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;mods_io_type_int8&quot;</span><span class="p">]</span>
    <span class="n">MODS_IO_TYPE_FP32_OR_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;mods_io_type_fp32_or_int8&quot;</span><span class="p">]</span>
    <span class="n">METHS_IO_TYPE_FP32_OR_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;meths_io_type_fp32_or_int8&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">FUNS_IO_TYPE_FP32</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">FUNS_IO_TYPE_FP16</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP16</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP16</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">FUNS_IO_TYPE_INT8</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">INT8</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">FUNS_IO_TYPE_FP32_OR_INT8</span><span class="p">:</span>
            <span class="n">first_arg</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_arg</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">_prev_node_input_type</span><span class="p">,</span>
                <span class="n">prev_node_output_type</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">get_node_first_input_and_output_type</span><span class="p">(</span>
                <span class="n">first_arg</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">logger_cls</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">prev_node_output_type</span><span class="p">,</span> <span class="n">prev_node_output_type</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="n">is_known_fp32_or_int8_input_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">target_type</span> <span class="ow">in</span> <span class="n">MODS_IO_TYPE_FP32_OR_INT8</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><span class="n">logger_cls</span><span class="p">,</span> <span class="n">ObserverBase</span><span class="p">,</span> <span class="n">FakeQuantizeBase</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="ow">or</span> <span class="n">is_known_fp32_or_int8_input_module</span>
        <span class="p">):</span>
            <span class="c1"># A logger or observer&#39;s input and output type is the output</span>
            <span class="c1"># type of the preceding node.</span>
            <span class="n">first_arg</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_arg</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">_prev_node_input_type</span><span class="p">,</span>
                <span class="n">prev_node_output_type</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">get_node_first_input_and_output_type</span><span class="p">(</span>
                <span class="n">first_arg</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">logger_cls</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">prev_node_output_type</span><span class="p">,</span> <span class="n">prev_node_output_type</span><span class="p">)</span>
        <span class="n">is_known_fp32_input_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">target_type</span> <span class="ow">in</span> <span class="n">MODS_IO_TYPE_FP32</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="n">is_known_int8_input_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">target_type</span> <span class="ow">in</span> <span class="n">MODS_IO_TYPE_INT8</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_known_fp32_input_module</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP32</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_known_int8_input_module</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">INT8</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_method&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;dequantize&quot;</span><span class="p">:</span>
            <span class="c1"># Dequantize is a special node because it allows multiple input types.</span>
            <span class="c1"># So, we look up the output type of the previous node and return that</span>
            <span class="c1"># as the input type of this node instance.</span>
            <span class="n">prev_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">_prev_node_input_type</span><span class="p">,</span>
                <span class="n">prev_node_output_type</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">get_node_first_input_and_output_type</span><span class="p">(</span>
                <span class="n">prev_node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">logger_cls</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">prev_node_output_type</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP32</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;to&quot;</span><span class="p">:</span>
            <span class="c1"># to is a special node because it allows multiple input types.</span>
            <span class="c1"># So, we look up the output type of the previous node and return that</span>
            <span class="c1"># as the input type of this node instance. We also look up the target</span>
            <span class="c1"># of to and return the correct output type.</span>
            <span class="n">prev_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">_prev_node_input_type</span><span class="p">,</span>
                <span class="n">prev_node_output_type</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">get_node_first_input_and_output_type</span><span class="p">(</span>
                <span class="n">prev_node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">logger_cls</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span>
            <span class="p">)</span>

            <span class="n">cur_node_dtype_target</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">cur_node_dtype_target</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cur_node_dtype_target</span><span class="si">}</span><span class="s2"> handling needs to be added&quot;</span>

            <span class="k">return</span> <span class="p">(</span><span class="n">prev_node_output_type</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">FP16</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="n">METHS_IO_TYPE_FP32_OR_INT8</span><span class="p">:</span>
            <span class="n">first_arg</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_arg</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">_prev_node_input_type</span><span class="p">,</span>
                <span class="n">prev_node_output_type</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">get_node_first_input_and_output_type</span><span class="p">(</span>
                <span class="n">first_arg</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">logger_cls</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">prev_node_output_type</span><span class="p">,</span> <span class="n">prev_node_output_type</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">,</span> <span class="n">NodeInputOrOutputType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_node_input_qparams</span><span class="p">(</span>
    <span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">,</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">node_type_to_io_type_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">NSNodeTargetType</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the qparams (scale, zero_point) of the first input to `node`,</span>
<span class="sd">    if they can be inferred from the graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prev_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">Node</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">MODS_IO_TYPE_FP32_OR_INT8</span> <span class="o">=</span> <span class="n">node_type_to_io_type_map</span><span class="p">[</span><span class="s2">&quot;mods_io_type_fp32_or_int8&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_scale_zp_from_function_args</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">scale_arg_idx</span><span class="p">,</span> <span class="n">zp_arg_idx</span><span class="p">):</span>
        <span class="n">scale_node</span><span class="p">,</span> <span class="n">zp_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="n">scale_arg_idx</span><span class="p">],</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="n">zp_arg_idx</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_node</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">zp_node</span><span class="p">,</span> <span class="n">Node</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">zp_node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">scale_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">scale_node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="n">zp_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">zp_node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">scale_obj</span><span class="p">,</span> <span class="n">zp_obj</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">prev_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>

        <span class="c1"># quantize - read the args directly</span>
        <span class="k">if</span> <span class="n">prev_node</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantize_per_tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_get_scale_zp_from_function_args</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">prev_node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">(</span><span class="n">toq</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">toq</span><span class="o">.</span><span class="n">add_relu</span><span class="p">,</span> <span class="n">toq</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">toq</span><span class="o">.</span><span class="n">mul_relu</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_get_scale_zp_from_function_args</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># TODO(future PR): handle more functionals</span>
        <span class="c1"># TODO(future PR): handle functional ops which inherit qparams from input</span>

    <span class="k">elif</span> <span class="n">prev_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>

        <span class="c1"># get type of the module</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prev_node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">module_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">prev_node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">module_obj</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">ConvReLU2d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">ELU</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">,</span>
                <span class="n">nnq</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">BNReLU2d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">BNReLU3d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">ConvReLU1d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">ConvReLU2d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">ConvReLU3d</span><span class="p">,</span>
                <span class="n">nniq</span><span class="o">.</span><span class="n">LinearReLU</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">module_obj</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">module_obj</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span>

        <span class="n">is_known_fp32_or_int8_input_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">module_obj</span><span class="p">,</span> <span class="n">target_type</span><span class="p">)</span> <span class="k">for</span> <span class="n">target_type</span> <span class="ow">in</span> <span class="n">MODS_IO_TYPE_FP32_OR_INT8</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_known_fp32_or_int8_input_module</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">get_node_input_qparams</span><span class="p">(</span><span class="n">prev_node</span><span class="p">,</span> <span class="n">gm</span><span class="p">,</span> <span class="n">node_type_to_io_type_map</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">return_first_non_observer_node</span><span class="p">(</span>
    <span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">,</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Node</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If node is not an observer, returns it.  If node is an observer,</span>
<span class="sd">    navigates up the graph and returns the first parent which is not an</span>
<span class="sd">    observer.  For example,</span>

<span class="sd">    graph: (node_non_obs), node = node_non_obs : returns node_non_obs</span>
<span class="sd">    graph: (node_non_obs -&gt; obs0), node = obs0 : returns node_non_obs</span>
<span class="sd">    graph: (node_non_obs -&gt; obs0 -&gt; fq0), node = fq0 : returns node_non_obs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
        <span class="n">node_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">if</span> <span class="n">is_activation_post_process</span><span class="p">(</span><span class="n">node_obj</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Node</span><span class="p">)</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># code duplication intended, not worth refactoring</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="n">node_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_activation_post_process</span><span class="p">(</span><span class="n">node_obj</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Node</span><span class="p">)</span>
                <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">node</span>


<span class="k">def</span> <span class="nf">get_number_of_non_param_args</span><span class="p">(</span>
    <span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">,</span>
    <span class="n">gm</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Assumes that all non-param args occur first. Returns the number of</span>
<span class="sd">    non-param args expected for a node.  For example, for</span>

<span class="sd">      F.linear(x, weight, bias)</span>

<span class="sd">    Returns 1, because x is a non-param arg and weight and bias are params.</span>
<span class="sd">    For</span>

<span class="sd">      lstm_mod(x, hid)</span>

<span class="sd">    Returns 2, because both x and hid are non-param args.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
        <span class="n">node_obj</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_obj</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">):</span>
            <span class="k">return</span> <span class="mi">2</span>

    <span class="c1"># default is 1</span>
    <span class="k">return</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">get_arg_indices_of_inputs_to_log</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of args of the node which we should attach</span>
<span class="sd">    loggers to, if input logging is enabled.</span>

<span class="sd">    For example,</span>
<span class="sd">    * for (x + y), returns [0, 1]</span>
<span class="sd">    * for (1 + y), returns [1]</span>
<span class="sd">    * for (x + 1), returns [0]</span>
<span class="sd">    * for (linear(x, w, b)) returns [0]</span>
<span class="sd">    * by default, returns [0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span> <span class="ow">and</span> <span class="p">(</span>
        <span class="c1"># TODO(future PR): use relationship map instead of hardcoding</span>
        <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Node</span><span class="p">:</span>
                <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_target_type_str</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">,</span> <span class="n">gm</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a string representation of the type of the function or module</span>
<span class="sd">    pointed to by this node, or &#39;&#39; for other node types.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">target_type</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;call_function&quot;</span><span class="p">,</span> <span class="s2">&quot;call_method&quot;</span><span class="p">):</span>
        <span class="n">target_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_module&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">target_mod</span> <span class="o">=</span> <span class="n">getattr_from_fqn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="n">target_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">target_mod</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">target_type</span>


<span class="k">def</span> <span class="nf">rekey_logger_info_on_node_name_of_model</span><span class="p">(</span>
    <span class="n">results</span><span class="p">:</span> <span class="n">NSResultsType</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NSResultsType</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rekeys the layer name of a results dictionary to use node names</span>
<span class="sd">    from `model_name`.</span>

<span class="sd">    For example, transforms</span>

<span class="sd">        {&#39;base_op_1_0&#39;: {&#39;node_output&#39;: {&#39;model_a&#39;:</span>
<span class="sd">          [{&#39;ref_node_name&#39;: &#39;linear1&#39;, ...}]}}}</span>

<span class="sd">    into</span>

<span class="sd">        {&#39;linear1&#39;: {&#39;node_output&#39;: {&#39;model_a&#39;:</span>
<span class="sd">          [{&#39;ref_node_name&#39;: &#39;linear1&#39;, ...}]}}}</span>

<span class="sd">    Note: we cannot use these node names directly because they are not</span>
<span class="sd">    guaranteed to be consistent across models. This is why we extract</span>
<span class="sd">    the results first and rekey afterwards.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">new_results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">old_layer_name</span><span class="p">,</span> <span class="n">result_type_to_results</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">new_layer_name</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">_result_type</span><span class="p">,</span> <span class="n">model_name_to_results</span> <span class="ow">in</span> <span class="n">result_type_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">cur_model_name</span><span class="p">,</span> <span class="n">list_of_results</span> <span class="ow">in</span> <span class="n">model_name_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">cur_model_name</span> <span class="o">==</span> <span class="n">model_name</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">list_of_results</span><span class="p">)</span>
                    <span class="n">new_layer_name</span> <span class="o">=</span> <span class="n">list_of_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;ref_node_name&quot;</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">continue</span>
        <span class="k">if</span> <span class="n">new_layer_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_results</span><span class="p">[</span><span class="n">new_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_type_to_results</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_results</span><span class="p">[</span><span class="n">old_layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_type_to_results</span>
    <span class="k">return</span> <span class="n">new_results</span>


<span class="k">def</span> <span class="nf">maybe_add_missing_fqns</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="n">NSResultsType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If `fqn` entries are filled in for one of the models in `results`, copies</span>
<span class="sd">    them over to any models which do not have them filled out.</span>

<span class="sd">    A common use case benefitting from this is comparing a model prepared by</span>
<span class="sd">    quantization to a quantized model. In this case, the model prepared by</span>
<span class="sd">    quantization would have `fqn` entries, and the quantized model would not.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Check in the first result to find any model with fqn entries defined.</span>
    <span class="n">model_name_with_fqns</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">result_type_to_results</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">result_type</span><span class="p">,</span> <span class="n">model_name_to_results</span> <span class="ow">in</span> <span class="n">result_type_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_results</span> <span class="ow">in</span> <span class="n">model_name_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_results</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">model_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;fqn&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">model_name_with_fqns</span> <span class="o">=</span> <span class="n">model_name</span>
                        <span class="k">break</span>
            <span class="k">break</span>
        <span class="k">break</span>

    <span class="k">if</span> <span class="n">model_name_with_fqns</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">result_type_to_results</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">result_type</span><span class="p">,</span> <span class="n">model_name_to_results</span> <span class="ow">in</span> <span class="n">result_type_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">ref_model_results</span> <span class="o">=</span> <span class="n">model_name_to_results</span><span class="p">[</span><span class="n">model_name_with_fqns</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_results</span> <span class="ow">in</span> <span class="n">model_name_to_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="n">model_name_with_fqns</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_results</span><span class="p">)):</span>
                        <span class="n">fqn</span> <span class="o">=</span> <span class="n">ref_model_results</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;fqn&quot;</span><span class="p">]</span>
                        <span class="n">model_results</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;fqn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fqn</span>


<span class="k">def</span> <span class="nf">maybe_dequantize_first_two_tensor_args_and_handle_tuples</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="o">*</span><span class="n">a_other</span> <span class="o">=</span> <span class="n">args</span>

        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">el0</span><span class="p">,</span> <span class="n">el1</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">):</span>
                <span class="n">new_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">el0</span><span class="p">,</span> <span class="n">el1</span><span class="p">,</span> <span class="o">*</span><span class="n">a_other</span><span class="p">)</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inner</span><span class="p">(</span><span class="o">*</span><span class="n">new_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">results</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">a0</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                <span class="n">a0</span> <span class="o">=</span> <span class="n">a0</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">a1</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                <span class="n">a1</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>

        <span class="c1"># for the purposes of this util, only handle floats</span>
        <span class="k">if</span> <span class="n">a0</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="ow">or</span> <span class="n">a1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">new_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="o">*</span><span class="n">a_other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">new_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>


<div class="viewcode-block" id="compute_sqnr"><a class="viewcode-back" href="../../../../../torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_sqnr">[docs]</a><span class="nd">@maybe_dequantize_first_two_tensor_args_and_handle_tuples</span>
<span class="k">def</span> <span class="nf">compute_sqnr</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the SQNR between `x` and `y`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor or tuple of tensors</span>
<span class="sd">        y: Tensor or tuple of tensors</span>

<span class="sd">    Return:</span>
<span class="sd">        float or tuple of floats</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Ps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">Pn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">Ps</span> <span class="o">/</span> <span class="n">Pn</span><span class="p">)</span></div>


<div class="viewcode-block" id="compute_normalized_l2_error"><a class="viewcode-back" href="../../../../../torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_normalized_l2_error">[docs]</a><span class="nd">@maybe_dequantize_first_two_tensor_args_and_handle_tuples</span>
<span class="k">def</span> <span class="nf">compute_normalized_l2_error</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized L2 error between `x` and `y`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor or tuple of tensors</span>
<span class="sd">        y: Tensor or tuple of tensors</span>

<span class="sd">    Return:</span>
<span class="sd">        float or tuple of floats</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span></div>


<div class="viewcode-block" id="compute_cosine_similarity"><a class="viewcode-back" href="../../../../../torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_cosine_similarity">[docs]</a><span class="nd">@maybe_dequantize_first_two_tensor_args_and_handle_tuples</span>
<span class="k">def</span> <span class="nf">compute_cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cosine similarity between `x` and `y`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor or tuple of tensors</span>
<span class="sd">        y: Tensor or tuple of tensors</span>

<span class="sd">    Return:</span>
<span class="sd">        float or tuple of floats</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># For convolutions, the shape of the quantized weight has one additional</span>
    <span class="c1"># dimension compared to the shape of the fp32 weight. Match the shapes</span>
    <span class="c1"># to enable cosine similarity comparison.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">op_type_supports_shadowing</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">Node</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;call_function&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">):</span>
            <span class="c1"># shadowing for ops with multiple tensor inputs is not implemented yet</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../_static/jquery.js"></script>
         <script src="../../../../../_static/underscore.js"></script>
         <script src="../../../../../_static/doctools.js"></script>
         <script src="../../../../../_static/clipboard.min.js"></script>
         <script src="../../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
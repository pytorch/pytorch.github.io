


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda.graphs &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/cuda/graphs.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../cuda.html">torch.cuda</a> &gt;</li>
        
      <li>torch.cuda.graphs</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.cuda.graphs</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_dummy_type</span>


<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s1">&#39;_CudaStreamBase&#39;</span><span class="p">):</span>
    <span class="c1"># Define dummy base classes</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_CUDAGraph&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_CUDAGraph&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_graph_pool_handle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_graph_pool_handle&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_cuda_isCurrentStreamCapturing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_cuda_isCurrentStreamCapturing&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_CUDAGraph</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_graph_pool_handle</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_cuda_isCurrentStreamCapturing</span>


<div class="viewcode-block" id="is_current_stream_capturing"><a class="viewcode-back" href="../../../generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing">[docs]</a><span class="k">def</span> <span class="nf">is_current_stream_capturing</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</span>

<span class="sd">    If a CUDA context does not exist on the current device, returns False without initializing the context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_cuda_isCurrentStreamCapturing</span><span class="p">()</span></div>

<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<div class="viewcode-block" id="graph_pool_handle"><a class="viewcode-back" href="../../../generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle">[docs]</a><span class="k">def</span> <span class="nf">graph_pool_handle</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an opaque token representing the id of a graph memory pool.</span>
<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_graph_pool_handle</span><span class="p">()</span></div>


<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<span class="k">class</span> <span class="nc">CUDAGraph</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_CUDAGraph</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper around a CUDA graph.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">capture_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Begins capturing CUDA work on the current stream.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_begin`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_begin`` internally.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or</span>
<span class="sd">                :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) that hints this graph may share memory</span>
<span class="sd">                with the indicated pool.  See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># I&#39;m not sure if pybind11 converts a None arg to the default defined on the C++ side,</span>
        <span class="c1"># so I&#39;m not taking any chances.</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">capture_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ends CUDA graph capture on the current stream.</span>
<span class="sd">        After ``capture_end``, ``replay`` may be called on this instance.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_end`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_end`` internally.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replays the CUDA work captured by this graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deletes the graph currently held by this instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">pool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns an opaque token representing the id of this graph&#39;s memory pool.</span>
<span class="sd">        This id can optionally be passed to another graph&#39;s ``capture_begin``,</span>
<span class="sd">        which hints the other graph may share the same memory pool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">pool</span><span class="p">()</span>


<div class="viewcode-block" id="graph"><a class="viewcode-back" href="../../../generated/torch.cuda.graph.html#torch.cuda.graph">[docs]</a><span class="k">class</span> <span class="nc">graph</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Context-manager that captures CUDA work into a :class:`torch.cuda.CUDAGraph`</span>
<span class="sd">    object for later replay.</span>

<span class="sd">    See :ref:`CUDA Graphs &lt;cuda-graph-semantics&gt;` for a general introduction,</span>
<span class="sd">    detailed use, and constraints.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        cuda_graph (torch.cuda.CUDAGraph): Graph object used for capture.</span>
<span class="sd">        pool (optional): Opaque token (returned by a call to :func:`~torch.cuda.graph_pool_handle()` or</span>
<span class="sd">            :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) hinting this graph&#39;s capture</span>
<span class="sd">            may share memory from the specified pool. See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">        stream (torch.cuda.Stream, optional): If supplied, will be set as the current stream in the context.</span>
<span class="sd">            If not supplied, ``graph`` sets its own internal side stream as the current stream in the context.</span>

<span class="sd">    .. note::</span>
<span class="sd">        For effective memory sharing, if you pass a ``pool`` used by a previous capture and the previous capture</span>
<span class="sd">        used an explicit ``stream`` argument, you should pass the same ``stream`` argument to this capture.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">default_capture_stream</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cuda_graph</span><span class="p">,</span>
                 <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Lazy-init of default_capture_stream helps avoid circular-import errors.</span>
        <span class="c1"># Not thread safe, but graphs already have the general (explicitly documented)</span>
        <span class="c1"># restriction that only one capture may be underway at a time in the process.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">pool</span><span class="p">,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="o">=</span> <span class="n">stream</span> <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span> <span class="o">=</span> <span class="n">cuda_graph</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Free as much memory as we can for the graph</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># Stackoverflow seems comfortable with this pattern</span>
        <span class="c1"># https://stackoverflow.com/questions/26635684/calling-enter-and-exit-manually#39172487</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">)</span>


    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">)</span></div>
        <span class="c1"># returning None should propagate exceptions from either capture_end or stream_ctx.__exit__()</span>


<div class="viewcode-block" id="make_graphed_callables"><a class="viewcode-back" href="../../../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables">[docs]</a><span class="k">def</span> <span class="nf">make_graphed_callables</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Accepts callables (functions or :class:`nn.Module&lt;torch.nn.Module&gt;`\ s)</span>
<span class="sd">    and returns graphed versions.</span>

<span class="sd">    Each graphed callable&#39;s forward pass runs its source callable&#39;s</span>
<span class="sd">    forward CUDA work as a CUDA graph inside a single autograd node.</span>

<span class="sd">    The graphed callable&#39;s forward pass also appends</span>
<span class="sd">    a backward node to the autograd graph. During backward, this node runs the</span>
<span class="sd">    callable&#39;s backward work as a CUDA graph.</span>

<span class="sd">    Therefore, each graphed callable should be a drop-in replacement for its source callable</span>
<span class="sd">    in an autograd-enabled training loop.</span>

<span class="sd">    See :ref:`Partial-network capture&lt;partial-network-capture&gt;` for detailed use and constraints.</span>

<span class="sd">    If you pass a tuple of several callables, their captures will use the same memory pool.</span>
<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when this is appropriate.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph.</span>
<span class="sd">            See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when passing a tuple of callables</span>
<span class="sd">            is appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order</span>
<span class="sd">            they&#39;ll run in the live workload.</span>
<span class="sd">        sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable.</span>
<span class="sd">            If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors.</span>
<span class="sd">            If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The ``requires_grad`` state of each Tensor in ``sample_args`` must match the state</span>
<span class="sd">        that&#39;s expected for the corresponding real input in the training loop.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        ``sample_args`` for each callable must be a tuple of Tensors. Other types and keyword args</span>
<span class="sd">        are not allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Returned callables do not support higher order differentiation (e.g., double backward).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In any :class:`~torch.nn.Module` passed to :func:`~make_graphed_callables`, only parameters</span>
<span class="sd">        may be trainable. Buffers must have ``requires_grad=False``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        After you pass a :class:`torch.nn.Module` through :func:`~make_graphed_callables`,</span>
<span class="sd">        you may not add or remove any of that Module&#39;s parameters or buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :class:`torch.nn.Module`\s passed to :func:`~torch.cuda.make_graphed_callables` must not have module hooks</span>
<span class="sd">        registered on them at the time they are passed. However, registering hooks on modules *after* passing them</span>
<span class="sd">        through :func:`~torch.cuda.make_graphed_callables` is allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When running a graphed callable, you must pass its arguments in the same order and format</span>
<span class="sd">        they appeared in that callable&#39;s ``sample_args``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        All Tensor outputs of graphed callables must require grad.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">callables</span> <span class="o">=</span> <span class="p">(</span><span class="n">callables</span><span class="p">,)</span>
        <span class="n">sample_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_args</span><span class="p">,)</span>

    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">args</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> \
                <span class="s2">&quot;Modules must not have hooks registered at the time they are passed. However, registering hooks &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;on modules after passing them through make_graphed_callables is allowed.&quot;</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">is</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">buffers</span><span class="p">()),</span> <span class="s2">&quot;In any :class:`~torch.nn.Module` passed to &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;:func:`~make_graphed_callables`, only parameters may be trainable. All buffers must have &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;``requires_grad=False``.&quot;</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">),</span> <span class="s2">&quot;In the beta API, sample_args &quot;</span> <span class="o">+</span> \
            <span class="s2">&quot;for each callable must be a tuple of Tensors. Other types and keyword args are not allowed.&quot;</span>


    <span class="c1"># If a callable is an nn.Module, its graph&#39;s full input surface is the args the user explicitly</span>
    <span class="c1"># passes to forward (ie, its sample_args) AND the module&#39;s parameter attributes.</span>
    <span class="n">per_callable_len_user_args</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">sample_args</span><span class="p">]</span>
    <span class="n">per_callable_module_params</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="p">()</span>
                                  <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callables</span><span class="p">]</span>
    <span class="n">per_callable_static_input_surfaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                                          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>

    <span class="n">fwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>
    <span class="n">bwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>

    <span class="n">mempool</span> <span class="o">=</span> <span class="n">graph_pool_handle</span><span class="p">()</span>

    <span class="c1"># Warmup</span>
    <span class="c1"># Hopefully prevents cudnn benchmarking and other lazy-initialization cuda work</span>
    <span class="c1"># from ending up in any captures.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">static_input_surface</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span>
                                                    <span class="n">sample_args</span><span class="p">,</span>
                                                    <span class="n">per_callable_static_input_surfaces</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">outputs</span>
                <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
                                                  <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                                  <span class="n">grad_outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">),</span>
                                                  <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                  <span class="n">allow_unused</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">grad_inputs</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># All captures here share a mempool. To avoid replays corrupting each other&#39;s memory,</span>
    <span class="c1"># the safest approach is to capture all passes in the same order they&#39;ll run:</span>
    <span class="c1"># fwd 1, fwd 2, ... fwd N, then bwd N, bwd N-1, ... bwd 1.</span>

    <span class="c1"># Capture forward graphs</span>
    <span class="n">per_callable_static_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_output_was_tensor</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">fwd_graph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span>
                                     <span class="n">sample_args</span><span class="p">,</span>
                                     <span class="n">fwd_graphs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">fwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="c1"># Assumes model output is a tensor or tuple of tensors</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">per_callable_output_was_tensor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">per_callable_output_was_tensor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">per_callable_static_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="c1"># Capture backward graphs in reverse order</span>
    <span class="n">per_callable_static_grad_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">static_input_surface</span><span class="p">,</span> <span class="n">static_outputs</span><span class="p">,</span> <span class="n">bwd_graph</span><span class="p">,</span> <span class="n">module_params</span> <span class="ow">in</span> \
            <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_input_surfaces</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_outputs</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">bwd_graphs</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_module_params</span><span class="p">)):</span>

        <span class="c1"># For now, assumes all static_outputs require grad</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">),</span> <span class="s2">&quot;Outputs of graphed callables must require grad.&quot;</span>
        <span class="n">static_grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">bwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
            <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">static_outputs</span><span class="p">,</span>
                                              <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                              <span class="n">grad_outputs</span><span class="o">=</span><span class="n">static_grad_outputs</span><span class="p">,</span>
                                              <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                              <span class="n">allow_unused</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Constructs a tuple suitable for returning from Graphed.backward:</span>
        <span class="c1"># Pads out the actually-needed grads with Nones in gradient slots for inputs that don&#39;t require grad.</span>
        <span class="c1"># I couldn&#39;t think of a slick one-liner for this pattern.</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">grad_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">static_input_surface</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">[</span><span class="n">grad_idx</span><span class="p">])</span>
                <span class="n">grad_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="n">per_callable_static_grad_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">)</span>
        <span class="n">per_callable_static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>

    <span class="c1"># Reverses the most recent two lists</span>
    <span class="n">per_callable_static_grad_outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_grad_outputs</span><span class="p">))</span>
    <span class="n">per_callable_static_grad_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_grad_inputs</span><span class="p">))</span>
    <span class="c1"># Now for every per_callable list, per_callable_*[i] holds the stuff for the ith callable.</span>

    <span class="k">def</span> <span class="nf">make_graphed_autograd_function</span><span class="p">(</span><span class="n">fwd_graph</span><span class="p">,</span>
                                       <span class="n">bwd_graph</span><span class="p">,</span>
                                       <span class="n">module_params</span><span class="p">,</span>
                                       <span class="n">len_user_args</span><span class="p">,</span>
                                       <span class="n">output_was_tensor</span><span class="p">,</span>
                                       <span class="n">static_input_surface</span><span class="p">,</span>
                                       <span class="n">static_outputs</span><span class="p">,</span>
                                       <span class="n">static_grad_outputs</span><span class="p">,</span>
                                       <span class="n">static_grad_inputs</span><span class="p">):</span>
        <span class="k">class</span> <span class="nc">Graphed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
            <span class="nd">@staticmethod</span>
            <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
                <span class="c1"># At this stage, only the user args may (potentially) be new tensors.</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_user_args</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                        <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">fwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">)</span>

            <span class="nd">@staticmethod</span>
            <span class="nd">@torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">once_differentiable</span>
            <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grads</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># don&#39;t copy if autograd gods have been kind and the</span>
                        <span class="c1"># incoming grad is already in the right place</span>
                        <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">grad</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                            <span class="n">g</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">bwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>

                <span class="c1"># Input args that didn&#39;t require grad expect a None gradient.</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">static_grad_inputs</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">functionalized</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
            <span class="c1"># Runs the autograd function with inputs == all inputs to the graph that might require grad</span>
            <span class="c1"># (explicit user args + module parameters)</span>
            <span class="c1"># Assumes module params didn&#39;t change since capture.</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">Graphed</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">user_args</span> <span class="o">+</span> <span class="n">module_params</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">output_was_tensor</span> <span class="k">else</span> <span class="n">out</span>

        <span class="k">return</span> <span class="n">functionalized</span>

    <span class="c1"># Put together the final graphed callables</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">callables</span><span class="p">):</span>
        <span class="n">graphed</span> <span class="o">=</span> <span class="n">make_graphed_autograd_function</span><span class="p">(</span><span class="n">fwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">bwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_len_user_args</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_output_was_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_input_surfaces</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_grad_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_grad_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph_training_state</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">orig_fwd</span><span class="p">):</span>
                <span class="k">def</span> <span class="nf">new_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
                    <span class="c1"># If the module&#39;s training-or-eval state matches what we graphed,</span>
                    <span class="c1"># run the graph, otherwise run the original forward method</span>
                    <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span> <span class="o">==</span> <span class="n">graph_training_state</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">graphed</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">orig_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">new_fwd</span>
            <span class="n">func</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graphed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">just_one_callable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
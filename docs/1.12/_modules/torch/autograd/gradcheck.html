


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.autograd.gradcheck &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/autograd/gradcheck.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../autograd.html">torch.autograd</a> &gt;</li>
        
      <li>torch.autograd.gradcheck</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.autograd.gradcheck</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">_TensorOrTensors</span>
<span class="kn">import</span> <span class="nn">torch.testing</span>
<span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">is_tensor_like</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">torch._vmap_internals</span> <span class="kn">import</span> <span class="n">vmap</span><span class="p">,</span> <span class="n">_vmap</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="c1"># Note: `get_*_jacobian` functions are added here even though we didn&#39;t intend to make them public</span>
<span class="c1"># since they have been exposed from before we added `__all__`  and we already maintain BC for them</span>
<span class="c1"># We should eventually deprecate them and remove them from `__all__`</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gradcheck&quot;</span><span class="p">,</span> <span class="s2">&quot;gradgradcheck&quot;</span><span class="p">,</span> <span class="s2">&quot;GradcheckError&quot;</span><span class="p">,</span> <span class="s2">&quot;get_numerical_jacobian&quot;</span><span class="p">,</span>
           <span class="s2">&quot;get_analytical_jacobian&quot;</span><span class="p">,</span> <span class="s2">&quot;get_numerical_jacobian_wrt_specific_input&quot;</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">GradcheckError</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Error raised by :func:`gradcheck` and :func:`gradgradcheck`&quot;&quot;&quot;</span>
    <span class="k">pass</span>


<span class="k">def</span> <span class="nf">_is_float_or_complex_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_complex</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_allocate_jacobians_with_inputs</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">numel_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Makes zero-filled tensors from inputs. If `numel_output` is not None, for</span>
    <span class="c1"># each tensor in `input_tensors`, returns a new zero-filled tensor with height</span>
    <span class="c1"># of `t.numel` and width of `numel_output`. Otherwise, for each tensor, returns</span>
    <span class="c1"># a 1-d tensor with size `(t.numel,)`. Each new tensor will be strided and have</span>
    <span class="c1"># the same dtype and device as those of the corresponding input.</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_is_float_or_complex_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">numel_output</span><span class="p">),</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">numel_input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Makes zero-filled tensors from outputs. If `dim` is not None, for each tensor</span>
    <span class="c1"># in `output_tensors`, returns a new zero-filled tensor with height of `dim` and</span>
    <span class="c1"># width of `t.numel`. Otherwise, for each tensor, returns a 1-d tensor with size</span>
    <span class="c1"># (t.numel,).</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;layout&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_is_float_or_complex_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="n">numel_input</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">()),</span> <span class="o">**</span><span class="n">options</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_iter_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
                  <span class="n">only_requiring_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># mypy doesn&#39;t narrow type of `x` to torch.Tensor</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">only_requiring_grad</span><span class="p">:</span>  <span class="c1"># type: ignore[union-attr]</span>
            <span class="k">yield</span> <span class="n">x</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">_iter_tensors</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="n">only_requiring_grad</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_iter_tensor</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">):</span>
    <span class="c1"># (Only used for slow gradcheck) Returns a generator that yields the following</span>
    <span class="c1"># elements at each iteration:</span>
    <span class="c1">#  1) a tensor: the same tensor is returned across all iterations. The tensor</span>
    <span class="c1">#     is not the same as the original x_tensor as given as input - it is</span>
    <span class="c1">#     prepared so that it can be modified in-place. Depending on whether the</span>
    <span class="c1">#     input tensor is strided, sparse, or dense, the returned tensor may or may</span>
    <span class="c1">#     not share storage with x_tensor.</span>
    <span class="c1">#  2) a tuple of indices that can be used with advanced indexing (yielded in</span>
    <span class="c1">#     dictionary order)</span>
    <span class="c1">#  3) flattened index that will be used to index into the Jacobian tensor</span>
    <span class="c1">#</span>
    <span class="c1"># For a tensor t with size (2, 2), _iter_tensor yields:</span>
    <span class="c1">#     `x, (0, 0), 0`, `x, (0, 1), 1`, `x, (1, 0), 2`, `x, (1, 1), 3`</span>
    <span class="c1">#</span>
    <span class="c1"># where x is the t.data of the original tensor. Perturbing the entry of x</span>
    <span class="c1"># at index (1, 1) yields the 3rd column of the overall Jacobian matrix.</span>
    <span class="k">if</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">get_stride</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)):</span>
                <span class="n">stride</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>
                <span class="n">tmp</span> <span class="o">*=</span> <span class="n">size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">stride</span>
        <span class="n">x_nnz</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
        <span class="n">x_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">x_indices</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        <span class="n">x_stride</span> <span class="o">=</span> <span class="n">get_stride</span><span class="p">(</span><span class="n">x_size</span><span class="p">)</span>
        <span class="c1"># Use .data here to get around the version check</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x_values</span><span class="o">.</span><span class="n">data</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_nnz</span><span class="p">):</span>
            <span class="n">x_value</span> <span class="o">=</span> <span class="n">x_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_values</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]]):</span>
                <span class="n">indices</span> <span class="o">=</span> <span class="n">x_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_idx</span><span class="p">)</span>
                <span class="n">d_idx</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_stride</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_size</span><span class="p">)))</span>
                <span class="k">yield</span> <span class="n">x_value</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>
    <span class="k">elif</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">for</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()])):</span>
            <span class="c1"># this is really inefficient, but without indexing implemented, there&#39;s</span>
            <span class="c1"># not really a better way than converting back and forth</span>
            <span class="n">x_tensor_dense</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">x_tensor_dense</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use .data here to get around the version check</span>
        <span class="n">x_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">data</span>
        <span class="k">for</span> <span class="n">d_idx</span><span class="p">,</span> <span class="n">x_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">x_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()])):</span>
            <span class="k">yield</span> <span class="n">x_tensor</span><span class="p">,</span> <span class="n">x_idx</span><span class="p">,</span> <span class="n">d_idx</span>


<span class="k">def</span> <span class="nf">_get_numerical_jacobian</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                            <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Computes the numerical Jacobian of `fn(inputs)` with respect to `target`. If</span>
<span class="sd">    not specified, targets are the input. Returns M * N Jacobians where N is the</span>
<span class="sd">    number of tensors in target that require grad and M is the number of non-integral</span>
<span class="sd">    outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the function to compute the jacobian for</span>
<span class="sd">        inputs: inputs to `fn`</span>
<span class="sd">        outputs: provide precomputed outputs to avoid one extra invocation of fn</span>
<span class="sd">        target: the Tensors wrt whom Jacobians are calculated (default=`inputs`)</span>
<span class="sd">        eps: the magnitude of the perturbation during finite differencing</span>
<span class="sd">             (default=`1e-3`)</span>
<span class="sd">        is_forward_ad: if this numerical jacobian is computed to be checked wrt</span>
<span class="sd">                       forward AD gradients (this is used for error checking only)</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of M N-tuples of tensors</span>

<span class="sd">    Note that `target` may not even be part of `input` to `fn`, so please be</span>
<span class="sd">    **very careful** in this to not clone `target`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">jacobians</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_forward_ad</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected output to be non-complex. get_numerical_jacobian no &quot;</span>
                         <span class="s2">&quot;longer supports functions that return complex outputs.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">inp_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">inp_indices</span><span class="p">)):</span>
        <span class="n">jacobians</span> <span class="o">+=</span> <span class="p">[</span><span class="n">get_numerical_jacobian_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                                                <span class="nb">input</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="n">is_forward_ad</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">jacobians</span>


<span class="k">def</span> <span class="nf">get_numerical_jacobian</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">grad_out</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated API to compute the numerical Jacobian for a given fn and its inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the function to compute the Jacobian for (must take inputs as a tuple)</span>
<span class="sd">        input: input to `fn`</span>
<span class="sd">        target: the Tensors wrt whom Jacobians are calculated (default=`input`)</span>
<span class="sd">        eps: the magnitude of the perturbation during finite differencing</span>
<span class="sd">             (default=`1e-3`)</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of Jacobians of `fn` (restricted to its first output) with respect to</span>
<span class="sd">        each input or target, if provided.</span>

<span class="sd">    Note that `target` may not even be part of `input` to `fn`, so please be</span>
<span class="sd">    **very careful** in this to not clone `target`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;get_numerical_jacobian was part of PyTorch&#39;s private API and not &quot;</span>
                  <span class="s2">&quot;meant to be exposed. We are deprecating it and it will be removed &quot;</span>
                  <span class="s2">&quot;in a future version of PyTorch. If you have a specific use for &quot;</span>
                  <span class="s2">&quot;this or feature request for this to be a stable API, please file &quot;</span>
                  <span class="s2">&quot;us an issue at https://github.com/pytorch/pytorch/issues/new&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad_out</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># grad_out param is only kept for backward compatibility reasons</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected grad_out to be 1.0. get_numerical_jacobian no longer &quot;</span>
                         <span class="s2">&quot;supports values of grad_out != 1.0.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fn_pack_inps</span><span class="p">(</span><span class="o">*</span><span class="n">inps</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">inps</span><span class="p">)</span>
    <span class="n">jacobians</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">fn_pack_inps</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">jacobian_for_each_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">jacobian_for_each_output</span> <span class="ow">in</span> <span class="n">jacobians</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_compute_numerical_gradient</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">entry</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">norm_v</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">):</span>
    <span class="c1"># Performs finite differencing by perturbing `entry` in-place by `v` and</span>
    <span class="c1"># returns the gradient of each of the outputs wrt to x at idx.</span>
    <span class="n">orig</span> <span class="o">=</span> <span class="n">entry</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">outa</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">outb</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
    <span class="n">entry</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="n">nbhd_checks_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">norm_v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">compute</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outa</span><span class="p">,</span> <span class="n">outb</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_compute_numerical_jvps_wrt_specific_input</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">input_is_complex</span><span class="p">,</span>
                                               <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="c1"># Computing the jacobian only works for real delta</span>
    <span class="c1"># For details on the algorithm used here, refer:</span>
    <span class="c1"># Section 3.5.3 https://arxiv.org/pdf/1701.00392.pdf</span>
    <span class="c1"># s = fn(z) where z = x for real valued input</span>
    <span class="c1"># and z = x + yj for complex valued input</span>
    <span class="n">jvps</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ds_dx_tup</span> <span class="o">=</span> <span class="n">jvp_fn</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="n">delta</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_is_complex</span><span class="p">:</span>  <span class="c1"># C -&gt; R</span>
        <span class="n">ds_dy_tup</span> <span class="o">=</span> <span class="n">jvp_fn</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="n">jvp_fn</span><span class="p">(</span><span class="n">delta</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ds_dx</span><span class="p">,</span> <span class="n">ds_dy</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ds_dx_tup</span><span class="p">,</span> <span class="n">ds_dy_tup</span><span class="p">):</span>
            <span class="k">assert</span><span class="p">(</span><span class="ow">not</span> <span class="n">ds_dx</span><span class="o">.</span><span class="n">is_complex</span><span class="p">())</span>
            <span class="c1"># conjugate wirtinger derivative</span>
            <span class="n">conj_w_d</span> <span class="o">=</span> <span class="n">ds_dx</span> <span class="o">+</span> <span class="n">ds_dy</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>
            <span class="n">jvps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conj_w_d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">ds_dx</span> <span class="ow">in</span> <span class="n">ds_dx_tup</span><span class="p">:</span>  <span class="c1"># R -&gt; R or (R -&gt; C for the forward AD case)</span>
            <span class="k">assert</span><span class="p">(</span><span class="n">is_forward_ad</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">ds_dx</span><span class="o">.</span><span class="n">is_complex</span><span class="p">())</span>
            <span class="n">jvps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ds_dx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jvps</span>


<span class="k">def</span> <span class="nf">_combine_jacobian_cols</span><span class="p">(</span><span class="n">jacobians_cols</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">outputs</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span>
                           <span class="n">numel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># jacobian_cols maps column_idx -&gt; output_idx -&gt; single column of jacobian Tensor</span>
    <span class="c1"># we return a list that maps output_idx -&gt; full jacobian Tensor</span>
    <span class="n">jacobians</span> <span class="o">=</span> <span class="n">_allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">jacobian</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jacobians</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">jacobians_cols</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">jacobian</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jacobians</span>


<span class="k">def</span> <span class="nf">_prepare_input</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">maybe_perturbed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                   <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Prepares the inputs to be passed into the function while including the new</span>
    <span class="c1"># modified input.</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined] # no attr _mkldnn</span>
        <span class="c1"># Convert back to mkldnn</span>
        <span class="k">if</span> <span class="n">maybe_perturbed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">maybe_perturbed_input</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">input</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fast_mode</span> <span class="ow">and</span> <span class="n">maybe_perturbed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># entry is already a &quot;cloned&quot; version of the original tensor</span>
            <span class="c1"># thus changes to entry are not reflected in the input</span>
            <span class="k">return</span> <span class="n">maybe_perturbed_input</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We cannot use entry (input.data) if we want gradgrad to work because</span>
        <span class="c1"># fn (in the gradgrad case) needs to compute grad wrt input</span>
        <span class="k">return</span> <span class="nb">input</span>


<span class="k">def</span> <span class="nf">_check_outputs_same_dtype_and_shape</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Check that the returned outputs don&#39;t have different dtype or shape when you</span>
    <span class="c1"># perturb the input</span>
    <span class="n">on_index</span> <span class="o">=</span> <span class="s2">&quot;on index </span><span class="si">{idx}</span><span class="s2"> &quot;</span> <span class="k">if</span> <span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">output1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
        <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected `func` to return outputs with the same shape&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; when inputs are perturbed </span><span class="si">{</span><span class="n">on_index</span><span class="si">}</span><span class="s2">by </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">, but got:&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; shapes </span><span class="si">{</span><span class="n">output1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">output1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">output2</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> \
        <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected `func` to return outputs with the same dtype&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; when inputs are perturbed </span><span class="si">{</span><span class="n">on_index</span><span class="si">}</span><span class="s2">by </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">, but got:&quot;</span>
         <span class="sa">f</span><span class="s2">&quot; dtypes </span><span class="si">{</span><span class="n">output1</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">output2</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_numerical_jacobian_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                              <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># Computes the numerical jacobians wrt to a single input. Returns N jacobian</span>
    <span class="c1"># tensors, where N is the number of outputs. We use a dictionary for</span>
    <span class="c1"># jacobian_cols because indices aren&#39;t necessarily consecutive for sparse inputs</span>
    <span class="c1"># When we perturb only a single element of the input tensor at a time, the jvp</span>
    <span class="c1"># is equivalent to a single col of the Jacobian matrix of fn.</span>
    <span class="n">jacobian_cols</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">input</span>
    <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">requires_grad</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">d_idx</span> <span class="ow">in</span> <span class="n">_iter_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">wrapped_fn</span> <span class="o">=</span> <span class="n">_with_prepare_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">nbhd_checks_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_outputs_same_dtype_and_shape</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">_get_numerical_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
        <span class="n">jacobian_cols</span><span class="p">[</span><span class="n">d_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">_compute_numerical_jvps_wrt_specific_input</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(),</span> <span class="n">is_forward_ad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_combine_jacobian_cols</span><span class="p">(</span><span class="n">jacobian_cols</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_get_analytical_jacobian_forward_ad</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                        <span class="n">all_u</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Computes the analytical Jacobian using forward mode AD of `fn(inputs)` using forward mode AD with respect</span>
<span class="sd">    to `target`. Returns N * M Jacobians where N is the number of tensors in target that require grad and</span>
<span class="sd">    M is the number of non-integral outputs.</span>
<span class="sd">    Contrary to other functions here, this function requires &quot;inputs&quot; to actually be used by the function.</span>
<span class="sd">    The computed value is expected to be wrong if the function captures the inputs by side effect instead of</span>
<span class="sd">    using the passed ones (many torch.nn tests do this).</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the function to compute the jacobian for</span>
<span class="sd">        inputs: inputs to `fn`</span>
<span class="sd">        outputs: provide precomputed outputs to avoid one extra invocation of fn</span>
<span class="sd">        check_grad_dtypes: if True, will check that the gradient dtype are valid</span>
<span class="sd">        all_u (optional): if provided, the Jacobian will be right multiplied with this vector</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of M N-tuples of tensors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># To avoid early import issues</span>
    <span class="n">fwAD</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">forward_ad</span>

    <span class="n">tensor_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">and</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tensor_inputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected inputs to be non-complex for _get_analytical_jacobian_forward_ad.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">all_u</span><span class="p">:</span>
        <span class="n">jacobians</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tensor_inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">jacobians</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_allocate_jacobians_with_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tensor_inputs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
        <span class="n">fw_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dual_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;MKLDNN inputs are not support for forward AD gradcheck.&quot;</span><span class="p">)</span>

                <span class="n">inp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inp</span><span class="p">))</span>
                <span class="c1"># If inp is a differentiable view, the dual might not be the tangent given to</span>
                <span class="c1"># make_dual, so read it explicitly from the dual tensor</span>
                <span class="n">fw_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">inp</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">dual_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">all_u</span><span class="p">:</span>
            <span class="c1"># Do the full reduction in one pass</span>
            <span class="c1"># To be consistent with numerical evaluation, we actually compute one reduction per input</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">fw_grad</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">fw_grads</span><span class="p">,</span> <span class="n">all_u</span><span class="p">)):</span>
                <span class="n">fw_grad</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">fw_grad</span><span class="p">))</span>
                <span class="n">raw_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">dual_inputs</span><span class="p">))</span>
                <span class="n">dual_outputs</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">_is_float_or_complex_tensor</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">index_o</span><span class="p">,</span> <span class="n">d_o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dual_outputs</span><span class="p">):</span>
                    <span class="n">val</span><span class="p">,</span> <span class="n">res</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">d_o</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">check_grad_dtypes</span> <span class="ow">and</span> <span class="n">res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">val</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="o">!=</span> <span class="n">res</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
                        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Forward AD gradient has dtype mismatch.&#39;</span><span class="p">)</span>

                    <span class="c1"># Remove extra dimension of size 1 corresponding to the reduced input</span>
                    <span class="n">jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index_o</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index_o</span><span class="p">]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index_o</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">fw_grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Reconstruct the full Jacobian column by column</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">fw_grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fw_grads</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">lin_idx</span><span class="p">,</span> <span class="n">grad_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">fw_grad</span><span class="o">.</span><span class="n">size</span><span class="p">()])):</span>
                    <span class="n">fw_grad</span><span class="p">[</span><span class="n">grad_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
                    <span class="n">raw_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">dual_inputs</span><span class="p">))</span>
                    <span class="n">dual_outputs</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">_is_float_or_complex_tensor</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">index_o</span><span class="p">,</span> <span class="n">d_o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dual_outputs</span><span class="p">):</span>
                        <span class="n">val</span><span class="p">,</span> <span class="n">res</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">d_o</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">check_grad_dtypes</span> <span class="ow">and</span> <span class="n">val</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="o">!=</span> <span class="n">res</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
                            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Forward AD gradient has dtype mismatch.&#39;</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index_o</span><span class="p">][</span><span class="n">lin_idx</span><span class="p">]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index_o</span><span class="p">][</span><span class="n">lin_idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                    <span class="n">fw_grad</span><span class="p">[</span><span class="n">grad_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="k">return</span> <span class="n">jacobians</span>

<span class="k">def</span> <span class="nf">_get_input_to_perturb</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="c1"># Prepare the input so that it can be modified in-place and do certain</span>
    <span class="c1"># operations that require the tensor to have strides. If fast_mode=False,</span>
    <span class="c1"># _iter_tensor would handle the below cases:</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined] # no attr _mkldnn</span>
        <span class="c1"># Convert to dense so we can perform operations that require strided tensors</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="c1"># Clone because input may require grad, and copy_ calls resize_,</span>
        <span class="c1"># which is not allowed for .data</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
    <span class="k">return</span> <span class="n">input_to_perturb</span>


<span class="k">def</span> <span class="nf">_with_prepare_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Wraps `fn` so that its inputs are already supplied</span>
    <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">():</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_prepare_input</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">input_to_perturb</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">input_idx</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">fast_mode</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">else</span> <span class="n">a</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inp</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">wrapped_fn</span>


<span class="k">def</span> <span class="nf">_get_numerical_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">):</span>
    <span class="c1"># Wraps jvp_fn so that certain arguments are already supplied</span>
    <span class="k">def</span> <span class="nf">jvp_fn</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_compute_numerical_gradient</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jvp_fn</span>


<span class="k">def</span> <span class="nf">_reshape_tensor_or_tuple</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="c1"># We don&#39;t need to reshape when input corresponding to u is sparse</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">u</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">u</span>


<span class="k">def</span> <span class="nf">_mul_tensor_or_tuple</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">k</span> <span class="o">*</span> <span class="n">u</span>


<span class="k">def</span> <span class="nf">_get_numerical_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span>
    <span class="n">input_to_perturb</span> <span class="o">=</span> <span class="n">_get_input_to_perturb</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">wrapped_fn</span> <span class="o">=</span> <span class="n">_with_prepare_inputs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">nbhd_checks_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_outputs_same_dtype_and_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">jvp_fn</span> <span class="o">=</span> <span class="n">_get_numerical_jvp_fn</span><span class="p">(</span><span class="n">wrapped_fn</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nbhd_checks_fn</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">_reshape_tensor_or_tuple</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">input_to_perturb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">_mul_tensor_or_tuple</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_compute_numerical_jvps_wrt_specific_input</span><span class="p">(</span><span class="n">jvp_fn</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(),</span> <span class="n">is_forward_ad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_numerical_vJu</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inp_indices</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="p">):</span>
    <span class="c1"># Note that all_v can also be None, in that case, this function only computes Ju.</span>
    <span class="n">reduced_jacobians</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inp_idx</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inp_indices</span><span class="p">,</span> <span class="n">all_u</span><span class="p">)):</span>
        <span class="n">all_Ju</span> <span class="o">=</span> <span class="n">_get_numerical_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="p">)</span>
        <span class="c1"># Filter out the Ju for non floating point outputs</span>
        <span class="n">filtered_Ju</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">func_out</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_Ju</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">func_out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">Ju</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_Ju</span><span class="p">,</span> <span class="n">func_out</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_is_float_or_complex_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
                <span class="n">filtered_Ju</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Ju</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># TODO: handle the other Ju</span>
                <span class="k">pass</span>
        <span class="k">if</span> <span class="n">all_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">jacobian_scalars</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">Ju</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_v</span><span class="p">,</span> <span class="n">filtered_Ju</span><span class="p">):</span>
                <span class="n">jacobian_scalars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_dot_with_type_promotion</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">Ju</span><span class="p">))</span>
            <span class="n">reduced_jacobians</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_scalars</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reduced_jacobians</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered_Ju</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduced_jacobians</span>


<span class="k">def</span> <span class="nf">_check_jacobians_equal</span><span class="p">(</span><span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
    <span class="c1"># Check whether the max difference between two Jacobian tensors are within some</span>
    <span class="c1"># tolerance `atol`.</span>
    <span class="k">for</span> <span class="n">j1_x</span><span class="p">,</span> <span class="n">j2_x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">j1</span><span class="p">,</span> <span class="n">j2</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">j1_x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="p">(</span><span class="n">j1_x</span> <span class="o">-</span> <span class="n">j2_x</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">atol</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_stack_and_check_tensors</span><span class="p">(</span><span class="n">list_of_list_of_tensors</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
                             <span class="n">numel_outputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="c1"># For the ith tensor in the inner list checks whether it has the same size and</span>
    <span class="c1"># dtype as the ith differentiable input.</span>
    <span class="n">out_jacobians</span> <span class="o">=</span> <span class="n">_allocate_jacobians_with_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">numel_outputs</span><span class="p">)</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">correct_grad_types</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor_list</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">list_of_list_of_tensors</span><span class="p">):</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="n">diff_input_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">out_jacobian</span> <span class="o">=</span> <span class="n">out_jacobians</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inp</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
                <span class="n">correct_grad_sizes</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">inp</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="n">correct_grad_types</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dense</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span> <span class="k">else</span> <span class="n">tensor</span>
                <span class="k">assert</span> <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">dense</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="n">out_jacobian</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_jacobians</span><span class="p">,</span> <span class="n">correct_grad_sizes</span><span class="p">,</span> <span class="n">correct_grad_types</span>


<span class="n">FAILED_NONDET_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\n</span><span class="s2"></span>
<span class="s2">NOTE: If your op relies on non-deterministic operations i.e., it is listed here:</span>
<span class="s2">https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html</span>
<span class="s2">this failure might be expected.</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.</span>
<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `nondet_tol=&lt;tol&gt;` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `gradcheck_nondet_tol=&lt;tol&gt;`.</span>
<span class="s2">- is a Module test (e.g., in common_nn.py), then modify the corresponding</span>
<span class="s2">  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">_check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span>
                                          <span class="n">fast_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># This is used by both fast and slow mode:</span>
    <span class="c1">#  - For slow mode, vjps[i][j] is the jth row the Jacobian wrt the ith</span>
    <span class="c1">#    input.</span>
    <span class="c1">#  - For fast mode, vjps[i][0] is a linear combination of the rows</span>
    <span class="c1">#    of the Jacobian wrt the ith input</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">vjp_fn</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span>
                                   <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Compute everything twice to check for nondeterminism (which we call reentrancy)</span>
    <span class="k">if</span> <span class="n">fast_mode</span><span class="p">:</span>
        <span class="n">vjps1</span> <span class="o">=</span> <span class="n">_get_analytical_vjps_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">vjps2</span> <span class="o">=</span> <span class="n">_get_analytical_vjps_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vjps1</span> <span class="o">=</span> <span class="n">_compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="n">vjps2</span> <span class="o">=</span> <span class="n">_compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>

    <span class="n">output_numel</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">fast_mode</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">jacobians1</span><span class="p">,</span> <span class="n">types_ok</span><span class="p">,</span> <span class="n">sizes_ok</span> <span class="o">=</span> <span class="n">_stack_and_check_tensors</span><span class="p">(</span><span class="n">vjps1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">jacobians2</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_stack_and_check_tensors</span><span class="p">(</span><span class="n">vjps2</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">reentrant</span> <span class="o">=</span> <span class="n">_check_jacobians_equal</span><span class="p">(</span><span class="n">jacobians1</span><span class="p">,</span> <span class="n">jacobians2</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">types_ok</span> <span class="ow">and</span> <span class="n">check_grad_dtypes</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Gradient has dtype mismatch&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes_ok</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Analytical gradient has incorrect size&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">reentrant</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Backward is not reentrant, i.e., running backward with &#39;</span>
                             <span class="s1">&#39;same input and grad_output multiple times gives different values, &#39;</span>
                             <span class="s1">&#39;although analytical gradient matches numerical gradient.&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;The tolerance for nondeterminism was </span><span class="si">{</span><span class="n">nondet_tol</span><span class="si">}</span><span class="s1">.&#39;</span> <span class="o">+</span>
                             <span class="n">FAILED_NONDET_MSG</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacobians1</span>


<span class="k">def</span> <span class="nf">_get_analytical_vJu_backward_mode</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">):</span>
    <span class="n">reduced_jacobians</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">output</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">all_v</span><span class="p">):</span>
        <span class="n">all_vJ</span> <span class="o">=</span> <span class="n">_check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span>
                                                       <span class="n">fast_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="n">jacobian_scalars</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">vJ</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_vJ</span><span class="p">,</span> <span class="n">all_u</span><span class="p">):</span>
            <span class="c1"># Why do we need squeeze here? vJ is a 2-d tensor so that we can reuse</span>
            <span class="c1"># the error checking logic from slow mode</span>
            <span class="n">vJ</span> <span class="o">=</span> <span class="n">vJ</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">vJ</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>  <span class="c1"># C -&gt; R</span>
                <span class="n">tv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">vJ</span><span class="o">.</span><span class="n">resolve_conj</span><span class="p">())</span>
                <span class="n">tr</span> <span class="o">=</span> <span class="n">tv</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">ti</span> <span class="o">=</span> <span class="n">tv</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">jacobian_scalars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span> <span class="o">*</span> <span class="n">ti</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># R -&gt; R</span>
                <span class="n">jacobian_scalars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vJ</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
        <span class="n">reduced_jacobians</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_scalars</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduced_jacobians</span>

<span class="k">def</span> <span class="nf">get_analytical_jacobian</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">grad_out</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Replicates the behavior of the old get_analytical_jacobian before the refactor</span>
    <span class="c1"># This shares much of its code with _check_analytical_jacobian_attributes</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;get_analytical_jacobian was part of PyTorch&#39;s private API and not &quot;</span>
                  <span class="s2">&quot;meant to be exposed. We are deprecating it and it will be removed &quot;</span>
                  <span class="s2">&quot;in a future version of PyTorch. If you have a specific use for &quot;</span>
                  <span class="s2">&quot;this or feature request for this to be a stable API, please file &quot;</span>
                  <span class="s2">&quot;us an issue at https://github.com/pytorch/pytorch/issues/new&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad_out</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># grad_out param is only kept for backward compatibility reasons</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected grad_out to be 1.0. get_analytical_jacobian no longer &quot;</span>
                         <span class="s2">&quot;supports values of grad_out != 1.0.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected output to be non-complex. get_analytical_jacobian no &quot;</span>
                         <span class="s2">&quot;longer supports functions that return complex outputs.&quot;</span><span class="p">)</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">vjp_fn</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span>
                                   <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Compute everything twice to check for nondeterminism (which we call reentrancy)</span>
    <span class="n">vjps1</span> <span class="o">=</span> <span class="n">_compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
    <span class="n">vjps2</span> <span class="o">=</span> <span class="n">_compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>

    <span class="n">output_numel</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="n">jacobians1</span><span class="p">,</span> <span class="n">types_ok</span><span class="p">,</span> <span class="n">sizes_ok</span> <span class="o">=</span> <span class="n">_stack_and_check_tensors</span><span class="p">(</span><span class="n">vjps1</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">jacobians2</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_stack_and_check_tensors</span><span class="p">(</span><span class="n">vjps2</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_numel</span><span class="p">)</span>
    <span class="n">reentrant</span> <span class="o">=</span> <span class="n">_check_jacobians_equal</span><span class="p">(</span><span class="n">jacobians1</span><span class="p">,</span> <span class="n">jacobians2</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jacobians1</span><span class="p">,</span> <span class="n">reentrant</span><span class="p">,</span> <span class="n">sizes_ok</span><span class="p">,</span> <span class="n">types_ok</span>


<span class="k">def</span> <span class="nf">_get_analytical_jacobian</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">):</span>
    <span class="c1"># Computes the analytical Jacobian in slow mode for a single input-output pair.</span>
    <span class="c1"># Forgoes performing checks on dtype, shape, and reentrancy.</span>
    <span class="n">jacobians</span> <span class="o">=</span> <span class="n">_check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="n">output_idx</span><span class="p">],</span>
                                                      <span class="n">nondet_tol</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacobians</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_compute_analytical_jacobian_rows</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">sample_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
    <span class="c1"># Computes Jacobian row-by-row using backward function `vjp_fn` = v^T J</span>
    <span class="c1"># NB: this function does not assume vjp_fn(v) to return tensors with the same</span>
    <span class="c1"># number of elements for different v. This is checked when we later combine the</span>
    <span class="c1"># rows into a single tensor.</span>
    <span class="n">grad_out_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sample_output</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span>
    <span class="n">flat_grad_out</span> <span class="o">=</span> <span class="n">grad_out_base</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># jacobians_rows[i][j] represents the jth row of the ith input</span>
    <span class="n">jacobians_rows</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flat_grad_out</span><span class="o">.</span><span class="n">numel</span><span class="p">()):</span>
        <span class="n">flat_grad_out</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">flat_grad_out</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">grad_out_base</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">jacobians_rows</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
            <span class="n">jacobians_rows</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="n">d_x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jacobians_rows</span>


<span class="k">def</span> <span class="nf">_get_analytical_vjps_wrt_specific_output</span><span class="p">(</span><span class="n">vjp_fn</span><span class="p">,</span> <span class="n">sample_output</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
    <span class="n">vjps</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_output</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">vjp</span> <span class="ow">in</span> <span class="n">grad_inputs</span><span class="p">:</span>
        <span class="n">vjps</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">vjp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vjp</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">vjps</span>


<span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_sparse_nnz</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_sparse_csr</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tupled_inputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;gradcheck expects all tensor inputs are dense when check_sparse_nnz is set to False.&#39;</span><span class="p">)</span>
    <span class="c1"># Make sure that gradients are saved for at least one input</span>
    <span class="n">any_input_requiring_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="ow">or</span> <span class="n">inp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Input #</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1"> requires gradient and &#39;</span>
                    <span class="s1">&#39;is not a double precision floating point or complex. &#39;</span>
                    <span class="s1">&#39;This check will likely fail if all the inputs are &#39;</span>
                    <span class="s1">&#39;not of double precision floating point or complex. &#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_sparse_csr</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">inp</span>
            <span class="c1"># TODO: To cover more problematic cases, replace stride = 0 check with</span>
            <span class="c1"># &quot;any overlap in memory&quot; once we have a proper function to check it.</span>
            <span class="k">if</span> <span class="n">content</span><span class="o">.</span><span class="n">layout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">st</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">sz</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">st</span><span class="p">,</span> <span class="n">sz</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">content</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">content</span><span class="o">.</span><span class="n">size</span><span class="p">())):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;The </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">th input has a dimension with stride 0. gradcheck only &#39;</span>
                        <span class="s1">&#39;supports inputs that are non-overlapping to be able to &#39;</span>
                        <span class="s1">&#39;compute the numerical gradients correctly. You should call &#39;</span>
                        <span class="s1">&#39;.contiguous on the input before passing it to gradcheck.&#39;</span><span class="p">)</span>
            <span class="n">any_input_requiring_grad</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">inp</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">any_input_requiring_grad</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;gradcheck expects at least one input tensor to require gradient, &#39;</span>
            <span class="s1">&#39;but none of the them have requires_grad=True.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_check_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>
        <span class="c1"># it is easier to call to_dense() on the sparse output than</span>
        <span class="c1"># to modify analytical jacobian</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Sparse output is not supported at gradcheck yet. &#39;</span>
                         <span class="s1">&#39;Please call to_dense() on the output of fn for gradcheck.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)):</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;MKLDNN output is not supported at gradcheck yet. &#39;</span>
                         <span class="s1">&#39;Please call to_dense() on the output of fn for gradcheck.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_no_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># When there are no differentiable outputs, numerical gradient for a function is</span>
    <span class="c1"># expected to be zero.</span>
    <span class="n">jacobians_all_inputs_outputs</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">jacobians_all_outputs_and_fixed_input</span> <span class="ow">in</span> <span class="n">jacobians_all_inputs_outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">jacobian</span> <span class="ow">in</span> <span class="n">jacobians_all_outputs_and_fixed_input</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">jacobian</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Numerical gradient for function expected to be zero&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_check_no_differentiable_outputs_fast</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">all_inputs</span><span class="p">,</span> <span class="n">inputs_indices</span><span class="p">,</span>
                                          <span class="n">all_u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs_indices</span><span class="p">,</span> <span class="n">all_u</span><span class="p">):</span>
        <span class="n">jvps</span> <span class="o">=</span> <span class="n">_get_numerical_jvp_wrt_specific_input</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inp_idx</span><span class="p">,</span> <span class="n">all_inputs</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">jvp</span> <span class="ow">in</span> <span class="n">jvps</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">jvp</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">jvp</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">jvp</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">nondet_tol</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;Numerical gradient for function expected to be zero&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="n">FAILED_BATCHED_GRAD_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">gradcheck or gradgradcheck failed while testing batched gradient computation.</span>
<span class="s2">This could have been invoked in a number of ways (via a test that calls</span>
<span class="s2">gradcheck/gradgradcheck directly or via an autogenerated test).</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.</span>
<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `check_batched_grad=False` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.</span>

<span class="s2">If you&#39;re modifying an existing operator that supports batched grad computation,</span>
<span class="s2">or wish to make a new operator work with batched grad computation, please read</span>
<span class="s2">the following.</span>

<span class="s2">To compute batched grads (e.g., jacobians, hessians), we vmap over the backward</span>
<span class="s2">computation. The most common failure case is if there is a &#39;vmap-incompatible</span>
<span class="s2">operation&#39; in the backward pass. Please see</span>
<span class="s2">NOTE: [How to write vmap-compatible backward formulas]</span>
<span class="s2">in the codebase for an explanation of how to fix this.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="n">FAILED_BATCHED_GRAD_MSG_FWD_AD</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">gradcheck failed while testing batched gradient computation with forward-mode AD.</span>
<span class="s2">This test is enabled automatically when both `check_batched_grad=True`</span>
<span class="s2">and `check_forward_ad=True`, but can be disabled in the following ways</span>
<span class="s2">dependong on how the test was invoked (via a test that calls gradcheck</span>
<span class="s2">directly or via an autogenerated test).</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.</span>
<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `check_batched_forward_grad=False` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `check_batched_forward_grad=False`</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">_get_failed_batched_grad_test_msg</span><span class="p">(</span><span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">For output </span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2"> and input </span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">:</span>

<span class="si">{</span><span class="n">FAILED_BATCHED_GRAD_MSG_FWD_AD</span> <span class="k">if</span> <span class="n">is_forward_ad</span> <span class="k">else</span> <span class="n">FAILED_BATCHED_GRAD_MSG</span><span class="si">}</span><span class="s2"></span>

<span class="s2">Got:</span>
<span class="si">{</span><span class="n">res</span><span class="si">}</span><span class="s2"></span>

<span class="s2">Expected:</span>
<span class="si">{</span><span class="n">exp</span><span class="si">}</span><span class="s2"></span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_test_batched_grad_forward_ad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">fwAD</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">forward_ad</span>   <span class="c1"># To avoid early import issues (do we need this?)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">current_input</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">is_tensor_like</span><span class="p">(</span><span class="n">current_input</span><span class="p">)</span> <span class="ow">and</span> <span class="n">current_input</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">tangent</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
                <span class="n">dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">current_input</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">tangent</span><span class="p">)</span>
                <span class="n">inputs_with_dual</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">dual</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">input_idx</span> <span class="k">else</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="k">else</span> <span class="n">inp</span><span class="p">)</span>
                                         <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
                <span class="n">dual_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs_with_dual</span><span class="p">))</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">dual_output</span> <span class="ow">in</span> <span class="n">dual_outputs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">dual_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">primal_out</span><span class="p">,</span> <span class="n">tangent_out</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">dual_output</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">tangent_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent_out</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">primal_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">primal_out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">primal_out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_float_or_complex_tensor</span><span class="p">(</span><span class="n">current_input</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="n">tangents</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">current_input</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">jvp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tangents</span><span class="p">]</span>
        <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="k">for</span> <span class="n">shards</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">expected</span><span class="p">)]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">_vmap</span><span class="p">(</span><span class="n">jvp</span><span class="p">)(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tangents</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
            <span class="c1"># Rethrow to provide a better error message</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;While computing batched gradients, got: </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">FAILED_BATCHED_GRAD_MSG_FWD_AD</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">input_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">expected</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="n">_get_failed_batched_grad_test_msg</span><span class="p">(</span><span class="n">input_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">_test_batched_grad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># NB: _test_batched_grad compares two autograd.grad invocations with a single</span>
    <span class="c1"># vmap(autograd.grad) invocation. It&#39;s not exactly a &quot;gradcheck&quot; in the</span>
    <span class="c1"># sense that we&#39;re not comparing an analytical jacobian with a numeric one,</span>
    <span class="c1"># but it is morally similar (we could have computed a full analytic jac</span>
    <span class="c1"># via vmap, but that is potentially slow)</span>
    <span class="n">diff_input_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">grad</span> <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inp</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inp</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="n">grad_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">vjp</span><span class="p">(</span><span class="n">gO</span><span class="p">)</span> <span class="k">for</span> <span class="n">gO</span> <span class="ow">in</span> <span class="n">grad_outputs</span><span class="p">]</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="k">for</span> <span class="n">shards</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">expected</span><span class="p">)]</span>

    <span class="c1"># Squash warnings since these are expected to happen in most cases</span>
    <span class="c1"># NB: this doesn&#39;t work for CUDA tests: https://github.com/pytorch/pytorch/issues/50209</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;There is a performance drop&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Please use functorch.vmap&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp</span><span class="p">)(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
            <span class="c1"># It&#39;s OK that we&#39;re not raising the error at the correct callsite.</span>
            <span class="c1"># That&#39;s because the callsite is always going to inside the Python</span>
            <span class="c1"># autograd.grad instead of the C++ traceback of what line in the</span>
            <span class="c1"># backward formula</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;While computing batched gradients, got: </span><span class="si">{</span><span class="n">ex</span><span class="si">}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">FAILED_BATCHED_GRAD_MSG</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">input_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">expected</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="n">_get_failed_batched_grad_test_msg</span><span class="p">(</span><span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">exp</span><span class="p">))</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_test_backward_mul_by_grad_output</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># Tests that backward is multiplied by grad_output</span>
    <span class="n">diff_input_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">diff_input_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s2">&quot;no Tensors requiring grad found in input&quot;</span><span class="p">)</span>
    <span class="n">grads_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span>
                                      <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">],</span>
                                      <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">gi</span><span class="p">,</span> <span class="n">di</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">gi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gi</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">layout</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;grad is incorrect layout (&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gi</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; is not &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">di</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;grad is sparse tensor, but has incorrect sparse_dim&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;grad is sparse tensor, but has incorrect dense_dim&#39;</span><span class="p">)</span>
            <span class="n">gi</span> <span class="o">=</span> <span class="n">gi</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="n">di</span> <span class="o">=</span> <span class="n">di</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">check_sparse_nnz</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gi</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gi</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;backward not multiplied by grad_output&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">gi</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;backward not multiplied by grad_output&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">gi</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">device</span> <span class="ow">or</span> <span class="n">gi</span><span class="o">.</span><span class="n">is_sparse</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s2">&quot;grad is incorrect type&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gi</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">di</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s1">&#39;grad is incorrect size&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">_test_undefined_forward_mode</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">fwAD</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">forward_ad</span>

    <span class="n">inp_tensors_idx</span><span class="p">,</span> <span class="n">inp_tensors</span> <span class="o">=</span> <span class="n">_get_inp_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">all_u_dense</span> <span class="o">=</span> <span class="n">_make_vectors</span><span class="p">(</span><span class="n">inp_tensors</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">tensor_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">and</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
        <span class="n">fw_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dual_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tensor_indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_mkldnn</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;MKLDNN inputs are not support for forward AD gradcheck.&quot;</span><span class="p">)</span>

                <span class="n">inp</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inp</span><span class="p">))</span>
                <span class="c1"># If inp is a differentiable view, the dual might not be the tangent given to</span>
                <span class="c1"># make_dual, so read it explicitly from the dual tensor</span>
                <span class="n">fw_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">inp</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">tensor_indices</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="n">dual_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">fw_grad</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">fw_grads</span><span class="p">,</span> <span class="n">all_u</span><span class="p">)):</span>
            <span class="n">fw_grad</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">fw_grad</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensor_indices</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">dual_inp_obj</span> <span class="o">=</span> <span class="n">dual_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="c1"># case 1 (Materialized Zero Tensor Tangent)</span>
            <span class="n">dual_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inp</span><span class="p">))</span>
            <span class="n">raw_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">dual_inputs</span><span class="p">))</span>
            <span class="n">dual_outputs1</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">_is_float_or_complex_tensor</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">)</span>

            <span class="c1"># case 2 (Efficient Zero Tensor Tangent since we don&#39;t make a dual object and pass a regular tensor)</span>
            <span class="n">dual_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">raw_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">dual_inputs</span><span class="p">))</span>
            <span class="n">dual_outputs2</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">_is_float_or_complex_tensor</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">)</span>

            <span class="c1"># reset</span>
            <span class="n">dual_inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">dual_inp_obj</span>

            <span class="k">for</span> <span class="n">index_o</span><span class="p">,</span> <span class="p">(</span><span class="n">d_o1</span><span class="p">,</span> <span class="n">d_o2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dual_outputs1</span><span class="p">,</span> <span class="n">dual_outputs2</span><span class="p">)):</span>
                <span class="n">val1</span><span class="p">,</span> <span class="n">res1</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">d_o1</span><span class="p">)</span>
                <span class="n">val2</span><span class="p">,</span> <span class="n">res2</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">d_o2</span><span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">res1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">res2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">res1</span><span class="p">,</span> <span class="n">res2</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s2">&quot;Mismatch in tangent values for output with index: &quot;</span><span class="p">,</span> <span class="n">index_o</span><span class="p">,</span>
                                             <span class="s2">&quot; when input: &quot;</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="s2">&quot; has an undefined tangent value. &quot;</span><span class="p">,</span>
                                             <span class="s2">&quot; Got: &quot;</span><span class="p">,</span> <span class="n">res1</span><span class="p">,</span> <span class="s2">&quot; but expected: &quot;</span><span class="p">,</span> <span class="n">res2</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">_test_undefined_backward_mode</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">diff_input_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_iter_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">diff_input_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="s2">&quot;no Tensors requiring grad found in input&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">warn_bc_breaking</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span>
            <span class="s1">&#39;Backwards compatibility: New undefined gradient support checking &#39;</span>
            <span class="s1">&#39;feature is enabled by default, but it may break existing callers &#39;</span>
            <span class="s1">&#39;of this function. If this is true for you, you can call this &#39;</span>
            <span class="s1">&#39;function with &quot;check_undefined_grad=False&quot; to disable the feature&#39;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">check_undefined_grad_support</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">):</span>
        <span class="n">grads_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">output_to_check</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">grads_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">,</span>
                                              <span class="n">grads_output</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="n">warn_bc_breaking</span><span class="p">()</span>
            <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">((</span>
                <span class="s1">&#39;Expected backward function to handle undefined output grads. &#39;</span>
                <span class="s1">&#39;Please look at &quot;Notes about undefined output gradients&quot; in &#39;</span>
                <span class="s1">&#39;&quot;tools/autograd/derivatives.yaml&quot;&#39;</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">gi</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads_input</span><span class="p">,</span> <span class="n">diff_input_list</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">gi</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">gi</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span>
                <span class="n">warn_bc_breaking</span><span class="p">()</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">((</span>
                    <span class="s1">&#39;Expected all input grads to be undefined or zero when all output grads are undefined &#39;</span>
                    <span class="s1">&#39;or zero. Please look at &quot;Notes about undefined output gradients&quot; in &#39;</span>
                    <span class="s1">&#39;&quot;tools/autograd/derivatives.yaml&quot;&#39;</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># All backward functions must work properly if all output grads are undefined</span>
    <span class="n">outputs_to_check</span> <span class="o">=</span> <span class="p">[[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">UndefinedGrad</span><span class="p">()(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
        <span class="c1"># This check filters out Tensor-likes that aren&#39;t instances of Tensor.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="p">]]</span>

    <span class="c1"># If there are multiple output grads, we should be able to undef one at a time without error</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">undef_grad_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)):</span>
            <span class="n">output_to_check</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="n">outputs_to_check</span><span class="o">.</span><span class="n">append</span><span class="p">([</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">UndefinedGrad</span><span class="p">()(</span><span class="n">o</span><span class="p">)</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">undef_grad_idx</span> <span class="k">else</span> <span class="n">o</span>
                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_to_check</span><span class="p">)])</span>

    <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">check_undefined_grad_support</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs_to_check</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_as_tuple</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span>


<span class="k">def</span> <span class="nf">_differentiable_outputs</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_notallclose_msg</span><span class="p">(</span><span class="n">analytical</span><span class="p">,</span> <span class="n">numerical</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span>
                         <span class="n">test_imag</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">out_is_complex</span> <span class="o">=</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_forward_ad</span><span class="p">)</span> <span class="ow">and</span> <span class="n">complex_indices</span> <span class="ow">and</span> <span class="n">output_idx</span> <span class="ow">in</span> <span class="n">complex_indices</span>
    <span class="n">inp_is_complex</span> <span class="o">=</span> <span class="n">is_forward_ad</span> <span class="ow">and</span> <span class="n">complex_indices</span> <span class="ow">and</span> <span class="n">input_idx</span> <span class="ow">in</span> <span class="n">complex_indices</span>
    <span class="n">part</span> <span class="o">=</span> <span class="s2">&quot;imaginary&quot;</span> <span class="k">if</span> <span class="n">test_imag</span> <span class="k">else</span> <span class="s2">&quot;real&quot;</span>
    <span class="n">element</span> <span class="o">=</span> <span class="s2">&quot;inputs&quot;</span> <span class="k">if</span> <span class="n">is_forward_ad</span> <span class="k">else</span> <span class="s2">&quot;outputs&quot;</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">out_is_complex</span> <span class="ow">or</span> <span class="n">inp_is_complex</span><span class="p">)</span> <span class="k">else</span> \
        <span class="sa">f</span><span class="s2">&quot;While considering the </span><span class="si">{</span><span class="n">part</span><span class="si">}</span><span class="s2"> part of complex </span><span class="si">{</span><span class="n">element</span><span class="si">}</span><span class="s2"> only, &quot;</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;computed with forward mode &quot;</span> <span class="k">if</span> <span class="n">is_forward_ad</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;Jacobian </span><span class="si">%s</span><span class="s1">mismatch for output </span><span class="si">%d</span><span class="s1"> with respect to input </span><span class="si">%d</span><span class="s1">,</span><span class="se">\n</span><span class="s1">&#39;</span> \
        <span class="s1">&#39;numerical:</span><span class="si">%s</span><span class="se">\n</span><span class="s1">analytical:</span><span class="si">%s</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">numerical</span><span class="p">,</span> <span class="n">analytical</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_transpose</span><span class="p">(</span><span class="n">matrix_of_tensors</span><span class="p">):</span>
    <span class="c1"># returns list of tuples</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">matrix_of_tensors</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_real_and_imag_output</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="c1"># returns new functions real(fn), and imag(fn) where real(fn) and imag(fn) behave the same as</span>
    <span class="c1"># the original fn, except torch.real or torch.imag are applied to the complex outputs</span>
    <span class="k">def</span> <span class="nf">apply_to_c_outs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">fn_to_apply</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fn_to_apply</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_fn</span>

    <span class="k">return</span> <span class="n">apply_to_c_outs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">real</span><span class="p">),</span> <span class="n">apply_to_c_outs</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">imag</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_real_and_imag_input</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">complex_inp_indices</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">):</span>
    <span class="c1"># returns new functions that take real inputs instead of complex inputs as</span>
    <span class="c1"># (x, y) -&gt; fn(x + y * 1j). And it computes: inp -&gt; fn(inp + y * 1j) and inp -&gt; fn(x + inp * 1j).</span>
    <span class="c1"># In each case, the other part is considered constant.</span>
    <span class="c1"># We do not use 0 for the constant here to make sure we always call the user function with a valid input.</span>
    <span class="k">def</span> <span class="nf">apply_to_c_inps</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">fn_to_apply</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">new_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">should_be_complex</span> <span class="ow">in</span> <span class="n">complex_inp_indices</span><span class="p">:</span>
                <span class="n">new_inputs</span><span class="p">[</span><span class="n">should_be_complex</span><span class="p">]</span> <span class="o">=</span> <span class="n">fn_to_apply</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">[</span><span class="n">should_be_complex</span><span class="p">],</span>
                                                            <span class="n">tupled_inputs</span><span class="p">[</span><span class="n">should_be_complex</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">new_inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">wrapped_fn</span>
    <span class="n">real_fn</span> <span class="o">=</span> <span class="n">apply_to_c_inps</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">inp</span><span class="p">,</span> <span class="n">orig</span><span class="p">:</span> <span class="n">inp</span> <span class="o">+</span> <span class="n">orig</span><span class="o">.</span><span class="n">imag</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
    <span class="n">imag_fn</span> <span class="o">=</span> <span class="n">apply_to_c_inps</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">inp</span><span class="p">,</span> <span class="n">orig</span><span class="p">:</span> <span class="n">orig</span><span class="o">.</span><span class="n">real</span> <span class="o">+</span> <span class="n">inp</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">real_fn</span><span class="p">,</span> <span class="n">imag_fn</span>


<span class="k">def</span> <span class="nf">_gradcheck_real_imag</span><span class="p">(</span><span class="n">gradcheck_fn</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                         <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">check_forward_ad</span><span class="p">,</span> <span class="n">check_backward_ad</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span>
                         <span class="n">check_undefined_grad</span><span class="p">):</span>
    <span class="n">complex_out_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()]</span>
    <span class="n">has_any_complex_output</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">check_backward_ad</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">has_any_complex_output</span><span class="p">:</span>
            <span class="n">real_fn</span><span class="p">,</span> <span class="n">imag_fn</span> <span class="o">=</span> <span class="n">_real_and_imag_output</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

            <span class="n">imag_func_out</span> <span class="o">=</span> <span class="n">imag_fn</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">)</span>
            <span class="n">imag_outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">imag_func_out</span><span class="p">)</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">imag_fn</span><span class="p">,</span> <span class="n">imag_func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">imag_outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span>
                         <span class="n">complex_indices</span><span class="o">=</span><span class="n">complex_out_indices</span><span class="p">,</span> <span class="n">test_imag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">real_func_out</span> <span class="o">=</span> <span class="n">real_fn</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">)</span>
            <span class="n">real_outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">real_func_out</span><span class="p">)</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">real_fn</span><span class="p">,</span> <span class="n">real_func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">real_outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">complex_indices</span><span class="o">=</span><span class="n">complex_out_indices</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">check_forward_ad</span><span class="p">:</span>
        <span class="n">complex_inp_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()]</span>
        <span class="k">if</span> <span class="n">complex_inp_indices</span><span class="p">:</span>
            <span class="n">real_fn</span><span class="p">,</span> <span class="n">imag_fn</span> <span class="o">=</span> <span class="n">_real_and_imag_input</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">complex_inp_indices</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">)</span>

            <span class="n">imag_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">imag</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">inp</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">tupled_inputs</span><span class="p">]</span>
            <span class="n">imag_func_out</span> <span class="o">=</span> <span class="n">imag_fn</span><span class="p">(</span><span class="o">*</span><span class="n">imag_inputs</span><span class="p">)</span>
            <span class="n">diff_imag_func_out</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">imag_func_out</span><span class="p">)</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">imag_fn</span><span class="p">,</span> <span class="n">imag_func_out</span><span class="p">,</span> <span class="n">imag_inputs</span><span class="p">,</span> <span class="n">diff_imag_func_out</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span>
                         <span class="n">complex_indices</span><span class="o">=</span><span class="n">complex_inp_indices</span><span class="p">,</span> <span class="n">test_imag</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">real_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">real</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">inp</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">tupled_inputs</span><span class="p">]</span>
            <span class="n">real_func_out</span> <span class="o">=</span> <span class="n">real_fn</span><span class="p">(</span><span class="o">*</span><span class="n">real_inputs</span><span class="p">)</span>
            <span class="n">diff_real_func_out</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">real_func_out</span><span class="p">)</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">real_fn</span><span class="p">,</span> <span class="n">real_func_out</span><span class="p">,</span> <span class="n">real_inputs</span><span class="p">,</span> <span class="n">diff_real_func_out</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">complex_indices</span><span class="o">=</span><span class="n">complex_inp_indices</span><span class="p">,</span>
                         <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">check_undefined_grad</span><span class="p">:</span>
                <span class="n">_test_undefined_forward_mode</span><span class="p">(</span><span class="n">imag_fn</span><span class="p">,</span> <span class="n">imag_func_out</span><span class="p">,</span> <span class="n">imag_inputs</span><span class="p">)</span>
                <span class="n">_test_undefined_forward_mode</span><span class="p">(</span><span class="n">real_fn</span><span class="p">,</span> <span class="n">real_func_out</span><span class="p">,</span> <span class="n">real_inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gradcheck_fn</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">check_undefined_grad</span><span class="p">:</span>
                <span class="n">_test_undefined_forward_mode</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_slow_gradcheck</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span>
                    <span class="n">nondet_tol</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">complex_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_imag</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">func_out</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_check_no_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="n">numerical</span> <span class="o">=</span> <span class="n">_transpose</span><span class="p">(</span><span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="n">use_forward_ad</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">use_forward_ad</span><span class="p">:</span>
        <span class="n">analytical_forward</span> <span class="o">=</span> <span class="n">_get_analytical_jacobian_forward_ad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="n">check_grad_dtypes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_per_out</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">numerical</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_per_out</span><span class="p">):</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">analytical_forward</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="n">_get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span> <span class="n">test_imag</span><span class="p">,</span>
                                                              <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
            <span class="n">analytical</span> <span class="o">=</span> <span class="n">_check_analytical_jacobian_attributes</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">analytical</span><span class="p">,</span> <span class="n">numerical</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="n">_get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span> <span class="n">test_imag</span><span class="p">))</span>

    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_dot_with_type_promotion</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">u</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">v</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">u</span> <span class="o">*</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">):</span>
    <span class="n">promoted_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">promoted_type</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">promoted_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_to_real_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dtype</span>

<span class="k">def</span> <span class="nf">_vec_from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">downcast_complex</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Create a random vector with the same number of elements as x and the same</span>
    <span class="c1"># dtype/device. If x is complex and downcast_complex is False, we create a</span>
    <span class="c1"># complex tensor with only real component.</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="c1"># For sparse, create a random sparse vec with random values in the same</span>
        <span class="c1"># indices. Make sure size is set so that it isn&#39;t inferred to be smaller.</span>
        <span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_to_real_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="k">if</span> <span class="n">downcast_complex</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">x_values</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">/=</span> <span class="n">values</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="n">values</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_to_real_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="k">if</span> <span class="n">downcast_complex</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">vec</span> <span class="o">/=</span> <span class="n">vec</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">vec</span>


<span class="k">def</span> <span class="nf">_get_inp_tensors</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">):</span>
    <span class="n">inp_idx_tup</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tup</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">inp_idx_tup</span><span class="p">],</span> <span class="p">[</span><span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">inp_idx_tup</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_adjusted_atol</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># In slow gradcheck, we compare A and B element-wise, i.e., for some a, b we</span>
    <span class="c1"># allow: |a - b| &lt; atol + rtol * b. But since we now compare q1 = v^T A u and</span>
    <span class="c1"># q2 = v^T B u, we must allow |q1 - q2| &lt; v^T E u + rtol * v^T B u, where E is</span>
    <span class="c1"># the correctly sized matrix in which each entry is atol.</span>
    <span class="c1">#</span>
    <span class="c1"># We see that atol needs to be scaled by v^T M u (where M is an all-ones M x N</span>
    <span class="c1"># matrix): v^T M u = \sum_{i} \sum_{j} u_i * v_j = (\sum_{i} u_i)(\sum_{i} v_i)</span>
    <span class="c1"># TODO: properly handle case when u is tuple instead of only taking first element</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="n">u</span>
    <span class="n">sum_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">else</span> <span class="n">u</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">sum_v</span> <span class="o">=</span> <span class="mf">1.</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span> <span class="k">else</span> <span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">atol</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">sum_u</span><span class="p">)</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">sum_v</span><span class="p">)</span>


<span class="n">FAST_FAIL_SLOW_OK_MSG</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Fast gradcheck failed but element-wise differences are small. This means that the</span>
<span class="s2">test might&#39;ve passed in slow_mode!</span>

<span class="s2">If you are adding a new operator, please file an issue and then use one of the</span>
<span class="s2">workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck:</span>

<span class="s2">If the test</span>
<span class="s2">- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck</span>
<span class="s2">  with `fast_mode=False` as a keyword argument.</span>
<span class="s2">- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test</span>
<span class="s2">  to have `gradcheck_fast_mode=False`</span>
<span class="s2">- is a Module test (e.g., in common_nn.py), then modify the corresponding</span>
<span class="s2">  module_test entry to have `gradcheck_fast_mode=False`</span>
<span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_run_slow_mode_and_get_error</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="p">):</span>
    <span class="c1"># Compute jacobians in slow mode for better error message</span>
    <span class="n">slow_numerical</span> <span class="o">=</span> <span class="n">_get_numerical_jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="n">is_forward_ad</span><span class="p">)[</span><span class="n">input_idx</span><span class="p">][</span><span class="n">output_idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">is_forward_ad</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">new_fn</span><span class="p">(</span><span class="n">inp</span><span class="p">):</span>
            <span class="n">new_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">)</span>
            <span class="n">new_inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">inp</span>
            <span class="k">return</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">new_inputs</span><span class="p">))[</span><span class="n">output_idx</span><span class="p">]</span>
        <span class="n">slow_analytical</span> <span class="o">=</span> <span class="n">_get_analytical_jacobian_forward_ad</span><span class="p">(</span><span class="n">new_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">tupled_inputs</span><span class="p">[</span><span class="n">input_idx</span><span class="p">],),</span> <span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">output_idx</span><span class="p">],))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">slow_analytical</span> <span class="o">=</span> <span class="n">_get_analytical_jacobian</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">input_idx</span><span class="p">,</span> <span class="n">output_idx</span><span class="p">)</span>


    <span class="c1"># Assume jacobians are non-empty and have the same shape</span>
    <span class="n">slow_max_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">slow_numerical</span> <span class="o">-</span> <span class="n">slow_analytical</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

    <span class="n">slow_allclose</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">slow_analytical</span><span class="p">,</span> <span class="n">slow_numerical</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">)</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The above quantities relating the numerical and analytical jacobians are computed </span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background </span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;Numerical:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">slow_numerical</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;Analytical:</span><span class="se">\n</span><span class="si">{</span><span class="n">slow_analytical</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
           <span class="sa">f</span><span class="s2">&quot;The max per-element difference (slow mode) is: </span><span class="si">{</span><span class="n">slow_max_diff</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">slow_allclose</span><span class="p">:</span>
        <span class="c1"># Slow gradcheck would&#39;ve passed!</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="n">FAST_FAIL_SLOW_OK_MSG</span>
    <span class="k">return</span> <span class="n">msg</span>


<span class="k">def</span> <span class="nf">_to_flat_dense_if_sparse</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">_make_vectors</span><span class="p">(</span><span class="n">inp_tensors</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="p">):</span>
    <span class="c1"># Use our own generator to avoid messing with the user&#39;s RNG state</span>
    <span class="n">g_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">all_u</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_u_dense</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inp_tensors</span><span class="p">:</span>
        <span class="n">ur</span> <span class="o">=</span> <span class="n">_vec_from_tensor</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">g_cpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">ur_dense</span> <span class="o">=</span> <span class="n">_to_flat_dense_if_sparse</span><span class="p">(</span><span class="n">ur</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
            <span class="n">ui</span> <span class="o">=</span> <span class="n">_vec_from_tensor</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">g_cpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">all_u</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">ur</span><span class="p">,</span> <span class="n">ui</span><span class="p">))</span>
            <span class="n">ui_dense</span> <span class="o">=</span> <span class="n">_to_flat_dense_if_sparse</span><span class="p">(</span><span class="n">ui</span><span class="p">)</span>
            <span class="n">all_u_dense</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">ur_dense</span><span class="p">,</span> <span class="n">ui_dense</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">all_u</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ur</span><span class="p">)</span>
            <span class="n">all_u_dense</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ur_dense</span><span class="p">)</span>
    <span class="n">all_v</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">use_forward_ad</span> <span class="k">else</span> <span class="p">[</span><span class="n">_vec_from_tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">g_cpu</span><span class="p">)</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">all_u_dense</span>


<span class="k">def</span> <span class="nf">_check_analytical_numerical_equal</span><span class="p">(</span><span class="n">all_analytical</span><span class="p">,</span> <span class="n">all_numerical</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span>
                                      <span class="n">func</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">test_imag</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">all_numerical_for_input_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_numerical</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_numerical_for_input_i</span><span class="p">):</span>
            <span class="c1"># Forward AD generates the transpose of what this function expects</span>
            <span class="k">if</span> <span class="n">is_forward_ad</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">all_analytical</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">all_analytical</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">updated_atol</span> <span class="o">=</span> <span class="n">_adjusted_atol</span><span class="p">(</span><span class="n">atol</span><span class="p">,</span> <span class="n">all_u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">all_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">all_v</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_allclose_with_type_promotion</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">updated_atol</span><span class="p">):</span>
                <span class="n">jacobians_str</span> <span class="o">=</span> <span class="n">_run_slow_mode_and_get_error</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">GradcheckError</span><span class="p">(</span><span class="n">_get_notallclose_msg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span> <span class="n">test_imag</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="p">)</span> <span class="o">+</span> <span class="n">jacobians_str</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fast_gradcheck</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span>
                    <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">complex_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_imag</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># See https://github.com/pytorch/pytorch/issues/53876 for details</span>
    <span class="n">inp_tensors_idx</span><span class="p">,</span> <span class="n">inp_tensors</span> <span class="o">=</span> <span class="n">_get_inp_tensors</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Backward mode computes v^T * J (VJP)</span>
    <span class="c1"># Since we computed J * u (JVP) through finite difference method, we perform an equality check</span>
    <span class="c1"># between VJP * u, v * JVP</span>
    <span class="c1"># ----</span>
    <span class="c1"># Forward mode computes J * u (JVP)</span>
    <span class="c1"># Since we already compute JVP through finite difference method,</span>
    <span class="c1"># we don&#39;t need v for correctness check here as asserted below</span>
    <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">all_u_dense</span> <span class="o">=</span> <span class="n">_make_vectors</span><span class="p">(</span><span class="n">inp_tensors</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">use_forward_ad</span><span class="o">=</span><span class="n">use_forward_ad</span><span class="p">)</span>

    <span class="n">numerical_vJu</span> <span class="o">=</span> <span class="n">_get_numerical_vJu</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inp_tensors_idx</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="n">use_forward_ad</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_forward_ad</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">all_v</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">analytical_vJu</span> <span class="o">=</span> <span class="n">_get_analytical_jacobian_forward_ad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func_out</span><span class="p">),</span>
                                                             <span class="n">all_u</span><span class="o">=</span><span class="n">all_u</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="n">check_grad_dtypes</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">_check_no_differentiable_outputs_fast</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inp_tensors_idx</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">)</span>

        <span class="n">analytical_vJu</span> <span class="o">=</span> <span class="n">_get_analytical_vJu_backward_mode</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u_dense</span><span class="p">)</span>

    <span class="n">_check_analytical_numerical_equal</span><span class="p">(</span><span class="n">analytical_vJu</span><span class="p">,</span> <span class="n">numerical_vJu</span><span class="p">,</span> <span class="n">complex_indices</span><span class="p">,</span>
                                      <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">all_v</span><span class="p">,</span> <span class="n">all_u</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">test_imag</span><span class="p">,</span> <span class="n">is_forward_ad</span><span class="o">=</span><span class="n">use_forward_ad</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># Note [VarArg of Tensors]</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># &#39;func&#39; accepts a vararg of tensors, which isn&#39;t expressable in the type system at the moment.</span>
<span class="c1"># If https://mypy.readthedocs.io/en/latest/additional_features.html?highlight=callable#extended-callable-types is accepted,</span>
<span class="c1"># the &#39;...&#39; first argument of Callable can be replaced with VarArg(Tensor).</span>
<span class="c1"># For now, we permit any input.</span>
<span class="c1"># the &#39;...&#39; first argument of Callable can be replaced with VarArg(Tensor).</span>
<span class="c1"># For now, we permit any input.</span>
<div class="viewcode-block" id="gradcheck"><a class="viewcode-back" href="../../../generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck">[docs]</a><span class="k">def</span> <span class="nf">gradcheck</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]],</span>  <span class="c1"># See Note [VarArg of Tensors]</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">raise_exception</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_sparse_nnz</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">nondet_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">check_undefined_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_grad_dtypes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_batched_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_batched_forward_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_forward_ad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_backward_ad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">fast_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check gradients computed via small finite differences against analytical</span>
<span class="sd">    gradients w.r.t. tensors in :attr:`inputs` that are of floating point or complex type</span>
<span class="sd">    and with ``requires_grad=True``.</span>

<span class="sd">    The check between numerical and analytical gradients uses :func:`~torch.allclose`.</span>

<span class="sd">    For most of the complex functions we consider for optimization purposes, no notion of</span>
<span class="sd">    Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of</span>
<span class="sd">    the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient</span>
<span class="sd">    computation is done under the assumption that the overall function has a real-valued</span>
<span class="sd">    output, we treat functions with complex output in a special way. For these functions,</span>
<span class="sd">    gradcheck is applied to two real-valued functions corresponding to taking the real</span>
<span class="sd">    components of the complex outputs for the first, and taking the imaginary components</span>
<span class="sd">    of the complex outputs for the second. For more details, check out</span>
<span class="sd">    :ref:`complex_autograd-doc`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The default values are designed for :attr:`input` of double precision.</span>
<span class="sd">        This check will likely fail if :attr:`input` is of less precision, e.g.,</span>
<span class="sd">        ``FloatTensor``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">       If any checked tensor in :attr:`input` has overlapping memory, i.e.,</span>
<span class="sd">       different indices pointing to the same memory address (e.g., from</span>
<span class="sd">       :func:`torch.expand`), this check will likely fail because the numerical</span>
<span class="sd">       gradients computed by point perturbation at such indices will change</span>
<span class="sd">       values at all other indices that share the same memory address.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor or a tuple of Tensors</span>
<span class="sd">        inputs (tuple of Tensor or Tensor): inputs to the function</span>
<span class="sd">        eps (float, optional): perturbation for finite differences</span>
<span class="sd">        atol (float, optional): absolute tolerance</span>
<span class="sd">        rtol (float, optional): relative tolerance</span>
<span class="sd">        raise_exception (bool, optional): indicating whether to raise an exception if</span>
<span class="sd">            the check fails. The exception gives more information about the</span>
<span class="sd">            exact nature of the failure. This is helpful when debugging gradchecks.</span>
<span class="sd">        check_sparse_nnz (bool, optional): if True, gradcheck allows for SparseTensor input,</span>
<span class="sd">            and for any SparseTensor at input, gradcheck will perform check at nnz positions only.</span>
<span class="sd">        nondet_tol (float, optional): tolerance for non-determinism. When running</span>
<span class="sd">            identical inputs through the differentiation, the results must either match</span>
<span class="sd">            exactly (default, 0.0) or be within this tolerance.</span>
<span class="sd">        check_undefined_grad (bool, optional): if True, check if undefined output grads</span>
<span class="sd">            are supported and treated as zeros, for ``Tensor`` outputs.</span>
<span class="sd">        check_batched_grad (bool, optional): if True, check if we can compute</span>
<span class="sd">            batched gradients using prototype vmap support. Defaults to False.</span>
<span class="sd">        check_batched_forward_grad (bool, optional): if True, checks if we can compute</span>
<span class="sd">            batched forward gradients using forward ad and prototype vmap support. Defaults to False.</span>
<span class="sd">        check_forward_ad (bool, optional): if True, check that the gradients computed with forward</span>
<span class="sd">            mode AD match the numerical ones. Defaults to False.</span>
<span class="sd">        check_backward_ad (bool, optional): if False, do not perform any checks that rely on</span>
<span class="sd">            backward mode AD to be implemented. Defaults to True.</span>
<span class="sd">        fast_mode (bool, optional): Fast mode for gradcheck and gradgradcheck is currently only</span>
<span class="sd">            implemented for R to R functions. If none of the inputs and outputs are complex</span>
<span class="sd">            a faster implementation of gradcheck that no longer computes the entire jacobian</span>
<span class="sd">            is run; otherwise, we fall back to the slow implementation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        True if all differences satisfy allclose condition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">check_forward_ad</span> <span class="ow">or</span> <span class="n">check_backward_ad</span><span class="p">,</span> \
        <span class="s2">&quot;Expected at least one of check_forward_ad or check_backward_ad to be True&quot;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">check_batched_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_backward_ad</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Setting check_batched_grad=True requires check_backward_ad to be True&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">check_batched_forward_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_forward_ad</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Setting check_batched_forward_grad=True requires check_forward_ad to be True&quot;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;raise_exception&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">raise_exception</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_gradcheck_helper</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">GradcheckError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_gradcheck_helper</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_gradcheck_helper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">rtol</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_undefined_grad</span><span class="p">,</span>
                      <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">check_batched_grad</span><span class="p">,</span> <span class="n">check_batched_forward_grad</span><span class="p">,</span> <span class="n">check_forward_ad</span><span class="p">,</span>
                      <span class="n">check_backward_ad</span><span class="p">,</span> <span class="n">fast_mode</span><span class="p">):</span>
    <span class="n">tupled_inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">_check_inputs</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span>

    <span class="n">func_out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func_out</span><span class="p">)</span>
    <span class="n">_check_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="n">gradcheck_fn</span> <span class="o">=</span> <span class="n">_fast_gradcheck</span> <span class="k">if</span> <span class="n">fast_mode</span> <span class="k">else</span> <span class="n">_slow_gradcheck</span>
    <span class="n">_gradcheck_real_imag</span><span class="p">(</span><span class="n">gradcheck_fn</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">func_out</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                         <span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">check_forward_ad</span><span class="o">=</span><span class="n">check_forward_ad</span><span class="p">,</span>
                         <span class="n">check_backward_ad</span><span class="o">=</span><span class="n">check_backward_ad</span><span class="p">,</span> <span class="n">nondet_tol</span><span class="o">=</span><span class="n">nondet_tol</span><span class="p">,</span>
                         <span class="n">check_undefined_grad</span><span class="o">=</span><span class="n">check_undefined_grad</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">check_batched_forward_grad</span><span class="p">:</span>
        <span class="n">_test_batched_grad_forward_ad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">)</span>

    <span class="c1"># Short circuit because remaining tests rely on backward AD to be implemented</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_backward_ad</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">check_batched_grad</span><span class="p">:</span>
            <span class="n">_test_batched_grad</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

    <span class="n">_test_backward_mul_by_grad_output</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">,</span> <span class="n">check_sparse_nnz</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">check_undefined_grad</span> <span class="ow">and</span> <span class="n">check_backward_ad</span><span class="p">:</span>
        <span class="n">_test_undefined_backward_mode</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">tupled_inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="gradgradcheck"><a class="viewcode-back" href="../../../generated/torch.autograd.gradgradcheck.html#torch.autograd.gradgradcheck">[docs]</a><span class="k">def</span> <span class="nf">gradgradcheck</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">_TensorOrTensors</span><span class="p">],</span>  <span class="c1"># See Note [VarArg of Tensors]</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">gen_non_contig_grad_outputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">raise_exception</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">nondet_tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">check_undefined_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">check_grad_dtypes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_batched_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_fwd_over_rev</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">check_rev_over_rev</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">fast_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check gradients of gradients computed via small finite differences</span>
<span class="sd">    against analytical gradients w.r.t. tensors in :attr:`inputs` and</span>
<span class="sd">    :attr:`grad_outputs` that are of floating point or complex type and with</span>
<span class="sd">    ``requires_grad=True``.</span>

<span class="sd">    This function checks that backpropagating through the gradients computed</span>
<span class="sd">    to the given :attr:`grad_outputs` are correct.</span>

<span class="sd">    The check between numerical and analytical gradients uses :func:`~torch.allclose`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The default values are designed for :attr:`input` and</span>
<span class="sd">        :attr:`grad_outputs` of double precision. This check will likely fail if</span>
<span class="sd">        they are of less precision, e.g., ``FloatTensor``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">       If any checked tensor in :attr:`input` and :attr:`grad_outputs` has</span>
<span class="sd">       overlapping memory, i.e., different indices pointing to the same memory</span>
<span class="sd">       address (e.g., from :func:`torch.expand`), this check will likely fail</span>
<span class="sd">       because the numerical gradients computed by point perturbation at such</span>
<span class="sd">       indices will change values at all other indices that share the same</span>
<span class="sd">       memory address.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor or a tuple of Tensors</span>
<span class="sd">        inputs (tuple of Tensor or Tensor): inputs to the function</span>
<span class="sd">        grad_outputs (tuple of Tensor or Tensor, optional): The gradients with</span>
<span class="sd">            respect to the function&#39;s outputs.</span>
<span class="sd">        eps (float, optional): perturbation for finite differences</span>
<span class="sd">        atol (float, optional): absolute tolerance</span>
<span class="sd">        rtol (float, optional): relative tolerance</span>
<span class="sd">        gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is</span>
<span class="sd">            ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the</span>
<span class="sd">            randomly generated gradient outputs are made to be noncontiguous</span>
<span class="sd">        raise_exception (bool, optional): indicating whether to raise an exception if</span>
<span class="sd">            the check fails. The exception gives more information about the</span>
<span class="sd">            exact nature of the failure. This is helpful when debugging gradchecks.</span>
<span class="sd">        nondet_tol (float, optional): tolerance for non-determinism. When running</span>
<span class="sd">            identical inputs through the differentiation, the results must either match</span>
<span class="sd">            exactly (default, 0.0) or be within this tolerance. Note that a small amount</span>
<span class="sd">            of nondeterminism in the gradient will lead to larger inaccuracies in</span>
<span class="sd">            the second derivative.</span>
<span class="sd">        check_undefined_grad (bool, optional): if True, check if undefined output grads</span>
<span class="sd">            are supported and treated as zeros</span>
<span class="sd">        check_batched_grad (bool, optional): if True, check if we can compute</span>
<span class="sd">            batched gradients using prototype vmap support. Defaults to False.</span>
<span class="sd">        fast_mode (bool, optional): if True, run a faster implementation of gradgradcheck that</span>
<span class="sd">            no longer computes the entire jacobian.</span>

<span class="sd">    Returns:</span>
<span class="sd">        True if all differences satisfy allclose condition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">check_fwd_over_rev</span> <span class="ow">or</span> <span class="n">check_rev_over_rev</span><span class="p">,</span> \
        <span class="s2">&quot;Expected at least one of check_fwd_over_rev or check_rev_over_rev to be True&quot;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">check_undefined_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_rev_over_rev</span><span class="p">),</span> \
        <span class="s2">&quot;Setting check_undefined_grad=True requires check_rev_over_rev to be True&quot;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">check_batched_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_rev_over_rev</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Setting check_batched_grad=True requires check_rev_over_rev to be True&quot;</span><span class="p">)</span>
    <span class="c1"># TODO: do we want to test this too?</span>
    <span class="c1"># assert not (check_batched_forward_grad and not check_fwd_over_rev), (</span>
    <span class="c1">#     &quot;Setting check_batched_forward_grad=True requires check_fwd_over_rev to be True&quot;)</span>
    <span class="n">tupled_inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># If grad_outputs is not specified, create random Tensors of the same shape, type, and device as the outputs</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">tupled_inputs</span><span class="p">))</span>
        <span class="n">tupled_grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">make_tensor</span><span class="p">(</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">noncontiguous</span><span class="o">=</span><span class="n">gen_non_contig_grad_outputs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tupled_grad_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">)</span>

    <span class="n">num_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tupled_grad_outputs</span><span class="p">)</span>

    <span class="c1"># NB: We need to save the requires_grad information about the inputs here because gradcheck detaches inputs</span>
    <span class="c1">#     before running forward mode AD</span>
    <span class="n">diff_input_args_indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tensor_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="n">diff_grad_output_indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tupled_grad_outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">new_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Restore the requires_grad information</span>
        <span class="n">input_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">diff_input_args_indices</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="o">-</span><span class="n">num_outputs</span><span class="p">]))</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_differentiable_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">input_args</span><span class="p">))</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">diff_grad_output_indices</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="n">num_outputs</span><span class="p">:]))</span>
        <span class="n">diff_input_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">diff_input_args_indices</span><span class="p">)</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">diff_input_args</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                          <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grad_inputs</span> <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_inputs</span>

    <span class="k">return</span> <span class="n">gradcheck</span><span class="p">(</span>
        <span class="n">new_func</span><span class="p">,</span> <span class="n">tupled_inputs</span> <span class="o">+</span> <span class="n">tupled_grad_outputs</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">raise_exception</span><span class="o">=</span><span class="n">raise_exception</span><span class="p">,</span>
        <span class="n">nondet_tol</span><span class="o">=</span><span class="n">nondet_tol</span><span class="p">,</span> <span class="n">check_undefined_grad</span><span class="o">=</span><span class="n">check_undefined_grad</span><span class="p">,</span>
        <span class="n">check_grad_dtypes</span><span class="o">=</span><span class="n">check_grad_dtypes</span><span class="p">,</span> <span class="n">check_batched_grad</span><span class="o">=</span><span class="n">check_batched_grad</span><span class="p">,</span> <span class="n">fast_mode</span><span class="o">=</span><span class="n">fast_mode</span><span class="p">,</span>
        <span class="n">check_forward_ad</span><span class="o">=</span><span class="n">check_fwd_over_rev</span><span class="p">,</span> <span class="n">check_backward_ad</span><span class="o">=</span><span class="n">check_rev_over_rev</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
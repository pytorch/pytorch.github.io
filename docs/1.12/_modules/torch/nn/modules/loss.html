


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.loss &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.loss</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.loss</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">.distance</span> <span class="kn">import</span> <span class="n">PairwiseDistance</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">_reduction</span> <span class="k">as</span> <span class="n">_Reduction</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>


<span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>


<span class="k">class</span> <span class="nc">_WeightedLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>


<div class="viewcode-block" id="L1Loss"><a class="viewcode-back" href="../../../../generated/torch.nn.L1Loss.html#torch.nn.L1Loss">[docs]</a><span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean absolute error (MAE) between each element in</span>
<span class="sd">    the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`n` elements each.</span>

<span class="sd">    The sum operation still operates over all the elements, and divides by :math:`n`.</span>

<span class="sd">    The division by :math:`n` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Supports real-valued and complex-valued inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(*)`, same shape as the input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss. It is useful to train a classification</span>
<span class="sd">    problem with `C` classes.</span>

<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning</span>
<span class="sd">    weight to each of the classes. This is particularly useful when you have an</span>
<span class="sd">    unbalanced training set.</span>

<span class="sd">    The `input` given through a forward call is expected to contain</span>
<span class="sd">    log-probabilities of each class. `input` has to be a Tensor of size either</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case. The latter is useful for</span>
<span class="sd">    higher dimension inputs, such as computing NLL loss per-pixel for 2D images.</span>

<span class="sd">    Obtaining log-probabilities in a neural network is easily achieved by</span>
<span class="sd">    adding a  `LogSoftmax`  layer in the last layer of your network.</span>
<span class="sd">    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra</span>
<span class="sd">    layer.</span>

<span class="sd">    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`</span>
<span class="sd">    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts</span>
<span class="sd">    this class index (this index may not necessarily be in the class range).</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_{y_n} x_{n,y_n}, \quad</span>
<span class="sd">        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</span>

<span class="sd">    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and</span>
<span class="sd">    :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;</span>
<span class="sd">            \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">            \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``None``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When</span>
<span class="sd">            :attr:`size_average` is ``True``, the loss is averaged over</span>
<span class="sd">            non-ignored targets.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``None``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` or :math:`(C)`, where `C = number of classes`, or</span>
<span class="sd">          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, where each value is</span>
<span class="sd">          :math:`0 \leq \text{targets}[i] \leq C-1`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of</span>
<span class="sd">          K-dimensional loss.</span>
<span class="sd">        - Output: If :attr:`reduction` is ``&#39;none&#39;``, shape :math:`(N)` or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of K-dimensional loss.</span>
<span class="sd">          Otherwise, scalar.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)</span>
<span class="sd">        &gt;&gt;&gt; N, C = 5, 4</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; # input is of size N x C x height x width</span>
<span class="sd">        &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)</span>
<span class="sd">        &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; m = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(conv(data)), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NLLLoss2d</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;NLLLoss2d has been deprecated. &quot;</span>
                      <span class="s2">&quot;Please use NLLLoss instead as a drop-in replacement and see &quot;</span>
                      <span class="s2">&quot;https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NLLLoss2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PoissonNLLLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Negative log likelihood loss with Poisson distribution of target.</span>

<span class="sd">    The loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{target} \sim \mathrm{Poisson}(\text{input})</span>

<span class="sd">        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})</span>
<span class="sd">                                    + \log(\text{target!})</span>

<span class="sd">    The last term can be omitted or approximated with Stirling formula. The</span>
<span class="sd">    approximation is used for target values more than 1. For targets less or</span>
<span class="sd">    equal to 1 zeros are added to the loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_input (bool, optional): if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target}*\text{input}`, if ``False`` the loss is</span>
<span class="sd">            :math:`\text{input} - \text{target}*\log(\text{input}+\text{eps})`.</span>
<span class="sd">        full (bool, optional): whether to compute full loss, i. e. to add the</span>
<span class="sd">            Stirling approximation term</span>

<span class="sd">            .. math::</span>
<span class="sd">                \text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input = False`. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.PoissonNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(log_input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar by default. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`,</span>
<span class="sd">          the same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_input&#39;</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoissonNLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_input</span> <span class="o">=</span> <span class="n">log_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">poisson_nll_loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_input</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span>
                                  <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GaussianNLLLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gaussian negative log likelihood loss.</span>

<span class="sd">    The targets are treated as samples from Gaussian distributions with</span>
<span class="sd">    expectations and variances predicted by the neural network. For a</span>
<span class="sd">    ``target`` tensor modelled as having Gaussian distribution with a tensor</span>
<span class="sd">    of expectations ``input`` and a tensor of positive variances ``var`` the loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},</span>
<span class="sd">        \ \text{eps}\right)\right) + \frac{\left(\text{input} - \text{target}\right)^2}</span>
<span class="sd">        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}</span>

<span class="sd">    where :attr:`eps` is used for stability. By default, the constant term of</span>
<span class="sd">    the loss function is omitted unless :attr:`full` is ``True``. If ``var`` is not the same</span>
<span class="sd">    size as ``input`` (due to a homoscedastic assumption), it must either have a final dimension</span>
<span class="sd">    of 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        full (bool, optional): include the constant term in the loss</span>
<span class="sd">            calculation. Default: ``False``.</span>
<span class="sd">        eps (float, optional): value used to clamp ``var`` (see note below), for</span>
<span class="sd">            stability. Default: 1e-6.</span>
<span class="sd">        reduction (string, optional): specifies the reduction to apply to the</span>
<span class="sd">            output:``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction</span>
<span class="sd">            will be applied, ``&#39;mean&#39;``: the output is the average of all batch</span>
<span class="sd">            member losses, ``&#39;sum&#39;``: the output is the sum of all batch member</span>
<span class="sd">            losses. Default: ``&#39;mean&#39;``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input</span>
<span class="sd">          but with one dimension equal to 1 (to allow for broadcasting)</span>
<span class="sd">        - Var: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input but</span>
<span class="sd">          with one dimension equal to 1, or same shape as the input but with one fewer</span>
<span class="sd">          dimension (to allow for broadcasting)</span>
<span class="sd">        - Output: scalar if :attr:`reduction` is ``&#39;mean&#39;`` (default) or</span>
<span class="sd">          ``&#39;sum&#39;``. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N, *)`, same</span>
<span class="sd">          shape as the input</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.GaussianNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; var = torch.ones(5, 2, requires_grad=True) #heteroscedastic</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target, var)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.GaussianNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; var = torch.ones(5, 1, requires_grad=True) #homoscedastic</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target, var)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Note:</span>
<span class="sd">        The clamping of ``var`` is ignored with respect to autograd, and so the</span>
<span class="sd">        gradients are unaffected by it.</span>

<span class="sd">    Reference:</span>
<span class="sd">        Nix, D. A. and Weigend, A. S., &quot;Estimating the mean and variance of the</span>
<span class="sd">        target probability distribution&quot;, Proceedings of 1994 IEEE International</span>
<span class="sd">        Conference on Neural Networks (ICNN&#39;94), Orlando, FL, USA, 1994, pp. 55-60</span>
<span class="sd">        vol.1, doi: 10.1109/ICNN.1994.374138.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianNLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">var</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gaussian_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss">[docs]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Kullback-Leibler divergence loss.</span>

<span class="sd">    For tensors of the same shape :math:`y_{\text{pred}},\ y_{\text{true}}`,</span>
<span class="sd">    where :math:`y_{\text{pred}}` is the :attr:`input` and :math:`y_{\text{true}}` is the</span>
<span class="sd">    :attr:`target`, we define the **pointwise KL-divergence** as</span>

<span class="sd">    .. math::</span>

<span class="sd">        L(y_{\text{pred}},\ y_{\text{true}})</span>
<span class="sd">            = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}}</span>
<span class="sd">            = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})</span>

<span class="sd">    To avoid underflow issues when computing this quantity, this loss expects the argument</span>
<span class="sd">    :attr:`input` in the log-space. The argument :attr:`target` may also be provided in the</span>
<span class="sd">    log-space if :attr:`log_target`\ `= True`.</span>

<span class="sd">    To summarise, this function is roughly equivalent to computing</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        if not log_target: # default</span>
<span class="sd">            loss_pointwise = target * (target.log() - input)</span>
<span class="sd">        else:</span>
<span class="sd">            loss_pointwise = target.exp() * (target - input)</span>

<span class="sd">    and then reducing this result depending on the argument :attr:`reduction` as</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        if reduction == &quot;mean&quot;:  # default</span>
<span class="sd">            loss = loss_pointwise.mean()</span>
<span class="sd">        elif reduction == &quot;batchmean&quot;:  # mathematically correct</span>
<span class="sd">            loss = loss_pointwise.sum() / input.size(0)</span>
<span class="sd">        elif reduction == &quot;sum&quot;:</span>
<span class="sd">            loss = loss_pointwise.sum()</span>
<span class="sd">        else:  # reduction == &quot;none&quot;</span>
<span class="sd">            loss = loss_pointwise</span>

<span class="sd">    .. note::</span>
<span class="sd">        As all the other losses in PyTorch, this function expects the first argument,</span>
<span class="sd">        :attr:`input`, to be the output of the model (e.g. the neural network)</span>
<span class="sd">        and the second, :attr:`target`, to be the observations in the dataset.</span>
<span class="sd">        This differs from the standard mathematical notation :math:`KL(P\ ||\ Q)` where</span>
<span class="sd">        :math:`P` denotes the distribution of the observations and :math:`Q` denotes the model.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :attr:`reduction`\ `= &quot;mean&quot;` doesn&#39;t return the true KL divergence value, please use</span>
<span class="sd">        :attr:`reduction`\ `= &quot;batchmean&quot;` which aligns with the mathematical definition.</span>
<span class="sd">        In a future release, `&quot;mean&quot;` will be changed to be the same as `&quot;batchmean&quot;`.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to `False`, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is `False`. Default: `True`</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is `False`, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: `True`</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output. Default: `&quot;mean&quot;`</span>
<span class="sd">        log_target (bool, optional): Specifies whether `target` is the log space. Default: `False`</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar by default. If :attr:`reduction` is `&#39;none&#39;`, then :math:`(*)`,</span>
<span class="sd">          same shape as the input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # input should be a distribution in the log space</span>
<span class="sd">        &gt;&gt;&gt; input = F.log_softmax(torch.randn(3, 5, requires_grad=True))</span>
<span class="sd">        &gt;&gt;&gt; # Sample a batch of distributions. Usually this would come from the dataset</span>
<span class="sd">        &gt;&gt;&gt; target = F.softmax(torch.rand(3, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = kl_loss(input, target)</span>

<span class="sd">        &gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;, log_target=True)</span>
<span class="sd">        &gt;&gt;&gt; log_target = F.log_softmax(torch.rand(3, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = kl_loss(input, log_target)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">log_target</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_target</span> <span class="o">=</span> <span class="n">log_target</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_target</span><span class="p">)</span></div>


<div class="viewcode-block" id="MSELoss"><a class="viewcode-back" href="../../../../generated/torch.nn.MSELoss.html#torch.nn.MSELoss">[docs]</a><span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean squared error (squared L2 norm) between</span>
<span class="sd">    each element in the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left( x_n - y_n \right)^2,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`n` elements each.</span>

<span class="sd">    The mean operation still operates over all the elements, and divides by :math:`n`.</span>

<span class="sd">    The division by :math:`n` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MSELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">BCELoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the Binary Cross Entropy between the target and</span>
<span class="sd">    the input probabilities:</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets :math:`y` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be</span>
<span class="sd">    mathematically undefined in the above loss equation. PyTorch chooses to set</span>
<span class="sd">    :math:`\log (0) = -\infty`, since :math:`\lim_{x\to 0} \log (x) = -\infty`.</span>
<span class="sd">    However, an infinite term in the loss equation is not desirable for several reasons.</span>

<span class="sd">    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be</span>
<span class="sd">    multiplying 0 with infinity. Secondly, if we have an infinite loss value, then</span>
<span class="sd">    we would also have an infinite term in our gradient, since</span>
<span class="sd">    :math:`\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty`.</span>
<span class="sd">    This would make BCELoss&#39;s backward method nonlinear with respect to :math:`x_n`,</span>
<span class="sd">    and using it for things like linear regression would not be straight-forward.</span>

<span class="sd">    Our solution is that BCELoss clamps its log function outputs to be greater than</span>
<span class="sd">    or equal to -100. This way, we can always have a finite loss value and a linear</span>
<span class="sd">    backward method.</span>


<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This loss combines a `Sigmoid` layer and the `BCELoss` in one single</span>
<span class="sd">    class. This version is more numerically stable than using a plain `Sigmoid`</span>
<span class="sd">    followed by a `BCELoss` as, by combining the operations into one layer,</span>
<span class="sd">    we take advantage of the log-sum-exp trick for numerical stability.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)</span>
<span class="sd">        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets `t[i]` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    It&#39;s possible to trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad</span>
<span class="sd">        l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})</span>
<span class="sd">        + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</span>

<span class="sd">    where :math:`c` is the class number (:math:`c &gt; 1` for multi-label binary classification,</span>
<span class="sd">    :math:`c = 1` for single-label binary classification),</span>
<span class="sd">    :math:`n` is the number of the sample in the batch and</span>
<span class="sd">    :math:`p_c` is the weight of the positive answer for the class :math:`c`.</span>

<span class="sd">    :math:`p_c &gt; 1` increases the recall, :math:`p_c &lt; 1` increases the precision.</span>

<span class="sd">    For example, if a dataset contains 100 positive and 300 negative examples of a single class,</span>
<span class="sd">    then `pos_weight` for the class should be equal to :math:`\frac{300}{100}=3`.</span>
<span class="sd">    The loss would act as if the dataset contains :math:`3\times 100=300` positive examples.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10</span>
<span class="sd">        &gt;&gt;&gt; output = torch.full([10, 64], 1.5)  # A prediction (logit)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1</span>
<span class="sd">        &gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(1.5))</span>
<span class="sd">        tensor(0.2014)</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples.</span>
<span class="sd">                Must be a vector with length equal to the number of classes.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">     Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">pos_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCEWithLogitsLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pos_weight&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                                                  <span class="n">pos_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span><span class="p">,</span>
                                                  <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="HingeEmbeddingLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss">[docs]</a><span class="k">class</span> <span class="nc">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>
<span class="sd">    This is usually used for measuring whether two inputs are similar or</span>
<span class="sd">    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically</span>
<span class="sd">    used for learning nonlinear embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">            x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">            \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of `1`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation</span>
<span class="sd">          operates over all the elements.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HingeEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MultiLabelMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>

<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>

<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>

<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`</span>
<span class="sd">          is the number of classes.</span>
<span class="sd">        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])</span>
<span class="sd">        &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1</span>
<span class="sd">        &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])</span>
<span class="sd">        &gt;&gt;&gt; loss(x, y)</span>
<span class="sd">        &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="sd">        tensor(0.8500)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiLabelMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below beta and an L1 term otherwise.</span>
<span class="sd">    It is less sensitive to outliers than :class:`torch.nn.MSELoss` and in some cases</span>
<span class="sd">    prevents exploding gradients (e.g. see the paper `Fast R-CNN`_ by Ross Girshick).</span>

<span class="sd">    For a batch of size :math:`N`, the unreduced loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1, ..., l_N\}^T</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        0.5 (x_n - y_n)^2 / beta, &amp; \text{if } |x_n - y_n| &lt; beta \\</span>
<span class="sd">        |x_n - y_n| - 0.5 * beta, &amp; \text{otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        Smooth L1 loss can be seen as exactly :class:`L1Loss`, but with the :math:`|x - y| &lt; beta`</span>
<span class="sd">        portion replaced with a quadratic function such that its slope is 1 at :math:`|x - y| = beta`.</span>
<span class="sd">        The quadratic segment smooths the L1 loss near :math:`|x - y| = 0`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Smooth L1 loss is closely related to :class:`HuberLoss`, being</span>
<span class="sd">        equivalent to :math:`huber(x, y) / beta` (note that Smooth L1&#39;s beta hyper-parameter is</span>
<span class="sd">        also known as delta for Huber). This leads to the following differences:</span>

<span class="sd">        * As beta -&gt; 0, Smooth L1 loss converges to :class:`L1Loss`, while :class:`HuberLoss`</span>
<span class="sd">          converges to a constant 0 loss. When beta is 0, Smooth L1 loss is equivalent to L1 loss.</span>
<span class="sd">        * As beta -&gt; :math:`+\infty`, Smooth L1 loss converges to a constant 0 loss, while</span>
<span class="sd">          :class:`HuberLoss` converges to :class:`MSELoss`.</span>
<span class="sd">        * For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.</span>
<span class="sd">          For :class:`HuberLoss`, the slope of the L1 segment is beta.</span>

<span class="sd">    .. _`Fast R-CNN`: https://arxiv.org/abs/1504.08083</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        beta (float, optional): Specifies the threshold at which to change between L1 and L2 loss.</span>
<span class="sd">            The value must be non-negative. Default: 1.0</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SmoothL1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>


<div class="viewcode-block" id="HuberLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss">[docs]</a><span class="k">class</span> <span class="nc">HuberLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below delta and a delta-scaled L1 term otherwise.</span>
<span class="sd">    This loss combines advantages of both :class:`L1Loss` and :class:`MSELoss`; the</span>
<span class="sd">    delta-scaled L1 region makes the loss less sensitive to outliers than :class:`MSELoss`,</span>
<span class="sd">    while the L2 region provides smoothness over :class:`L1Loss` near 0. See</span>
<span class="sd">    `Huber loss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;`_ for more information.</span>

<span class="sd">    For a batch of size :math:`N`, the unreduced loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1, ..., l_N\}^T</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        0.5 (x_n - y_n)^2, &amp; \text{if } |x_n - y_n| &lt; delta \\</span>
<span class="sd">        delta * (|x_n - y_n| - 0.5 * delta), &amp; \text{otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        When delta is set to 1, this loss is equivalent to :class:`SmoothL1Loss`.</span>
<span class="sd">        In general, this loss differs from :class:`SmoothL1Loss` by a factor of delta (AKA beta</span>
<span class="sd">        in Smooth L1).</span>
<span class="sd">        See :class:`SmoothL1Loss` for additional discussion on the differences in behavior</span>
<span class="sd">        between the two losses.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>
<span class="sd">        delta (float, optional): Specifies the threshold at which to change between delta-scaled L1 and L2 loss.</span>
<span class="sd">            The value must be positive.  Default: 1.0</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">huber_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion computes the cross entropy loss between input and target.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    The `input` is expected to contain raw, unnormalized scores for each class.</span>
<span class="sd">    `input` has to be a Tensor of size :math:`(C)` for unbatched input,</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1` for the</span>
<span class="sd">    `K`-dimensional case. The last being useful for higher dimension inputs, such</span>
<span class="sd">    as computing cross entropy loss per-pixel for 2D images.</span>

<span class="sd">    The `target` that this criterion expects should contain either:</span>

<span class="sd">    - Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if</span>
<span class="sd">      `ignore_index` is specified, this loss also accepts this class index (this index</span>
<span class="sd">      may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`</span>
<span class="sd">      set to ``&#39;none&#39;``) loss for this case can be described as:</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}</span>

<span class="sd">      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,</span>
<span class="sd">      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as</span>
<span class="sd">      :math:`d_1, ..., d_k` for the `K`-dimensional case. If</span>
<span class="sd">      :attr:`reduction` is not ``&#39;none&#39;`` (default ``&#39;mean&#39;``), then</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}} l_n, &amp;</span>
<span class="sd">               \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">                \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">                \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">      Note that this case is equivalent to the combination of :class:`~torch.nn.LogSoftmax` and</span>
<span class="sd">      :class:`~torch.nn.NLLLoss`.</span>

<span class="sd">    - Probabilities for each class; useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with</span>
<span class="sd">      :attr:`reduction` set to ``&#39;none&#39;``) loss for this case can be described as:</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,</span>
<span class="sd">      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as</span>
<span class="sd">      :math:`d_1, ..., d_k` for the `K`-dimensional case. If</span>
<span class="sd">      :attr:`reduction` is not ``&#39;none&#39;`` (default ``&#39;mean&#39;``), then</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">               \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">                \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">                \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        The performance of this criterion is generally better when `target` contains class</span>
<span class="sd">        indices, as this allows for optimized computation. Consider providing `target` as</span>
<span class="sd">        class probabilities only when a single class label per minibatch item is too restrictive.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each class.</span>
<span class="sd">            If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Note that</span>
<span class="sd">            :attr:`ignore_index` is only applicable when the target contains class indices.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount</span>
<span class="sd">            of smoothing when computing the loss, where 0.0 means no smoothing. The targets</span>
<span class="sd">            become a mixture of the original ground truth and a uniform distribution as described in</span>
<span class="sd">            `Rethinking the Inception Architecture for Computer Vision &lt;https://arxiv.org/abs/1512.00567&gt;`__. Default: :math:`0.0`.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with</span>
<span class="sd">          :math:`K \geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.</span>
<span class="sd">          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.</span>
<span class="sd">        - Output: If reduction is &#39;none&#39;, same shape as the target. Otherwise, scalar.</span>

<span class="sd">        where:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{aligned}</span>
<span class="sd">                C ={} &amp; \text{number of classes} \\</span>
<span class="sd">                N ={} &amp; \text{batch size} \\</span>
<span class="sd">            \end{aligned}</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # Example of target with class indices</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example of target with class probabilities</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5).softmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="s1">&#39;label_smoothing&#39;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">label_smoothing</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">label_smoothing</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                               <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
                               <span class="n">label_smoothing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultiLabelSoftMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-label one-versus-all</span>
<span class="sd">    loss based on max-entropy, between input :math:`x` and target :math:`y` of size</span>
<span class="sd">    :math:`(N, C)`.</span>
<span class="sd">    For each sample in the minibatch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})</span>
<span class="sd">                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</span>

<span class="sd">    where :math:`i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}`,</span>
<span class="sd">    :math:`y[i] \in \left\{0, \; 1\right\}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.</span>
<span class="sd">        - Target: :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiLabelSoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given input tensors</span>
<span class="sd">    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.</span>
<span class="sd">    This is used for measuring whether two inputs are similar or dissimilar,</span>
<span class="sd">    using the cosine similarity, and is typically used for learning nonlinear</span>
<span class="sd">    embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for each sample is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\</span>
<span class="sd">        \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,</span>
<span class="sd">            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the</span>
<span class="sd">            default value is :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input1: :math:`(N, D)` or :math:`(D)`, where `N` is the batch size and `D` is the embedding dimension.</span>
<span class="sd">        - Input2: :math:`(N, D)` or :math:`(D)`, same shape as Input1.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`.</span>
<span class="sd">        - Output: If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`, otherwise scalar.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="MarginRankingLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss">[docs]</a><span class="k">class</span> <span class="nc">MarginRankingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given</span>
<span class="sd">    inputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,</span>
<span class="sd">    and a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).</span>

<span class="sd">    If :math:`y = 1` then it assumed the first input should be ranked higher</span>
<span class="sd">    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.</span>

<span class="sd">    The loss function for each pair of samples in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input1: :math:`(N)` or :math:`()` where `N` is the batch size.</span>
<span class="sd">        - Input2: :math:`(N)` or :math:`()`, same shape as the Input1.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, same shape as the inputs.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;`` and Input size is not :math:`()`, then :math:`(N)`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MarginRankingLoss()</span>
<span class="sd">        &gt;&gt;&gt; input1 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; input2 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3).sign()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MarginRankingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i])^p}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>

<span class="sd">    Optionally, you can give non-equal weighting on the classes by passing</span>
<span class="sd">    a 1D :attr:`weight` tensor into the constructor.</span>

<span class="sd">    The loss function then becomes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`</span>
<span class="sd">            are the only supported values.</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`1`.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` or :math:`(C)`, where :math:`N` is the batch size and :math:`C` is the number of classes.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, where each value is :math:`0 \leq \text{targets}[i] \leq C-1`.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the target.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MultiMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])</span>
<span class="sd">        &gt;&gt;&gt; y = torch.tensor([3])</span>
<span class="sd">        &gt;&gt;&gt; loss(x, y)</span>
<span class="sd">        &gt;&gt;&gt; # 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="sd">        tensor(0.3250)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;only p == 1 and p == 2 supported&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span>
                                   <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>


<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the</span>
<span class="sd">    triplet margin loss for input tensors using a custom distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Default: :math:`1`.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.</span>
<span class="sd">        swap (bool, optional): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: ``False``.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` or :math:`(D)` where :math:`D` is the vector dimension.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;`` and</span>
<span class="sd">          input shape is :math:`(N, D)`; a scalar otherwise.</span>

<span class="sd">    Examples::</span>

<span class="sd">    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)</span>
<span class="sd">    &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;swap&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
                                     <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TripletMarginWithDistanceLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given input</span>
<span class="sd">    tensors :math:`a`, :math:`p`, and :math:`n` (representing anchor,</span>
<span class="sd">    positive, and negative examples, respectively), and a nonnegative,</span>
<span class="sd">    real-valued function (&quot;distance function&quot;) used to compute the relationship</span>
<span class="sd">    between the anchor and positive example (&quot;positive distance&quot;) and the</span>
<span class="sd">    anchor and negative example (&quot;negative distance&quot;).</span>

<span class="sd">    The unreduced loss (i.e., with :attr:`reduction` set to ``&#39;none&#39;``)</span>
<span class="sd">    can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where :math:`N` is the batch size; :math:`d` is a nonnegative, real-valued function</span>
<span class="sd">    quantifying the closeness of two tensors, referred to as the :attr:`distance_function`;</span>
<span class="sd">    and :math:`margin` is a nonnegative margin representing the minimum difference</span>
<span class="sd">    between the positive and negative distances that is required for the loss to</span>
<span class="sd">    be 0.  The input tensors have :math:`N` elements each and can be of any shape</span>
<span class="sd">    that the distance function can handle.</span>

<span class="sd">    If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginLoss`, which computes the triplet</span>
<span class="sd">    loss for input tensors using the :math:`l_p` distance as the distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        distance_function (callable, optional): A nonnegative, real-valued function that</span>
<span class="sd">            quantifies the closeness of two tensors. If not specified,</span>
<span class="sd">            `nn.PairwiseDistance` will be used.  Default: ``None``</span>
<span class="sd">        margin (float, optional): A nonnegative margin representing the minimum difference</span>
<span class="sd">            between the positive and negative distances required for the loss to be 0. Larger</span>
<span class="sd">            margins penalize cases where the negative examples are not distant enough from the</span>
<span class="sd">            anchors, relative to the positives. Default: :math:`1`.</span>
<span class="sd">        swap (bool, optional): Whether to use the distance swap described in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. If True, and if the positive example is closer to the</span>
<span class="sd">            negative example than the anchor is, swaps the positive example and the anchor in</span>
<span class="sd">            the loss computation. Default: ``False``.</span>
<span class="sd">        reduction (string, optional): Specifies the (optional) reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>


<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` represents any number of additional dimensions</span>
<span class="sd">          as supported by the distance function.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;``, or a scalar</span>
<span class="sd">          otherwise.</span>

<span class="sd">    Examples::</span>

<span class="sd">    &gt;&gt;&gt; # Initialize embeddings</span>
<span class="sd">    &gt;&gt;&gt; embedding = nn.Embedding(1000, 128)</span>
<span class="sd">    &gt;&gt;&gt; anchor_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; positive_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; negative_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; anchor = embedding(anchor_ids)</span>
<span class="sd">    &gt;&gt;&gt; positive = embedding(positive_ids)</span>
<span class="sd">    &gt;&gt;&gt; negative = embedding(negative_ids)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Built-in Distance Function</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function</span>
<span class="sd">    &gt;&gt;&gt; def l_infinity(x1, x2):</span>
<span class="sd">    &gt;&gt;&gt;     return torch.max(torch.abs(x1 - x2), dim=1).values</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function (Lambda)</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(</span>
<span class="sd">    &gt;&gt;&gt;         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;swap&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">distance_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginWithDistanceLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> \
            <span class="n">distance_function</span> <span class="k">if</span> <span class="n">distance_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">PairwiseDistance</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_with_distance_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span>
                                                   <span class="n">distance_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">,</span>
                                                   <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Connectionist Temporal Classification loss.</span>

<span class="sd">    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the</span>
<span class="sd">    probability of possible alignments of input to target, producing a loss value which is differentiable</span>
<span class="sd">    with respect to each input node. The alignment of input to target is assumed to be &quot;many-to-one&quot;, which</span>
<span class="sd">    limits the length of the target sequence such that it must be :math:`\leq` the input length.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int, optional): blank label. Default :math:`0`.</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the output losses will be divided by the target lengths and</span>
<span class="sd">            then the mean over the batch is taken. Default: ``&#39;mean&#39;``</span>
<span class="sd">        zero_infinity (bool, optional):</span>
<span class="sd">            Whether to zero infinite losses and the associated gradients.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">            Infinite losses mainly occur when the inputs are too short</span>
<span class="sd">            to be aligned to the targets.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Log_probs: Tensor of size :math:`(T, N, C)` or :math:`(T, C)`,</span>
<span class="sd">          where :math:`T = \text{input length}`,</span>
<span class="sd">          :math:`N = \text{batch size}`, and</span>
<span class="sd">          :math:`C = \text{number of classes (including blank)}`.</span>
<span class="sd">          The logarithmized probabilities of the outputs (e.g. obtained with</span>
<span class="sd">          :func:`torch.nn.functional.log_softmax`).</span>
<span class="sd">        - Targets: Tensor of size :math:`(N, S)` or</span>
<span class="sd">          :math:`(\operatorname{sum}(\text{target\_lengths}))`,</span>
<span class="sd">          where :math:`N = \text{batch size}` and</span>
<span class="sd">          :math:`S = \text{max target length, if shape is } (N, S)`.</span>
<span class="sd">          It represent the target sequences. Each element in the target</span>
<span class="sd">          sequence is a class index. And the target index cannot be blank (default=0).</span>
<span class="sd">          In the :math:`(N, S)` form, targets are padded to the</span>
<span class="sd">          length of the longest sequence, and stacked.</span>
<span class="sd">          In the :math:`(\operatorname{sum}(\text{target\_lengths}))` form,</span>
<span class="sd">          the targets are assumed to be un-padded and</span>
<span class="sd">          concatenated within 1 dimension.</span>
<span class="sd">        - Input_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represent the lengths of the</span>
<span class="sd">          inputs (must each be :math:`\leq T`). And the lengths are specified</span>
<span class="sd">          for each sequence to achieve masking under the assumption that sequences</span>
<span class="sd">          are padded to equal lengths.</span>
<span class="sd">        - Target_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represent lengths of the targets.</span>
<span class="sd">          Lengths are specified for each sequence to achieve masking under the</span>
<span class="sd">          assumption that sequences are padded to equal lengths. If target shape is</span>
<span class="sd">          :math:`(N,S)`, target_lengths are effectively the stop index</span>
<span class="sd">          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for</span>
<span class="sd">          each target in a batch. Lengths must each be :math:`\leq S`</span>
<span class="sd">          If the targets are given as a 1d tensor that is the concatenation of individual</span>
<span class="sd">          targets, the target_lengths must add up to the total length of the tensor.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(N)` if input is batched or :math:`()` if input is unbatched, where :math:`N = \text{batch size}`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # Target are to be padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16      # Batch size</span>
<span class="sd">        &gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch (padding length)</span>
<span class="sd">        &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Target are to be un-padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16      # Batch size</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Target are to be un-padded and unbatched (effectively N=1)</span>
<span class="sd">        &gt;&gt;&gt; T = 50      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.tensor(T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(target_lengths,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        A. Graves et al.: Connectionist Temporal Classification:</span>
<span class="sd">        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:</span>
<span class="sd">        https://www.cs.toronto.edu/~graves/icml_2006.pdf</span>

<span class="sd">    Note:</span>
<span class="sd">        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be</span>
<span class="sd">        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,</span>
<span class="sd">        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of</span>
<span class="sd">        dtype :attr:`torch.int32`.</span>

<span class="sd">        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.</span>


<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blank&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CTCLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blank</span> <span class="o">=</span> <span class="n">blank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span> <span class="o">=</span> <span class="n">zero_infinity</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span><span class="p">)</span>

<span class="c1"># TODO: L1HingeEmbeddingCriterion</span>
<span class="c1"># TODO: MSECriterion weight</span>
<span class="c1"># TODO: ClassSimplexCriterion</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
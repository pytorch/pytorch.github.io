


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.fsdp.fully_sharded_data_parallel &mdash; PyTorch 1.12 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.12 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.fsdp.fully_sharded_data_parallel</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.fsdp.fully_sharded_data_parallel</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">NamedTuple</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.distributed.utils</span> <span class="kn">import</span> <span class="n">_to_kwargs</span><span class="p">,</span> <span class="n">_sync_params_and_buffers</span><span class="p">,</span> <span class="n">_replace_by_prefix</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.distributed</span> <span class="kn">import</span> <span class="n">ProcessGroup</span>
<span class="kn">from</span> <span class="nn">torch.distributed._shard.sharded_tensor</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Shard</span><span class="p">,</span>
    <span class="n">ShardedTensor</span><span class="p">,</span>
    <span class="n">init_from_local_shards</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="kn">import</span> <span class="n">_get_default_group</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>

<span class="kn">from</span> <span class="nn">._optim_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_broadcast_pos_dim_tensor_states</span><span class="p">,</span>
    <span class="n">_broadcast_processed_optim_state_dict</span><span class="p">,</span>
    <span class="n">_flatten_full_optim_state_dict</span><span class="p">,</span>
    <span class="n">_get_flat_param_to_fsdp_module</span><span class="p">,</span>
    <span class="n">_get_param_id_to_param</span><span class="p">,</span>
    <span class="n">_get_param_to_param_id</span><span class="p">,</span>
    <span class="n">_process_pos_dim_tensor_state</span><span class="p">,</span>
    <span class="n">_unflatten_optim_state</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_apply_to_modules</span><span class="p">,</span> <span class="n">_apply_to_tensors</span><span class="p">,</span>
    <span class="n">_override_batchnorm_mixed_precision</span><span class="p">,</span> <span class="n">_contains_batchnorm</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.flatten_params_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FLAT_PARAM</span><span class="p">,</span>
    <span class="n">FPW_MODULE</span><span class="p">,</span>
    <span class="n">FlatParameter</span><span class="p">,</span>
    <span class="n">FlattenParamsWrapper</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.wrap</span> <span class="kn">import</span> <span class="n">_recursive_wrap</span><span class="p">,</span> <span class="n">_wrap_batchnorm_individually</span><span class="p">,</span> <span class="n">_or_policy</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>  <span class="c1"># noqa: F401</span>

<span class="n">_TORCHDISTX_AVAIL</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchdistx</span> <span class="kn">import</span> <span class="n">deferred_init</span><span class="p">,</span> <span class="n">fake</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_TORCHDISTX_AVAIL</span> <span class="o">=</span> <span class="kc">False</span>


<span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">=</span> <span class="s2">&quot;_fsdp_wrapped_module&quot;</span>
<span class="n">FSDP_PREFIX</span> <span class="o">=</span> <span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">FPW_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>

<span class="n">_PARAM_BROADCAST_BUCKET_SIZE</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_default_meta_device_init_fn</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Default initializer for modules initialized on the meta device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: move module to device_id here once device_id is available.</span>
    <span class="n">module</span><span class="o">.</span><span class="n">to_empty</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Unable to call reset_parameters() for module on meta device with error </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
            <span class="s2">&quot;Please ensure your module implements a ``reset_parameters`` function.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>


<span class="k">class</span> <span class="nc">ShardingStrategy</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specify which sharding strategy will be used for the distributed training.</span>
<span class="sd">    FULL_SHARD: Shards parameters, gradients and optimizer states. This algorithm</span>
<span class="sd">                inserts ``all_gather`` before forward and backward computation to gather</span>
<span class="sd">                parameters, also inserts ``reduce_scatter`` after backward computation for</span>
<span class="sd">                synchronizing and sharding gradients. Sharded optimizer states are</span>
<span class="sd">                updated locally.</span>
<span class="sd">    SHARD_GRAD_OP: Shard optimizer states and gradients, this algorithm inserts all_gather</span>
<span class="sd">                   before forward computation and keeps the full parameters in</span>
<span class="sd">                   GPU memory until backward computation is done. It inserts reduce_scater</span>
<span class="sd">                   after backward computation for synchronizing and sharding gradients.</span>
<span class="sd">                   Sharded optimizer states are updated locally.</span>
<span class="sd">    NO_SHARD: This is similar to PyTorch ``DistributedDataParallel`` API. Parameters, gradients</span>
<span class="sd">              and optimizer states are replicated among ranks, ``all_reduce`` is inserted after</span>
<span class="sd">              backward computation is done for synchronizing gradients. Full optimizer states</span>
<span class="sd">              are updated in each rank.</span>
<span class="sd">    HYBRID_SHARD(future support): apply FULL_SHARD algorithm in the intra node and</span>
<span class="sd">                                  apply NO_SHARD algorithm in the inter nodes.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">FULL_SHARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SHARD_GRAD_OP</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">NO_SHARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="c1"># TODO</span>
    <span class="c1"># HYBRID_SHARD = auto()</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">MixedPrecision</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A config to enable mixed precision training with FullyShardedDataParallel.</span>
<span class="sd">    This class can be constructed with three flags:</span>
<span class="sd">        ``param_dtype`` controls the precision of model parameters, inputs, and</span>
<span class="sd">        therefore the precision under which computation happens. After forward</span>
<span class="sd">        and backward passes, FSDP parameters point to full precision shards</span>
<span class="sd">        that are kept in memory. Full precision parameters are always</span>
<span class="sd">        checkpointed.</span>
<span class="sd">        ``reduce_dtype`` controls the precision under which gradient reduction</span>
<span class="sd">        would occur, which can potentially be different than ``param_dtype``</span>
<span class="sd">        for use cases such as communication efficiency.</span>
<span class="sd">        ``buffer_dtype`` controls the precision that buffers are cast to. Note</span>
<span class="sd">        that buffers are unsharded and are cast in the first forward pass, and</span>
<span class="sd">        remain in their reduced precision state even after forward/backward</span>
<span class="sd">        passes. However, when taking checkpoints with ``state_dict``, buffers</span>
<span class="sd">        are checkpointed in their full precision (and then restored back to</span>
<span class="sd">        to their reduced precision) as expected. Note that this checkpoint</span>
<span class="sd">        support is currently limited to ``StateDictType.FULL_STATE_DICT``.</span>

<span class="sd">    .. note:: In ``summon_full_params``, parameters are summoned in full</span>
<span class="sd">        precision but buffers are not.</span>

<span class="sd">    .. note:: Parameters and buffers are checkpointed in full precision. For</span>
<span class="sd">        buffers, this is only guaranteed to work for ``StateDictType.FULL_STATE_DICT``.</span>

<span class="sd">    .. note:: This API is experimental and subject to change.</span>

<span class="sd">    .. note:: Specification of reduced precision types must be explicit, in that</span>
<span class="sd">        if, for example, ``param_dtype`` is not specified, it will not be cast by</span>
<span class="sd">        FSDP. Thus, a config such as ``MixedPrecision(reduce_dtype=torch.float16)``</span>
<span class="sd">        will not cast buffers or parameters. Note that if a ``MixedPrecision``</span>
<span class="sd">        config is specified without a ``reduce_dtype``, gradient communication</span>
<span class="sd">        would occur in the `param_dtype` precision, if given, otherwise, in the</span>
<span class="sd">        original parameter precision.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># maintain a tensor of this dtype that the fp32 param shard will be cast to.</span>
    <span class="c1"># Will control the precision of model params, inputs, and thus compute as</span>
    <span class="c1"># well.</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Gradient communication precision.</span>
    <span class="n">reduce_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Buffer precision.</span>
    <span class="c1"># TODO: buffer + param are usually of the same type, if user specifies</span>
    <span class="c1"># param but not buffer, should we automatically make buffer be the same?</span>
    <span class="n">buffer_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CPUOffload</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CPU offloading config. Currently, only parameter and gradient CPU</span>
<span class="sd">    offload are supported.</span>
<span class="sd">    offload_params: Offloading parameters to CPUs when these parameters are</span>
<span class="sd">                    not used for computation on GPUs. This implicitly enables</span>
<span class="sd">                    gradient offloading to CPUs in order for parameters and</span>
<span class="sd">                    gradients to be on the same device to work with optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">offload_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">class</span> <span class="nc">BackwardPrefetch</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specify where to prefetch next layer&#39;s full parameters</span>
<span class="sd">    during backward pass.</span>
<span class="sd">    BACKWARD_PRE: prefetch right before current layer&#39;s backward computation</span>
<span class="sd">                  starts, this approach will increase backward communication</span>
<span class="sd">                  and computation overalpping and potentialy improve training</span>
<span class="sd">                  performance, but it may increase the peak memory usage as</span>
<span class="sd">                  the prefetched full parameters will be kept in the GPU memory</span>
<span class="sd">                  until next layer&#39;s backward computation is done.</span>
<span class="sd">    BACKWARD_POST: prefetch right after current layer&#39;s backward computation finishes,</span>
<span class="sd">                   this approach will not increase peak memory as prefetching happens</span>
<span class="sd">                   after current layer&#39;s full parameters are freed.</span>
<span class="sd">                   It could potentially improve backward communication and computation</span>
<span class="sd">                   overlapping as it avoids all_gather and reduce_scatter are blocked</span>
<span class="sd">                   each other in the single NCCL stream. However, based on our experiments,</span>
<span class="sd">                   for some models, the backward post backward hook fire order is not always</span>
<span class="sd">                   the reversed forward computation order, so this</span>
<span class="sd">                   approach may prefetch full parameters for layers ahead of next layer,</span>
<span class="sd">                   this &#39;ahead&#39; all_gather could delay next layer&#39;s all_gather in the</span>
<span class="sd">                   single NCCL stream and cause the next layer&#39;s computation delay. So it may</span>
<span class="sd">                   cause some performance regession for some models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BACKWARD_PRE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_POST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="c1"># TODO, BACKWARD_PRE_CPU, prefetch full parameters and keep them in the CPU memory</span>


<span class="k">class</span> <span class="nc">TrainingState_</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple enum to indicate what state FSDP is in. Used for asserting</span>
<span class="sd">    to make sure APIs are called in the correct state.</span>
<span class="sd">    ..note::</span>
<span class="sd">        ``BACKWARD_PRE`` and ``BACKWARD_POST`` states are used to ensure we</span>
<span class="sd">        receives backward hooks in the correct order. It is used to catch</span>
<span class="sd">        unexpected order of hooks being called (likely due to our</span>
<span class="sd">        hook registration logic or autograd engine logic changes).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">IDLE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">FORWARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_PRE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">BACKWARD_POST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SUMMON_FULL_PARAMS</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">StateDictType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This enum indicates that which type of ``state_dict`` the FSDP module is</span>
<span class="sd">    currently processing (returning or loading).</span>
<span class="sd">    The default value is FULL_STATE_DICT to comply the PyTorch convention.</span>
<span class="sd">    ..note::</span>
<span class="sd">        FSDP currently supports two types of ``state_dict``:</span>
<span class="sd">            1. ``state_dict/load_state_dict`: this pair of APIs return and load</span>
<span class="sd">               the non-sharded, unflattened parameters. The semantics is the</span>
<span class="sd">               same as using DDP.</span>
<span class="sd">            2. ``_local_state_dict/_load_local_state_dict``: this pair of APIs return</span>
<span class="sd">               and load local sharded, flattened parameters. The values returned</span>
<span class="sd">               by ``_local_state_dict`` can be directly used by FSDP and is only</span>
<span class="sd">               meaningful to FSDP (because parameters are flattened). Note that</span>
<span class="sd">               these APIs are meant for use via the :func:`state_dict_type`</span>
<span class="sd">               context manager as follows:</span>
<span class="sd">                   &gt;&gt;&gt; with fsdp.state_dict_type(StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">                   &gt;&gt;&gt;     state = fsdp.state_dict()  # loads local state dict</span>
<span class="sd">            3. ``_sharded_state_dict/_load_sharded_state_dict``: this pair of APIs</span>
<span class="sd">               return and load sharded, unflattened parameters. The ``state_dict``</span>
<span class="sd">               return by ``sharded_state_dict`` can be used by all other parallel</span>
<span class="sd">               schemes (resharding may be required).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FULL_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">LOCAL_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SHARDED_STATE_DICT</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StateDictConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ``StateDictConfig`` is the base class for all state_dict configuration classes.</span>
<span class="sd">    Users should instantiate a child version (i.e. ``FullStateDictConfig``) in</span>
<span class="sd">    order to configure settings for the particular type of ``state_dict``</span>
<span class="sd">    implementation FSDP will use.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">FullStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ``FullStateDictConfig`` is a config class meant to be used with</span>
<span class="sd">    ``StateDictType.FULL_STATE_DICT``. Currently, it accepts two parameters,</span>
<span class="sd">    ``offload_to_cpu`` and ``rank0_only`` which can be configured to offload</span>
<span class="sd">    the full ``state_dict`` to CPU and to materialize the ``state_dict`` on</span>
<span class="sd">    rank 0 only. When used, it is recommended to enable both of these flags</span>
<span class="sd">    together to optimize memory savings when taking checkpoints. Note that</span>
<span class="sd">    this config class is meant for user via the :func:`state_dict_type`</span>
<span class="sd">    context manager as follows:</span>
<span class="sd">        &gt;&gt;&gt; fsdp = FSDP(model, auto_wrap_policy=...)</span>
<span class="sd">        &gt;&gt;&gt; cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</span>
<span class="sd">        &gt;&gt;&gt; with FullyShardedDataParallel.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):</span>
<span class="sd">        &gt;&gt;&gt;     state = fsdp.state_dict()</span>
<span class="sd">        &gt;&gt;&gt;     # state will be empty on non rank 0 and contain CPU tensors on rank 0.</span>
<span class="sd">        &gt;&gt;&gt; # To reload checkpoint for inference, finetuning, transfer learning, etc:</span>
<span class="sd">        &gt;&gt;&gt; model = model_fn() # Initialize model on CPU in preparation for wrapping with FSDP</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Load checkpoint only on rank 0 to avoid memory redundancy</span>
<span class="sd">        &gt;&gt;&gt;     state_dict = torch.load(&quot;my_checkpoint.pt&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     model.load_state_dict(state_dict)</span>
<span class="sd">        &gt;&gt;&gt; # All ranks initialize FSDP module as usual. ``sync_module_states`` argument</span>
<span class="sd">        &gt;&gt;&gt; # communicates loaded checkpoint states from rank 0 to rest of the world.</span>
<span class="sd">        &gt;&gt;&gt; fsdp = FSDP(model, device_id=torch.cuda.current_device(), auto_wrap_policy=..., sync_module_states=True)</span>
<span class="sd">        &gt;&gt;&gt; # After this point, all ranks have FSDP model with loaded checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LocalStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ShardedStateDictConfig</span><span class="p">(</span><span class="n">StateDictConfig</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">_state_dict_type_to_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="n">FullStateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="n">LocalStateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="n">ShardedStateDictConfig</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">OptimStateKeyType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PARAM_NAME</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">PARAM_ID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_ExecOrderWarnStatus</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Used internally for execution order validation.&quot;&quot;&quot;</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>     <span class="c1"># no deviation yet</span>
    <span class="n">WARNING</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>  <span class="c1"># deviated this iteration; currently issuing warnings</span>
    <span class="n">WARNED</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>   <span class="c1"># deviated in a previous iteration</span>


<span class="k">class</span> <span class="nc">_ExecOrderData</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This contains the data used for validating execution order across ranks.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        _all_flat_params (List[FlatParameter]): A :class:`list` of all</span>
<span class="sd">            flattened parameters contained in the FSDP module hierarchy with</span>
<span class="sd">            the list index implicitly giving a unique parameter index.</span>
<span class="sd">        _param_to_unflat_param_names (Dict[FlatParameter, List[str]]): A</span>
<span class="sd">            mapping from flattened parameter to the comprising unflattened</span>
<span class="sd">            parameters&#39; names.</span>
<span class="sd">        is_first_iter (bool): Whether executing in the first iteration or not.</span>
<span class="sd">        param_order (List[int]): Order that parameters participate in the</span>
<span class="sd">            forward pass; constructed on the first iteration and validated</span>
<span class="sd">            against in subsequent iterations.</span>
<span class="sd">        index (int): Index tracking the position in ``param_order``</span>
<span class="sd">            when validating the forward pass execution order in subsequent</span>
<span class="sd">            iterations.</span>
<span class="sd">        warn_status (_ExecOrderWarnStatus): To avoid flooding the console, we</span>
<span class="sd">            only issue warnings throughout the first deviating iteration and no</span>
<span class="sd">            longer check thereafter; this tracks the warning status.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_flat_params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_unflat_param_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Modified in the first iteration:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_order</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Modified in the subsequent iterations:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span><span class="p">:</span> <span class="n">_ExecOrderWarnStatus</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">NONE</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root_module</span><span class="p">:</span> <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">root_module</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;This data structure should only be &quot;</span> \
            <span class="s2">&quot;initialized on an FSDP root module&quot;</span>
        <span class="c1"># Save `root_modules.parameters()` to `_all_flat_params` instead of</span>
        <span class="c1"># re-materializing each time to avoid the result depending on the</span>
        <span class="c1"># calling context (e.g. when some parameters have been rebuilt)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_flat_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">root_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_unflat_param_names</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">Dict</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
            <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span><span class="n">root_module</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># `root_module.parameters()` should only contain `FlatParameter`s</span>

    <span class="k">def</span> <span class="nf">get_param_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">FlatParameter</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a unique non-negative parameter index for ``param`` if it is</span>
<span class="sd">        valid or -1 otherwise. Critically, this index assignment must be the</span>
<span class="sd">        same across ranks.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">),</span> \
            <span class="sa">f</span><span class="s2">&quot;Expects `param` is a `FlatParameter` but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_flat_params</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="n">param</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">i</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span> <span class="nf">get_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the parameter corresponding to ``param_index`` or ``None``</span>
<span class="sd">        if the index is invalid.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_flat_params</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">param_index</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">p</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">get_unflat_param_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns a :class:`list` of unflattened parameter names comprising</span>
<span class="sd">        the flattened parameter with index ``param_index`` or an empty</span>
<span class="sd">        :class:`list` if ``param_index`` is invalid.&quot;&quot;&quot;</span>
        <span class="n">param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">param_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">assert</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_unflat_param_names</span><span class="p">,</span> \
            <span class="s2">&quot;Internal data structures out of sync; check `init()`&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called in :meth:`_wait_for_post_backward` to reset data for the next</span>
<span class="sd">        iteration.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iter</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># `reset()` marks the end of an iteration, so transition if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">==</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNING</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNED</span>


<div class="viewcode-block" id="FullyShardedDataParallel"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper for sharding Module parameters across data parallel workers. This</span>
<span class="sd">    is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.</span>
<span class="sd">    FullyShardedDataParallel is commonly shortened to FSDP.</span>

<span class="sd">    .. _`Xu et al.`: https://arxiv.org/abs/2004.13336</span>
<span class="sd">    .. _DeepSpeed: https://www.deepspeed.ai/</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">        &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">        &gt;&gt;&gt; optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span class="sd">        &gt;&gt;&gt; x = sharded_module(x, y=3, z=torch.Tensor([1]))</span>
<span class="sd">        &gt;&gt;&gt; loss = x.sum()</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; optim.step()</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The optimizer must be initialized *after* the module has been wrapped,</span>
<span class="sd">        since FSDP will shard parameters in-place and this will break any</span>
<span class="sd">        previously initialized optimizers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the destination CUDA device has ID ``dev_id``, either (1)</span>
<span class="sd">        ``module`` should already be placed on that device, (2) the device</span>
<span class="sd">        should be set using ``torch.cuda.set_device(dev_id)``, or (3)</span>
<span class="sd">        ``dev_id`` should be passed into the ``device_id`` constructor</span>
<span class="sd">        argument. This FSDP instance&#39;s compute device will be that destination</span>
<span class="sd">        device. For (1) and (3), the FSDP initialization always occurs on GPU.</span>
<span class="sd">        For (2), the FSDP initialization happens on ``module`` &#39;s current</span>
<span class="sd">        device, which may be CPU.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        FSDP currently does not support gradient accumulation outside</span>
<span class="sd">        ``no_sync()`` when using CPU offloading. Trying to do so yields</span>
<span class="sd">        incorrect results since FSDP will use the newly-reduced gradient</span>
<span class="sd">        instead of accumulating with any existing gradient.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Changing the original parameter variable names after construction will</span>
<span class="sd">        lead to undefined behavior.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Passing in `sync_module_states=True` flag requires module to be put</span>
<span class="sd">        on GPU, or to use ``device_id`` argument to specify a CUDA device that</span>
<span class="sd">        FSDP will move module to. This is because ``sync_module_states=True``</span>
<span class="sd">        requires GPU communication.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        As of PyTorch 1.12, FSDP only offers limited support for shared parameters</span>
<span class="sd">        (for example, setting one ``Linear`` layer&#39;s weight to another&#39;s). In</span>
<span class="sd">        particular, modules that share parameters must be wrapped as part of the</span>
<span class="sd">        same FSDP unit. If enhanced shared parameter support is needed for your</span>
<span class="sd">        use case, please ping https://github.com/pytorch/pytorch/issues/77724</span>

<span class="sd">    .. note::</span>
<span class="sd">        Inputs into FSDP ``forward`` function will be moved to compute device</span>
<span class="sd">        (same device FSDP module is on) before running ``forward``, so user does</span>
<span class="sd">        not have to manually move inputs from CPU -&gt; GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module):</span>
<span class="sd">            module to be wrapped with FSDP.</span>
<span class="sd">        process_group (Optional[ProcessGroup]):</span>
<span class="sd">            process group for sharding</span>
<span class="sd">        sharding_strategy (Optional[ShardingStrategy]):</span>
<span class="sd">            Config sharding algorithm, different sharding algorithm has trade</span>
<span class="sd">            off between memory saving and communication overhead. ``FULL_SHARD``</span>
<span class="sd">            will be chosen if sharding_strategy is not specified.</span>
<span class="sd">        cpu_offload (Optional[CPUOffload]):</span>
<span class="sd">            CPU offloading config. Currently, only parameter and gradient CPU</span>
<span class="sd">            offload is supported. It can be enabled via passing in</span>
<span class="sd">            ``cpu_offload=CPUOffload(offload_params=True)``. Note that this</span>
<span class="sd">            currently implicitly enables gradient offloading to CPU in order for</span>
<span class="sd">            params and grads to be on same device to work with optimizer. This</span>
<span class="sd">            API is subject to change. Default is ``None`` in which case there</span>
<span class="sd">            will be no offloading.</span>
<span class="sd">        auto_wrap_policy (Optional[Callable]):</span>
<span class="sd">            A callable specifying a policy to recursively wrap layers with FSDP.</span>
<span class="sd">            Note that this policy currently will only apply to child modules of</span>
<span class="sd">            the passed in module. The remainder modules are always wrapped in</span>
<span class="sd">            the returned FSDP root instance.</span>
<span class="sd">            ``size_based_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is</span>
<span class="sd">            an example of ``auto_wrap_policy`` callable, this policy wraps layers</span>
<span class="sd">            with the number of parameters larger than 100M. ``transformer_auto_wrap_policy``</span>
<span class="sd">            written in ``torch.distributed.fsdp.wrap`` is an example of ``auto_wrap_policy``</span>
<span class="sd">            callable for tranformer-like model architectures. Users can supply the customized</span>
<span class="sd">            ``auto_wrap_policy`` callable that should accept following arguments:</span>
<span class="sd">            ``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``,</span>
<span class="sd">            extra customized arguments could be added to the customized</span>
<span class="sd">            ``auto_wrap_policy`` callable as well. It is a good practice to print out</span>
<span class="sd">            the sharded model and check whether the sharded model is what</span>
<span class="sd">            the application wants and then adjust accordingly.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; def custom_auto_wrap_policy(</span>
<span class="sd">                &gt;&gt;&gt;     module: nn.Module,</span>
<span class="sd">                &gt;&gt;&gt;     recurse: bool,</span>
<span class="sd">                &gt;&gt;&gt;     unwrapped_params: int,</span>
<span class="sd">                &gt;&gt;&gt;     # These are customizable for this policy function.</span>
<span class="sd">                &gt;&gt;&gt;     min_num_params: int = int(1e8),</span>
<span class="sd">                &gt;&gt;&gt; ) -&gt; bool:</span>
<span class="sd">                &gt;&gt;&gt;     return unwrapped_params &gt;= min_num_params</span>

<span class="sd">        backward_prefetch (Optional[BackwardPrefetch]):</span>
<span class="sd">            This is an experimental feature that is subject to change in the</span>
<span class="sd">            the near future. It allows users to enable two different backward_prefetch</span>
<span class="sd">            algorithms to help backward communication and computation overlapping.</span>
<span class="sd">            Pros and cons of each algorithm is explained in the class ``BackwardPrefetch``.</span>
<span class="sd">        mixed_precision (Optional[MixedPrecision]): A ``MixedPrecision`` instance</span>
<span class="sd">            describing the mixed precision training config to be used. ``MixedPrecision``</span>
<span class="sd">            supports configuring parameter, buffer, and gradient communication dtype. Note</span>
<span class="sd">            that only floating point data is cast to the reduced precision. This allows</span>
<span class="sd">            users potential memory saving and training speedup while trading off</span>
<span class="sd">            accuracy during model training. If ``None``, no mixed precision is applied.</span>
<span class="sd">            Note that if ``mixed_precision`` is enabled for FSDP model that</span>
<span class="sd">            contains ``BatchNorm`` with ``auto_wrap_policy``, FSDP will take</span>
<span class="sd">            care to disable mixed precision for ``BatchNorm`` units by wrapping</span>
<span class="sd">            them separately in their own FSDP unit with ``mixed_precision=None``.</span>
<span class="sd">            This is done because several ``BatchNorm`` kernels do not implement</span>
<span class="sd">            reduced type support at the moment. If individually wrapping the model,</span>
<span class="sd">            users must take care to set ``mixed_precision=None`` for</span>
<span class="sd">            ``BatchNorm`` units.</span>
<span class="sd">            (Default: ``None``)</span>
<span class="sd">        ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose</span>
<span class="sd">            own parameters and child modules&#39; parameters and buffers are</span>
<span class="sd">            ignored by this instance. None of the modules directly in</span>
<span class="sd">            ``ignored_modules`` should be :class:`FullyShardedDataParallel`</span>
<span class="sd">            instances, and any child modules that are already-constructed</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances will not be ignored if</span>
<span class="sd">            they are nested under this instance. This argument may be used to</span>
<span class="sd">            avoid sharding specific parameters when using an</span>
<span class="sd">            ``auto_wrap_policy`` or if parameters&#39; sharding is not managed by</span>
<span class="sd">            FSDP. (Default: ``None``)</span>
<span class="sd">        param_init_fn (Optional[Callable[[nn.Module], None]]):</span>
<span class="sd">            A ``Callable[torch.nn.Module] -&gt; None`` that</span>
<span class="sd">            specifies how modules that are currently on the meta device should be initialized</span>
<span class="sd">            onto an actual device. Note that as of v1.12, we detect modules on the meta</span>
<span class="sd">            device via ``is_meta`` check and apply a default initialization that calls</span>
<span class="sd">            ``reset_parameters`` method on the passed in ``nn.Module`` if ``param_init_fn``</span>
<span class="sd">            is not specified, otherwise we run ``param_init_fn`` to initialize the passed</span>
<span class="sd">            in ``nn.Module``. In particular, this means that if ``is_meta=True`` for any</span>
<span class="sd">            module parameters for modules that will be wrapped with FSDP and ``param_init_fn``</span>
<span class="sd">            is not specified, we assume your module properly implements a ``reset_paramters()``</span>
<span class="sd">            and will throw errors if not. Note that additionally, we offer support for modules</span>
<span class="sd">            initialized with torchdistX&#39;s (https://github.com/pytorch/torchdistX)</span>
<span class="sd">            ``deferred_init`` API. In this case, deferred modules would be initialized</span>
<span class="sd">            by a default initialization function that calls torchdistX&#39;s</span>
<span class="sd">            ``materialize_module``, or the passed in ``param_init_fn``, if it is not</span>
<span class="sd">            ``None``. The same ``Callable`` is applied to initialize all meta modules.</span>
<span class="sd">            Note that this initialization function is applied before doing any FSDP sharding</span>
<span class="sd">            logic.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; module = MyModule(device=&quot;meta&quot;)</span>
<span class="sd">                &gt;&gt;&gt; def my_init_fn(module):</span>
<span class="sd">                &gt;&gt;&gt;     # responsible for initializing a module, such as with reset_parameters</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)</span>
<span class="sd">                &gt;&gt;&gt; print(next(fsdp_model.parameters()).device) # current CUDA device</span>
<span class="sd">                &gt;&gt;&gt; # With torchdistX</span>
<span class="sd">                &gt;&gt;&gt; module = deferred_init.deferred_init(MyModule, device=&quot;cuda&quot;)</span>
<span class="sd">                &gt;&gt;&gt; # Will initialize via deferred_init.materialize_module().</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)</span>

<span class="sd">        device_id (Optional[Union[int, torch.device]]): An ``int`` or ``torch.device``</span>
<span class="sd">            describing the CUDA device the FSDP module should be moved to determining where</span>
<span class="sd">            initialization such as sharding takes place. If this argument is not specified</span>
<span class="sd">            and ``module`` is on CPU, we will move ``module`` to current CUDA device for faster</span>
<span class="sd">            initialization and move ``module`` back to CPU before returning.</span>
<span class="sd">            If specified, resulting FSDP instances will reside on this device.</span>
<span class="sd">            Note that if ``device_id`` is specified but ``module`` is already</span>
<span class="sd">            on a different CUDA device, an error will be thrown. (Default: ``None``)</span>

<span class="sd">        sync_module_states (bool): If ``True``, each individually wrapped FSDP unit will broadcast</span>
<span class="sd">            module parameters from rank 0 to ensure they are the same across all ranks after</span>
<span class="sd">            initialization. This helps ensure model parameters are the same across ranks</span>
<span class="sd">            before starting training, but adds communication overhead to ``__init__``, as at least</span>
<span class="sd">            one broadcast is triggered per individually wrapped FSDP unit.</span>
<span class="sd">            This can also help load checkpoints taken by ``state_dict`` and to be loaded by</span>
<span class="sd">            ``load_state_dict`` in a memory efficient way. See documentation for</span>
<span class="sd">            :class:`FullStateDictConfig` for an example of this. (Default: ``False``)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShardingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cpu_offload</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CPUOffload</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackwardPrefetch</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MixedPrecision</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignored_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">param_init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sync_module_states</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.distributed.fsdp&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Validate the ignored modules and derive the ignored parameters/buffers</span>
        <span class="n">ignored_modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ignored_modules</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_modules</span><span class="p">)</span>
        <span class="n">ignored_params</span><span class="p">,</span> <span class="n">ignored_param_names</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_ignored_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">ignored_modules</span><span class="p">)</span>
        <span class="n">buffer_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_buffer_names</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="c1"># Compute the names to ignore for full state dict cloning (i.e. those</span>
        <span class="c1"># of the ignored modules&#39; parameters and of all modules&#39; buffers)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_param_names</span> <span class="o">=</span> <span class="n">ignored_param_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_names</span> <span class="o">=</span> <span class="n">buffer_names</span>
        <span class="c1"># NOTE: Since the names are computed at construction time, if the user</span>
        <span class="c1"># changes them later, then FSDP will not properly ignore them. However,</span>
        <span class="c1"># the `FlatParameter` implementation already relies on this assumption.</span>
        <span class="c1"># We do this at construction time since we want the fully prefixed</span>
        <span class="c1"># parameter names matching the keys in the model state dict (namely,</span>
        <span class="c1"># including the wrapped module&#39;s name in the prefix), which may be done</span>
        <span class="c1"># most non-intrusively here before flattening.</span>

        <span class="c1"># if auto_wrap_policy is specified, submodules should not be</span>
        <span class="c1"># already wrapped, otherwise we&#39;d attempt to double wrap them resulting</span>
        <span class="c1"># in errors.</span>
        <span class="k">if</span> <span class="n">auto_wrap_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_wrapped</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span>
                <span class="n">check_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mod</span><span class="p">:</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">),</span>
                <span class="n">err_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mod</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">mod</span><span class="si">}</span><span class="s2"> to NOT be FullyShardedDataParallel if auto_wrap is enabled.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">_contains_batchnorm</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">_override_batchnorm_mixed_precision</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                <span class="n">policy_to_use</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_or_policy</span><span class="p">,</span>
                    <span class="n">policies</span><span class="o">=</span><span class="p">[</span><span class="n">_wrap_batchnorm_individually</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Mixed precision was specified for FSDP module with&quot;</span>
                    <span class="s2">&quot; batchnorm submodules wrapped via ``auto_wrap_policy``.&quot;</span>
                    <span class="s2">&quot; BatchNorm units will be wrapped as a separate FSDP unit,&quot;</span>
                    <span class="s2">&quot; with mixed_precision disabled (i.e. set to ``None``)&quot;</span>
                    <span class="s2">&quot; as several BatchNorm kernels would raise errors when&quot;</span>
                    <span class="s2">&quot; operating on reduced precision inputs.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">policy_to_use</span> <span class="o">=</span> <span class="n">auto_wrap_policy</span>
            <span class="n">_recursive_wrap</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span>
                <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">policy_to_use</span><span class="p">,</span>
                <span class="n">wrapper_cls</span><span class="o">=</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span>
                <span class="n">ignored_modules</span><span class="o">=</span><span class="n">ignored_modules</span><span class="p">,</span>
                <span class="n">ignored_params</span><span class="o">=</span><span class="n">ignored_params</span><span class="p">,</span>
                <span class="c1"># Note that we have the recursive_wrap skip wrapping for</span>
                <span class="c1"># the outermost (this) module otherwise it will result in a</span>
                <span class="c1"># double-wrap causing issues.</span>
                <span class="n">only_wrap_children</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="c1"># FSDP arguments follow.</span>
                <span class="n">process_group</span><span class="o">=</span><span class="n">process_group</span><span class="p">,</span>
                <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">sharding_strategy</span><span class="p">,</span>
                <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
                <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">backward_prefetch</span><span class="p">,</span>
                <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mixed_precision</span><span class="p">,</span>
                <span class="n">param_init_fn</span><span class="o">=</span><span class="n">param_init_fn</span><span class="p">,</span>
                <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
                <span class="n">sync_module_states</span><span class="o">=</span><span class="n">sync_module_states</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">device_id</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_id</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># If user passed in something like torch.device(&quot;cuda&quot;),</span>
            <span class="c1"># device index of current device is unclear, make it explicit.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Passed in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="si">}</span><span class="s2"> does not have explicit index, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;setting it to current index: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;If this is not correct, please explicitly call torch.cuda.set_device()&quot;</span>
                    <span class="s2">&quot;before FSDP initialization or pass in explicit device index as device_id argument.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="kc">None</span>


        <span class="n">is_meta_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">is_meta</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">is_torchdistX_deferred_init</span> <span class="o">=</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">is_meta_module</span> <span class="ow">and</span> <span class="n">_TORCHDISTX_AVAIL</span>
            <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">is_fake</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">_run_param_init_fn</span><span class="p">():</span>
            <span class="c1"># Call user-specified initialization function.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">param_init_fn</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">param_init_fn</span><span class="si">}</span><span class="s2"> to be callable, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">param_init_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">param_init_fn</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_meta_module</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">param_init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_run_param_init_fn</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Call default initialization function that is dependent on</span>
                <span class="c1"># reset_parameters.</span>
                <span class="n">_default_meta_device_init_fn</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_torchdistX_deferred_init</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">_TORCHDISTX_AVAIL</span><span class="p">,</span> <span class="s2">&quot;Got torchdistX initialized module but torchdistX lib is not available.&quot;</span>
            <span class="k">if</span> <span class="n">param_init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_run_param_init_fn</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Call default torchdistX initialization function. Omit re-initialization of FSDP submodules</span>
                <span class="c1"># which is unnecessary.</span>
                <span class="n">check_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span>  <span class="c1"># noqa: E731</span>
                <span class="n">deferred_init</span><span class="o">.</span><span class="n">materialize_module</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">check_fn</span><span class="o">=</span><span class="n">check_fn</span><span class="p">)</span>

        <span class="c1"># Check that module was placed onto a single device.</span>
        <span class="n">module_devices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;FSDP only supports single device modules, but got params on </span><span class="si">{</span><span class="n">module_devices</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Move module appropriately depending on device_id and whether module is on CPU.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_move_module_if_needed</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># device for computation, if module is on GPU, use module.device;</span>
        <span class="c1"># if module is on CPU, use current device;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span> <span class="o">=</span> <span class="n">_get_default_cuda_device</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># if device_id is specified, ensure it is the same</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Inconsistent compute_device and device_id: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># Enum to indicate if we&#39;re in the forward/backward pass, idle, etc.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>

        <span class="c1"># setting two factors to avoid underflow and overflow</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gradient_predivide_factor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_postdivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">numel_padded_per_param</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">cpu_offload</span> <span class="ow">or</span> <span class="n">CPUOffload</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">=</span> <span class="n">backward_prefetch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">=</span> <span class="n">sharding_strategy</span> <span class="ow">or</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">mixed_precision</span>
        <span class="c1"># Original buffer type (mapping since all buffers may not be of same type). In</span>
        <span class="c1"># the case of mixed precision training, this is used to restore buffers</span>
        <span class="c1"># to their original type (which may not be the same as that of the</span>
        <span class="c1"># parameters in the model) when checkpointing.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_orig_buffer_dtypes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Only handle params which are not already sharded. This enables</span>
        <span class="c1"># sharding individual layers of a Module, with an outer wrapper to</span>
        <span class="c1"># shard any leftover parameters.</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">sync_module_states</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">params</span> <span class="o">!=</span> <span class="p">[]</span> <span class="ow">and</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Module has CPU parameters, but sync_module_states=True is specified.&quot;</span>
                    <span class="s2">&quot;This only works for GPU module, please specify `device_id` argument or move&quot;</span>
                    <span class="s2">&quot; module to GPU before init.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Collect buffers we have to synchronize, avoiding buffers that have already</span>
            <span class="c1"># been synchronized to avoid redundant synchronization.</span>
            <span class="n">bufs_to_sync</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="s1">&#39;_fsdp_has_been_sync&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="n">buf</span><span class="o">.</span><span class="n">_fsdp_has_been_sync</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">bufs_to_sync</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

            <span class="n">states_to_sync</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
            <span class="n">states_to_sync</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bufs_to_sync</span><span class="p">)</span>
            <span class="n">_sync_params_and_buffers</span><span class="p">(</span>
                <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                <span class="n">module_states</span><span class="o">=</span><span class="n">states_to_sync</span><span class="p">,</span>
                <span class="c1"># Same bucket size as DDP</span>
                <span class="n">broadcast_bucket_size</span><span class="o">=</span><span class="n">_PARAM_BROADCAST_BUCKET_SIZE</span><span class="p">,</span>
                <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">:</span> <span class="n">FlattenParamsWrapper</span> <span class="o">=</span> <span class="n">FlattenParamsWrapper</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="n">param_list</span><span class="o">=</span><span class="n">params</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">FSDP_WRAPPED_MODULE</span><span class="p">)</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span>
        <span class="k">del</span> <span class="n">module</span>  <span class="c1"># free original module in case it helps garbage collection</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="n">flat_param</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Shard module parameters in place</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shard_parameters</span><span class="p">()</span>

        <span class="c1"># Make sure all parameters are sharded.</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignored_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;found unflattened parameter: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> ; </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_lazy_init</span><span class="p">()</span>

        <span class="c1"># Flag indicating if we require gradient reduction in the backward</span>
        <span class="c1"># pass (set to `False` in the `no_sync()` context manager)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">FullStateDictConfig</span><span class="p">()</span>

        <span class="c1"># FSDP currently provides three different state_dicts. The actual</span>
        <span class="c1"># state_dict that will be saved/loaded is decided by</span>
        <span class="c1"># self._state_dict_type. And the main logic of each state_dict is</span>
        <span class="c1"># implemented in the hook. Therefore, for each hook (post-save and</span>
        <span class="c1"># pre-load), there is a dispatcher dictionary to dispatch the execution</span>
        <span class="c1"># flow to the correct implementation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_post_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_post_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_post_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_load_state_dict_pre_hook</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook</span><span class="p">,</span> <span class="n">with_module</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_pre_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_pre_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_pre_load_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_load_state_dict_post_hook</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook_fn</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_post_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_post_load_state_dict_hook</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sharded_post_load_state_dict_hook</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Flag to guard against preparing gradients multiple times per backward pass.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_backward_hook_has_run</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Used for prefetching all gather full params in post backward hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_need_rebuild_full_params</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># If specified, offload parameter shard to CPU.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_offload_to_cpu</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># For validating execution order across ranks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span> <span class="o">=</span> <span class="n">_ExecOrderData</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_move_module_if_needed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Moves module appropriately depending on device_id and</span>
<span class="sd">        whether module is on CPU. Returns a ``bool`` indicating</span>
<span class="sd">        whether the module needs to be moved back to CPU before</span>
<span class="sd">        returning to user.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Move module to device specified. Note that this is done prior to</span>
        <span class="c1"># setting compute_device to ensure that they align.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">param</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Get the next unflat param</span>
                <span class="n">param_gen</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
                <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">param_gen</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">):</span>
                        <span class="k">break</span>

                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="c1"># this FSDP instance manages no parameters.</span>
                <span class="k">pass</span>

            <span class="c1"># For GPU modules, module device should match device_id.</span>
            <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Module on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> is given device_id argument &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="si">}</span><span class="s2">, but is on </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot; Either move module before FSDP init or omit device_id argument.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># device_id argument is not specified</span>
            <span class="c1"># If module is on CPU, log a warning asking user to use `device_id` for faster</span>
            <span class="c1"># GPU init.</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Get the next unflat param</span>
                <span class="n">param_gen</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
                <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">param_gen</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">):</span>
                        <span class="k">break</span>

                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;Module is put on CPU and will thus have flattening and sharding&quot;</span>
                        <span class="s2">&quot; run on CPU, which is less efficient than on GPU. We recommend passing in &quot;</span>
                        <span class="s2">&quot;`device_id` argument which will enable FSDP to put module on GPU device,&quot;</span>
                        <span class="s2">&quot; module must also be on GPU device to work with `sync_module_states=True` flag&quot;</span>
                        <span class="s2">&quot; which requires GPU communication.&quot;</span>
                    <span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="c1"># this FSDP instance manages no parameters</span>
                <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_init_reshard_after_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">:</span>
            <span class="c1"># Free full params and keep shard only after forward</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">SHARD_GRAD_OP</span><span class="p">:</span>
            <span class="c1"># Keep full params in the GPU memory until backward</span>
            <span class="c1"># computation is done</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">:</span>
            <span class="c1"># self.reshard_after_forward is not used when NO_SHARD</span>
            <span class="c1"># is set, just setting it as False here</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;sharding_strategy only supports FULL_SHARD, SHARD_GRAD_OP and NO_SHARD right now.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_ignored_modules</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">root_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">_ignored_modules</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks that ``_ignored_modules`` (1) is an iterable of</span>
<span class="sd">        ``torch.nn.Module`` s without any :class:`FullyShardedDataParallel`</span>
<span class="sd">        instances and does not contain the top-level ``module`` itself, and</span>
<span class="sd">        then returns them and their children as a :class:`set`, excluding</span>
<span class="sd">        nested :class:`FullyShardedDataParallel` instances.</span>

<span class="sd">        We include the child modules of modules in ``_ignored_modules`` to be</span>
<span class="sd">        more intuitive since ignoring a module should ignore its child modules</span>
<span class="sd">        as well, and we exclude :class:`FullyShardedDataParallel` instances</span>
<span class="sd">        since ``self`` may be the intended root instance that manages them.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_ignored_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">msg_prefix</span> <span class="o">=</span> <span class="s2">&quot;`ignored_modules` should be an iterable of &quot;</span> \
            <span class="s2">&quot;`torch.nn.Module`s &quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ignored_root_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">_ignored_modules</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg_prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">_ignored_modules</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">ignored_root_modules</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="n">msg_prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;but got an iterable with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`ignored_modules` should not include FSDP modules&quot;</span>
                <span class="p">)</span>
        <span class="c1"># Include child modules and exclude nested FSDP modules</span>
        <span class="n">ignored_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">child</span> <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">ignored_root_modules</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="ow">and</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">FlattenParamsWrapper</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">root_module</span> <span class="ow">in</span> <span class="n">ignored_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Trying to ignore the top-level module passed into the FSDP &quot;</span>
                <span class="s2">&quot;constructor itself will result in all parameters being &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;ignored and is not supported: </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">ignored_modules</span>

    <span class="k">def</span> <span class="nf">_get_ignored_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">root_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">ignored_modules</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the parameters of the modules in ``ignored_modules``,</span>
<span class="sd">        excluding any :class:`FlatParameter` s and their fully prefixed names,</span>
<span class="sd">        both as :class:`set` s.</span>

<span class="sd">        Args:</span>
<span class="sd">            root_module (torch.nn.Module): Top-level module passed into the</span>
<span class="sd">                FSDP constructor from which to derive the fully prefixed names.</span>
<span class="sd">            ignored_modules (Set[torch.nn.Module]): Modules to ignore.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ignored_params</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">p</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">ignored_modules</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">param_to_unflat_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span>
            <span class="n">root_module</span><span class="p">,</span> <span class="n">dedup_shared_params</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ignored_param_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ignored_params</span><span class="p">:</span>
            <span class="n">unflat_param_names</span> <span class="o">=</span> <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
            <span class="n">ignored_param_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">unflat_param_names</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ignored_params</span><span class="p">,</span> <span class="n">ignored_param_names</span>

    <span class="k">def</span> <span class="nf">_get_buffer_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root_module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the fully prefixed names of all buffers in the module hierarchy</span>
<span class="sd">        rooted at ``root_module`` as a class:`set`.</span>

<span class="sd">        Args:</span>
<span class="sd">            root_module (torch.nn.Module): Top-level module passed into the</span>
<span class="sd">                FSDP constructor from which to derive the fully prefixed names.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">module_fn</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">buffer_names</span><span class="p">):</span>
            <span class="c1"># For FSDP modules, only add the entry when considering the</span>
            <span class="c1"># contained `FlattenParamsWrapper` to avoid duplication</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                    <span class="n">prefixed_buffer_name</span> <span class="o">=</span> <span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">buffer_name</span><span class="p">)</span>
                    <span class="n">buffer_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">prefixed_buffer_name</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">return_fn</span><span class="p">(</span><span class="n">buffer_names</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">buffer_names</span>

        <span class="n">buffer_names</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">_apply_to_modules</span><span class="p">(</span>
            <span class="n">root_module</span><span class="p">,</span> <span class="n">module_fn</span><span class="p">,</span> <span class="n">return_fn</span><span class="p">,</span> <span class="n">buffer_names</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_check_wrapped</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">begin_module</span><span class="p">,</span> <span class="n">check_fn</span><span class="p">,</span> <span class="n">err_fn</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">begin_module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">check_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">err_fn</span><span class="p">(</span><span class="n">mod</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FlattenParamsWrapper</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;make model.module accessible, just like DDP.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">FlattenParamsWrapper</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span>

    <span class="k">def</span> <span class="nf">check_is_root</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>

<div class="viewcode-block" id="FullyShardedDataParallel.fsdp_modules"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">fsdp_modules</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">root_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all nested FSDP instances, possibly including ``module`` itself</span>
<span class="sd">        and only including FSDP root modules if ``root_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module, which may or may not be an</span>
<span class="sd">                ``FSDP`` module.</span>
<span class="sd">            root_only (bool): Whether to return only FSDP root modules.</span>
<span class="sd">                (Default: ``False``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[FullyShardedDataParallel]: FSDP modules that are nested in</span>
<span class="sd">            the input ``module``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">submodule</span> <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="ow">and</span>
            <span class="p">(</span><span class="ow">not</span> <span class="n">root_only</span> <span class="ow">or</span> <span class="n">submodule</span><span class="o">.</span><span class="n">check_is_root</span><span class="p">())</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.apply"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.apply">[docs]</a>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">        as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">        (see also :ref:`nn-init-doc`).</span>

<span class="sd">        Compared to ``torch.nn.Module.apply``, this version additionally gathers</span>
<span class="sd">        the full parameters before applying ``fn``. It should not be called from</span>
<span class="sd">        within another ``summon_full_params`` context.</span>

<span class="sd">        Args:</span>
<span class="sd">            fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>

<span class="sd">        Returns:</span>
<span class="sd">            Module: self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">uninitialized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="c1"># Reset lazy init that might be called by _summon_full_params, since</span>
        <span class="c1"># it could have set is_root incorrectly for non-root FSDP instances.</span>
        <span class="k">if</span> <span class="n">uninitialized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_reset_lazy_init</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ret</span></div>

    <span class="c1"># setting two factors &#39;self.gradient_predivide_factor&#39;</span>
    <span class="c1"># and &#39;self.gradient_postdivide_factor&#39; to avoid underflow and overflow</span>
    <span class="k">def</span> <span class="nf">_get_gradient_predivide_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">world_size</span> <span class="o">%</span> <span class="n">factor</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">world_size</span> <span class="o">/</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="n">factor</span><span class="p">:</span>
            <span class="n">factor</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">factor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_offload_to_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Offloads parameter to CPU from self.compute_device. If the parameter is</span>
<span class="sd">        already on CPU then this is a noop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cpu_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">cpu_device</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        parameters or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        buffers or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">buffer_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether user explicitly enabled mixed precision for</span>
<span class="sd">        gradient reduction or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">reduce_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_cast_fp_inputs_to_precision</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Casts floating point tensors in args and kwargs to precision given by dtype.</span>
<span class="sd">        requires_grad field is respected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">cast_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="c1"># Explicitly copy over requires_grad context since this is happening</span>
            <span class="c1"># within torch.no_grad.</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
                <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="k">return</span> <span class="n">y</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span>
                <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_cast_param_shards_to_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Allocates a mixed precision paramter shard and casts parameter shards to</span>
<span class="sd">        reduced precision by copying into this mixed precision shard. Note that</span>
<span class="sd">        if we are CPU offloading, this also implicitly loads the parameter shard</span>
<span class="sd">        back to GPU.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Expected to only be called when mixed precision for parameters is enabled.&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;mixed_precision_params&quot;</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">_alloc_storage</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                <span class="c1"># Cast is done by copy</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
                    <span class="c1"># no-op if not CPU offloading, otherwise nonblocking because</span>
                    <span class="c1"># p._local_shard is pinned in _init_param_attributes.</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="c1"># Point p to the mp shard</span>
                <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span>
        <span class="c1"># Block current stream on this copy work.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;mixed_precision_params&quot;</span><span class="p">])</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_free_mp_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deallocate storage for parameter&#39;s mixed precision shard.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Expected to only be called when mixed precision for parameters is enabled.&quot;</span>
        <span class="n">current_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># mp_shard should always be allocated.</span>
            <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="c1"># Shard is allocated in &quot;mixed_precision_stream&quot; and then we block</span>
            <span class="c1"># current stream on this stream, so don&#39;t free it until work in the</span>
            <span class="c1"># current stream is completed.</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_cast_buffers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Move all buffers to the given *device* and *dtype*.</span>
<span class="sd">        If *device* is not given, then it will default to</span>
<span class="sd">        ``self.compute_device``, otherwise buffer will be moved to ``device``.</span>
<span class="sd">        In the case of nested FSDP instances, we will respect the child instance&#39;s</span>
<span class="sd">        ``compute_device`` configuration.</span>
<span class="sd">        If *dtype* is given, it must be a mapping of buffer name to buffer dtype,</span>
<span class="sd">            and this argument is currently only given to restore back to original</span>
<span class="sd">            buffer types during checkpoint. If *dtype* is not given, and we are</span>
<span class="sd">            in mixed precision training, the buffer will be cast to buffer_dtype,</span>
<span class="sd">            otherwise the buffer will not be cast.</span>
<span class="sd">        Args:</span>
<span class="sd">            device (torch.device, Optional):</span>
<span class="sd">                device to cast buffers to (defaults to compute_device)</span>
<span class="sd">            dtype: (Dict[str, torch.dtype], Optional):</span>
<span class="sd">                Mapping of buffer name to their dtype to cast to.</span>
<span class="sd">            memo (Set, Optional):</span>
<span class="sd">                set of modules that have already been processed</span>
<span class="sd">            recurse (bool, Optional):</span>
<span class="sd">                Whether to call _cast_buffers recursively on nested FSDP</span>
<span class="sd">                instances (default is True).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="ow">and</span> <span class="n">recurse</span><span class="p">:</span>
                <span class="c1"># Allow any child FSDP instances to handle their own buffers.</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">memo</span><span class="o">=</span><span class="n">memo</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">buf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_orig_buffer_dtypes</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_orig_buffer_dtypes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="c1"># If given, cast buffer to the given dtype. This is used to</span>
                    <span class="c1"># suppport mixed precision for buffers</span>
                    <span class="c1"># (given by self.mixed_precision.buffer_dtype) and also used</span>
                    <span class="c1"># to restore the buffer dtype to the original precision for</span>
                    <span class="c1"># state_dict() calls.</span>
                    <span class="c1"># Note that non-floating point buffers are not casted.</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">buf</span><span class="p">):</span>
                        <span class="c1"># We are restoring the original buffer type in</span>
                        <span class="c1"># preparation for checkpoint.</span>
                        <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
                            <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
                        <span class="c1"># Note that we don&#39;t pass in self.mixed_precision.buffer_dtype</span>
                        <span class="c1"># recursively into _cast_buffers, as we want to respect</span>
                        <span class="c1"># mp config for child FSDP instances.</span>
                        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">():</span>
                            <span class="n">buf</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">buffer_dtype</span><span class="p">)</span>

                    <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_shard_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        At initialization we wrap a module with full parameters and shard the</span>
<span class="sd">        parameters in-place. Sharding is implemented by viewing each parameter</span>
<span class="sd">        as a 1D Tensor and retaining only a single slice, where the slice size</span>
<span class="sd">        is determined by the number of data parallel workers.</span>
<span class="sd">        After this initial sharding is complete, the user can initialize a</span>
<span class="sd">        ``torch.optim.Optimizer`` in the usual way, i.e.::</span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">            optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span class="sd">        The optimizer will see only a single slice of parameters and will thus</span>
<span class="sd">        allocate less memory for optimizer state, avoiding redundancy across</span>
<span class="sd">        data parallel workers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">,</span> <span class="s2">&quot;Param should have not been sharded yet.&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;Autograd does not support operations for integer type.&quot;</span>

            <span class="c1"># Sharding is done only when world_size is larger than 1 and</span>
            <span class="c1"># sharding_strategy!=NO_SHARD.</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">!=</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_orig_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">numel_padded_per_param</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Save the original storage and free it later on.</span>
            <span class="c1"># Since we&#39;re modifying the tensor&#39;s storage directly,</span>
            <span class="c1"># make sure the tensor is the sole occupant of the storage.</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="s2">&quot;The tensor is not the sole occupant of the storage.&quot;</span>
            <span class="n">orig_storage</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>

            <span class="c1"># Replace p with the relevant shard.</span>
            <span class="n">local_shard</span><span class="p">,</span> <span class="n">num_padded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_shard</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">local_shard</span><span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>
            <span class="n">p</span><span class="o">.</span><span class="n">shard_by_offsets</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="n">local_shard</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">local_shard</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">num_padded</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">numel_padded_per_param</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_padded</span><span class="p">)</span>

            <span class="c1"># Free storage that contains the original full data.</span>
            <span class="k">if</span> <span class="n">orig_storage</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">orig_storage</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numel_padded_per_param</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="p">),</span> <span class="s2">&quot;numel_padded_per_param is not populated correctly.&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_chunk</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the unpadded chunk as a view and the number of padding</span>
<span class="sd">        elements of a full tensor for the given rank and world size.&quot;&quot;&quot;</span>
        <span class="c1"># Shard using `torch.chunk()` to match all-gather/reduce-scatter.</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># If there are not enough chunks to shard across ranks, create an</span>
            <span class="c1"># empty chunk that will just be padded with zeros to be the</span>
            <span class="c1"># appropriate size.</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
        <span class="c1"># Determine number of padding elements.</span>
        <span class="n">num_to_pad</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="n">chunk</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_to_pad</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> \
            <span class="s2">&quot;Chunk&#39;s size should at most the first chunk&#39;s size&quot;</span>
        <span class="k">return</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">num_to_pad</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_shard_functional</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Functional version of :meth:`_get_shard`.&quot;&quot;&quot;</span>
        <span class="n">chunk</span><span class="p">,</span> <span class="n">num_to_pad</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_get_chunk</span><span class="p">(</span>
            <span class="n">tensor</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># We always need to clone here regardless of the padding and even</span>
        <span class="c1"># though `chunk` is a view of `tensor` because `tensor` may be</span>
        <span class="c1"># deallocated after this method returns</span>
        <span class="n">shard</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num_to_pad</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">shard</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">shard</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_to_pad</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">shard</span><span class="p">,</span> <span class="n">num_to_pad</span>

    <span class="k">def</span> <span class="nf">_get_shard</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the local shard and the number of padding elements of a full</span>
<span class="sd">        tensor for the calling rank if ``rank=None`` or for the rank ``rank``</span>
<span class="sd">        if not ``None``.&quot;&quot;&quot;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">rank</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_get_shard_functional</span><span class="p">(</span>
            <span class="n">tensor</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward missing attributes to wrapped module.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward indexing calls in case the module is a nn.Sequential.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

    <span class="k">def</span> <span class="nf">_reset_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reset instance so :func:`_lazy_init` will run on the next forward.</span>
<span class="sd">        Currently this is only called in __init__</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_local_shard&quot;</span><span class="p">):</span>
                <span class="c1"># reset attributes that are added in _init_param_attributes, as</span>
                <span class="c1"># part of _lazy_init</span>
                <span class="k">del</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="c1"># set &#39;self.reshard_after_forward&#39; flag based on self.sharding_strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_reshard_after_forward</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Initialization steps that should happen lazily, typically right</span>
<span class="sd">        before the first forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize param attributes lazily, in case the param&#39;s dtype or</span>
        <span class="c1"># device changes after __init__.</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_param_attributes</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># Initialize _is_root and setup streams. These steps would ideally</span>
        <span class="c1"># happen in __init__, but _is_root can only be determined after the</span>
        <span class="c1"># entire model hierarchy is setup, thus we run it lazily.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># _is_root means that we are in the outermost module&#39;s forward.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_is_root</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_setup_streams</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="c1"># Buffers stay on GPU, and don&#39;t get sharded. Since _cast_buffers</span>
            <span class="c1"># applies recursively, we only call this from the root instance.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Don&#39;t free the full params for the outer-most (root) instance,</span>
            <span class="c1"># In most cases, root instance contains params in the last layers</span>
            <span class="c1"># or has no params. In these cases, those params will be needed</span>
            <span class="c1"># immediately after for the backward pass. Note that this only</span>
            <span class="c1"># applies currently when freeing parameters at end of layer&#39;s</span>
            <span class="c1"># forward pass.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># Due to the use of streams, we need to make sure the previous</span>
            <span class="c1"># ``optim.step()`` is done before we all-gather parameters.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_for_previous_optim_step</span><span class="p">()</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_init_param_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        We manage several attributes on each Parameter instance. The first two</span>
<span class="sd">        are set by :func:`_shard_parameters`:</span>
<span class="sd">            ``_is_sharded``: ``True`` if the Parameter is sharded or ``False``</span>
<span class="sd">                if the Parameter is intentionally not sharded (in which case we</span>
<span class="sd">                will all-reduce grads for this param). Currently the way</span>
<span class="sd">                `_is_sharded = False` is if world_size = 1 or sharding strategy</span>
<span class="sd">                is NO_SHARD.</span>
<span class="sd">            ``_orig_size``: the size of the original Parameter (before sharding)</span>
<span class="sd">        A few attributes are set here:</span>
<span class="sd">            ``_local_shard``: a single shard of the parameter. This is needed to</span>
<span class="sd">                recover the shard after rebuilding full parameter in forward</span>
<span class="sd">                and backward.</span>
<span class="sd">            ``_full_param_padded``: the full weight (padded to be evenly</span>
<span class="sd">                divisible by ``world_size``), used for computation in the</span>
<span class="sd">                forward and backward pass. It is initialized with the</span>
<span class="sd">                appropriate size and then has its storage freed. This will be</span>
<span class="sd">                resized in place and only materialized (via all-gather) as needed.</span>
<span class="sd">        Another attribute is set by :func:`_register_post_backward_hooks`:</span>
<span class="sd">            ``_shard_bwd_hook``: it holds the parameter&#39;s AccumulateGrad object</span>
<span class="sd">                and the registered post hook handle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_is_sharded&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
            <span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_orig_size&quot;</span>
        <span class="p">),</span> <span class="s2">&quot;Parameters should have been sharded during construction.&quot;</span>
        <span class="c1"># If _local_shard has been set in the first lazy init and</span>
        <span class="c1"># current parameter is pointed to _local_shard, no need to</span>
        <span class="c1"># set the _local_shard again.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_local_shard&quot;</span><span class="p">):</span>
            <span class="c1"># If CPU offloading, p._local_shard should have been placed on CPU</span>
            <span class="c1"># during its first lazy construction.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="s2">&quot;cpu&quot;</span>
                <span class="p">),</span> <span class="p">(</span>
                    <span class="s2">&quot;Expected p._local_shard to be on CPU, &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="sa">f</span><span class="s2">&quot;but it&#39;s on </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># A single shard of the parameters. Also makes p._local_shard to be on</span>
        <span class="c1"># CPU if we are CPU offloading, since p.data would be on CPU during</span>
        <span class="c1"># init.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Expected param to be on CPU when cpu_offloading is enabled. &quot;</span>
                <span class="s2">&quot;If CPU offloading is enabled correctly, you may be &quot;</span>
                <span class="s2">&quot;accidentally moving the model to CUDA after FSDP initialization.&quot;</span>
            <span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="c1"># If CPU offloading, pin the memory to enable faster CPU -&gt; GPU device</span>
        <span class="c1"># transfer.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="c1"># When offloading parameters, also move the grad shard to CPU during</span>
            <span class="c1"># backward pass. In this case, it&#39;s important to pre-allocate the</span>
            <span class="c1"># CPU grad shard in pinned memory so that we can do a non-blocking</span>
            <span class="c1"># transfer.</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">p</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>

        <span class="c1"># If mixed_precision, maintain reduced precision param shard on</span>
        <span class="c1"># compute_device for computation in fwd/bwd. We resize storage to 0 here</span>
        <span class="c1"># and rematerialize before building the full param when needed. After</span>
        <span class="c1"># fwd/bwd, it is freed and we only hold on to the full precision shard.</span>
        <span class="c1"># As a result, this reduced precision shard is not allocated if we are</span>
        <span class="c1"># not in the forward/backward pass.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="p">)</span>

        <span class="c1"># We also maintain a full-sized parameter of type self.compute_dtype.</span>
        <span class="c1"># We resize the storage to size 0 at init (here) and only materialize</span>
        <span class="c1"># as needed. The storage may contain padding elements so that it is</span>
        <span class="c1"># evenly divisible by world_size, although these padding elements will</span>
        <span class="c1"># be removed before the relevant computation.</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="c1"># We set p._full_param_padded&#39;s dtype to the desired parameter dtype</span>
            <span class="c1"># in the case of mixed precision. This is so that when we all_gather</span>
            <span class="c1"># into full_param_padded it can occur without issues and result in</span>
            <span class="c1"># full_param_padded having the expected param_dtype.</span>
            <span class="n">full_param_dtype</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">full_param_dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">_set_is_root</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;If ``True``, implies that no other :class:`FullyShardedDataParallel`</span>
<span class="sd">        instance wraps this one. Called once by :func:`_lazy_init`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="c1"># No FSDP instance wraps this, else _is_root would be set to False.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># If final backward callback is never been queued, state should be IDLE.</span>
        <span class="c1"># If final backward callback is queued, the callback should be finished</span>
        <span class="c1"># and the state was reset to be IDLE.</span>
        <span class="c1"># This should be asserted at the beginning of forward pass in the root instance only.</span>
        <span class="c1"># For children instances, if they are checkpointed, state will not be reset to</span>
        <span class="c1"># IDLE after each inner forward/backward.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="c1"># We relax the assert for non-root instance, when the nested initialized module is wrapped</span>
                <span class="c1"># again in FSDP later, for example after training to run inference.</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span>
                <span class="p">),</span> <span class="s2">&quot;Non-root instance&#39;s _is_root flag should have not been set yet&quot;</span> \
                    <span class="s2">&quot;or has already been set as False.&quot;</span>
                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_setup_streams</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Create streams to overlap data transfer and computation.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="c1"># Stream for all-gathering parameters.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
            <span class="c1"># Stream for overlapping grad reduction with the backward pass.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
            <span class="c1"># Stream to move main params to self.mixed_precision.param_dtype</span>
            <span class="c1"># for forward pass.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;mixed_precision_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>

        <span class="c1"># We share streams with all children instances, which allows them to</span>
        <span class="c1"># overlap transfers across the forward pass without synchronizing with</span>
        <span class="c1"># the default stream.</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_streams</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_fsdp_graph_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span>
                <span class="c1"># Give each non-root FSDP module an alias to the root&#39;s</span>
                <span class="c1"># execution order data structure and the root&#39;s ignored</span>
                <span class="c1"># parameters and all buffer names since only the root&#39;s names</span>
                <span class="c1"># are fully prefixed like the state dict keys</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_exec_order_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_ignored_param_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_param_names</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_buffer_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_names</span>

    <span class="k">def</span> <span class="nf">_wait_for_previous_optim_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The outer-most :class:`FullyShardedDataParallel` instance (i.e., the root</span>
<span class="sd">        instance) needs to synchronize with the default stream to ensure the</span>
<span class="sd">        previous optimizer step is done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;mixed_precision_params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_need_prefetch_pre_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">==</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">training_state</span>
            <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_need_prefetch_post_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backward_prefetch</span> <span class="o">==</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">training_state</span>
            <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="p">]</span><span class="o">.</span><span class="n">_need_rebuild_full_params</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

<div class="viewcode-block" id="FullyShardedDataParallel.state_dict_type"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">state_dict_type</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict_type</span><span class="p">:</span> <span class="n">StateDictType</span><span class="p">,</span>
        <span class="n">state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to set the ``state_dict_type`` of all the descendant</span>
<span class="sd">        FSDP modules of the target module. The target module does not have to</span>
<span class="sd">        be a FSDP module. If the target module is a FSDP module, its</span>
<span class="sd">        ``state_dict_type`` will also be changed.</span>

<span class="sd">        .. note:: This API should be called for only the top-level (root)</span>
<span class="sd">            module.</span>

<span class="sd">        .. note:: This API enables users to transparently use the conventional</span>
<span class="sd">            ``state_dict`` API to take model checkpoints in cases where the</span>
<span class="sd">            root FSDP module is wrapped by another ``nn.Module``. For example,</span>
<span class="sd">            the following will ensure ``state_dict``  is called on all non-FSDP</span>
<span class="sd">            instances, while dispatching into `local_state_dict` implementation</span>
<span class="sd">            for FSDP:</span>

<span class="sd">        Example::</span>

<span class="sd">        &gt;&gt;&gt; model = DDP(FSDP(...))</span>
<span class="sd">        &gt;&gt;&gt; with FSDP.state_dict_type(model, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">        &gt;&gt;&gt;     checkpoint = model.state_dict()</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module.</span>
<span class="sd">            state_dict_type (StateDictType): the desired ``state_dict_type`` to set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Use default config a state_dict config is not set.</span>
        <span class="k">if</span> <span class="n">state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict_config</span> <span class="o">=</span> <span class="n">_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]()</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">prev_state_dict_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span>
            <span class="k">if</span> <span class="n">prev_state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span>
            <span class="k">if</span> <span class="n">prev_state_dict_type</span> <span class="o">!=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All FSDP module should the same state_dict_type.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">prev_state_dict_config</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;All FSDP modules should have the same type of state_dict_config.&quot;</span>
                <span class="p">)</span>

            <span class="n">expected_state_dict_config_type</span> <span class="o">=</span> <span class="n">_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">expected_state_dict_config_type</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expected state_dict_config of type </span><span class="si">{</span><span class="n">expected_state_dict_config_type</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">state_dict_type</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">state_dict_config</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">prev_state_dict_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># Avoid mypy warning</span>
            <span class="k">assert</span> <span class="n">prev_state_dict_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># Avoid mypy warning</span>
            <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">prev_state_dict_type</span>
                <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">prev_state_dict_config</span></div>

    <span class="k">def</span> <span class="nf">_full_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hook that runs after model.state_dict() is called before returning result to</span>
<span class="sd">        user. For FSDP, we may have to clone the tensors in state_dict as params go</span>
<span class="sd">        back to sharded version after _summon_full_params ends, and also remove</span>
<span class="sd">        &quot;_fsdp_wrapped_module&quot; prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">])</span>
        <span class="c1"># state_dict is empty for nonzero ranks if `rank0_only` was enabled.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="n">offload_to_cpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="o">.</span><span class="n">offload_to_cpu</span>
        <span class="n">cpu_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="n">clean_key</span> <span class="o">=</span> <span class="n">clean_tensor_name</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="c1"># Do not need to clone buffers since they are not sharded</span>
            <span class="k">if</span> <span class="n">clean_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffer_names</span><span class="p">:</span>
                <span class="c1"># Offload the buffer to CPU if needed -- we do not do this in</span>
                <span class="c1"># `_summon_full_params()` since without care, that would free</span>
                <span class="c1"># the original buffer&#39;s GPU memory and require reallocating</span>
                <span class="c1"># that memory later; this only affects the state dict&#39;s buffer</span>
                <span class="c1"># variable and leaves the original buffer&#39;s GPU memory intact</span>
                <span class="k">if</span> <span class="n">offload_to_cpu</span> <span class="ow">and</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">cpu_device</span><span class="p">:</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu_device</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="c1"># Clone non-ignored parameters before exiting the</span>
            <span class="c1"># `_summon_full_params()` context</span>
            <span class="k">if</span> <span class="n">clean_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_param_names</span> <span class="ow">and</span> \
                    <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="s2">&quot;_has_been_cloned&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">_has_been_cloned</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">except</span> <span class="ne">BaseException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Failed to clone() tensor with name </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">. This may mean &quot;</span>
                        <span class="s2">&quot;that this state_dict entry could point to invalid memory &quot;</span>
                        <span class="s2">&quot;regions after returning from state_dict() call if this &quot;</span>
                        <span class="s2">&quot;parameter is managed by FSDP. Please check clone &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;implementation of </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">. Error: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">_local_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This hook create a ShardedTensor from the local flat_param and replace</span>
<span class="sd">        the state_dict[f&quot;{prefix}{FLAT_PARAM}] with the ShardedTensor. No copy</span>
<span class="sd">        will happen. The underlying storage is the same.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">no_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="c1"># state_dict[f&quot;{prefix}{FLAT_PARAM}&quot;] exists and has the same tensor</span>
        <span class="c1"># value as the flat_param but it is a pure Tensor because</span>
        <span class="c1"># nn.Module.state_dict() will detach the parameter. Therefore, we need</span>
        <span class="c1"># to get flat_param from the FlattenParamsWrapper to get the metadata.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">FLAT_PARAM</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># Construct a ShardedTensor from the flat_param.</span>
        <span class="n">full_numel</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">full_numel</span>
        <span class="n">shard_offset</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
        <span class="n">valid_data_size</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span>
        <span class="k">if</span> <span class="n">valid_data_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">flat_param</span> <span class="o">=</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">valid_data_size</span><span class="p">)</span>
        <span class="n">local_shards</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">Shard</span><span class="o">.</span><span class="n">from_tensor_and_offsets</span><span class="p">(</span><span class="n">flat_param</span><span class="p">,</span> <span class="p">[</span><span class="n">shard_offset</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FLAT_PARAM</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_from_local_shards</span><span class="p">(</span>
            <span class="n">local_shards</span><span class="p">,</span> <span class="n">full_numel</span><span class="p">,</span> <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
        <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_sharded_post_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The hook replaces the unflattened, unsharded parameter in the state_dict</span>
<span class="sd">        with a unflattened, sharded parameter (a ShardedTensor).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">no_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">orig_flat_param</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">param_info</span><span class="p">:</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module_name</span><span class="p">:</span>
                <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">module_name</span><span class="si">}{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># Create a ShardedTensor for the unflattened, non-sharded parameter.</span>
            <span class="n">param</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span>
            <span class="n">local_shard</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()]</span>
            <span class="n">offsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
            <span class="n">local_shards</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">Shard</span><span class="o">.</span><span class="n">from_tensor_and_offsets</span><span class="p">(</span><span class="n">local_shard</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_from_local_shards</span><span class="p">(</span>
                <span class="n">local_shards</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
            <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_post_state_dict_hook</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        _post_state_dict_hook() is called after the state_dict() of this</span>
<span class="sd">        FSDP module is executed. ``self._state_dict_type`` is used to decide</span>
<span class="sd">        what postprocessing will be done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="n">processed_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">](</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>
        <span class="c1"># Restore buffers, which currently are in their full precision type,</span>
        <span class="c1"># back to their mixed precision type. This is because buffers are cast</span>
        <span class="c1"># during lazy_init() and stay at their mixed precision type before/after</span>
        <span class="c1"># forward/backward. As a result state_dict() should maintain this.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">processed_state_dict</span>

<div class="viewcode-block" id="FullyShardedDataParallel.state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is the entry point of all three FSDP ``state_dict`` APIs: full,</span>
<span class="sd">        local, and sharded. For the full state dict</span>
<span class="sd">        (``StateDictType.FULL_STATE_DICT``), FSDP attempts to unshard the model</span>
<span class="sd">        on all ranks, which may result in an OOM error if the full model cannot</span>
<span class="sd">        fit on a single GPU. In that case, users may pass in a</span>
<span class="sd">        :class:`FullStateDictConfig` to only save the checkpoint on rank 0 and/</span>
<span class="sd">        or to offload it to CPU memory layer by layer, enabling much larger</span>
<span class="sd">        checkpoints. If the full model cannot fit in CPU memory, then users may</span>
<span class="sd">        instead take a local state dict (``StateDictType.LOCAL_STATE_DICT``)</span>
<span class="sd">        that only saves the local shard of the model. The sharded state dict</span>
<span class="sd">        (``StateDictType.SHARDED_STATE_DICT``) saves the model parameters as</span>
<span class="sd">        ``ShardedTensor`` s. The ``state_dict`` type can be configured using</span>
<span class="sd">        the :meth:`state_dict_type` context manager.</span>

<span class="sd">        Example::</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">        &gt;&gt;&gt; my_module = nn.Linear(...)</span>
<span class="sd">        &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">        &gt;&gt;&gt; full_state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</span>
<span class="sd">        &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.FULL_STATE_DICT, full_state_dict_config):</span>
<span class="sd">        &gt;&gt;&gt;     full_dict = sharded_module.state_dict()</span>
<span class="sd">        &gt;&gt;&gt; full_dict.keys()</span>
<span class="sd">        &gt;&gt;&gt; odict_keys([&#39;weight&#39;, &#39;bias&#39;])</span>
<span class="sd">        &gt;&gt;&gt; # using local state dict</span>
<span class="sd">        &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">        &gt;&gt;&gt;     local_dict = sharded_module.state_dict()</span>
<span class="sd">        &gt;&gt;&gt; local_dict.keys()</span>
<span class="sd">        &gt;&gt;&gt; odict_keys([&#39;flat_param&#39;, &#39;inner.flat_param&#39;])</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives may be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO (rohan-varma): separate these out once a state_dict pre-hook</span>
        <span class="c1"># is available.</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span>
            <span class="c1"># Get config args</span>
            <span class="n">full_state_dict_config</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">FullStateDictConfig</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">rank0_only</span> <span class="o">=</span> <span class="n">full_state_dict_config</span><span class="o">.</span><span class="n">rank0_only</span>
            <span class="n">offload_to_cpu</span> <span class="o">=</span> <span class="n">full_state_dict_config</span><span class="o">.</span><span class="n">offload_to_cpu</span>
            <span class="n">summon_ctx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                    <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span> <span class="k">else</span>
                <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">with</span> <span class="n">summon_ctx</span><span class="p">:</span>
                <span class="c1"># Since buffers are not sharded and stay casted, restore them to their</span>
                <span class="c1"># original user module specified types for checkpoint. We take care to</span>
                <span class="c1"># recast in post_state_dict_hook for consistency with the fact that</span>
                <span class="c1"># buffers stay casted after forward/backward. We must have the</span>
                <span class="c1"># call here instead of above because _summon_full_params itself</span>
                <span class="c1"># calls _lazy_init() which would cast the buffers.</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_buffers</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cast_buffers</span><span class="p">(</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_orig_buffer_dtypes</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span>
                    <span class="p">)</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># TODO: support offload to CPU in post state dict hook.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">rank0_only</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">state_dict</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{}</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_is_sharded</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;local_state_dict can only be called &quot;</span>
                    <span class="s2">&quot;when parameters are flatten and sharded.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span>
            <span class="n">summon_ctx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span> <span class="k">else</span>
                <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">with</span> <span class="n">summon_ctx</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown StateDictType </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_local_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the local state of the module. Parameters are flattened and</span>
<span class="sd">        sharded, so the resulting state_dict can only be loaded after the module</span>
<span class="sd">        has been wrapped with FSDP.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_full_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We should exit summon_full_params context.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">])</span>
        <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_full_param_ctx&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_sharded_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sharded states of the module. Parameters are unflattened and</span>
<span class="sd">        sharded, so the resulting state_dict can be used with any parallelism</span>
<span class="sd">        (e.g., DPP, model parallelism, and single trainer) after a valid</span>
<span class="sd">        resharding.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span><span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_full_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We do not expect to be calling pre-hooks twice without post-hook</span>
        <span class="c1"># call in between.</span>
        <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_full_param_ctx&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="c1"># Note that it needs writeback=True to persist.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
            <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">writeback</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_full_param_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_local_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_local_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This hook finds the local flat_param for this FSDP module from the</span>
<span class="sd">        state_dict. The flat_param should be a ShardedTensor. This hook converts</span>
<span class="sd">        the ShardedTensor to a tensor. No copy happen unless padding is required.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">FLAT_PARAM</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">fqn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">FLAT_PARAM</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;No flat parameter in state_dict but self.module.flat_param is not None&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">load_tensor</span><span class="p">,</span> <span class="n">ShardedTensor</span>
        <span class="p">),</span> <span class="s2">&quot;Tensors in local_state_dict should be ShardedTensor.&quot;</span>

        <span class="c1"># Convert the ShardedTensor to a Tensor.</span>
        <span class="n">shards</span> <span class="o">=</span> <span class="n">load_tensor</span><span class="o">.</span><span class="n">local_shards</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">),</span> <span class="s2">&quot;load_local_state_dict assume one shard per ShardedTensor.&quot;</span>
        <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Get the metada of the flat_param to decide whether to pad the loaded</span>
        <span class="c1"># tensor.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="k">assert</span> <span class="n">flat_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()):</span>
            <span class="k">assert</span> <span class="n">load_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Local shard size = </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2"> and the tensor in &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;the state_dict is </span><span class="si">{</span><span class="n">load_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
            <span class="n">load_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">load_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="n">fqn</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_tensor</span>

    <span class="k">def</span> <span class="nf">_sharded_post_load_state_dict_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_sharded_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The hook combines the unflattened, sharded parameters (ShardedTensor) to</span>
<span class="sd">        a new FlatParameter and shards the new FlatParameter to the local chunk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_replace_by_prefix</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">no_params</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;load_sharded_state_dict can only be called when parameters &quot;</span>
                <span class="s2">&quot;are flatten and sharded.&quot;</span>
            <span class="p">)</span>

        <span class="n">nonsharded_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO: Reduce the communication by using only one _all_gather_base to</span>
        <span class="c1"># gather all the parameters in this layer. This can be achieved by</span>
        <span class="c1"># concatenated all the local shards and then append the padding.</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/77461</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_param_infos</span><span class="p">:</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="n">module_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">FPW_MODULE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module_name</span><span class="p">:</span>
                <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">FSDP_WRAPPED_MODULE</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">param</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">fqn</span><span class="p">)</span>

            <span class="c1"># All-gather the param (ShardedTensor)</span>
            <span class="n">shards</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">local_shards</span><span class="p">()</span>
            <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">dim_0_size</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">param_numel</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">dim_0_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">param_numel</span> <span class="o">//</span> <span class="n">dim_0_size</span>
            <span class="p">)</span>
            <span class="n">num_padding</span> <span class="o">=</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="n">local_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">num_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">local_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">local_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_padding</span><span class="p">])</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">chunk_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">local_tensor</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">local_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">param_numel</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
            <span class="n">nonsharded_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Create a new flat_param from the loaded, non-sharded tensors.</span>
        <span class="n">flat_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">flat_param</span>
        <span class="n">loaded_flat_param</span> <span class="o">=</span> <span class="n">FlatParameter</span><span class="p">(</span><span class="n">nonsharded_tensors</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Get the chunk from the loaded flat_param for the local rank.</span>
        <span class="n">loaded_flat_param</span><span class="p">,</span> <span class="n">num_to_pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_shard</span><span class="p">(</span><span class="n">loaded_flat_param</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">loaded_flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The loaded local chunk has different numel(</span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">) &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;from the local chunk </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span> <span class="o">==</span> <span class="n">num_to_pad</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The loaded local chunk has different padding(</span><span class="si">{</span><span class="n">num_to_pad</span><span class="si">}</span><span class="s2">) &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;from the local chunk </span><span class="si">{</span><span class="n">flat_param</span><span class="o">.</span><span class="n">num_padded</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_fsdp_wrapped_module.flat_param&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loaded_flat_param</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_pre_load_state_dict_hook</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ``_pre_state_dict_hook` is called before ``self._load_from_state_dict()``</span>
<span class="sd">        is called. ``self._state_dict_type`` is used to decide what preprocessing</span>
<span class="sd">        will be done.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Code that is common for all state_dict impls</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="c1"># Dispatch into state_dict specific implementation of pre-hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_load_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">](</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_post_load_state_dict_hook</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Code that is common for all state_dict impls</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">FullyShardedDataParallel</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="c1"># Dispatch into state_dict type specific implementation of post-hook for</span>
        <span class="c1"># loading state_dict.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_load_state_dict_hook_fn</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">]()</span>

<div class="viewcode-block" id="FullyShardedDataParallel.load_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The entry point of all three FSDP ``load_state_dict`` APIs. By default,</span>
<span class="sd">        calling ``load_state_dict`` on an FSDP module will result in FSDP</span>
<span class="sd">        attempting to load a &quot;full&quot; state_dict, i.e. a state_dict consisting of</span>
<span class="sd">        full, unsharded, unflattened original module parameters. This requires</span>
<span class="sd">        FSDP to load the full parameter context on each rank which could result</span>
<span class="sd">        in GPU OOM. As a result, :func:`state_dict_type` API is available to</span>
<span class="sd">        configure between ``load_state_dict`` implementations. User can thus use</span>
<span class="sd">        ``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`` context</span>
<span class="sd">        manager to load a local state dict checkpoint that will restore only</span>
<span class="sd">        local shards of the module. Currently, the only supported</span>
<span class="sd">        implementations are ``StateDictType.LOCAL_STATE_DICT`` and</span>
<span class="sd">        ``StateDictType.FULL_STATE_DICT`` (default). Please see :func:`state_dict`</span>
<span class="sd">        for documentation around creating an FSDP checkpoint.</span>

<span class="sd">        Example::</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">        &gt;&gt;&gt; my_module = nn.Linear(...)</span>
<span class="sd">        &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">        &gt;&gt;&gt; checkpoint = torch.load(PATH)</span>
<span class="sd">        &gt;&gt;&gt; full_state_dict = checkpoint[&#39;full_state_dict&#39;]</span>
<span class="sd">        &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.FULL_STATE_DICT):</span>
<span class="sd">        &gt;&gt;&gt;     sharded_module.load_state_dict(full_state_dict)</span>
<span class="sd">        &gt;&gt;&gt; full_dict.keys()</span>
<span class="sd">        &gt;&gt;&gt; odict_keys([&#39;weight&#39;, &#39;bias&#39;])</span>
<span class="sd">        &gt;&gt;&gt; # using local state dict</span>
<span class="sd">        &gt;&gt;&gt; local_state_dict = checkpoint[&#39;local_state_dict&#39;]</span>
<span class="sd">        &gt;&gt;&gt; with FSDP.state_dict_type(sharded_module, StateDictType.LOCAL_STATE_DICT):</span>
<span class="sd">        &gt;&gt;&gt;     sharded_module.load_state_dict(local_state_dict)</span>
<span class="sd">        &gt;&gt;&gt; local_dict.keys()</span>
<span class="sd">        &gt;&gt;&gt; odict_keys([&#39;flat_param&#39;, &#39;inner.flat_param&#39;])</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives may be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_load_local_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load states from a flattened, sharded state dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_sharded_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="s2">&quot;OrderedDict[str, torch.Tensor]&quot;</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load states from a unflattened, sharded state dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span><span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;FullyShardedDataParallel.forward&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>

            <span class="c1"># Start of a forward pass.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">FORWARD</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
                <span class="c1"># TODO: disabling side stream for tensor copies for now, investigate</span>
                <span class="c1"># perf with it on / off.</span>
                <span class="c1"># Place inputs on compute_device. This is a noop if inputs are already</span>
                <span class="c1"># on compute_device. Note that when device_id is specified,</span>
                <span class="c1"># device_id == self.compute_device is guaranteed.</span>
                <span class="c1"># TODO: for mixed precision, move inputs to right device + cast might</span>
                <span class="c1"># be done in one go for performance.</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_to_kwargs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Cast inputs to their mixed precision type.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">input_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cast_fp_inputs_to_precision</span><span class="p">(</span>
                    <span class="n">input_dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>

            <span class="c1"># All-gather full parameters, moving them to compute_device if</span>
            <span class="c1"># necessary.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rebuild_full_params</span><span class="p">()</span>
            <span class="c1"># Wait for all_gather full parameters to finish before computation</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

            <span class="c1"># Register backward hooks to reshard params and reduce-scatter grads.</span>
            <span class="c1"># These need to be re-registered every forward pass in some cases where grad_fn</span>
            <span class="c1"># is mutated.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_register_post_backward_hooks</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_free_full_params</span><span class="p">()</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_free_mp_shard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
            <span class="c1"># Switch to original local shards of params. We maintain this invariant throughout</span>
            <span class="c1"># the code, i.e., ``p.data == p._local_shard`` after each function. This</span>
            <span class="c1"># also ensures that after the first forward, the optimizer state will be</span>
            <span class="c1"># initialized with the correct dtype and (sharded) size, since optimizer</span>
            <span class="c1"># state is typically initialized lazily in ``optim.step()``. Note that</span>
            <span class="c1"># when CPU offload is enabled, _use_param_local_shard implicitly</span>
            <span class="c1"># offloads the local shard to CPU by making p.data point to</span>
            <span class="c1"># p._local_shard, which would reside on CPU.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_param_local_shard</span><span class="p">()</span>

            <span class="c1"># Register pre-backward hooks to all-gather the params for the backward</span>
            <span class="c1"># pass (if output&#39;s grad was needed). This won&#39;t register anything if</span>
            <span class="c1"># we are in eval mode.</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_register_pre_backward_hooks</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

            <span class="c1"># Done with a forward pass.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_write_back_current_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Writes back full_params into self.params.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">full_param</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">full_params</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">continue</span>  <span class="c1"># Already copied because no sharding.</span>

            <span class="c1"># TODO: Might be able to refactor to use _get_shard.</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="n">full_param</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">_summon_full_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">writeback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">writeback</span> <span class="ow">and</span> <span class="n">rank0_only</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;writeback=True and rank0_only=True is not supported, as model &quot;</span>
                <span class="s2">&quot;parameter shapes will be different across ranks, and writing &quot;</span>
                <span class="s2">&quot;to them can lead to inconsistencies across ranks when the &quot;</span>
                <span class="s2">&quot;context is exited.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">offload_to_cpu</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">rank0_only</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;offload_to_cpu and rank0_only=False will result in &quot;</span>
                <span class="s2">&quot;full parameters being redundantly copied to CPU memory for &quot;</span>
                <span class="s2">&quot;GPUs that reside on the same machine, which may incur the risk of &quot;</span>
                <span class="s2">&quot;CPU OOM. It is recommended to use ``offload_to_cpu`` with &quot;</span>
                <span class="s2">&quot;rank0_only=True.&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">_free_full_params_and_use_local_shard</span><span class="p">(</span><span class="n">params_to_free</span><span class="p">):</span>
            <span class="c1"># We may not always be able to free the full param, for example in</span>
            <span class="c1"># the case where world_size == 1 and the shard actually points to</span>
            <span class="c1"># the full parameter.</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">can_free</span><span class="p">)</span> <span class="ow">in</span> <span class="n">params_to_free</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">can_free</span><span class="p">:</span>
                    <span class="n">current_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                    <span class="c1"># Don&#39;t let PyTorch reuse this memory until all work in the</span>
                    <span class="c1"># current stream is complete</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
                    <span class="n">_free_storage</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

            <span class="c1"># when CPU offload is enabled, _use_param_local_shard implicitly</span>
            <span class="c1"># offloads the local shard to CPU by making p.data point to</span>
            <span class="c1"># p._local_shard, which would reside on CPU.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_param_local_shard</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">recurse</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
                <span class="c1"># Summon all params for any nested FSDP instances.</span>
                <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                            <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">writeback</span><span class="o">=</span><span class="n">writeback</span><span class="p">,</span>
                            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
                            <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="c1"># Yield to the caller, with full params in all nested instances.</span>
                <span class="k">yield</span>
            <span class="c1"># Exiting from the ExitStack will re-shard params.</span>
            <span class="k">return</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
            <span class="c1"># Set the state so that we assert when trying to go into</span>
            <span class="c1"># forward/backward.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>

            <span class="c1"># Even if rank0_only = True, we need to materialize all params here</span>
            <span class="c1"># and free them right after as full param materialization requires</span>
            <span class="c1"># collective comm.</span>
            <span class="n">currently_local_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rebuild_full_params</span><span class="p">()</span>
            <span class="c1"># Wait for all_gather to finish before computation</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>
            <span class="n">my_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">offload_to_cpu</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">rank0_only</span> <span class="ow">or</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>
                        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                            <span class="c1"># Note that we avoid using p._full_param_padded</span>
                            <span class="c1"># directly here as we may not be using that param</span>
                            <span class="c1"># as the full_param from _rebuild_full_params (i.e.)</span>
                            <span class="c1"># in mixed precision.</span>
                            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">full_param</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">currently_local_params</span>
                            <span class="p">):</span>
                                <span class="n">full_param</span> <span class="o">=</span> <span class="n">full_param</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_update_p_data</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">output_tensor</span><span class="o">=</span><span class="n">full_param</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">rank0_only</span> <span class="ow">and</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">_free_full_params_and_use_local_shard</span><span class="p">(</span><span class="n">currently_local_params</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">yield</span>
                <span class="k">finally</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># FSDP now has the full flattened parameter. Unflatten it to get the</span>
                <span class="c1"># full parameters.</span>
                <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
                    <span class="c1"># Invariant: rank == 0 or !rank0_only</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">unflatten_params</span><span class="p">())</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">yield</span>
                    <span class="k">finally</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">offload_to_cpu</span><span class="p">:</span>
                            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>
                                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                                        <span class="c1"># Note that we avoid using</span>
                                        <span class="c1"># p._full_param_padded directly here as</span>
                                        <span class="c1"># we may not be using that param</span>
                                        <span class="c1"># as the full_param from</span>
                                        <span class="c1"># _rebuild_full_params (i.e. in mixed</span>
                                        <span class="c1"># precision.</span>
                                        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">full_param</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">currently_local_params</span>
                                        <span class="p">):</span>
                                            <span class="n">full_param</span> <span class="o">=</span> <span class="n">full_param</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">)</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">_update_p_data</span><span class="p">(</span>
                                                <span class="n">p</span><span class="p">,</span> <span class="n">output_tensor</span><span class="o">=</span><span class="n">full_param</span><span class="p">,</span>
                                            <span class="p">)</span>

                        <span class="k">if</span> <span class="n">writeback</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_write_back_current_shard</span><span class="p">(</span><span class="n">currently_local_params</span><span class="p">)</span>
                        <span class="n">stack</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                        <span class="n">_free_full_params_and_use_local_shard</span><span class="p">(</span><span class="n">currently_local_params</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>

<div class="viewcode-block" id="FullyShardedDataParallel.summon_full_params"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">summon_full_params</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">writeback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; A context manager to expose full params for FSDP instances.</span>
<span class="sd">        Can be useful *after* forward/backward for a model to get</span>
<span class="sd">        the params for additional processing or checking. It can take a non-FSDP</span>
<span class="sd">        module and will summon full params for all contained FSDP modules as</span>
<span class="sd">        well as their children, depending on the ``recurse`` argument.</span>

<span class="sd">        .. note:: This can be used on inner FSDPs.</span>
<span class="sd">        .. note:: This can *not* be used within a forward or backward pass. Nor</span>
<span class="sd">            can forward and backward be started from within this context.</span>
<span class="sd">        .. note:: Parameters will revert to their local shards after the context</span>
<span class="sd">            manager exits, storage behavior is the same as forward.</span>
<span class="sd">        .. note:: The full parameters can be modified, but only the portion</span>
<span class="sd">            corresponding to the local param shard will persist after the</span>
<span class="sd">            context manager exits (unless ``writeback=False``, in which case</span>
<span class="sd">            changes will be discarded). In the case where FSDP does not shard</span>
<span class="sd">            the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``</span>
<span class="sd">            config, the modification is persisted regardless of ``writeback``.</span>
<span class="sd">        .. note:: This method works on modules which are not FSDP themselves but</span>
<span class="sd">            may contain multiple independent FSDP units. In that case, the given</span>
<span class="sd">            arguments will apply to all contained FSDP units.</span>

<span class="sd">        .. warning:: Note that ``rank0_only=True`` in conjunction with</span>
<span class="sd">            ``writeback=True`` is not currently supported and will raise an</span>
<span class="sd">            error. This is because model parameter shapes would be different</span>
<span class="sd">            across ranks within the context, and writing to them can lead to</span>
<span class="sd">            inconsistency across ranks when the context is exited.</span>

<span class="sd">        .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will</span>
<span class="sd">            result in full parameters being redundantly copied to CPU memory for</span>
<span class="sd">            GPUs that reside on the same machine, which may incur the risk of</span>
<span class="sd">            CPU OOM. It is recommended to use ``offload_to_cpu`` with</span>
<span class="sd">            ``rank0_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            recurse (bool, Optional): recursively summon all params for nested</span>
<span class="sd">                FSDP instances (default: True).</span>
<span class="sd">            writeback (bool, Optional): if ``False``, modifications to params are</span>
<span class="sd">                discarded after the context manager exists;</span>
<span class="sd">                disabling this can be slightly more efficient (default: True)</span>
<span class="sd">            rank0_only (bool, Optional): if ``True``, full parameters are</span>
<span class="sd">                materialized on only global rank 0. This means that within the</span>
<span class="sd">                context, only rank 0 will have full parameters and the other</span>
<span class="sd">                ranks will have sharded parameters. Note that setting</span>
<span class="sd">                ``rank0_only=True`` with ``writeback=True`` is not supported,</span>
<span class="sd">                as model parameter shapes will be different across ranks</span>
<span class="sd">                within the context, and writing to them can lead to</span>
<span class="sd">                inconsistency across ranks when the context is exited.</span>
<span class="sd">            offload_to_cpu (bool, Optional): If ``True``, full parameters are</span>
<span class="sd">                offloaded to CPU. Note that this offloading currently only</span>
<span class="sd">                occurs if the parameter is sharded (which is only not the case</span>
<span class="sd">                for world_size = 1 or ``NO_SHARD`` config). It is recommended</span>
<span class="sd">                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid</span>
<span class="sd">                redundant copies of model parameters being offloaded to the same CPU memory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note that we specify root_only as FSDP roots will handle summoning</span>
        <span class="c1"># child FSDP instances based on recurse argument.</span>
        <span class="n">fsdp_modules</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="n">root_only</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># Summon all params for all FSDP instances</span>
        <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">fsdp_modules</span><span class="p">:</span>
                <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">_summon_full_params</span><span class="p">(</span>
                        <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">,</span>
                        <span class="n">writeback</span><span class="o">=</span><span class="n">writeback</span><span class="p">,</span>
                        <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
                        <span class="n">offload_to_cpu</span><span class="o">=</span><span class="n">offload_to_cpu</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="c1"># Yield to the caller, with full params in all FSDP instances.</span>
            <span class="k">yield</span>
        <span class="c1"># Exiting from the ExitStack will reshard all params.</span>
        <span class="k">return</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.named_buffers"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers">[docs]</a>    <span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_buffers()` to intercept buffer names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened buffer prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">in_summon_full_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">in_summon_full_params</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">buffer_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.named_parameters"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_parameters()` to intercept parameter names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened parameter prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Determine which logic to use based on the context at call time</span>
        <span class="n">in_summon_full_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">in_summon_full_params</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_register_pre_backward_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Register pre-backward hook to run before the wrapped module&#39;s</span>
<span class="sd">        backward. Hooks should be attached to all outputs from the forward.</span>
<span class="sd">        Returns:</span>
<span class="sd">            outputs: new outputs with hooks registered if they requires gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reset before each backward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_need_rebuild_full_params</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">outputs</span>  <span class="c1"># don&#39;t register hooks if grad isn&#39;t enabled</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="c1"># This actually means that only root instance has</span>
            <span class="c1"># _post_backward_callback_queued defined. Accidentally accessing this field</span>
            <span class="c1"># will assert on all other instances, giving us a nice bug checker.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Reset before each backward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_backward_hook_has_run</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">def</span> <span class="nf">_pre_backward_hook</span><span class="p">(</span><span class="o">*</span><span class="n">unused</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Run ``_pre_backward_hook`` only once per backward pass</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_backward_hook_has_run</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="c1"># try to queue final backward callback only once for root, so</span>
            <span class="c1"># that final backward callback is attached to the outer most</span>
            <span class="c1"># backward graph task and called after all the backward</span>
            <span class="c1"># calls are completed.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_queue_wait_for_post_backward</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_prefetch_pre_backward_hook</span><span class="p">():</span>
                <span class="c1"># Always wait for all_gather before rebuilding full params, just</span>
                <span class="c1"># in case full params have already been prefetched in previous layer&#39;s</span>
                <span class="c1"># pre-backward hook.</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

            <span class="c1"># Start of a backward pass for the first time in an backward pass.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span>

            <span class="c1"># All-gather full parameters, moving them to compute device if</span>
            <span class="c1"># necessary.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rebuild_full_params</span><span class="p">()</span>
            <span class="c1"># Wait for all_gather to finish before computation</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

            <span class="c1"># Prefetch next layer&#39;s full params in backward pass,</span>
            <span class="c1"># since it is prefetching, no need to wait for all_gather stream.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_prefetch_pre_backward_hook</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_rebuild_full_params</span><span class="p">()</span>  <span class="c1"># type: ignore[operator]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_backward_hook_has_run</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Prepare p.grad so that it is in the right shape, device, accumulated values, etc.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prep_grads_for_backward</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_register_hook</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">t</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">_pre_backward_hook</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_need_rebuild_full_params</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">t</span>

        <span class="c1"># Attach hooks to Tensor outputs.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_apply_to_tensors</span><span class="p">(</span><span class="n">_register_hook</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">_register_post_backward_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Register backward hooks to reshard params and reduce-scatter grads.</span>
<span class="sd">        This is called during forward pass. The goal is to attach a hook</span>
<span class="sd">        on each of the parameter&#39;s gradient generating function (``grad_acc``</span>
<span class="sd">        below) so that the hook is called *after* all gradients for that</span>
<span class="sd">        param are computed.</span>
<span class="sd">        Goals:</span>
<span class="sd">        1. We want the hook to fire once and only once *after* all gradients</span>
<span class="sd">        are accumulated for a param.</span>
<span class="sd">        2. If it fires more than once, we end up incorrectly shard the grad</span>
<span class="sd">        multiple times. (could lead to dimension too small)</span>
<span class="sd">        3. If it fires once but too early or doesn&#39;t fire, we leave gradients</span>
<span class="sd">        unsharded. (could lead to dimension too large)</span>
<span class="sd">        Due to multiple-pass forward, this function can be called on</span>
<span class="sd">        the same parameter multiple times in a single forward pass. If we register</span>
<span class="sd">        the hook multiple time, we end up getting called multiple times. We</span>
<span class="sd">        could try to get a new hook every time and delete the previous one</span>
<span class="sd">        registered. However, due to *unknown reason* (I have debugged it for</span>
<span class="sd">        a long time!), in mixed precision mode, we get two different ``grad_acc``</span>
<span class="sd">        objects below during different calls of this function (in the same</span>
<span class="sd">        forward pass). If we keep the last one, the hook end up firing too</span>
<span class="sd">        early. In full precision mode, we luckily get the *same* ``grad_acc``</span>
<span class="sd">        object, so deleting and re-registering still ensured the hook fire</span>
<span class="sd">        once after all gradients are generated.</span>
<span class="sd">        Empirically, keep the first hook register per forward pass seems to</span>
<span class="sd">        work the best. We do need to remove the hook at the end of the</span>
<span class="sd">        backward pass. Otherwise, the next forward pass will not register</span>
<span class="sd">        a new hook, which is needed for a new forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">():</span>
            <span class="k">return</span>  <span class="c1"># don&#39;t register grad hooks if grad isn&#39;t enabled</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_shard_bwd_hook&quot;</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="c1"># Register a hook on the first call, empirically, autograd</span>
                <span class="c1"># fires it at the end for this param, which makes sense.</span>
                <span class="n">p_tmp</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># Get a grad_fn on p_tmp.</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">p_tmp</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">),</span> <span class="s2">&quot;p_tmp grad_fn should not be None, it is used to access </span><span class="se">\</span>
<span class="s2">                    p&#39;s AccumulateGrad object and register post hook on it.&quot;</span>
                <span class="n">grad_acc</span> <span class="o">=</span> <span class="n">p_tmp</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span>
                    <span class="mi">0</span>
                <span class="p">]</span>  <span class="c1"># Gets its AccumulateGrad object.</span>
                <span class="n">handle</span> <span class="o">=</span> <span class="n">grad_acc</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span>
                    <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_hook</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_shard_bwd_hook</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_acc</span><span class="p">,</span> <span class="n">handle</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_post_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Parameter</span><span class="p">,</span> <span class="o">*</span><span class="n">unused</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        At the start of :func:`_post_backward_hook`, ``param.grad`` contains the</span>
<span class="sd">        full gradient for the local batch. The reduce-scatter op will replace</span>
<span class="sd">        ``param.grad`` with a single shard of the summed gradient across all</span>
<span class="sd">        GPUs. This shard will align with the current GPU rank. For example::</span>
<span class="sd">            before reduce_scatter:</span>
<span class="sd">                param.grad (GPU #0): [1, 2, 3, 4]</span>
<span class="sd">                param.grad (GPU #1): [5, 6, 7, 8]</span>
<span class="sd">            after reduce_scatter:</span>
<span class="sd">                param.grad (GPU #0): [6, 8]    # 1+5, 2+6</span>
<span class="sd">                param.grad (GPU #1): [10, 12]  # 3+7, 4+8</span>
<span class="sd">        The local GPU&#39;s ``optim.step`` is responsible for updating a single</span>
<span class="sd">        shard of params, also corresponding to the current GPU&#39;s rank. This</span>
<span class="sd">        alignment is created by :func:`_shard_parameters`, which ensures that</span>
<span class="sd">        the local optimizer only sees the relevant parameter shard.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># First hook callback will see PRE state. If we have multiple params,</span>
        <span class="c1"># then subsequent hook callbacks will see POST state.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;FSDP only works with gradients that don&#39;t require gradients&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span> <span class="ow">or</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">:</span>
            <span class="c1"># We free full parameters unless we are in `no_sync()` (i.e. when</span>
            <span class="c1"># `_require_backward_grad_sync=False`) and not using the</span>
            <span class="c1"># `FULL_SHARD` strategy. If we are not using the `FULL_SHARD`</span>
            <span class="c1"># strategy (e.g. instead using `SHARD_GRAD_OP`), then we keep the</span>
            <span class="c1"># full parameters in memory and save network overhead.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_free_full_params</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">],</span> <span class="p">[</span><span class="n">param</span><span class="p">]))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">():</span>
            <span class="c1"># Noop if reshard_after_forward=True because we&#39;d free the param</span>
            <span class="c1"># shard when rebuilding the full params in the pre_beckward_hook.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_free_mp_shard</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">],</span> <span class="p">[</span><span class="n">param</span><span class="p">]))</span>

        <span class="c1"># Switch to local shard after backward. Note that</span>
        <span class="c1"># when CPU offload is enabled, _use_param_local_shard implicitly</span>
        <span class="c1"># offloads the local shard to CPU by making p.data point to</span>
        <span class="c1"># p._local_shard, which would reside on CPU.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_param_local_shard</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">],</span> <span class="p">[</span><span class="n">param</span><span class="p">]))</span>

        <span class="c1"># Prefetch previous layer&#39;s full params in backward pass post backward hook,</span>
        <span class="c1"># If next layer&#39;s backward computation is done and full params are freed,</span>
        <span class="c1"># no need to prefetch the full params again.</span>
        <span class="c1"># Only prefetch full params if any of the next layer&#39;s outputs requires grad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_need_prefetch_post_backward_hook</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_graph_order</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_my_fsdp_idx_in_graph</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_rebuild_full_params</span><span class="p">()</span>  <span class="c1"># type: ignore[operator]</span>
            <span class="c1"># Next layer&#39;s computation will start right after this all_gather,</span>
            <span class="c1"># Wait for all_gather to finish before computation.</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Wait for all work in the current stream to finish, then start the</span>
        <span class="c1"># reductions in post_backward stream.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">]):</span>
            <span class="n">orig_grad_data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_reduce</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="c1"># Cast gradient to precision in which it should be communicated.</span>
                <span class="c1"># TODO: Make this a communication hook when communication hooks</span>
                <span class="c1"># are implemented for FSDP. Note that this is a noop if the</span>
                <span class="c1"># reduce_dtype matches the param dtype.</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">reduce_dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Average grad by world_size for consistency with PyTorch DDP.</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_predivide_factor</span><span class="p">)</span>

            <span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="c1"># We clear `param.grad` to permit repeated gradient</span>
                <span class="c1"># computations when this FSDP module is called multiple times.</span>
                <span class="c1"># This is to avoid a race among multiple re-entrant backward</span>
                <span class="c1"># passes. For example, the second backward pass computation</span>
                <span class="c1"># precedes ahead of the first backward pass reduction, which is</span>
                <span class="c1"># possible since the reduction is in a different stream and is</span>
                <span class="c1"># async. Then, the first backward pass may be incorrectly</span>
                <span class="c1"># reducing the second backward pass&#39;s `param.grad`.</span>
                <span class="c1"># The reduced gradients are accumulated in</span>
                <span class="c1"># `param._saved_grad_shard`, and the gradient reductions can</span>
                <span class="c1"># happen in arbitrary order, though we tolerate this due to the</span>
                <span class="c1"># (approximate) commutativity of floating-point addition.</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">grad_flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">chunks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">grad_flatten</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">))</span>
                <span class="n">num_pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">*</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="n">grad</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="n">input_flattened</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">grad_flatten</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_pad</span><span class="p">])</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">_reduce_scatter_base</span><span class="p">(</span>
                    <span class="n">output</span><span class="p">,</span> <span class="n">input_flattened</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_postdivide_factor</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Average grad by world_size for consistency with PyTorch DDP.</span>
                    <span class="n">output</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_postdivide_factor</span><span class="p">)</span>

                <span class="c1"># Note that we need to cast grads back to the full precision if</span>
                <span class="c1"># 1) parameters were in reduced precision during fwd, as grads</span>
                <span class="c1"># would thus be in this reduced precision, or</span>
                <span class="c1"># 2) parameters did not have precision reduced, but grads</span>
                <span class="c1"># had reduced precision for communication.</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_reduce</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="c1"># Cast gradients back to the full parameter precision so that</span>
                    <span class="c1"># optimizer.step() happens in full precision.</span>
                    <span class="n">orig_param_grad_data</span> <span class="o">=</span> <span class="n">output</span>
                    <span class="n">output</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="c1"># Don&#39;t let this memory get reused until after the transfer.</span>
                    <span class="n">orig_param_grad_data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

                <span class="c1"># To support gradient accumulation outside `no_sync()`, we save</span>
                <span class="c1"># the gradient data to `param._saved_grad_shard` before the</span>
                <span class="c1"># backward pass, accumulate gradients into it here, and set</span>
                <span class="c1"># `param.grad` with the accumulated value at the end of the</span>
                <span class="c1"># backward pass in preparation for the optimizer step.</span>
                <span class="n">accumulate_grad</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">accumulate_grad</span><span class="p">:</span>
                    <span class="n">p_assert</span><span class="p">(</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="s2">&quot;Shape mismatch when accumulating gradients: &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="sa">f</span><span class="s2">&quot;existing grad shape=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;new grad shape=</span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="p">)</span>
                    <span class="n">p_assert</span><span class="p">(</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="s2">&quot;Device mismatch when accumulating gradients: &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="sa">f</span><span class="s2">&quot;existing grad device=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;new grad device=</span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span> <span class="o">+=</span> <span class="n">output</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span> <span class="o">=</span> <span class="n">output</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">_saved_grad_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Currently the way for _is_sharded to be False is if</span>
                <span class="c1"># world_size == 1 or sharding_strategy is NO_SHARD.</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span>
                <span class="p">),</span> <span class="s2">&quot;Currently the way for _is_sharded to be False is </span><span class="se">\</span>
<span class="s2">                    world_size == 1 or sharding_stratagy is set to be NO_SHARD&quot;</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">NO_SHARD</span><span class="p">:</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_postdivide_factor</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="c1"># Average grad by world_size for consistency with PyTorch DDP.</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_postdivide_factor</span><span class="p">)</span>
                <span class="c1"># Note that we need to cast grads back to the full precision if</span>
                <span class="c1"># 1) parameters were in reduced precision during fwd, as grads</span>
                <span class="c1"># would thus be in this reduced precision, or</span>
                <span class="c1"># 2) parameters did not have precision reduced, but grads</span>
                <span class="c1"># had reduced precision for communication.</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_reduce</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="c1"># Cast gradients back to the full parameter precision so that</span>
                    <span class="c1"># optimizer.step() happens in full precision.</span>
                    <span class="n">orig_param_grad_data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="c1"># Don&#39;t let this memory get reused until after the transfer.</span>
                    <span class="n">orig_param_grad_data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

            <span class="c1"># Regardless of sharding or not, offload the grad to CPU if we are</span>
            <span class="c1"># offloading params. This is so param and grad reside on same device</span>
            <span class="c1"># which is needed for the optimizer step.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="c1"># We specify non_blocking=True</span>
                <span class="c1"># and ensure the appropriate synchronization is done by waiting</span>
                <span class="c1"># streams in _wait_for_post_backward.</span>
                <span class="n">param</span><span class="o">.</span><span class="n">_cpu_grad</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
                <span class="c1"># Don&#39;t let this memory get reused until after the transfer.</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

            <span class="c1"># After _post_backward_hook returns, orig_grad_data will eventually</span>
            <span class="c1"># go out of scope, at which point it could otherwise be freed for</span>
            <span class="c1"># further reuse by the main stream while the div/reduce_scatter/copy</span>
            <span class="c1"># are underway in the post_backward stream. See:</span>
            <span class="c1"># github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py</span>
            <span class="n">orig_grad_data</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_queue_wait_for_post_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Try to queue a `wait_for_post_backward` callback.</span>
<span class="sd">        Only called on root and only queue one callback at the beginning of</span>
<span class="sd">        outer most backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span>
        <span class="p">),</span> <span class="s2">&quot;_queue_wait_for_post_backward can only be called on root.&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wait_for_post_backward</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_wait_for_post_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Wait for post-backward to finish. Only called on root instance.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;_wait_for_post_backward can only be called on root.&quot;</span>
        <span class="c1"># Check if the root module has params and if any of them has</span>
        <span class="c1"># the `requires_grad` field set. If `requires_grad=False` for</span>
        <span class="c1"># all the params, the post_backward hook will not fire and the</span>
        <span class="c1"># state will remain in `TrainingState_.BACKWARD_PRE`.</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;post_backward&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="c1"># We need to wait for the non-blocking GPU -&gt;</span>
                <span class="c1"># CPU grad transfers to finish. We need to do this for GPU -&gt; CPU</span>
                <span class="c1"># copies because when grad is on CPU, it won&#39;t wait for any CUDA</span>
                <span class="c1"># stream to finish GPU -&gt; CPU copies unless we explicitly block the</span>
                <span class="c1"># host-side with synchronize().</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="c1"># A backward pass is done, clean up below.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_finalize_params</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">:</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="sd">&quot;&quot;&quot;Helper used below on all fsdp modules.&quot;&quot;&quot;</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">fsdp_module</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_shard_bwd_hook&quot;</span><span class="p">):</span>
                        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_shard_bwd_hook</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">_shard_bwd_hook</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="p">),</span> <span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="s2">&quot;p._shard_bwd_hook fields are not valid.&quot;</span>
                        <span class="p">)</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">_shard_bwd_hook</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="nb">delattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_shard_bwd_hook&quot;</span><span class="p">)</span>
                    <span class="c1"># Preserve the gradient accumulation state if not</span>
                    <span class="c1"># synchronizing: `p.grad` remains the unsharded gradient</span>
                    <span class="c1"># accumulated from prior `no_sync()` iterations, and</span>
                    <span class="c1"># `p._saved_grad_shard` remains the sharded gradient from</span>
                    <span class="c1"># the last synchronized iteration</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="c1"># Set `p.grad` as needed to ensure optimizer correctness</span>
                    <span class="c1"># since optimizers operate on the `grad` attribute</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_cpu_grad&quot;</span><span class="p">):</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                            <span class="sa">f</span><span class="s2">&quot;Device mismatch: p=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;p._cpu_grad=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_cpu_grad</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">):</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;Device mismatch: p=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> &quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                            <span class="sa">f</span><span class="s2">&quot;p._saved_grad_shard=</span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p_assert</span><span class="p">(</span>
                            <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">,</span> <span class="s2">&quot;All sharded parameters should &quot;</span>
                            <span class="s2">&quot;use `_saved_grad_shard`&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">):</span>
                        <span class="nb">delattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_saved_grad_shard&quot;</span><span class="p">)</span>

        <span class="c1"># Update root and nested FSDP&#39;s hooks and flags.</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>  <span class="c1"># includes self</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="n">_finalize_params</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_pre_backward_hook_has_run</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                    <span class="c1"># Check if the module has params and if any of them has</span>
                    <span class="c1"># the `requires_grad` field set. If `requires_grad=False` for</span>
                    <span class="c1"># all the params, the post_backward hook will not fire and the</span>
                    <span class="c1"># state will remain in `TrainingState_.BACKWARD_PRE`.</span>
                    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">params</span><span class="p">]):</span>
                        <span class="n">m</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_POST</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">m</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># When `m` and its children have no non-ignored params or</span>
                    <span class="c1"># have non-ignored params but none with `requires_grad==True`,</span>
                    <span class="c1"># there are two cases:</span>
                    <span class="c1"># 1. output tensors are `requires_grad==True`. In this case,</span>
                    <span class="c1"># pre-backward hook is still registered, so it is in BACKWARD_PRE state.</span>
                    <span class="c1"># 2. output tensors are `requires_grad==False`. In this case,</span>
                    <span class="c1"># pre-backward hook is not registered, so it is in IDLE state.</span>
                    <span class="n">m</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">([</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">])</span>
                <span class="n">m</span><span class="o">.</span><span class="n">training_state</span> <span class="o">=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span>

                <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
                    <span class="c1"># reset this flag for cases like &quot;one forward pass + multiple backward passes&quot;</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_post_backward_callback_queued</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_update_p_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function to update p.data pointer.</span>
<span class="sd">        Args:</span>
<span class="sd">            output_tensor (torch.Tensor): this tensor contains the data we just gathered.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">output_tensor</span>
        <span class="c1"># Trim any padding and reshape to match original size.</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span> <span class="n">p</span><span class="o">.</span><span class="n">_orig_size</span><span class="o">.</span><span class="n">numel</span><span class="p">()]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_orig_size</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_rebuild_full_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gather all shards of params.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># _summon_full_params must do a full precision rebuild even under mixed</span>
        <span class="c1"># precision, because it is used for e.g. checkpoint where we&#39;d like to</span>
        <span class="c1"># checkpoint in full precision.</span>
        <span class="n">force_full_precision</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span><span class="p">)</span>
        <span class="c1"># full param output tensors and a flag indicating whether</span>
        <span class="c1"># _summon_full_params can free them or not. It is possible that we can&#39;t</span>
        <span class="c1"># free the full param, which currently occurs when the returned</span>
        <span class="c1"># parameter points to the unsharded param when world_size == 1, or when</span>
        <span class="c1"># we&#39;re returning the full parameter and reshard_after_forward=False</span>
        <span class="c1"># (because we need to ensure p._full_param_padded stays intact)</span>
        <span class="n">output_tensors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_streams</span><span class="p">[</span><span class="s2">&quot;all_gather&quot;</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                <span class="n">mixed_precision_cast_ran</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_full_precision</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">mixed_precision_cast_ran</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cast_param_shards_to_dtype</span><span class="p">()</span>
                    <span class="c1"># TODO: remove below</span>
                    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
                <span class="c1"># We can skip moving params to GPU if mixed precision, as p.data</span>
                <span class="c1"># would then be pointing to p._mp_shard which is already on</span>
                <span class="c1"># self.compute_device.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">mixed_precision_cast_ran</span><span class="p">:</span>
                    <span class="c1"># Move params to GPU if needed. Note that we don&#39;t use</span>
                    <span class="c1"># self._full_param_padded.device here because the attr is</span>
                    <span class="c1"># not set always, i.e. when world_size=1 and</span>
                    <span class="c1"># p._is_sharded = False. However when it is set, the</span>
                    <span class="c1"># device is always self.compute_device.</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="c1"># Check the validity of this `_rebuild_full_params()` call in</span>
                <span class="c1"># terms of execution order (regardless of if FSDP actually</span>
                <span class="c1"># needs to all-gather or not)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_check_rebuild_full_params</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                <span class="c1"># e.g., when world_size == 1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">if</span> <span class="n">mixed_precision_cast_ran</span><span class="p">:</span>
                        <span class="c1"># p.data should be the same type as p._mp_shard, and it</span>
                        <span class="c1"># is safe to free.</span>
                        <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">_mp_shard</span><span class="o">.</span><span class="n">dtype</span>
                        <span class="c1"># Safe to free because p.data points to the mp shard.</span>
                        <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># p.data points to the unsharded parameter, so not safe to</span>
                        <span class="c1"># free.</span>
                        <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
                    <span class="k">continue</span>
                <span class="c1"># If full param has been rebuilt or has not been freed, no need to call all gather</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="p">):</span>
                    <span class="c1"># Check that the full param is in the expected precision, if</span>
                    <span class="c1"># training with mixed precision</span>
                    <span class="k">if</span> <span class="n">mixed_precision_cast_ran</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="s2">&quot;_rebuild_full_params: Expected full param to be &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span><span class="si">}</span><span class="s2">, &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">!&quot;</span>
                            <span class="p">)</span>
                    <span class="c1"># output is full_param_padded which can be freed depending</span>
                    <span class="c1"># on reshard_after_forward (this path is exercised by tests</span>
                    <span class="c1"># in test_fsdp_summon_full_params).</span>
                    <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshard_after_forward</span><span class="p">))</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_p_data</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">output_tensor</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If full param has not been rebuilt or has been freed, call all gather</span>
                    <span class="n">p_data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="n">p_full_size</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">p_full_size</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">p_data</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>
                    <span class="p">),</span> <span class="s2">&quot;Param full size should be equal to its shard size multiply world_size.&quot;</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="p">),</span> <span class="s2">&quot;Full param&#39;s storage should have been freed before if all gather is needed.&quot;</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                        <span class="ow">and</span> <span class="n">force_full_precision</span>
                    <span class="p">):</span>
                        <span class="c1"># p._full_param_padded has the reduced precision type,</span>
                        <span class="c1"># but we need full precision rebuild as we&#39;re in</span>
                        <span class="c1"># _summon_full_params. Note that this is why</span>
                        <span class="c1"># _summon_full_params collects locally used params from</span>
                        <span class="c1"># _rebuild_full_params instead of relying on</span>
                        <span class="c1"># p._full_param_padded, as it may not always be</span>
                        <span class="c1"># allocated such as during mixed precision.</span>
                        <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">p_data</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">p_full_size</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Allocate based on full size from all shards.</span>
                        <span class="n">_alloc_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">p_full_size</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
                        <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="c1"># Fill output_tensor with (p.data for each shard in self.world_size)</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span>
                        <span class="n">output_tensor</span><span class="p">,</span> <span class="n">p_data</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
                    <span class="p">)</span>

                    <span class="c1"># The full parameter, which can be freed. Note that we</span>
                    <span class="c1"># append here before update_p_data so as to not saved the</span>
                    <span class="c1"># tensor with padding trimmed, which causes issues with</span>
                    <span class="c1"># writeback in _summon_full_params.</span>
                    <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">output_tensor</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>
                    <span class="c1"># Set p.data = output_tensor (with padding trimmed)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_p_data</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">output_tensor</span><span class="o">=</span><span class="n">output_tensor</span><span class="p">)</span>
                    <span class="c1"># We can free the reduced precision shard as we have the</span>
                    <span class="c1"># full precision parameter.</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                    <span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_free_mp_shard</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">output_tensors</span>

    <span class="k">def</span> <span class="nf">_check_rebuild_full_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">FlatParameter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks the validity of a call to :meth:`_rebuild_full_params` in terms</span>
<span class="sd">        of the execution order. If on the first iteration, this uses an</span>
<span class="sd">        all-gather to check that all ranks are running ``forward()`` with the</span>
<span class="sd">        same parameter, erroring if not, and on subsequent iterations, if the</span>
<span class="sd">        forward order differs from that of the first iteration (meaning that we</span>
<span class="sd">        can no longer guarantee correct execution since all-gathers may be</span>
<span class="sd">        mismatched), then we issue a warning to the user. This only issues</span>
<span class="sd">        warnings on the first deviating iteration and stops checking</span>
<span class="sd">        thereafter.</span>

<span class="sd">        Only the :meth:`_rebuild_full_params` calls in the forward pass are</span>
<span class="sd">        checked since a correct forward order should imply a correct</span>
<span class="sd">        pre-backward order for typical cases.</span>

<span class="sd">        Executing in ``no_sync()`` does not affect this check for</span>
<span class="sd">        ``FULL_SHARD`` and ``SHARD_GRAD_OP``: (1) Being in ``no_sync()`` in the</span>
<span class="sd">        first iteration does not yield a different forward</span>
<span class="sd">        :meth:`_rebuild_full_params()` sequence, and (2) being in ``no_sync()``</span>
<span class="sd">        in a later iteration does not give false positive warnings since the</span>
<span class="sd">        forward :meth:`_rebuild_full_params()` sequence still matches the first</span>
<span class="sd">        iteration sequence (for ``FULL_SHARD``) or the first iteration</span>
<span class="sd">        sequence&#39;s prefix (for ``SHARD_GRAD_OP``).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Only check when rebuilding the full parameters in the forward pass,</span>
        <span class="c1"># and skip the check (1) when in eval mode since then there is not a</span>
        <span class="c1"># safe point at which to reset the execution order data and (2) if</span>
        <span class="c1"># world size is 1 since then there is no chance of desynchronization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">!=</span> <span class="n">TrainingState_</span><span class="o">.</span><span class="n">FORWARD</span> <span class="ow">or</span> \
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">eod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exec_order_data</span>
        <span class="n">param_index</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">get_param_index</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eod</span><span class="o">.</span><span class="n">is_first_iter</span><span class="p">:</span>
            <span class="c1"># Only issue warnings on the first deviating iteration and stop</span>
            <span class="c1"># checking thereafter to avoid flooding the console</span>
            <span class="k">if</span> <span class="n">eod</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">==</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNED</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="c1"># However, we may issue multiple warnings on the first deviating</span>
            <span class="c1"># iteration to help debugging, where either:</span>
            <span class="c1"># 1. This iteration sees an extra `_rebuild_full_params()` in</span>
            <span class="c1"># `forward()` compared to the first iteration</span>
            <span class="n">msg_prefix</span> <span class="o">=</span> <span class="n">curr_param_order</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># non-`None` means we warn</span>
            <span class="k">if</span> <span class="n">eod</span><span class="o">.</span><span class="n">index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eod</span><span class="o">.</span><span class="n">param_order</span><span class="p">):</span>
                <span class="n">msg_prefix</span> <span class="o">=</span> <span class="s2">&quot;Expected to not rebuild any more parameters &quot;</span> \
                    <span class="s2">&quot;in `forward()` for this module but trying to rebuild &quot;</span> \
                    <span class="s2">&quot;parameters for &quot;</span>
                <span class="n">curr_param_order</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">param_order</span> <span class="o">+</span> <span class="p">[</span><span class="n">param_index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">expected_param_index</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">param_order</span><span class="p">[</span><span class="n">eod</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
                <span class="c1"># 2. This iteration sees the same number of</span>
                <span class="c1"># `_rebuild_full_params()` (so far) but the current parameter</span>
                <span class="c1"># differs</span>
                <span class="k">if</span> <span class="n">param_index</span> <span class="o">!=</span> <span class="n">expected_param_index</span><span class="p">:</span>
                    <span class="n">expected_param_names</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">expected_param_index</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> \
                        <span class="s2">&quot;Expected parameter should always be valid&quot;</span>
                    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="s2">&quot;Expected to rebuild parameters in &quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot;`forward()` for </span><span class="si">{</span><span class="n">expected_param_names</span><span class="si">}</span><span class="s2"> but &quot;</span> \
                        <span class="s2">&quot;instead trying to rebuild parameters for &quot;</span>
                    <span class="n">curr_param_order</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">param_order</span><span class="p">[:</span><span class="n">eod</span><span class="o">.</span><span class="n">index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">param_index</span><span class="p">]</span>
            <span class="n">to_issue_warning</span> <span class="o">=</span> <span class="n">msg_prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">to_issue_warning</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">curr_param_order</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">param_names</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">param_index</span><span class="p">)</span>
                <span class="n">is_added_param</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">is_added_param</span><span class="p">:</span>
                    <span class="n">msg_suffix</span> <span class="o">=</span> <span class="s2">&quot;a newly-added parameter since construction time&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg_suffix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">sub_msg</span> <span class="o">=</span> <span class="n">msg_prefix</span> <span class="o">+</span> <span class="n">msg_suffix</span>
                <span class="n">first_iter_param_names</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">eod</span><span class="o">.</span><span class="n">param_order</span>
                <span class="p">]</span>
                <span class="n">curr_iter_param_names</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">curr_param_order</span>
                <span class="p">]</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Forward order differs from that of the first iteration &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> -- collectives are unchecked and may &quot;</span>
                    <span class="s2">&quot;give incorrect results or hang</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">sub_msg</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>
                    <span class="sa">f</span><span class="s2">&quot;First iteration&#39;s forward order: </span><span class="si">{</span><span class="n">first_iter_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">This iteration&#39;s forward order (so far): &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">curr_iter_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="n">eod</span><span class="o">.</span><span class="n">warn_status</span> <span class="o">=</span> <span class="n">_ExecOrderWarnStatus</span><span class="o">.</span><span class="n">WARNING</span>
            <span class="n">eod</span><span class="o">.</span><span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use `compute_device` instead of the parameter&#39;s device in case it</span>
            <span class="c1"># is offloaded on CPU and we are using NCCL backend, which requires</span>
            <span class="c1"># communicated tensors be on GPU</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">param_index</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_all_gather_base</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="c1"># Check that all ranks plan to all-gather the same parameter index</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">i1</span><span class="p">),</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">i2</span><span class="p">)</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span>
                <span class="p">((</span><span class="n">rank</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span> <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)),</span> <span class="mi">2</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">):</span>
                    <span class="n">r1_param_names</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">i1</span><span class="p">)</span>
                    <span class="n">r2_param_names</span> <span class="o">=</span> <span class="n">eod</span><span class="o">.</span><span class="n">get_unflat_param_names</span><span class="p">(</span><span class="n">i2</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Forward order differs across ranks: rank </span><span class="si">{</span><span class="n">r1</span><span class="si">}</span><span class="s2"> is &quot;</span>
                        <span class="s2">&quot;rebuilding full parameters in `forward()` for &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">r1_param_names</span><span class="si">}</span><span class="s2"> while rank </span><span class="si">{</span><span class="n">r2</span><span class="si">}</span><span class="s2"> is rebuilding full &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;parameters in `forward()` for </span><span class="si">{</span><span class="n">r2_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="n">eod</span><span class="o">.</span><span class="n">param_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_index</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_prep_grads_for_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Make sure p.grad has the correct size/device, otherwise set it to None.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">_orig_size</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span>
            <span class="p">):</span>
                <span class="n">offloaded</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span>
                <span class="k">if</span> <span class="n">offloaded</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">,</span> \
                        <span class="s2">&quot;`p.grad.device` and `p.device` should be the same &quot;</span> \
                        <span class="s2">&quot;if not offloading parameters to CPU&quot;</span>
                <span class="n">prev_iter_outside_no_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> \
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="c1"># As long as the previous iteration was outside `no_sync()`,</span>
                <span class="c1"># then we must save the gradient in `_saved_grad_shard`, even</span>
                <span class="c1"># if the current iteration is inside `no_sync()`. This is to</span>
                <span class="c1"># prepare for the next iteration outside `no_sync()`, which may</span>
                <span class="c1"># try to accumulate gradients. FSDP accumulates gradients in</span>
                <span class="c1"># the separate variable `p._saved_grad_shard` to leave `p.grad`</span>
                <span class="c1"># for the per-iteration gradient.</span>
                <span class="k">if</span> <span class="n">prev_iter_outside_no_sync</span><span class="p">:</span>
                    <span class="c1"># FSDP currently does not support gradient accumulation</span>
                    <span class="c1"># outside `no_sync()` when using CPU offloading (see the</span>
                    <span class="c1"># warning in the class&#39;s docstring).</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">offloaded</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">_saved_grad_shard</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_free_full_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Free up storage for full parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="n">current_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># e.g., world_size == 1 or self.sharding_strategy = NO_SHARD</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">_is_sharded</span><span class="p">:</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_enabled_for_params</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_free_mp_shard</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">]))</span>
                <span class="k">continue</span>
            <span class="c1"># Don&#39;t let PyTorch reuse this memory until all work in the current</span>
            <span class="c1"># stream is complete.</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="c1"># There may be external references to the Tensor Storage that we</span>
            <span class="c1"># can&#39;t modify, such as references that are created by</span>
            <span class="c1"># ctx.save_for_backward in the forward pass. Thus when we</span>
            <span class="c1"># unshard parameters, we should reuse the original Tensor</span>
            <span class="c1"># Storage object and unshard it in-place. For now, just resize</span>
            <span class="c1"># the Storage to 0 to save memory.</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">_full_param_padded</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_use_param_local_shard</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Use local shard for a list of params. Also implicitly offloads</span>
<span class="sd">        parameters back to CPU if we are CPU offloading.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
                <span class="c1"># Ensure local_shard resides in CPU if we are offloading params.</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                    <span class="s2">&quot;cpu&quot;</span>
                <span class="p">),</span> <span class="s2">&quot;Expected p._local_shard to be on CPU&quot;</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">_local_shard</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">_assert_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TrainingState_</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TrainingState_</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Assert we are in the given state.&quot;&quot;&quot;</span>
        <span class="c1"># Since assert can be turned off and this error checking</span>
        <span class="c1"># is really important, we use explicit error checking</span>
        <span class="c1"># and raise a ValueError if needed.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">TrainingState_</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;expected to be in states </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2"> but current state &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="c1"># In case we are failing in the context of autograd hook, asserting</span>
            <span class="c1"># may not generate useful msg. So, let&#39;s print it to be sure.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Asserting FSDP instance is: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">traceback</span><span class="o">.</span><span class="n">print_stack</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.no_sync"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">no_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to disable gradient synchronizations across FSDP</span>
<span class="sd">        instances. Within this context, gradients will be accumulated in module</span>
<span class="sd">        variables, which will later be synchronized in the first</span>
<span class="sd">        forward-backward pass after exiting the context. This should only be</span>
<span class="sd">        used on the root FSDP instance and will recursively apply to all</span>
<span class="sd">        children FSDP instances.</span>

<span class="sd">        .. note:: This likely results in higher memory usage because FSDP will</span>
<span class="sd">            accumulate the full model gradients (instead of gradient shards)</span>
<span class="sd">            until the eventual sync.</span>

<span class="sd">        .. note:: When used with CPU offloading, the gradients will not be</span>
<span class="sd">            offloaded to CPU when inside the context manager. Instead, they</span>
<span class="sd">            will only be offloaded right after the eventual sync.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;`no_sync()` on inner FSDP instances is not supported&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="n">old_flags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="n">old_flags</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">))</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">old_flag</span> <span class="ow">in</span> <span class="n">old_flags</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">m</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span><span class="p">,</span> <span class="p">(</span>
                    <span class="s2">&quot;`_require_backward_grad_sync` was incorrectly set to &quot;</span>
                    <span class="s2">&quot;`True` while in the `no_sync()` context manager&quot;</span>
                <span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_require_backward_grad_sync</span> <span class="o">=</span> <span class="n">old_flag</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params_with_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively returns a list of all module parameters that have a gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

<div class="viewcode-block" id="FullyShardedDataParallel.clip_grad_norm_"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">clip_grad_norm_</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clip all gradients at this point in time. The norm is computed over all</span>
<span class="sd">        gradients together, as if they were concatenated into a single vector.</span>
<span class="sd">        Gradients are modified in-place.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_norm (float or int): max norm of the gradients</span>
<span class="sd">            norm_type (float or int): type of the used p-norm. Can be ``&#39;inf&#39;``</span>
<span class="sd">                for infinity norm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Total norm of the parameters (viewed as a single vector).</span>

<span class="sd">        .. note:: This is analogous to ``torch.nn.utils.clip_grad_norm_`` but</span>
<span class="sd">            handles the partitioning and multiple devices per rank under the</span>
<span class="sd">            hood. The default torch util is not applicable here, because each</span>
<span class="sd">            rank only has a partial view of all the grads in the model, so</span>
<span class="sd">            calling it for FSDP models would lead to different scaling being</span>
<span class="sd">            applied per subset of model parameters.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks, since synchronization</span>
<span class="sd">            primitives will be used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Call `_lazy_init` to ensure the stream synchronization is done appropriately.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">,</span> <span class="s2">&quot;clip_grad_norm should only be called on the root (parent) instance&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState_</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>

        <span class="n">max_norm</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_norm</span><span class="p">)</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
        <span class="c1"># Computes the max norm for this shard&#39;s gradients and sync&#39;s across workers</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">_calc_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_with_grad</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">local_norm</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">local_norm</span> <span class="o">**</span> <span class="n">norm_type</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">max_norm</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">total_norm</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">total_norm</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">clip_coef</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># multiply by clip_coef, aka, (max_norm/total_norm).</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_with_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">))</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">full_optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Consolidates the full optimizer state on rank 0 and returns it</span>
<span class="sd">        as a :class:`dict` following the convention of</span>
<span class="sd">        :meth:`torch.optim.Optimizer.state_dict`, i.e. with keys ``&quot;state&quot;``</span>
<span class="sd">        and ``&quot;param_groups&quot;``. The flattened parameters in ``FSDP`` modules</span>
<span class="sd">        contained in ``model`` are mapped back to their unflattened parameters.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks since synchronization</span>
<span class="sd">            primitives are used. However, if ``rank0_only=True``, then the</span>
<span class="sd">            state dict is only populated on rank 0, and all other ranks return</span>
<span class="sd">            an empty :class:`dict`.</span>

<span class="sd">        .. warning:: Unlike ``torch.optim.Optimizer.state_dict()``, this method</span>
<span class="sd">            uses full parameter names as keys instead of parameter IDs.</span>

<span class="sd">        .. warning:: If you do not pass ``model.parameters()`` as the first</span>
<span class="sd">            argument to the optimizer, then you should pass that same value to</span>
<span class="sd">            this method as ``optim_input``.</span>

<span class="sd">        .. note:: Like in :meth:`torch.optim.Optimizer.state_dict`, the tensors</span>
<span class="sd">            contained in the optimizer state dict are not cloned, so there may</span>
<span class="sd">            be aliasing surprises. For best practices, consider saving the</span>
<span class="sd">            returned optimizer state dict immediately, e.g. using</span>
<span class="sd">            ``torch.save()``.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                were passed into the optimizer ``optim``.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer ``optim`` representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. (Default: ``None``)</span>
<span class="sd">            rank0_only (bool): If ``True``, saves the populated :class:`dict`</span>
<span class="sd">                only on rank 0; if ``False``, saves it on all ranks. (Default:</span>
<span class="sd">                ``True``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: A :class:`dict` containing the optimizer state for</span>
<span class="sd">            ``model`` &#39;s original unflattened parameters and including keys</span>
<span class="sd">            &quot;state&quot; and &quot;param_groups&quot; following the convention of</span>
<span class="sd">            :meth:`torch.optim.Optimizer.state_dict`. If ``rank0_only=True``,</span>
<span class="sd">            then nonzero ranks return an empty :class:`dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">osd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">osd_state</span><span class="p">,</span> <span class="n">osd_param_groups</span> <span class="o">=</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">],</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>  <span class="c1"># alias</span>

        <span class="n">group</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">process_group</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;process_group&quot;</span><span class="p">)</span> \
            <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># not all `torch.nn.Module`s have `process_group`</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">to_save</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">rank0_only</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">full_osd</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="p">{},</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="p">[]}</span> <span class="k">if</span> <span class="n">to_save</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="n">full_osd_state</span> <span class="o">=</span> <span class="n">full_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">to_save</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># alias</span>

        <span class="c1"># Handle the &quot;state&quot; part of the optimizer state dict</span>
        <span class="n">param_to_unflat_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">flat_param_id_to_param</span> <span class="o">=</span> <span class="n">_get_param_id_to_param</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
        <span class="n">flat_param_to_fsdp_module</span> <span class="o">=</span> <span class="n">_get_flat_param_to_fsdp_module</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">flat_param_id</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_param_id_to_param</span><span class="p">):</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="c1"># Do not include parameters without state to avoid empty mappings</span>
            <span class="k">if</span> <span class="n">flat_param_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">osd_state</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">assert</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_unflat_param_names</span><span class="p">,</span> \
                <span class="s2">&quot;Check the `param_to_unflat_params` construction</span><span class="se">\n</span><span class="s2">&quot;</span> \
                <span class="sa">f</span><span class="s2">&quot;param: </span><span class="si">{</span><span class="n">param</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">unflat_param_names</span> <span class="o">=</span> <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
            <span class="c1"># For FSDP parameters, we need to unflatten</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">flat_param_to_fsdp_module</span><span class="p">,</span> \
                    <span class="s2">&quot;Check the `flat_param_to_fsdp_module` construction</span><span class="se">\n</span><span class="s2">&quot;</span> \
                    <span class="sa">f</span><span class="s2">&quot;param: </span><span class="si">{</span><span class="n">param</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">unflat_state</span> <span class="o">=</span> <span class="n">_unflatten_optim_state</span><span class="p">(</span>
                    <span class="n">flat_param_to_fsdp_module</span><span class="p">[</span><span class="n">param</span><span class="p">],</span> <span class="n">param</span><span class="p">,</span>
                    <span class="n">osd_state</span><span class="p">[</span><span class="n">flat_param_id</span><span class="p">],</span> <span class="n">to_save</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">to_save</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">unflat_state</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">unflat_param_names</span><span class="p">)</span> <span class="ow">and</span> \
                        <span class="nb">len</span><span class="p">(</span><span class="n">unflat_state</span><span class="p">)</span> <span class="o">==</span> <span class="n">param</span><span class="o">.</span><span class="n">_num_unflattened_params</span><span class="p">,</span> \
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">unflat_state</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">unflat_param_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">_num_unflattened_params</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">for</span> <span class="n">unflat_param_name</span><span class="p">,</span> <span class="n">unflat_param_state</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                        <span class="n">unflat_param_names</span><span class="p">,</span> <span class="n">unflat_state</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">full_osd_state</span><span class="p">[</span><span class="n">unflat_param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">unflat_param_state</span>
            <span class="c1"># For parameters from non-FSDP modules, we do not need to unflatten</span>
            <span class="k">elif</span> <span class="n">to_save</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">unflat_param_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="n">unflat_param_name</span> <span class="o">=</span> <span class="n">unflat_param_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># Do not `deepcopy()` to avoid unnecessarily duplicating</span>
                <span class="c1"># tensor storage</span>
                <span class="n">full_osd_state</span><span class="p">[</span><span class="n">unflat_param_name</span><span class="p">]</span> <span class="o">=</span> \
                    <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">osd_state</span><span class="p">[</span><span class="n">flat_param_id</span><span class="p">])</span>
                <span class="c1"># Move all tensor state to CPU</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="n">full_osd_state</span><span class="p">[</span><span class="n">unflat_param_name</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">state_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">param_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="n">state_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="c1"># Non-target ranks may return since there is no more communication</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">to_save</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">full_osd</span>

        <span class="c1"># Handle the &quot;param_groups&quot; part of the optimizer state dict</span>
        <span class="n">full_osd_param_groups</span> <span class="o">=</span> <span class="n">full_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>  <span class="c1"># alias</span>
        <span class="k">for</span> <span class="n">flat_param_group</span> <span class="ow">in</span> <span class="n">osd_param_groups</span><span class="p">:</span>
            <span class="n">unflat_param_group</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">flat_param_group</span><span class="p">)</span>
            <span class="n">param_group_params</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">flat_param_id_to_param</span><span class="p">[</span><span class="n">flat_param_id</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">flat_param_id</span> <span class="ow">in</span> <span class="n">flat_param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="n">nested_unflat_param_names</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group_params</span>
            <span class="p">]</span>
            <span class="n">unflat_param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">unflat_param_name</span>
                <span class="k">for</span> <span class="n">unflat_param_names</span> <span class="ow">in</span> <span class="n">nested_unflat_param_names</span>
                <span class="k">for</span> <span class="n">unflat_param_name</span> <span class="ow">in</span> <span class="n">unflat_param_names</span>
            <span class="p">]</span>  <span class="c1"># flatten the list of lists</span>
            <span class="n">full_osd_param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">unflat_param_group</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">full_osd</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.shard_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">shard_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shards the full optimizer state dict ``full_optim_state_dict`` by</span>
<span class="sd">        remapping the state to flattened parameters instead of unflattened</span>
<span class="sd">        parameters and restricting to only this rank&#39;s part of the optimizer</span>
<span class="sd">        state. The first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)</span>
<span class="sd">            &gt;&gt;&gt; torch.save(full_osd, PATH)</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = torch.load(PATH)</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. warning:: If you do not pass ``model.parameters()`` as the first</span>
<span class="sd">            argument to the optimizer, then you should pass that same value to</span>
<span class="sd">            this method as ``optim_input``.</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Dict[str, Any]): Optimizer state dict</span>
<span class="sd">                corresponding to the unflattened parameters and holding the</span>
<span class="sd">                full non-sharded optimizer state.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_flatten_full_optim_state_dict</span><span class="p">(</span>
            <span class="n">full_optim_state_dict</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.scatter_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">scatter_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Scatters the full optimizer state dict from rank 0 to all other ranks,</span>
<span class="sd">        returning the sharded optimizer state dict on each rank. The return</span>
<span class="sd">        value is the same as :meth:`shard_full_optim_state_dict`, and on rank</span>
<span class="sd">        0, the first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim, new_group = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Optional[Dict[str, Any]]): Optimizer state</span>
<span class="sd">                dict corresponding to the unflattened parameters and holding</span>
<span class="sd">                the full non-sharded optimizer state if on rank 0; the argument</span>
<span class="sd">                is ignored on nonzero ranks.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``; the argument is ignored on nonzero</span>
<span class="sd">                ranks. (Default: ``None``)</span>
<span class="sd">            group (Optional[Any]): Model&#39;s process group or ``None`` if using</span>
<span class="sd">                the default process group. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Try to use the passed-in process group, the model&#39;s process group,</span>
        <span class="c1"># or the default process group (i.e. ``None``) in that priority order</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;process_group&quot;</span><span class="p">):</span>
            <span class="n">group</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">process_group</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="c1"># Check for a valid broadcast device, preferring GPU when available</span>
        <span class="n">using_nccl</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_check_for_nccl_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">broadcast_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> \
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">using_nccl</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;NCCL requires a GPU for collectives&quot;</span><span class="p">)</span>
        <span class="c1"># Flatten the optimizer state dict and construct a copy with the</span>
        <span class="c1"># positive-dimension tensors&#39; shapes in place of the tensors themselves</span>
        <span class="c1"># since those tensors will be broadcast separately to avoid copying</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">full_optim_state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rank 0 must pass in the full optimizer state dict&quot;</span><span class="p">)</span>
            <span class="n">flat_osd</span><span class="p">,</span> <span class="n">fsdp_flat_param_ids</span> <span class="o">=</span> <span class="n">_flatten_full_optim_state_dict</span><span class="p">(</span>
                <span class="n">full_optim_state_dict</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">processed_osd</span> <span class="o">=</span> <span class="n">_process_pos_dim_tensor_state</span><span class="p">(</span>
                <span class="n">flat_osd</span><span class="p">,</span> <span class="n">fsdp_flat_param_ids</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Broadcast the optim state dict without positive-dimension tensor</span>
        <span class="c1"># state and the FSDP parameter IDs from rank 0 to all ranks</span>
        <span class="n">processed_osd</span><span class="p">,</span> <span class="n">fsdp_flat_param_ids</span> <span class="o">=</span> \
            <span class="n">_broadcast_processed_optim_state_dict</span><span class="p">(</span>
                <span class="n">processed_osd</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">fsdp_flat_param_ids</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Broadcast positive-dimension tensor state (both sharded tensors for</span>
        <span class="c1"># FSDP parameters and unsharded tensors for non-FSDP parameters)</span>
        <span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">_broadcast_pos_dim_tensor_states</span><span class="p">(</span>
            <span class="n">processed_osd</span><span class="p">,</span> <span class="n">fsdp_flat_param_ids</span><span class="p">,</span>
            <span class="n">flat_osd</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span>
            <span class="n">broadcast_device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sharded_osd</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.rekey_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">rekey_optim_state_dict</span><span class="p">(</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">optim_state_key_type</span><span class="p">:</span> <span class="n">OptimStateKeyType</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
        <span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Re-keys the optimizer state dict ``optim_state_dict`` to use the key</span>
<span class="sd">        type ``optim_state_key_type``. This can be used to achieve</span>
<span class="sd">        compatibility between optimizer state dicts from models with FSDP</span>
<span class="sd">        instances and ones without.</span>

<span class="sd">        To re-key an FSDP full optimizer state dict (i.e. from</span>
<span class="sd">        :meth:`full_optim_state_dict`) to use parameter IDs and be loadable to</span>
<span class="sd">        a non-wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_optim.load_state_dict(rekeyed_osd)</span>

<span class="sd">        To re-key a normal optimizer state dict from a non-wrapped model to be</span>
<span class="sd">        loadable to a wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; osd = nonwrapped_optim.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The optimizer state dict re-keyed using the</span>
<span class="sd">            parameter keys specified by ``optim_state_key_type``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">optim_state_key_type</span> <span class="ow">in</span> \
            <span class="p">(</span><span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">,</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">)</span>
        <span class="n">osd</span> <span class="o">=</span> <span class="n">optim_state_dict</span>  <span class="c1"># alias</span>
        <span class="c1"># Validate that the existing parameter keys are uniformly typed</span>
        <span class="n">uses_param_name_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="n">uses_param_id_mask</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">))</span> <span class="ow">or</span> \
                <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)):</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Invalid parameter keys: </span><span class="si">{</span><span class="n">osd</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>
        <span class="c1"># Return directly if the existing key type matches the target key type</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span> <span class="ow">and</span>
            <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">))</span> <span class="ow">or</span> \
            <span class="p">(</span><span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span> <span class="ow">and</span>
                <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">osd</span>
        <span class="c1"># Otherwise, actually perform the re-keying</span>
        <span class="n">new_osd</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">:</span>  <span class="c1"># ID -&gt; name</span>
            <span class="n">param_id_to_param</span> <span class="o">=</span> <span class="n">_get_param_id_to_param</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
            <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_param_name</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_id_to_param_name</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">param_to_param_name</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_id_to_param</span>
            <span class="p">]</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span>
                    <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">param_id</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                <span class="p">])</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">elif</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">:</span>  <span class="c1"># name -&gt; ID</span>
            <span class="n">param_name_to_param</span> <span class="o">=</span> <span class="n">_get_param_name_to_param</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_to_param_id</span> <span class="o">=</span> <span class="n">_get_param_to_param_id</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
            <span class="c1"># Because not all model parameters may be passed as the optimizer</span>
            <span class="c1"># input, we may need to drop some parameters from this mapping</span>
            <span class="n">param_name_to_param_id</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name</span><span class="p">:</span> <span class="n">param_to_param_id</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_name_to_param</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_param_id</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span>
                    <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                <span class="p">])</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">return</span> <span class="n">new_osd</span>  <span class="c1"># should never reach here</span></div></div>


<span class="k">def</span> <span class="nf">_get_default_cuda_device</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Try to infer CUDA device from module parameters.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">compute_device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="n">compute_device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">compute_device</span>
    <span class="c1"># e.g., if module does not have parameters, it will throw StopIteration,</span>
    <span class="c1"># in this case, instead of raising exception, return cuda device.</span>
    <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="c1"># Fall back to current CUDA device</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_free_storage</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Free underlying storage of a Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Since we&#39;re modifying the Tensor&#39;s Storage directly, make sure the Tensor</span>
        <span class="c1"># is the sole occupant of the Storage.</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">data</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;The tensor is not the sole occupant of the storage.&quot;</span>
        <span class="n">data</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_alloc_storage</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Allocate storage for a tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">size</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>  <span class="c1"># no need to reallocate</span>
        <span class="k">return</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">data</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">),</span> <span class="s2">&quot;Then tensor storage should have been resized to be 0.&quot;</span>
    <span class="n">data</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">size</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>  <span class="c1"># type: ignore[attr-defined]</span>

<span class="k">def</span> <span class="nf">p_assert</span><span class="p">(</span><span class="n">cond</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;This is used as an alternate to ``assert`` when in the backward context</span>
<span class="sd">    to print the error message ``s`` since otherwise, it is swallowed.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cond</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span>

<span class="k">def</span> <span class="nf">_calc_grad_norm</span><span class="p">(</span><span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate gradient norm of an iterable of parameters.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Total norm of the parameters (viewed as a single vector).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">par</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute the norm in full precision no matter what</span>
        <span class="n">local_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">par</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">parameters</span>
                <span class="p">]</span>
            <span class="p">),</span>
            <span class="n">p</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">local_norm</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">local_norm</span>


<span class="k">def</span> <span class="nf">_get_param_to_unflat_param_names</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">dedup_shared_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a mapping from flattened parameter (including non-FSDP-module</span>
<span class="sd">    parameters) to its unflattened parameter names. For non-FSDP-module</span>
<span class="sd">    parameters, these mapped-to lists always contain a single element. The</span>
<span class="sd">    unflattened parameter names should match the keys of the model state dict.</span>

<span class="sd">    For shared parameters, only the first parameter name is included (following</span>
<span class="sd">    the ``torch.nn.Module.parameters()`` order).</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">            :class:`FullyShardedDataParallel` instance).</span>
<span class="sd">        dedup_shared_params (bool): If ``True``, only includes the first</span>
<span class="sd">            list of unflattened parameter names corresponding to a parameter</span>
<span class="sd">            in the module walk order; if ``False``, then includes all of the</span>
<span class="sd">            unflattened parameter names.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_clean_param_name</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">param_info</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This replicates the parameter name cleaning logic in model state</span>
<span class="sd">        dict but avoids gathering any parameters.&quot;&quot;&quot;</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">clean_tensor_name</span><span class="p">(</span>
            <span class="n">prefix</span> <span class="o">+</span> <span class="n">param_info</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">param_info</span><span class="o">.</span><span class="n">param_name</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">module_fn</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">param_to_unflat_param_names</span><span class="p">):</span>
        <span class="c1"># For FSDP modules, only add the entry when considering the contained</span>
        <span class="c1"># `FlattenParamsWrapper` to avoid duplication</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">prefixed_param_names</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">_clean_param_name</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">param_info</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">param_info</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">_param_infos</span>
                <span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">FlatParameter</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">param_name</span><span class="p">]</span>
                <span class="c1"># If this parameter has already been visited, then it is a</span>
                <span class="c1"># shared parameter; then, only take the first parameter name</span>
                <span class="n">is_shared_param</span> <span class="o">=</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_unflat_param_names</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shared_param</span><span class="p">:</span>
                    <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">prefixed_param_names</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">dedup_shared_params</span><span class="p">:</span>
                    <span class="n">param_to_unflat_param_names</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">prefixed_param_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">return_fn</span><span class="p">(</span><span class="n">param_to_unflat_param_names</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">param_to_unflat_param_names</span>

    <span class="n">param_to_unflat_param_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">return</span> <span class="n">_apply_to_modules</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">module_fn</span><span class="p">,</span> <span class="n">return_fn</span><span class="p">,</span> <span class="n">param_to_unflat_param_names</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_param_to_param_name</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a mapping from parameters to their parameter names. ``model``</span>
<span class="sd">    should not contain any :class:`FullyShardedDataParallel` instances, which</span>
<span class="sd">    means that none of the parameters should be ``FlatParameter`` s. As a</span>
<span class="sd">    result, compared to :meth:`_get_param_to_unflat_param_names`, the mapped</span>
<span class="sd">    values may be flattened from singleton :class:`list` s to the contained</span>
<span class="sd">    names themselves.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): Root module, which should not contain any</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">param_to_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_unflat_param_names</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`_get_param_to_unflat_param_names()` &quot;</span> \
            <span class="s2">&quot;should not construct empty lists&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Each parameter should only map to one parameter name but got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">param</span><span class="p">:</span> <span class="n">param_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">param_to_param_name</span>


<span class="k">def</span> <span class="nf">_get_param_name_to_param</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Constructs the inverse mapping of :meth:`_get_param_to_param_name`.&quot;&quot;&quot;</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_param_name</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">param_to_param_name</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">param_to_param_name</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">clean_tensor_name</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Cleans the parameter or buffer name by removing any FSDP-related</span>
<span class="sd">    prefixes.&quot;&quot;&quot;</span>
    <span class="c1"># FSDP full tensor names may not have both (i.e. `FSDP_PREFIX`), so we</span>
    <span class="c1"># call `replace()` twice separately</span>
    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">tensor_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_WRAPPED_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">tensor_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FPW_MODULE</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_name</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Probability distributions - torch.distributions &mdash; PyTorch master documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multiprocessing package - torch.multiprocessing" href="multiprocessing.html" />
    <link rel="prev" title="Automatic differentiation package - torch.autograd" href="autograd.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                <a href="http://pytorch.org/docs/versions.html">master (0.4.0a0+409b1c8 ) &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id18"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id19"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id20"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id21"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id22"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id23"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id24"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id25"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id26"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id27"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id28"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id29"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id30"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id31"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id32"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id33"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id34"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id35"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id36"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id37"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id38"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id39"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id40"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id41">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id42">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id44"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Probability distributions - torch.distributions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/distributions.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-torch.distributions">
<span id="probability-distributions-torch-distributions"></span><h1>Probability distributions - torch.distributions<a class="headerlink" href="#module-torch.distributions" title="Permalink to this headline"></a></h1>
<p>The <code class="docutils literal"><span class="pre">distributions</span></code> package contains parameterizable probability distributions
and sampling functions.</p>
<p>Policy gradient methods can be implemented using the
<a class="reference internal" href="#torch.distributions.Distribution.log_prob" title="torch.distributions.Distribution.log_prob"><code class="xref py py-meth docutils literal"><span class="pre">log_prob()</span></code></a> method, when the probability
density function is differentiable with respect to its parameters. A basic
method is the REINFORCE rule:</p>
<div class="math">
\[\Delta\theta  = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}\]</div>
<p>where <span class="math">\(\theta\)</span> are the parameters, <span class="math">\(\alpha\)</span> is the learning rate,
<span class="math">\(r\)</span> is the reward and <span class="math">\(p(a|\pi^\theta(s))\)</span> is the probability of
taking action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span> given policy <span class="math">\(\pi^\theta\)</span>.</p>
<p>In practice we would sample an action from the output of a network, apply this
action in an environment, and then use <code class="docutils literal"><span class="pre">log_prob</span></code> to construct an equivalent
loss function. Note that we use a negative because optimisers use gradient
descent, whilst the rule above assumes gradient ascent. With a categorical
policy, the code for implementing REINFORCE would be as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">policy_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="c1"># NOTE: this is equivalent to what used to be called multinomial</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">m</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">*</span> <span class="n">reward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="distribution">
<h2><span class="hidden-section">Distribution</span><a class="headerlink" href="#distribution" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Distribution">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Distribution</code><span class="sig-paren">(</span><em>batch_shape=torch.Size([])</em>, <em>event_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution" title="Permalink to this definition"></a></dt>
<dd><p>Distribution is the abstract base class for probability distributions.</p>
<dl class="attribute">
<dt id="torch.distributions.Distribution.batch_shape">
<code class="descname">batch_shape</code><a class="headerlink" href="#torch.distributions.Distribution.batch_shape" title="Permalink to this definition"></a></dt>
<dd><p>Returns the shape over which parameters are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.entropy" title="Permalink to this definition"></a></dt>
<dd><p>Returns entropy of distribution, batched over batch_shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Tensor or Variable of shape batch_shape.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.enumerate_support"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.enumerate_support" title="Permalink to this definition"></a></dt>
<dd><p>Returns tensor containing all values supported by a discrete
distribution. The result will enumerate over dimension 0, so the shape
of the result will be <cite>(cardinality,) + batch_shape + event_shape</cite>
(where <cite>event_shape = ()</cite> for univariate distributions).</p>
<p>Note that this enumerates over all batched variables in lock-step
<cite>[[0, 0], [1, 1], ]</cite>. To iterate over the full Cartesian product
use <cite>itertools.product(m.enumerate_support())</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Variable or Tensor iterating over dimension 0.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.Distribution.event_shape">
<code class="descname">event_shape</code><a class="headerlink" href="#torch.distributions.Distribution.event_shape" title="Permalink to this definition"></a></dt>
<dd><p>Returns the shape of a single sample (without batching).</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.log_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.log_prob" title="Permalink to this definition"></a></dt>
<dd><p>Returns the log of the probability density/mass function evaluated at
<cite>value</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  </td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.Distribution.params">
<code class="descname">params</code><a class="headerlink" href="#torch.distributions.Distribution.params" title="Permalink to this definition"></a></dt>
<dd><p>Returns a dictionary from param names to <cite>Constraint</cite> objects that
should be satisfied by each parameter of this distribution. For
distributions with multiple parameterization, only one complete
set of parameters should be specified in <cite>.params</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.rsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.rsample" title="Permalink to this definition"></a></dt>
<dd><p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.sample" title="Permalink to this definition"></a></dt>
<dd><p>Generates a sample_shape shaped sample or sample_shape shaped batch of
samples if the distribution parameters are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.Distribution.sample_n">
<code class="descname">sample_n</code><span class="sig-paren">(</span><em>n</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/distribution.html#Distribution.sample_n"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Distribution.sample_n" title="Permalink to this definition"></a></dt>
<dd><p>Generates n samples or n batches of samples if the distribution
parameters are batched.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.Distribution.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.Distribution.support" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <cite>Constraint</cite> object representing this distributions support.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bernoulli">
<h2><span class="hidden-section">Bernoulli</span><a class="headerlink" href="#bernoulli" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Bernoulli">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Bernoulli</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/bernoulli.html#Bernoulli"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Bernoulli" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Bernoulli distribution parameterized by <cite>probs</cite> or <cite>logits</cite>.</p>
<p>Samples are binary (0 or 1). They take the value <cite>1</cite> with probability <cite>p</cite>
and <cite>0</cite> with probability <cite>1 - p</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># 30% chance 1; 70% chance 0</span>
<span class="go"> 0.0</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  the probabilty of sampling <cite>1</cite></li>
<li><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  the log-odds of sampling <cite>1</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="beta">
<h2><span class="hidden-section">Beta</span><a class="headerlink" href="#beta" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Beta">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Beta</code><span class="sig-paren">(</span><em>concentration1</em>, <em>concentration0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/beta.html#Beta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Beta" title="Permalink to this definition"></a></dt>
<dd><p>Beta distribution parameterized by <cite>concentration1</cite> and <cite>concentration0</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Beta distributed with concentration concentration1 and concentration0</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  1st concentration parameter of the distribution
(often referred to as alpha)</li>
<li><strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  2nd concentration parameter of the distribution
(often referred to as beta)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="binomial">
<h2><span class="hidden-section">Binomial</span><a class="headerlink" href="#binomial" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Binomial">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Binomial</code><span class="sig-paren">(</span><em>total_count=1</em>, <em>probs=None</em>, <em>logits=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/binomial.html#Binomial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Binomial" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Binomial distribution parameterized by <cite>total_count</cite> and
either <cite>probs</cite> or <cite>logits</cite> (but not both).</p>
<ul class="simple">
<li>Requires a single shared <cite>total_count</cite> for all
parameters and samples.</li>
</ul>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span> <span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go"> 0</span>
<span class="go"> 22</span>
<span class="go"> 71</span>
<span class="go"> 100</span>
<span class="go">[torch.FloatTensor of size 4]]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>)  number of Bernoulli trials</li>
<li><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Event probabilities</li>
<li><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Event log-odds</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="categorical">
<h2><span class="hidden-section">Categorical</span><a class="headerlink" href="#categorical" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Categorical">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Categorical</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/categorical.html#Categorical"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Categorical" title="Permalink to this definition"></a></dt>
<dd><p>Creates a categorical distribution parameterized by either <cite>probs</cite> or
<cite>logits</cite> (but not both).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is equivalent to the distribution that <a class="reference internal" href="torch.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal"><span class="pre">torch.multinomial()</span></code></a>
samples from.</p>
</div>
<p>Samples are integers from <cite>0  K-1</cite> where <cite>K</cite> is probs.size(-1).</p>
<p>If <cite>probs</cite> is 1D with length-<cite>K</cite>, each element is the relative probability
of sampling the class at that index.</p>
<p>If <cite>probs</cite> is 2D, it is treated as a batch of probability vectors.</p>
<p>See also: <a class="reference internal" href="torch.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal"><span class="pre">torch.multinomial()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go"> 3</span>
<span class="go">[torch.LongTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  event probabilities</li>
<li><strong>logits</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  event log probabilities</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cauchy">
<h2><span class="hidden-section">Cauchy</span><a class="headerlink" href="#cauchy" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Cauchy">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Cauchy</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/cauchy.html#Cauchy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Cauchy" title="Permalink to this definition"></a></dt>
<dd><p>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of
independent normally distributed random variables with means <cite>0</cite> follows a
Cauchy distribution.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Cauchy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Cauchy distribution with loc=0 and scale=1</span>
<span class="go"> 2.3214</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  mode or median of the distribution.</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  half width at half maximum.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="chi2">
<h2><span class="hidden-section">Chi2</span><a class="headerlink" href="#chi2" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Chi2">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Chi2</code><span class="sig-paren">(</span><em>df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/chi2.html#Chi2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Chi2" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Chi2 distribution parameterized by shape parameter <cite>df</cite>.
This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Chi2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Chi2 distributed with shape df=1</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  shape parameter of the distribution</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="dirichlet">
<h2><span class="hidden-section">Dirichlet</span><a class="headerlink" href="#dirichlet" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Dirichlet">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Dirichlet</code><span class="sig-paren">(</span><em>concentration</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/dirichlet.html#Dirichlet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Dirichlet" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Dirichlet distribution parameterized by concentration <cite>concentration</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Dirichlet distributed with concentrarion concentration</span>
<span class="go"> 0.1046</span>
<span class="go"> 0.8954</span>
<span class="go">[torch.FloatTensor of size 2]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>concentration</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  concentration parameter of the distribution
(often referred to as alpha)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="exponential">
<h2><span class="hidden-section">Exponential</span><a class="headerlink" href="#exponential" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Exponential">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Exponential</code><span class="sig-paren">(</span><em>rate</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/exponential.html#Exponential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Exponential" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Exponential distribution parameterized by <cite>rate</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Exponential distributed with rate=1</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  rate = 1 / scale of the distribution</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="fishersnedecor">
<h2><span class="hidden-section">FisherSnedecor</span><a class="headerlink" href="#fishersnedecor" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.FisherSnedecor">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">FisherSnedecor</code><span class="sig-paren">(</span><em>df1</em>, <em>df2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/fishersnedecor.html#FisherSnedecor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.FisherSnedecor" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Fisher-Snedecor distribution parameterized by <cite>df1</cite> and <cite>df2</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FisherSnedecor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Fisher-Snedecor-distributed with df1=1 and df2=2</span>
<span class="go"> 0.2453</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>df1</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  degrees of freedom parameter 1</li>
<li><strong>df2</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  degrees of freedom parameter 2</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="gamma">
<h2><span class="hidden-section">Gamma</span><a class="headerlink" href="#gamma" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Gamma">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Gamma</code><span class="sig-paren">(</span><em>concentration</em>, <em>rate</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/gamma.html#Gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Gamma" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Gamma distribution parameterized by shape <cite>concentration</cite> and <cite>rate</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Gamma distributed with concentration=1 and rate=1</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  shape parameter of the distribution
(often referred to as alpha)</li>
<li><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  rate = 1 / scale of the distribution
(often referred to as beta)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="geometric">
<h2><span class="hidden-section">Geometric</span><a class="headerlink" href="#geometric" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Geometric">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Geometric</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/geometric.html#Geometric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Geometric" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Geometric distribution parameterized by <cite>probs</cite>, where <cite>probs</cite> is the probability of success of Bernoulli
trials. It represents the probability that in k + 1 Bernoulli trials, the first k trials failed, before
seeing a success.</p>
<p>Samples are non-negative integers [0, inf).</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Geometric</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># underlying Bernoulli has 30% chance 1; 70% chance 0</span>
<span class="go"> 2</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  the probabilty of sampling <cite>1</cite>. Must be in range (0, 1]</li>
<li><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  the log-odds of sampling <cite>1</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="gumbel">
<h2><span class="hidden-section">Gumbel</span><a class="headerlink" href="#gumbel" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Gumbel">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Gumbel</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/gumbel.html#Gumbel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Gumbel" title="Permalink to this definition"></a></dt>
<dd><p>Samples from a Gumbel Distribution.</p>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gumbel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from Gumbel distribution with loc=1, scale=2</span>
<span class="go"> 1.0124</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Location parameter of the distribution</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Scale parameter of the distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="laplace">
<h2><span class="hidden-section">Laplace</span><a class="headerlink" href="#laplace" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Laplace">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/laplace.html#Laplace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Laplace" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Laplace distribution parameterized by <cite>loc</cite> and scale.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Laplace</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Laplace distributed with loc=0, scale=1</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  mean of the distribution</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  scale of the distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="normal">
<h2><span class="hidden-section">Normal</span><a class="headerlink" href="#normal" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Normal">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Normal</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/normal.html#Normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Normal" title="Permalink to this definition"></a></dt>
<dd><p>Creates a normal (also called Gaussian) distribution parameterized by
<cite>loc</cite> and <cite>scale</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with loc=0 and scale=1</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  mean of the distribution (often referred to as mu)</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  standard deviation of the distribution
(often referred to as sigma)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="onehotcategorical">
<h2><span class="hidden-section">OneHotCategorical</span><a class="headerlink" href="#onehotcategorical" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.OneHotCategorical">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">OneHotCategorical</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.OneHotCategorical" title="Permalink to this definition"></a></dt>
<dd><p>Creates a one-hot categorical distribution parameterized by <cite>probs</cite>.</p>
<p>Samples are one-hot coded vectors of size probs.size(-1).</p>
<p>See also: <a class="reference internal" href="#torch.distributions.Categorical" title="torch.distributions.Categorical"><code class="xref py py-func docutils literal"><span class="pre">torch.distributions.Categorical()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">OneHotCategorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go"> 1</span>
<span class="go"> 0</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>probs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  event probabilities</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pareto">
<h2><span class="hidden-section">Pareto</span><a class="headerlink" href="#pareto" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Pareto">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Pareto</code><span class="sig-paren">(</span><em>scale</em>, <em>alpha</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/pareto.html#Pareto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Pareto" title="Permalink to this definition"></a></dt>
<dd><p>Samples from a Pareto Type 1 distribution.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Pareto</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Pareto distribution with scale=1 and alpha=1</span>
<span class="go"> 1.5623</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Scale parameter of the distribution</li>
<li><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  Shape parameter of the distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="studentt">
<h2><span class="hidden-section">StudentT</span><a class="headerlink" href="#studentt" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.StudentT">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">StudentT</code><span class="sig-paren">(</span><em>df</em>, <em>loc=0.0</em>, <em>scale=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/studentT.html#StudentT"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.StudentT" title="Permalink to this definition"></a></dt>
<dd><p>Creates a Students t-distribution parameterized by <cite>df</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">StudentT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Student&#39;s t-distributed with degrees of freedom=2</span>
<span class="go"> 0.1046</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  degrees of freedom</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="uniform">
<h2><span class="hidden-section">Uniform</span><a class="headerlink" href="#uniform" title="Permalink to this headline"></a></h2>
<dl class="class">
<dt id="torch.distributions.Uniform">
<em class="property">class </em><code class="descclassname">torch.distributions.</code><code class="descname">Uniform</code><span class="sig-paren">(</span><em>low</em>, <em>high</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/uniform.html#Uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.Uniform" title="Permalink to this definition"></a></dt>
<dd><p>Generates uniformly distributed random samples from the half-open interval
<cite>[low, high)</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">5.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># uniformly distributed in the range [0.0, 5.0)</span>
<span class="go"> 2.3418</span>
<span class="go">[torch.FloatTensor of size 1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>low</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  lower range (inclusive).</li>
<li><strong>high</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>)  upper range (exclusive).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-torch.distributions.kl">
<span id="kl-divergence"></span><h2><cite>KL Divergence</cite><a class="headerlink" href="#module-torch.distributions.kl" title="Permalink to this headline"></a></h2>
<dl class="function">
<dt id="torch.distributions.kl.kl_divergence">
<code class="descclassname">torch.distributions.kl.</code><code class="descname">kl_divergence</code><span class="sig-paren">(</span><em>p</em>, <em>q</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/kl.html#kl_divergence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.kl.kl_divergence" title="Permalink to this definition"></a></dt>
<dd><p>Compute Kullback-Leibler divergence <span class="math">\(KL(p \| q)\)</span> between two distributions.</p>
<div class="math">
\[KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>p</strong> (<a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><em>Distribution</em></a>)  A <a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code></a> object.</li>
<li><strong>q</strong> (<a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><em>Distribution</em></a>)  A <a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code></a> object.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A batch of KL divergences of shape <cite>batch_shape</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first"><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable">Variable</a> or <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">NotImplementedError</span></code>  If the distribution types have not been registered via
<a class="reference internal" href="#torch.distributions.kl.register_kl" title="torch.distributions.kl.register_kl"><code class="xref py py-meth docutils literal"><span class="pre">register_kl()</span></code></a>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.distributions.kl.register_kl">
<code class="descclassname">torch.distributions.kl.</code><code class="descname">register_kl</code><span class="sig-paren">(</span><em>type_p</em>, <em>type_q</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributions/kl.html#register_kl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.distributions.kl.register_kl" title="Permalink to this definition"></a></dt>
<dd><p>Decorator to register a pairwise function with <a class="reference internal" href="#torch.distributions.kl.kl_divergence" title="torch.distributions.kl.kl_divergence"><code class="xref py py-meth docutils literal"><span class="pre">kl_divergence()</span></code></a>.
Usage:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">Normal</span><span class="p">,</span> <span class="n">Normal</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_normal_normal</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="c1"># insert implementation here</span>
</pre></div>
</div>
<p>Lookup returns the most specific (type,type) match ordered by subclass. If
the match is ambiguous, a <cite>RuntimeWarning</cite> is raised. For example to
resolve the ambiguous situation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">BaseP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_version1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
<span class="nd">@register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">BaseQ</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_version2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>you should register a third most-specific implementation, e.g.:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)(</span><span class="n">kl_version1</span><span class="p">)</span>  <span class="c1"># Break the tie.</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>type_p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a>)  A subclass of <a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code></a>.</li>
<li><strong>type_q</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a>)  A subclass of <a class="reference internal" href="#torch.distributions.Distribution" title="torch.distributions.Distribution"><code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code></a>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="multiprocessing.html" class="btn btn-neutral float-right" title="Multiprocessing package - torch.multiprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="autograd.html" class="btn btn-neutral" title="Automatic differentiation package - torch.autograd" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
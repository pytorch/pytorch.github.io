


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._utils &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/_utils.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

<!--
  Search engines should not index the master version of documentation.
  Stable documentation are built without release == 'master'.
-->
<meta name="robots" content="noindex">


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>master (2.1.0a0+gitbe0b12e ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/_utils.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ir.html">IRs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_dynamo.html">torch._dynamo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch._utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._utils</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copyreg</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">DefaultDict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">def</span> <span class="nf">_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the type if `dtype` is not provided, else casts this object to</span>
<span class="sd">    the specified type.</span>

<span class="sd">    If this is already of the correct type, no copy is performed and the</span>
<span class="sd">    original object is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype (type or string): The desired type</span>
<span class="sd">        non_blocking (bool): If ``True``, and the source is in pinned memory</span>
<span class="sd">            and destination is on the GPU or vice versa, the copy is performed</span>
<span class="sd">            asynchronously with respect to the host. Otherwise, the argument</span>
<span class="sd">            has no effect.</span>
<span class="sd">        **kwargs: For compatibility, may contain the key ``async`` in place of</span>
<span class="sd">            the ``non_blocking`` argument. The ``async`` arg is deprecated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">non_blocking</span> <span class="o">=</span> <span class="n">_get_async_or_non_blocking</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">_import_dotted_name</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot cast sparse tensor to dense tensor&quot;</span><span class="p">)</span>
        <span class="n">new_module_name</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="vm">__module__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.sparse&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="n">new_values_type_name</span> <span class="o">=</span> <span class="n">new_module_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">dtype</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">new_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">new_values_type_name</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="n">new_indices_type_name</span> <span class="o">=</span> <span class="n">new_module_name</span> <span class="o">+</span> <span class="s2">&quot;.LongTensor&quot;</span>
        <span class="n">new_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span>
            <span class="n">new_indices_type_name</span><span class="p">,</span> <span class="n">non_blocking</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">dtype</span><span class="p">(</span><span class="n">new_indices</span><span class="p">,</span> <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot cast dense tensor to sparse tensor&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this object in CUDA memory.</span>

<span class="sd">    If this object is already in CUDA memory and on the correct device, then</span>
<span class="sd">    no copy is performed and the original object is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (int): The destination GPU id. Defaults to the current device.</span>
<span class="sd">        non_blocking (bool): If ``True`` and the source is in pinned memory,</span>
<span class="sd">            the copy will be asynchronous with respect to the host. Otherwise,</span>
<span class="sd">            the argument has no effect.</span>
<span class="sd">        **kwargs: For compatibility, may contain the key ``async`` in place of</span>
<span class="sd">            the ``non_blocking`` argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">non_blocking</span> <span class="o">=</span> <span class="n">_get_async_or_non_blocking</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">new_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">new_type</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">untyped_storage</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">untyped_storage</span>


<span class="k">def</span> <span class="nf">_get_async_or_non_blocking</span><span class="p">(</span><span class="n">function_name</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the non-blocking flag given the function name and kwargs.</span>

<span class="sd">    Args:</span>
<span class="sd">        function_name (str): the name of the function being used.</span>
<span class="sd">        non_blocking (bool): the default value.</span>
<span class="sd">        **kwargs (dict): the kwargs passed to the function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">non_blocking</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="s2">&quot;async&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">() got an unexpected keyword argument &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
        <span class="n">argument</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">function_name</span><span class="p">,</span> <span class="n">argument</span><span class="p">))</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&#39;async&#39; is deprecated; use &#39;non_blocking&#39;&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;async&quot;</span><span class="p">]</span>


<span class="c1"># Note [Don&#39;t serialize hooks]</span>
<span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1"># Since time immemorial, we have serialized the backward hooks associated with</span>
<span class="c1"># variables.  This kind of half-worked--Python can pickle global functions</span>
<span class="c1"># (but not closures!)--but there were problems.</span>
<span class="c1">#</span>
<span class="c1">#   - It&#39;s fragile.  If you serialize a backward hook into a saved</span>
<span class="c1">#     model, and then you rename the function associated with the hook,</span>
<span class="c1">#     now your saved model is broken and you can&#39;t load it anymore.</span>
<span class="c1">#</span>
<span class="c1">#   - It&#39;s not actually used.  The standard recommendation is to</span>
<span class="c1">#     serialize the *state_dict* of a model, not the model itself</span>
<span class="c1">#     (since this is more stable to code changes affecting the model</span>
<span class="c1">#     serialization), and the state dict saves &quot;data&quot; only, thus</span>
<span class="c1">#     stripping the the backward hooks.  In some cases, hooks are</span>
<span class="c1">#     essential to the well-functioning of a model (e.g., DDP),</span>
<span class="c1">#     but DDP already manages readding the hooks!</span>
<span class="c1">#</span>
<span class="c1">#   - We didn&#39;t serialize them in many cases.  Prior to #10220, we</span>
<span class="c1">#     were dropping backward hooks in ForkingPickler.  We &quot;fixed&quot; this</span>
<span class="c1">#     to be convenient with other serialization sites, but lack of</span>
<span class="c1">#     serializing backward hooks wasn&#39;t actually the root cause of</span>
<span class="c1">#     the bug.</span>
<span class="c1">#</span>
<span class="c1"># With these cases in mind, we have decided that a better strategy</span>
<span class="c1"># is to just NOT serialize hooks at all.</span>
<span class="c1">#</span>
<span class="c1"># Since this is a BC-breaking change, we should warn when we previously</span>
<span class="c1"># serialized a hook, but no longer do so. This will be done by adding a special</span>
<span class="c1"># sentinel property to hooks will be used to suppress this warning. If a hook</span>
<span class="c1"># has the property _torch_serialize_ignore, we will not emit a warning if we</span>
<span class="c1"># attempt to serialize a Tensor with this hook attached to it.</span>
<span class="c1">#</span>
<span class="c1"># By the way, when _backward_hooks is skipped, we must give an EMPTY</span>
<span class="c1"># OrderedDict(), if you pass a None you&#39;ll run afoul #12219.</span>


<span class="c1"># TODO: Once we decide to break serialization FC, `storage` no longer needs to</span>
<span class="c1"># be a TypedStorage</span>
<span class="k">def</span> <span class="nf">_rebuild_tensor</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
    <span class="c1"># first construct a tensor with the correct dtype/device</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_tensor_metadata</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="c1"># Tensor&#39;s Metadata for serializing.</span>
    <span class="c1"># Currently, this only returns a dict[string, bool] specifing whether</span>
    <span class="c1"># `conj` or `neg` bit is set.</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tensor_metadata</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>


<span class="k">def</span> <span class="nf">set_tensor_metadata</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">metadata</span><span class="p">):</span>
    <span class="c1"># See `get_tensor_metadata` above</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_set_tensor_metadata</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>


<span class="k">def</span> <span class="nf">_rebuild_tensor_v2</span><span class="p">(</span>
    <span class="n">storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">backward_hooks</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">_rebuild_tensor</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
    <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
        <span class="n">set_tensor_metadata</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>

    <span class="c1"># NB: This line exists only for backwards compatibility; the</span>
    <span class="c1"># general expectation is that backward_hooks is an empty</span>
    <span class="c1"># OrderedDict.  See Note [Don&#39;t serialize hooks]</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">backward_hooks</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="n">_sparse_tensors_to_validate</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;torch.Tensor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>


<span class="c1"># In _legacy_load() in serialization.py we unpickle storages after the sparse</span>
<span class="c1"># tensors have been already unpickled. Those storages contain data necessary for</span>
<span class="c1"># validating sparse tensors: indices and values. That&#39;s why sparse tensors are</span>
<span class="c1"># first unpickled without any validation, and then this function is called just</span>
<span class="c1"># before _legacy_load() returns, so that all the sparse tensors can be validated</span>
<span class="c1"># in bulk.</span>
<span class="c1">#</span>
<span class="c1"># The same procedure must be followed by _load() in serialization.py because due</span>
<span class="c1"># to Pickler semantics, we have to use the same (non-validating) function for</span>
<span class="c1"># unpickling sparse tensors, regardless of the caller.</span>
<span class="k">def</span> <span class="nf">_validate_loaded_sparse_tensors</span><span class="p">():</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">_sparse_tensors_to_validate</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_validate_sparse_coo_tensor_args</span><span class="p">(</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc</span><span class="p">,</span>
            <span class="p">}:</span>
                <span class="c1"># TODO: Validation currently involves an expensive traversal</span>
                <span class="c1"># on CPU, which may include a device transfer.</span>
                <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">}:</span>
                    <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">ccol_indices</span><span class="p">(),</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">row_indices</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_validate_sparse_compressed_tensor_args</span><span class="p">(</span>
                    <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">layout</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;_validate_loaded_sparse_tensors for layout `</span><span class="si">%s</span><span class="s2">`&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span>
                <span class="p">)</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_sparse_tensors_to_validate</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_rebuild_sparse_tensor</span><span class="p">(</span><span class="n">layout</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rebuilds a sparse tensor from its sparse storage representation.</span>

<span class="sd">    Args:</span>
<span class="sd">        layout (str): The sparse storage layout of the tensor.</span>
<span class="sd">        data (tuple): The tensor&#39;s sparse storage representation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
        <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">check_invariants</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">_sparse_tensors_to_validate</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">elif</span> <span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc</span><span class="p">,</span>
    <span class="p">}:</span>
        <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_compressed_tensor</span><span class="p">(</span>
            <span class="n">compressed_indices</span><span class="p">,</span>
            <span class="n">plain_indices</span><span class="p">,</span>
            <span class="n">values</span><span class="p">,</span>
            <span class="n">size</span><span class="p">,</span>
            <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
            <span class="n">check_invariants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">_sparse_tensors_to_validate</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;rebuilding sparse tensor for layout </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layout</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_rebuild_device_tensor_from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="c1"># Should not be used, only here to be able to load Tensors serialized with older versions of pytorch</span>
<span class="n">_rebuild_xla_tensor</span> <span class="o">=</span> <span class="n">_rebuild_device_tensor_from_numpy</span>


<span class="k">def</span> <span class="nf">_rebuild_meta_tensor_no_storage</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_strided</span><span class="p">(</span>
        <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;meta&quot;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_rebuild_wrapper_subclass</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">layout</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_make_wrapper_subclass</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">storage_offset</span><span class="o">=</span><span class="n">storage_offset</span><span class="p">,</span>
        <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># TODO: Once we decide to break serialization FC, `storage` no longer needs to</span>
<span class="c1"># be a TypedStorage</span>
<span class="k">def</span> <span class="nf">_rebuild_qtensor</span><span class="p">(</span>
    <span class="n">storage</span><span class="p">,</span>
    <span class="n">storage_offset</span><span class="p">,</span>
    <span class="n">size</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">,</span>
    <span class="n">quantizer_params</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">,</span>
    <span class="n">backward_hooks</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">qscheme</span> <span class="o">=</span> <span class="n">quantizer_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quantizer_params</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_empty_affine_quantized</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">zero_point</span><span class="o">=</span><span class="n">zero_point</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">qscheme</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">zero_points</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">quantizer_params</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">zero_points</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">:</span>
                <span class="n">scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">zero_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">zero_points</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_empty_per_channel_affine_quantized</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span>
            <span class="n">scales</span><span class="o">=</span><span class="n">scales</span><span class="p">,</span>
            <span class="n">zero_points</span><span class="o">=</span><span class="n">zero_points</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Can&#39;t deserialize quantized tensor with qscheme </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">qscheme</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
    <span class="c1"># NB: This line exists only for backwards compatibility; the</span>
    <span class="c1"># general expectation is that backward_hooks is an empty</span>
    <span class="c1"># OrderedDict.  See Note [Don&#39;t serialize hooks]</span>
    <span class="n">tensor</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">backward_hooks</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">_rebuild_parameter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">backward_hooks</span><span class="p">):</span>
    <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">)</span>
    <span class="c1"># NB: This line exists only for backwards compatibility; the</span>
    <span class="c1"># general expectation is that backward_hooks is an empty</span>
    <span class="c1"># OrderedDict.  See Note [Don&#39;t serialize hooks]</span>
    <span class="n">param</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">backward_hooks</span>

    <span class="k">return</span> <span class="n">param</span>


<span class="k">def</span> <span class="nf">_rebuild_parameter_with_state</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">backward_hooks</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">)</span>
    <span class="c1"># NB: This line exists only for backwards compatibility; the</span>
    <span class="c1"># general expectation is that backward_hooks is an empty</span>
    <span class="c1"># OrderedDict.  See Note [Don&#39;t serialize hooks]</span>
    <span class="n">param</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">backward_hooks</span>

    <span class="c1"># Restore state on Parameter like python attr.</span>
    <span class="n">param</span> <span class="o">=</span> <span class="n">_set_obj_state</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>


<span class="k">def</span> <span class="nf">_get_obj_state</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="c1"># Get the state of the python subclass</span>
    <span class="c1"># This loosely mimicks the function on the object class but since Tensor do not inherit</span>
    <span class="c1"># from it, we cannot call that function directly</span>
    <span class="c1"># https://github.com/python/cpython/blob/c83919bd635f4433f1c6ae8504996a9fe3c215e5/Objects/typeobject.c#L4891</span>
    <span class="c1"># Note that starting with Python 3.11, this `__getstate__` is always defined and thus</span>
    <span class="c1"># the else branch will never be taken.</span>
    <span class="n">getstate_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;__getstate__&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">getstate_fn</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">getstate_fn</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">copyreg</span><span class="o">.</span><span class="n">_slotnames</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">if</span> <span class="n">slots_to_save</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">obj</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
                <span class="p">{</span>
                    <span class="n">name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">slots_to_save</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="p">},</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__dict__</span>

    <span class="k">return</span> <span class="n">state</span>


<span class="k">def</span> <span class="nf">_set_obj_state</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid serialized state: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">dict_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">slots_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dict_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">slots_state</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Starting with Python 3.11, the __dict__ attribute is lazily created</span>
    <span class="c1"># and is serialized as None when not needed.</span>
    <span class="k">if</span> <span class="n">dict_state</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dict_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">slots_state</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">slots_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obj</span>


<span class="k">def</span> <span class="nf">_import_dotted_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="n">components</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obj</span>


<span class="c1"># Taken from python 3.5 docs</span>
<span class="k">def</span> <span class="nf">_accumulate</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">):</span>
    <span class="s2">&quot;Return running totals&quot;</span>
    <span class="c1"># _accumulate([1,2,3,4,5]) --&gt; 1 3 6 10 15</span>
    <span class="c1"># _accumulate([1,2,3,4,5], operator.mul) --&gt; 1 2 6 24 120</span>
    <span class="n">it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">yield</span> <span class="n">total</span>
    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">it</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">total</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">total</span>


<span class="k">def</span> <span class="nf">_flatten_dense_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of</span>
<span class="sd">    same dense type.</span>

<span class="sd">    Since inputs are dense, the resulting tensor will be a concatenated 1D</span>
<span class="sd">    buffer. Element-wise operation on this buffer will be equivalent to</span>
<span class="sd">    operating individually.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Iterable[Tensor]): dense tensors to flatten.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A contiguous 1D buffer containing input tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">flatten_dense_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_flatten_sparse_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten sparse tensors into two contiguous 1D buffers, one of indices and</span>
<span class="sd">    one of values. Assume tensors are of same sparse type.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Iterable[Tensor]): sparse tensors to flatten.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of two contiguous 1D buffers, one containing input tensors&#39;</span>
<span class="sd">        indices and the other containing the values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flat_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">flatten_dense_tensors</span><span class="p">(</span>
        <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">flat_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">flatten_dense_tensors</span><span class="p">(</span>
        <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">flat_indices</span><span class="p">,</span> <span class="n">flat_values</span>


<span class="k">def</span> <span class="nf">_unflatten_dense_tensors</span><span class="p">(</span><span class="n">flat</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;View a flat buffer using the sizes of tensors. Assume that tensors are of</span>
<span class="sd">    same dense type, and that flat is given by _flatten_dense_tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        flat (Tensor): flattened dense tensors to unflatten.</span>
<span class="sd">        tensors (Iterable[Tensor]): dense tensors whose sizes will be used to</span>
<span class="sd">          unflatten flat.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Unflattened dense tensors with sizes same as tensors and values from</span>
<span class="sd">        flat.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">unflatten_dense_tensors</span><span class="p">(</span><span class="n">flat</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_unflatten_sparse_tensors</span><span class="p">(</span><span class="n">flat</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;View flat buffer (containing indices and values) using the sizes of</span>
<span class="sd">    tensors. Assume that tensors are of same sparse type, and that flat is given</span>
<span class="sd">    by _flatten_sparse_tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        flat (tuple(Tensor, Tensor)): flattened indices and values of sparse</span>
<span class="sd">          tensors to unflatten.</span>
<span class="sd">        tensors (Iterable[Tensor]): sparse tensors whose sizes will be used to</span>
<span class="sd">          unflatten flat.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Unflattened sparse tensors with sizes same as tensors and values from</span>
<span class="sd">        flat.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flat_indices</span><span class="p">,</span> <span class="n">flat_values</span> <span class="o">=</span> <span class="n">flat</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">unflatten_dense_tensors</span><span class="p">(</span>
        <span class="n">flat_indices</span><span class="p">,</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">unflatten_dense_tensors</span><span class="p">(</span>
        <span class="n">flat_values</span><span class="p">,</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_reorder_tensors_as</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">ordered_tensors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assume that tensors are of same order as ordered_tensors within their</span>
<span class="sd">    types, e.g., from _take_tensors. Reorder them to be of same order as</span>
<span class="sd">    ordered_tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Iterable[Tensor]): tensors to be reordered. They should be of</span>
<span class="sd">          the same order as ordered_tensors within their own types.</span>
<span class="sd">        ordered_tensors (Iterable[Tensor]): tensors whose order will be the</span>
<span class="sd">          reference.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Ordered tuple of tensors with contents from tensors and order of</span>
<span class="sd">        ordered_tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">type_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="n">type_dict</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">()]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="n">type_dict_</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">coll</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">coll</span> <span class="ow">in</span> <span class="n">type_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">type_dict_</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">()])</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">ordered_tensors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_take_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">size_limit</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Group tensors into chunks. This generator yields a chunk at each time,</span>
<span class="sd">    each containing tensors of same type up to certain byte limit in total size.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Sequence): A sequence of tensors to be separated into chunks.</span>
<span class="sd">        size_limit (int): The limit of each chunk in bytes.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Blocks of tensors of same type and within size_limit. The yielded</span>
<span class="sd">        tensors are only ordered as the original sequence within its types.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">buf_dict</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">[[],</span> <span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_indices</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_values</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">indices</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
                <span class="o">+</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">values</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
        <span class="n">buf_and_size</span> <span class="o">=</span> <span class="n">buf_dict</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">buf_and_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">size_limit</span> <span class="ow">and</span> <span class="n">buf_and_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">buf_and_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">buf_and_size</span> <span class="o">=</span> <span class="n">buf_dict</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[],</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">buf_and_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">buf_and_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">size</span>
    <span class="k">for</span> <span class="n">buf</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">buf_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">buf</span>


<span class="c1"># annotation decorator to get annotations in a way that is compatible</span>
<span class="c1"># with both Python 2 and 3</span>
<span class="k">def</span> <span class="nf">annotate</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">dec</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
        <span class="n">fun</span><span class="o">.</span><span class="vm">__annotations__</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">fun</span><span class="o">.</span><span class="vm">__annotations__</span><span class="p">[</span><span class="s2">&quot;return&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ret</span>
        <span class="k">return</span> <span class="n">fun</span>

    <span class="k">return</span> <span class="n">dec</span>


<span class="c1"># NOTE [ Python Traceback Reference Cycle Problem ]</span>
<span class="c1">#</span>
<span class="c1"># When using sys.exc_info(), it is important to **not** store the exc_info[2],</span>
<span class="c1"># which is the traceback, because otherwise you will run into the traceback</span>
<span class="c1"># reference cycle problem, i.e., the traceback holding reference to the frame,</span>
<span class="c1"># and the frame (which holds reference to all the object in its temporary scope)</span>
<span class="c1"># holding reference the traceback.</span>


<span class="k">class</span> <span class="nc">KeyErrorMessage</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;str subclass that returns itself in repr&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">ExceptionWrapper</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wraps an exception plus traceback to communicate across threads&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s2">&quot;in background&quot;</span><span class="p">):</span>
        <span class="c1"># It is important that we don&#39;t store exc_info, see</span>
        <span class="c1"># NOTE [ Python Traceback Reference Cycle Problem ]</span>
        <span class="k">if</span> <span class="n">exc_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">exc_info</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span> <span class="o">=</span> <span class="n">exc_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exc_msg</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">traceback</span><span class="o">.</span><span class="n">format_exception</span><span class="p">(</span><span class="o">*</span><span class="n">exc_info</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">where</span> <span class="o">=</span> <span class="n">where</span>

    <span class="k">def</span> <span class="nf">reraise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reraises the wrapped exception in the current thread&quot;&quot;&quot;</span>
        <span class="c1"># Format a message such as: &quot;Caught ValueError in DataLoader worker</span>
        <span class="c1"># process 2. Original Traceback:&quot;, followed by the traceback.</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Caught </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">Original </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">where</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exc_msg</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span> <span class="o">==</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="c1"># KeyError calls repr() on its argument (usually a dict key). This</span>
            <span class="c1"># makes stack traces unreadable. It will not be changed in Python</span>
            <span class="c1"># (https://bugs.python.org/issue2651), so we work around it.</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">KeyErrorMessage</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span><span class="p">,</span> <span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="c1"># Some exceptions have first argument as non-str but explicitly</span>
            <span class="c1"># have message field</span>
            <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">exception</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="c1"># If the exception takes multiple arguments, don&#39;t try to</span>
            <span class="c1"># instantiate since we don&#39;t know how to</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
        <span class="k">raise</span> <span class="n">exception</span>


<span class="k">def</span> <span class="nf">_get_available_device_type</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;cuda&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;xpu&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">return</span> <span class="s2">&quot;xpu&quot;</span>
    <span class="n">custom_backend_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
    <span class="n">custom_device_mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">custom_device_mod</span> <span class="ow">and</span> <span class="n">custom_device_mod</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">custom_backend_name</span>
    <span class="c1"># add more available device types here</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_get_device_attr</span><span class="p">(</span><span class="n">get_member</span><span class="p">):</span>
    <span class="n">device_type</span> <span class="o">=</span> <span class="n">_get_available_device_type</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">and</span> <span class="n">device_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_member</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="ow">and</span> <span class="n">device_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_member</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">get_member</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">device_type</span><span class="p">))</span>
    <span class="c1"># add more available device types here</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_get_current_device_index</span><span class="p">():</span>
    <span class="c1"># current device index</span>
    <span class="k">return</span> <span class="n">_get_device_attr</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_get_all_device_indices</span><span class="p">():</span>
    <span class="c1"># all device index</span>
    <span class="k">return</span> <span class="n">_get_device_attr</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">device_count</span><span class="p">())))</span>


<span class="k">def</span> <span class="nf">_get_devices_properties</span><span class="p">(</span><span class="n">device_ids</span><span class="p">):</span>
    <span class="c1"># all device properties</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_get_device_attr</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_current_device_index</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checks if there are CUDA devices available and</span>
<span class="sd">    returns the device index of the current default CUDA device.</span>
<span class="sd">    Returns -1 in case there are no CUDA devices available.</span>
<span class="sd">    Arguments: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>


<span class="k">def</span> <span class="nf">_get_device_index</span><span class="p">(</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">optional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">allow_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gets the device index from :attr:`device`, which can be a torch.device</span>
<span class="sd">    object, a Python integer, or ``None``.</span>

<span class="sd">    If :attr:`device` is a torch.device object, returns the device index if it</span>
<span class="sd">    has index. Note that for a device without a specified index,</span>
<span class="sd">    i.e., ``torch.device(&#39;xxx&#39;)``, this will return the current default</span>
<span class="sd">    device of that type if :attr:`optional` is ``True``. If :attr:`allow_cpu` is ``True``,</span>
<span class="sd">    CPU devices will be accepted and ``-1`` will be returned in this case.</span>

<span class="sd">    If :attr:`device` is a Python integer, it is returned as is.</span>

<span class="sd">    If :attr:`device` is ``None``, this will return the current default</span>
<span class="sd">    device of the supported runtime platform if :attr:`optional` is ``True``.</span>
<span class="sd">    i.e., the current default CUDA device will be returned if CUDA runtime is supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">device_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_cpu</span> <span class="ow">and</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected a non cpu device, but got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">device_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="k">else</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">device_idx</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">if</span> <span class="n">device_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">optional</span><span class="p">:</span>
            <span class="c1"># The eager API _get_current_device_index uses `lambda` functions which are</span>
            <span class="c1"># not supported in JIT and hence not scriptable. The JIT equivalent API to get</span>
            <span class="c1"># the current device index is `get_current_device_index()` which can</span>
            <span class="c1"># be scripted. We use is_scripting to check the mode we are in and call the</span>
            <span class="c1"># appropriate API.</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
                <span class="n">device_idx</span> <span class="o">=</span> <span class="n">get_current_device_index</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">device_idx</span> <span class="o">=</span> <span class="n">_get_current_device_index</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected a torch.device with a specified index &quot;</span>
                <span class="s2">&quot;or an integer, but got:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">device_idx</span>


<span class="k">def</span> <span class="nf">_handle_complex</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a real view of a tensor if complex dtype else just the tensor</span>
<span class="sd">    need to check if a UninitializedParameter because otherwise checking is_complex is an error for a LazyModule</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">UninitializedParameter</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span>
        <span class="k">else</span> <span class="n">tensor</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element size for a dtype, in bytes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;expected torch.dtype, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">bits</span> <span class="o">&gt;&gt;</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">bits</span> <span class="o">&gt;&gt;</span> <span class="mi">3</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="c1"># NOTE: torch.bool is not supported in torch.iinfo()</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">bits</span> <span class="o">&gt;&gt;</span> <span class="mi">3</span>


<span class="k">class</span> <span class="nc">_ClassPropertyDescriptor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fget</span><span class="p">,</span> <span class="n">fset</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fget</span> <span class="o">=</span> <span class="n">fget</span>

    <span class="k">def</span> <span class="fm">__get__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">instance</span><span class="p">,</span> <span class="n">owner</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">owner</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">owner</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fget</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">owner</span><span class="p">)()</span>


<span class="k">def</span> <span class="nf">classproperty</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="p">(</span><span class="nb">classmethod</span><span class="p">,</span> <span class="nb">staticmethod</span><span class="p">)):</span>
        <span class="n">func</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_ClassPropertyDescriptor</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>


<span class="c1"># Whether we are compiling with torch.compile or not</span>
<span class="k">def</span> <span class="nf">is_compiling</span><span class="p">():</span>
    <span class="k">return</span> <span class="kc">False</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
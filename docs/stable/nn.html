

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nn.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.optim" href="optim.html" />
    <link rel="prev" title="torch.Storage" href="storage.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.4.1 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-with-autograd">In-place operations with autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal notranslate"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C++ extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#id1">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory">My model reports “cuda runtime error(2): out of memory”</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-gpu-memory-isn-t-freed-properly">My GPU memory isn’t freed properly</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-data-loader-workers-return-identical-random-numbers">My data loader workers return identical random numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism">My recurrent network doesn’t work with data parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#building-from-source">Building from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#include-optional-components">Include optional components</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#speeding-cuda-build-for-windows">Speeding CUDA build for Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#one-key-install-script">One key install script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#extension">Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cffi-extension">CFFI Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cpp-extension">Cpp Extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#package-not-found-in-win-32-channel">Package not found in win-32 channel.</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#why-are-there-no-python-2-packages-for-windows">Why are there no Python 2 packages for Windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#import-error">Import error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#usage-multiprocessing">Usage (multiprocessing)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-without-if-clause-protection">Multiprocessing error without if-clause protection</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-broken-pipe">Multiprocessing error “Broken pipe”</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-driver-shut-down">Multiprocessing error “driver shut down”</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cuda-ipc-operations">CUDA IPC operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-dtype">torch.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-device">torch.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-layout">torch.layout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#moduledict"><span class="hidden-section">ModuleDict</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameterdict"><span class="hidden-section">ParameterDict</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#convolution-layers">Convolution layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#unfold"><span class="hidden-section">Unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#fold"><span class="hidden-section">Fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pooling-layers">Pooling layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#padding-layers">Padding layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activations-weighted-sum-nonlinearity">Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activations-other">Non-linear activations (other)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivelogsoftmaxwithloss"><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#groupnorm"><span class="hidden-section">GroupNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameters-to-vector"><span class="hidden-section">parameters_to_vector</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#vector-to-parameters"><span class="hidden-section">vector_to_parameters</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#spectral-norm"><span class="hidden-section">spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-spectral-norm"><span class="hidden-section">remove_spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id20"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id22"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23"><span class="hidden-section">unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id24"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id25"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id26"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id27"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id28"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id29"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id32"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id33"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id34"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id35"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id36"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id37"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id38"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id39"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id40"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id41"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id42"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id43"><span class="hidden-section">linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id44"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id45"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id46"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id47"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sparse-functions">Sparse functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id48"><span class="hidden-section">embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id49">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id50">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id51"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal notranslate"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#in-place-operations-on-tensors">In-place operations on Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable-deprecated">Variable (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#tensor-autograd-functions">Tensor autograd functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#numerical-gradient-checking">Numerical gradient checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#anomaly-detection">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#score-function">Score function</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pathwise-derivative">Pathwise derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#halfcauchy"><span class="hidden-section">HalfCauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#halfnormal"><span class="hidden-section">HalfNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#independent"><span class="hidden-section">Independent</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#lognormal"><span class="hidden-section">LogNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multinomial"><span class="hidden-section">Multinomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#poisson"><span class="hidden-section">Poisson</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.transforms"><cite>Transforms</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraints"><cite>Constraints</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal notranslate"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#multi-gpu-collective-functions">Multi-GPU collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#launch-utility">Launch utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#module-torchvision.transforms.functional">Functional Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>torch.nn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-torch.nn">
<span id="torch-nn"></span><h1>torch.nn<a class="headerlink" href="#module-torch.nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.nn.Parameter">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Parameter</code><a class="reference internal" href="_modules/torch/nn/parameter.html#Parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>A kind of Tensor that is to be considered a module parameter.</p>
<p>Parameters are <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> subclasses, that have a
very special property when used with <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> s - when they’re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <a class="reference internal" href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parameters()</span></code></a> iterator.
Assigning a Tensor doesn’t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – parameter tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if the parameter requires gradient. See
<a class="reference internal" href="notes/autograd.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="containers">
<h2>Containers<a class="headerlink" href="#containers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module">
<h3><span class="hidden-section">Module</span><a class="headerlink" href="#module" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Module">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Module</code><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call <cite>.cuda()</cite>, etc.</p>
<dl class="method">
<dt id="torch.nn.Module.add_module">
<code class="descname">add_module</code><span class="sig-paren">(</span><em>name</em>, <em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.add_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – child module to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.apply">
<code class="descname">apply</code><span class="sig-paren">(</span><em>fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">torch-nn-init</span>).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>fn</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None) – function to be applied to each submodule</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="go">        print(m)</span>
<span class="go">        if type(m) == nn.Linear:</span>
<span class="go">            m.weight.data.fill_(1.0)</span>
<span class="go">            print(m.weight)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.children">
<code class="descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> – a child module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cpu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cuda"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.double"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="torch.nn.Module.dump_patches">
<code class="descname">dump_patches</code><em class="property"> = False</em><a class="headerlink" href="#torch.nn.Module.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.extra_repr">
<code class="descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.float"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to float datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.half"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em>, <em>strict=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – a dict containing parameters and
persistent buffers.</li>
<li><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.modules">
<code class="descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> – a module in the network</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear (2 -&gt; 2)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_children">
<code class="descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> – Tuple containing a name and child module</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_modules">
<code class="descname">named_modules</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> – Tuple of name and module</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear (2 -&gt; 2))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_parameters">
<code class="descname">named_parameters</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Parameter)</em> – Tuple containing the name and parameter</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Parameter</em> – module parameter</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_backward_hook">
<code class="descname">register_backward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_backward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_buffer">
<code class="descname">register_buffer</code><span class="sig-paren">(</span><em>name</em>, <em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_buffer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</li>
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – buffer to be registered.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_hook">
<code class="descname">register_forward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input or output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_pre_hook">
<code class="descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_parameter">
<code class="descname">register_parameter</code><span class="sig-paren">(</span><em>name</em>, <em>param</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>Parameter</em></a>) – parameter to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><em>destination=None</em>, <em>prefix=''</em>, <em>keep_vars=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a dictionary containing a whole state of the module</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)">dict</a></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>device=None</em>, <em>dtype=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>dtype</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>tensor</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<p>Its signature is similar to <a class="reference internal" href="tensors.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code></a>, but only accepts
floating point desired <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> s. In addition, this method will
only cast the floating point parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This method modifies the module in-place.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</li>
<li><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point type of
the floating point parameters and buffers in this module</li>
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">self</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dst_type</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – the desired type</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential">
<h3><span class="hidden-section">Sequential</span><a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sequential">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

<span class="c1"># Example of using Sequential with OrderedDict</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="modulelist">
<h3><span class="hidden-section">ModuleList</span><a class="headerlink" href="#modulelist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ModuleList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleList</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p>ModuleList can be indexed like a regular Python list, but modules it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of modules to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given module to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends modules from a Python iterable to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em>) – iterable of modules to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="moduledict">
<h3><span class="hidden-section">ModuleDict</span><a class="headerlink" href="#moduledict" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ModuleDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleDict</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a dictionary.</p>
<p>ModuleDict can be indexed like a regular Python dictionary, but modules it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of (string: module)
or an iterable of key/value pairs of type (string, module)</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
                <span class="s1">&#39;conv&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s1">&#39;pool&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">([</span>
                <span class="p">[</span><span class="s1">&#39;lrelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()],</span>
                <span class="p">[</span><span class="s1">&#39;prelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()]</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="n">choice</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ModuleDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.items"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ModuleDict and return its module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>key</strong> (<em>string</em>) – key to pop from the ModuleDict</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the ModuleDict with the key/value pairs from a mapping or
an iterable, overwriting existing keys.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em>) – a mapping (dictionary) of (string: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module`</span></code>) or
an iterable of key/value pairs of type (string, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module`</span></code>)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict values.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterlist">
<h3><span class="hidden-section">ParameterList</span><a class="headerlink" href="#parameterlist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ParameterList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterList</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a list.</p>
<p>ParameterList can be indexed like a regular Python list, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code> to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ParameterList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>parameter</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given parameter at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) – parameter to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends parameters from a Python iterable to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em>) – iterable of parameters to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterdict">
<h3><span class="hidden-section">ParameterDict</span><a class="headerlink" href="#parameterdict" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ParameterDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterDict</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a dictionary.</p>
<p>ParameterDict can be indexed like a regular Python dictionary, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of
(string : <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code>) or an iterable of key,value pairs
of type (string, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code>)</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span>
                <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="p">})</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ParameterDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.items"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ParameterDict and return its parameter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>key</strong> (<em>string</em>) – key to pop from the ParameterDict</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the ParameterDict with the key/value pairs from a mapping or
an iterable, overwriting existing keys.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em>) – a mapping (dictionary) of
(string : <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code>) or an iterable of
key/value pairs of type (string, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code>)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict values.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="convolution-layers">
<h2>Convolution layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conv1d">
<h3><span class="hidden-section">Conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, L)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> can be
precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*},\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(L\)</span> is a length of signal sequence.</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor \frac{\text{out_channels}}{\text{in_channels}} \right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math notranslate nohighlight">\((\text{in_channels}=C_{in}, \text{out_channels}=C_{in} * K, ..., \text{groups}=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size)</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape
(out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h3><span class="hidden-section">Conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*},\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math notranslate nohighlight">\((\text{in_channels}=C_{in}, \text{out_channels}=C_{in} * K, ..., \text{groups}=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h3><span class="hidden-section">Conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*},\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math notranslate nohighlight">\((\text{in_channels}=C_{in}, \text{out_channels}=C_{in} * K, ..., \text{groups}=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to all three sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose1d">
<h3><span class="hidden-section">ConvTranspose1d</span><a class="headerlink" href="#convtranspose1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding}
      + \text{kernel_size} + \text{output_padding}\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convtranspose2d">
<h3><span class="hidden-section">ConvTranspose2d</span><a class="headerlink" href="#convtranspose2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimensions</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0]
      + \text{kernel_size}[0] + \text{output_padding}[0]\\W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1]
      + \text{kernel_size}[1] + \text{output_padding}[1]\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose3d">
<h3><span class="hidden-section">ConvTranspose3d</span><a class="headerlink" href="#convtranspose3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimensions</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0]
      + \text{kernel_size}[0] + \text{output_padding}[0]\\H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1]
      + \text{kernel_size}[1] + \text{output_padding}[1]\\W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2]
      + \text{kernel_size}[2] + \text{output_padding}[2]\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unfold">
<h3><span class="hidden-section">Unfold</span><a class="headerlink" href="#unfold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Unfold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Unfold</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/fold.html#Unfold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from a batched input tensor.</p>
<p>Consider an batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor of shape <span class="math notranslate nohighlight">\((N, C, *)\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is the batch dimension, <span class="math notranslate nohighlight">\(C\)</span> is the channel dimension,
and <span class="math notranslate nohighlight">\(*\)</span> represent arbitrary spatial dimensions. This operation flattens
each sliding <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>-sized block within the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into a column (i.e., last dimension) of a 3-D <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>
tensor of shape <span class="math notranslate nohighlight">\((N, C \times \prod(\text{kernel_size}), L)\)</span>, where
<span class="math notranslate nohighlight">\(C \times \prod(\text{kernel_size})\)</span> is the total number of values
with in each block (a block has <span class="math notranslate nohighlight">\(\prod(\text{kernel_size})\)</span> spatial
locations each containing a <span class="math notranslate nohighlight">\(C\)</span>-channeled vector), and <span class="math notranslate nohighlight">\(L\)</span> is
the total number of such blocks:</p>
<div class="math notranslate nohighlight">
\[L = \prod_d \left\lfloor\frac{\text{input_spatial_size}[d] + 2 \times \text{padding}[d] \
    - \text{dilation}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{input_spatial_size}\)</span> is formed by the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (<span class="math notranslate nohighlight">\(*\)</span> above), and <span class="math notranslate nohighlight">\(d\)</span> is over all spatial
dimensions.</p>
<p>Therefore, indexing <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> at the last dimension (column dimension)
gives all values within a certain block.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or
<code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1, their values will be
replicated across all spatial dimensions.</li>
<li>For the case of two input spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">im2col</span></code>.</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, *)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C \times \prod(\text{kernel_size}), L)\)</span> as described above</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unfold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Unfold</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 4 blocks (2x3 kernels) in total in the 3x4 input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 30, 4])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp_unf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_unf</span> <span class="o">=</span> <span class="n">inp_unf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fold</span><span class="p">(</span><span class="n">out_unf</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or equivalently (and avoiding a copy),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># out = out_unf.view(1, 2, 7, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="go">tensor(1.9073e-06)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fold">
<h3><span class="hidden-section">Fold</span><a class="headerlink" href="#fold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Fold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Fold</code><span class="sig-paren">(</span><em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/fold.html#Fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<p>Consider a batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor containing sliding local blocks,
e.g., patches of images, of shape <span class="math notranslate nohighlight">\((N, C \times  \prod(\text{kernel_size}), L)\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is batch dimension, <span class="math notranslate nohighlight">\(C \times \prod(\text{kernel_size})\)</span>
is the number of values with in a block (a block has <span class="math notranslate nohighlight">\(\prod(\text{kernel_size})\)</span>
spatial locations each containing a <span class="math notranslate nohighlight">\(C\)</span>-channeled vector), and
<span class="math notranslate nohighlight">\(L\)</span> is the total number of blocks. (This is exacly the
same specification as the output shape of <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>.) This
operation combines these local blocks into the large <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> tensor
of shape <span class="math notranslate nohighlight">\((N, C, \text{output_size}[0], \text{output_size}[1], \dots)\)</span>.
Similar to <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>, the arguments must satisfy</p>
<div class="math notranslate nohighlight">
\[L = \prod_d \left\lfloor\frac{\text{output_size}[d] + 2 \times \text{padding}[d] \
    - \text{dilation}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is over all spatial dimensions.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> describes the spatial shape of the large containing
tensor of the sliding local blocks. It is useful to resolve the ambiguity
when multiple input shapes map to same number of sliding blocks, e.g.,
with <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>.</li>
</ul>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the shape of the spatial dimensions [2:] of the output</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1 then
their values will be replicated across all spatial dimensions.</li>
<li>For the case of two output spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">col2im</span></code>.</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C \times \prod(\text{kernel_size}), L)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, \text{output_size}[0], \text{output_size}[1], \dots)\)</span> as described above</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Fold</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-layers">
<h2>Pooling layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="maxpool1d">
<h3><span class="hidden-section">MaxPool1d</span><a class="headerlink" href="#maxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, L)\)</span>
and output <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_j, k)  = \max_{m=0, \ldots, \text{kernel_size}-1}
        \text{input}(N_i, C_j, \text{stride} * k + m)
\end{equation*}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 * \text{padding} - \text{dilation}
      * (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool2d">
<h3><span class="hidden-section">MaxPool2d</span><a class="headerlink" href="#maxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_j, h, w)  = \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1}
                       \text{input}(N_i, C_j, \text{stride}[0] * h + m, \text{stride}[1] * w + n)
\end{equation*}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding}[0] - \text{dilation}[0]
      * (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding}[1] - \text{dilation}[1]
      * (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool3d">
<h3><span class="hidden-section">MaxPool3d</span><a class="headerlink" href="#maxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{out}(N_i, C_j, d, h, w) &amp;= \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1}
        \text{input}(N_i, C_j, \text{stride}[0] * k + d,\\ &amp;\text{stride}[1] * h + m, \text{stride}[2] * w + n)
\end{align*}\end{split}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on all three sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = \left\lfloor\frac{D_{in} + 2 * \text{padding}[0] - \text{dilation}[0] *
  (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding}[1] - \text{dilation}[1] *
  (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding}[2] - \text{dilation}[2] *
  (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool1d">
<h3><span class="hidden-section">MaxUnpool1d</span><a class="headerlink" href="#maxunpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool1d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool1d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[H_{out} = (H_{in} - 1) * \text{stride}[0] - 2 * \text{padding}[0] + \text{kernel_size}[0]\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example showcasing the use of output_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool2d">
<h3><span class="hidden-section">MaxUnpool2d</span><a class="headerlink" href="#maxunpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool2d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool2d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = (H_{in} - 1) * \text{stride}[0] - 2 * \text{padding}[0] + \text{kernel_size}[0]\\W_{out} = (W_{in} - 1) * \text{stride}[1] - 2 * \text{padding}[1] + \text{kernel_size}[1]\end{aligned}\end{align} \]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
<span class="go">                            [ 5,  6,  7,  8],</span>
<span class="go">                            [ 9, 10, 11, 12],</span>
<span class="go">                            [13, 14, 15, 16]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   6.,   0.,   8.],</span>
<span class="go">          [  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,  14.,   0.,  16.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># specify a different output size than input size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  6.,   0.,   8.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,  14.,   0.],</span>
<span class="go">          [ 16.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,   0.,   0.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool3d">
<h3><span class="hidden-section">MaxUnpool3d</span><a class="headerlink" href="#maxunpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> is not fully invertible, since the non-maximal values are lost.
<a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool3d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs section below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool3d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = (D_{in} - 1) * \text{stride}[0] - 2 * \text{padding}[0] + \text{kernel_size}[0]\\H_{out} = (H_{in} - 1) * \text{stride}[1] - 2 * \text{padding}[1] + \text{kernel_size}[1]\\W_{out} = (W_{in} - 1) * \text{stride}[2] - 2 * \text{padding}[2] + \text{kernel_size}[2]\end{aligned}\end{align} \]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20, 16, 51, 33, 15])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool1d">
<h3><span class="hidden-section">AvgPool1d</span><a class="headerlink" href="#avgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, L)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\(k\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_j, l)  = \frac{1}{k} \sum_{m=0}^{k}
                       \text{input}(N_i, C_j, \text{stride} * l + m)
\end{equation*}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can each be
an <code class="docutils literal notranslate"><span class="pre">int</span></code> or a one-element tuple.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor \frac{L_{in} +
2 * \text{padding} - \text{kernel_size}}{\text{stride}} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool with window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]))</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool2d">
<h3><span class="hidden-section">AvgPool2d</span><a class="headerlink" href="#avgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       \text{input}(N_i, C_j, \text{stride}[0] * h + m, \text{stride}[1] * w + n)
\end{equation*}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in}  + 2 * \text{padding}[0] -
  \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in}  + 2 * \text{padding}[1] -
  \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool3d">
<h3><span class="hidden-section">AvgPool3d</span><a class="headerlink" href="#avgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_j, d, h, w)  = \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
        \frac{\text{input}(N_i, C_j, \text{stride}[0] * d + k, \text{stride}[1] * h + m,
                \text{stride}[2] * w + n)}
             {kD * kH * kW}
\end{equation*}\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on all three sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on all three sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = \left\lfloor\frac{D_{in} + 2 * \text{padding}[0] -
      \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\\H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding}[1] -
      \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding}[2] -
      \text{kernel_size}[2]}{\text{stride}[2]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fractionalmaxpool2d">
<h3><span class="hidden-section">FractionalMaxPool2d</span><a class="headerlink" href="#fractionalmaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.FractionalMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">FractionalMaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>output_size=None</em>, <em>output_ratio=None</em>, <em>return_indices=False</em>, <em>_random_samples=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.FractionalMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p>
<p>Fractional MaxPooling is described in detail in the paper <a class="reference external" href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham</p>
<p>The max-pooling operation is applied in <span class="math notranslate nohighlight">\(kHxkW\)</span> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple <cite>(kh x kw)</cite></li>
<li><strong>output_size</strong> – the target output size of the image of the form <cite>oH x oW</cite>.
Can be a tuple <cite>(oH, oW)</cite> or a single number oH for a square image <cite>oH x oH</cite></li>
<li><strong>output_ratio</strong> – If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to <code class="xref py py-meth docutils literal notranslate"><span class="pre">nn.MaxUnpool2d()</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, and target output size 13x12</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window and target output size being half of input image size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lppool1d">
<h3><span class="hidden-section">LPPool1d</span><a class="headerlink" href="#lppool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LPPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool1d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#LPPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LPPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math notranslate nohighlight">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}\]</div>
<ul class="simple">
<li>At p = infinity, one gets Max Pooling</li>
<li>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – a single int, the size of the window</li>
<li><strong>stride</strong> – a single int, the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, L_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[L_{out} = \left\lfloor\frac{L_{in} +
2 * \text{padding} - \text{kernel_size}}{\text{stride}} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
<dt>Examples::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of window of length 3, with stride 2.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="lppool2d">
<h3><span class="hidden-section">LPPool2d</span><a class="headerlink" href="#lppool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LPPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool2d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#LPPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LPPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math notranslate nohighlight">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}\]</div>
<ul class="simple">
<li>At p = <span class="math notranslate nohighlight">\(\infty\)</span>, one gets Max Pooling</li>
<li>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in}  + 2 * \text{padding}[0] - \text{dilation}[0] *
      (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in}  + 2 * \text{padding}[1] - \text{dilation}[1] *
      (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window of power 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool1d">
<h3><span class="hidden-section">AdaptiveMaxPool1d</span><a class="headerlink" href="#adaptivemaxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool1d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size H</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool1d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool2d">
<h3><span class="hidden-section">AdaptiveMaxPool2d</span><a class="headerlink" href="#adaptivemaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool2d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool3d">
<h3><span class="hidden-section">AdaptiveMaxPool3d</span><a class="headerlink" href="#adaptivemaxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool3d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size of the image of the form D x H x W.
Can be a tuple (D, H, W) or a single D for a cube D x D x D.
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool3d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool1d">
<h3><span class="hidden-section">AdaptiveAvgPool1d</span><a class="headerlink" href="#adaptiveavgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool1d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size H</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool2d">
<h3><span class="hidden-section">AdaptiveAvgPool2d</span><a class="headerlink" href="#adaptiveavgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool2d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool3d">
<h3><span class="hidden-section">AdaptiveAvgPool3d</span><a class="headerlink" href="#adaptiveavgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool3d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size of the form D x H x W.
Can be a tuple (D, H, W) or a single number D for a cube D x D x D
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="padding-layers">
<h2>Padding layers<a class="headerlink" href="#padding-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="reflectionpad1d">
<h3><span class="hidden-section">ReflectionPad1d</span><a class="headerlink" href="#reflectionpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReflectionPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReflectionPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReflectionPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  0  1  2  3</span>
<span class="go">  4  5  6  7</span>
<span class="go">[torch.FloatTensor of size (1,2,4)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">   2   1   0   1   2   3   2   1</span>
<span class="go">   6   5   4   5   6   7   6   5</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">   3   2   1   0   1   2   3   2</span>
<span class="go">   7   6   5   4   5   6   7   6</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="reflectionpad2d">
<h3><span class="hidden-section">ReflectionPad2d</span><a class="headerlink" href="#reflectionpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReflectionPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReflectionPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>,
<cite>paddingTop</cite>, <cite>paddingBottom</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  0  1  2</span>
<span class="go">  3  4  5</span>
<span class="go">  6  7  8</span>
<span class="go">[torch.FloatTensor of size (1,1,3,3)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   8   7   6   7   8   7   6</span>
<span class="go">   5   4   3   4   5   4   3</span>
<span class="go">   2   1   0   1   2   1   0</span>
<span class="go">   5   4   3   4   5   4   3</span>
<span class="go">   8   7   6   7   8   7   6</span>
<span class="go">   5   4   3   4   5   4   3</span>
<span class="go">   2   1   0   1   2   1   0</span>
<span class="go">[torch.FloatTensor of size (1,1,7,7)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  7  6  7  8  7</span>
<span class="go">  4  3  4  5  4</span>
<span class="go">  1  0  1  2  1</span>
<span class="go">  4  3  4  5  4</span>
<span class="go">  7  6  7  8  7</span>
<span class="go">[torch.FloatTensor of size (1,1,5,5)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad1d">
<h3><span class="hidden-section">ReplicationPad1d</span><a class="headerlink" href="#replicationpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  0  1  2  3</span>
<span class="go">  4  5  6  7</span>
<span class="go">[torch.FloatTensor of size (1,2,4)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">   0   0   0   1   2   3   3   3</span>
<span class="go">   4   4   4   5   6   7   7   7</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">   0   0   0   0   1   2   3   3</span>
<span class="go">   4   4   4   4   5   6   7   7</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad2d">
<h3><span class="hidden-section">ReplicationPad2d</span><a class="headerlink" href="#replicationpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>,
<cite>paddingTop</cite>, <cite>paddingBottom</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  0  1  2</span>
<span class="go">  3  4  5</span>
<span class="go">  6  7  8</span>
<span class="go">[torch.FloatTensor of size (1,1,3,3)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   0   0   0   1   2   2   2</span>
<span class="go">   0   0   0   1   2   2   2</span>
<span class="go">   0   0   0   1   2   2   2</span>
<span class="go">   3   3   3   4   5   5   5</span>
<span class="go">   6   6   6   7   8   8   8</span>
<span class="go">   6   6   6   7   8   8   8</span>
<span class="go">   6   6   6   7   8   8   8</span>
<span class="go">[torch.FloatTensor of size (1,1,7,7)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  0  0  1  2  2</span>
<span class="go">  0  0  1  2  2</span>
<span class="go">  0  0  1  2  2</span>
<span class="go">  3  3  4  5  5</span>
<span class="go">  6  6  7  8  8</span>
<span class="go">[torch.FloatTensor of size (1,1,5,5)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad3d">
<h3><span class="hidden-section">ReplicationPad3d</span><a class="headerlink" href="#replicationpad3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad3d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>,
<cite>paddingTop</cite>, <cite>paddingBottom</cite>, <cite>paddingFront</cite>, <cite>paddingBack</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(D_{out} = D_{in} + \textit{paddingFront} + \textit{paddingBack}\)</span>
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="zeropad2d">
<h3><span class="hidden-section">ZeroPad2d</span><a class="headerlink" href="#zeropad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ZeroPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ZeroPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ZeroPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with zero.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>,
<cite>paddingTop</cite>, <cite>paddingBottom</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1.4418 -1.9812 -0.3815</span>
<span class="go"> -0.3828 -0.6833 -0.2376</span>
<span class="go">  0.1433  0.0211  0.4311</span>
<span class="go">[torch.FloatTensor of size (1,1,3,3)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  1.4418 -1.9812 -0.3815  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000 -0.3828 -0.6833 -0.2376  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  0.1433  0.0211  0.4311  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">[torch.FloatTensor of size (1,1,7,7)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">  0.0000  0.0000  0.0000  0.0000  0.0000</span>
<span class="go">  0.0000  1.4418 -1.9812 -0.3815  0.0000</span>
<span class="go">  0.0000 -0.3828 -0.6833 -0.2376  0.0000</span>
<span class="go">  0.0000  0.1433  0.0211  0.4311  0.0000</span>
<span class="go">[torch.FloatTensor of size (1,1,5,5)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad1d">
<h3><span class="hidden-section">ConstantPad1d</span><a class="headerlink" href="#constantpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad1d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in both boundaries. If a 2-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  0.1875  0.5046 -1.0074  2.0005</span>
<span class="go"> -0.3540 -1.8645  1.1530  0.0632</span>
<span class="go">[torch.FloatTensor of size (1,2,4)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  3.5000  3.5000  0.1875  0.5046 -1.0074  2.0005  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000 -0.3540 -1.8645  1.1530  0.0632  3.5000  3.5000</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  3.5000  3.5000  3.5000  0.1875  0.5046 -1.0074  2.0005  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000 -0.3540 -1.8645  1.1530  0.0632  3.5000</span>
<span class="go">[torch.FloatTensor of size (1,2,8)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad2d">
<h3><span class="hidden-section">ConstantPad2d</span><a class="headerlink" href="#constantpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad2d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<cite>paddingLeft</cite>, <cite>paddingRight</cite>,
<cite>paddingTop</cite>, <cite>paddingBottom</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>

<span class="go">(0 ,.,.) =</span>
<span class="go"> -0.2295 -0.9774</span>
<span class="go"> -0.3335 -1.4178</span>
<span class="go">[torch.FloatTensor of size (1,2,2)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000 -0.2295 -0.9774  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000 -0.3335 -1.4178  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">[torch.FloatTensor of size (1,6,6)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">(0 ,.,.) =</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">  3.5000  3.5000  3.5000 -0.2295 -0.9774</span>
<span class="go">  3.5000  3.5000  3.5000 -0.3335 -1.4178</span>
<span class="go">  3.5000  3.5000  3.5000  3.5000  3.5000</span>
<span class="go">[torch.FloatTensor of size (1,5,5)]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad3d">
<h3><span class="hidden-section">ConstantPad3d</span><a class="headerlink" href="#constantpad3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad3d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N`d-padding, use :func:`torch.nn.functional.pad()</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<cite>paddingLeft</cite>, <cite>paddingRight</cite>, <cite>paddingTop</cite>, <cite>paddingBottom</cite>, <cite>paddingFront</cite>, <cite>paddingBack</cite>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math notranslate nohighlight">\(D_{out} = D_{in} + \textit{paddingFront} + \textit{paddingBack}\)</span>
<span class="math notranslate nohighlight">\(H_{out} = H_{in} + \textit{paddingTop} + \textit{paddingBottom}\)</span>
<span class="math notranslate nohighlight">\(W_{out} = W_{in} + \textit{paddingLeft} + \textit{paddingRight}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-weighted-sum-nonlinearity">
<h2>Non-linear activations (weighted sum, nonlinearity)<a class="headerlink" href="#non-linear-activations-weighted-sum-nonlinearity" title="Permalink to this headline">¶</a></h2>
<div class="section" id="elu">
<h3><span class="hidden-section">ELU</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>alpha</strong> – the <span class="math notranslate nohighlight">\(\alpha\)</span> value for the ELU formulation. Default: 1.0</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/ELU.png" src="_images/ELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardshrink">
<h3><span class="hidden-section">Hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Hardshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise
Hardshrink is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{HardShrink}(x) =
\begin{cases}
x, &amp; \text{ if } x &gt; \lambda \\
x, &amp; \text{ if } x &lt; -\lambda \\
0, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> – the <span class="math notranslate nohighlight">\(\lambda\)</span> value for the Hardshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Hardshrink.png" src="_images/Hardshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h3><span class="hidden-section">Hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Hardtanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardtanh</code><span class="sig-paren">(</span><em>min_val=-1</em>, <em>max_val=1</em>, <em>inplace=False</em>, <em>min_value=None</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise</p>
<p>HardTanh is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{HardTanh}(x) = \begin{cases}
    1 &amp; \text{ if } x &gt; 1 \\
    -1 &amp; \text{ if } x &lt; -1 \\
    x &amp; \text{ otherwise } \\
\end{cases}\end{split}\]</div>
<p>The range of the linear region <span class="math notranslate nohighlight">\([-1, 1]\)</span> can be adjusted using
<code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<img alt="_images/Hardtanh.png" src="_images/Hardtanh.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min_val</strong> – minimum value of the linear region range. Default: -1</li>
<li><strong>max_val</strong> – maximum value of the linear region range. Default: 1</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Keyword arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code>
have been deprecated in favor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="leakyrelu">
<h3><span class="hidden-section">LeakyReLU</span><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LeakyReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative_slope} * \min(0, x)\)</span> or</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyRELU}(x) =
\begin{cases}
x, &amp; \text{ if } x \geq 0 \\
\text{negative_slope} \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>negative_slope</strong> – Controls the angle of the negative slope. Default: 1e-2</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/LeakyReLU.png" src="_images/LeakyReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsigmoid">
<h3><span class="hidden-section">LogSigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math notranslate nohighlight">\(\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/LogSigmoid.png" src="_images/LogSigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="prelu">
<h3><span class="hidden-section">PReLU</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>num_parameters=1</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math notranslate nohighlight">\(\text{PReLU}(x) = \max(0,x) + a * \min(0,x)\)</span> or</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{PReLU}(x) =
\begin{cases}
x, &amp; \text{ if } x \geq 0 \\
ax, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(a\)</span> is a learnable parameter. When called without arguments, <cite>nn.PReLU()</cite> uses a single
parameter <span class="math notranslate nohighlight">\(a\)</span> across all input channels. If called with <cite>nn.PReLU(nChannels)</cite>,
a separate <span class="math notranslate nohighlight">\(a\)</span> is used for each input channel.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">weight decay should not be used when learning <span class="math notranslate nohighlight">\(a\)</span> for good performance.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_parameters</strong> – number of <span class="math notranslate nohighlight">\(a\)</span> to learn. Default: 1</li>
<li><strong>init</strong> – the initial value of <span class="math notranslate nohighlight">\(a\)</span>. Default: 0.25</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/PReLU.png" src="_images/PReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu">
<h3><span class="hidden-section">ReLU</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise
<span class="math notranslate nohighlight">\(\text{ReLU}(x)= \max(0, x)\)</span></p>
<img alt="_images/ReLU.png" src="_images/ReLU.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu6">
<h3><span class="hidden-section">ReLU6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU6">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math notranslate nohighlight">\(\text{ReLU6}(x) = \min(\max(0,x), 6)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/ReLU6.png" src="_images/ReLU6.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rrelu">
<h3><span class="hidden-section">RReLU</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RReLU</code><span class="sig-paren">(</span><em>lower=0.125</em>, <em>upper=0.3333333333333333</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#RReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the randomized leaky rectified liner unit function element-wise
described in the paper
<a class="reference external" href="https://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a>.</p>
<p>The function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{RReLU}(x) = \begin{cases}
    x &amp; \text{if } x \geq 0 \\
    ax &amp; \text{ otherwise }
\end{cases},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(a\)</span> is randomly sampled from uniform distribution
<span class="math notranslate nohighlight">\(\mathcal{U}(\text{lower}, \text{upper})\)</span>.</p>
<blockquote>
<div>See: <a class="reference external" href="https://arxiv.org/pdf/1505.00853.pdf">https://arxiv.org/pdf/1505.00853.pdf</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>lower</strong> – lower bound of the uniform distribution. Default: <span class="math notranslate nohighlight">\(\frac{1}{8}\)</span></li>
<li><strong>upper</strong> – upper bound of the uniform distribution. Default: <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="selu">
<h3><span class="hidden-section">SELU</span><a class="headerlink" href="#selu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SELU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#SELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\)</span>,
with <span class="math notranslate nohighlight">\(\alpha = 1.6732632423543772848170429916717\)</span> and
<span class="math notranslate nohighlight">\(\text{scale} = 1.0507009873554804934193349852946\)</span>.</p>
<img alt="_images/SELU.png" src="_images/SELU.png" />
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h3><span class="hidden-section">Sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math notranslate nohighlight">\(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Sigmoid.png" src="_images/Sigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softplus">
<h3><span class="hidden-section">Softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softplus">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softplus</code><span class="sig-paren">(</span><em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math notranslate nohighlight">\(\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))\)</span></p>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> – the <span class="math notranslate nohighlight">\(\beta\)</span> value for the Softplus formulation. Default: 1</li>
<li><strong>threshold</strong> – values above this revert to a linear function. Default: 20</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softplus.png" src="_images/Softplus.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h3><span class="hidden-section">Softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>SoftShrinkage function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{SoftShrinkage}(x) =
\begin{cases}
x - \lambda, &amp; \text{ if } x &gt; \lambda \\
x + \lambda, &amp; \text{ if } x &lt; -\lambda \\
0, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> – the <span class="math notranslate nohighlight">\(\lambda\)</span> value for the Softshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softshrink.png" src="_images/Softshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softsign">
<h3><span class="hidden-section">Softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softsign">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softsign</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math notranslate nohighlight">\(\text{SoftSign}(x) = \frac{x}{ 1 + |x|}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softsign.png" src="_images/Softsign.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanh">
<h3><span class="hidden-section">Tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanh</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{Tanh}(x) = \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Tanh.png" src="_images/Tanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h3><span class="hidden-section">Tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanhshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanhshrink</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math notranslate nohighlight">\(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Tanhshrink.png" src="_images/Tanhshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="threshold">
<h3><span class="hidden-section">Threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Threshold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor</p>
<p>Threshold is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y =
\begin{cases}
x, &amp;\text{ if } x &gt; \text{threshold} \\
\text{value}, &amp;\text{ otherwise }
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>threshold</strong> – The value to threshold at</li>
<li><strong>value</strong> – The value to replace with</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-other">
<h2>Non-linear activations (other)<a class="headerlink" href="#non-linear-activations-other" title="Permalink to this headline">¶</a></h2>
<div class="section" id="softmin">
<h3><span class="hidden-section">Softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmin">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmin</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <cite>(0, 1)</cite> and sum to 1</p>
<p><span class="math notranslate nohighlight">\(\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmin will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input, with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax">
<h3><span class="hidden-section">Softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>
<p>Softmax is defined as
<span class="math notranslate nohighlight">\(\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
<tr class="field-even field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use <cite>LogSoftmax</cite> instead (it’s faster and has better numerical properties).</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax2d">
<h3><span class="hidden-section">Softmax2d</span><a class="headerlink" href="#softmax2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax2d</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies SoftMax over features to each spatial location.</p>
<p>When given an image of <code class="docutils literal notranslate"><span class="pre">Channels</span> <span class="pre">x</span> <span class="pre">Height</span> <span class="pre">x</span> <span class="pre">Width</span></code>, it will
apply <cite>Softmax</cite> to each location <span class="math notranslate nohighlight">\((Channels, h_i, w_j)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you softmax over the 2nd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsoftmax">
<h3><span class="hidden-section">LogSoftmax</span><a class="headerlink" href="#logsoftmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSoftmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSoftmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <cite>Log(Softmax(x))</cite> function to an n-dimensional input Tensor.
The LogSoftmax formulation can be simplified as</p>
<p><span class="math notranslate nohighlight">\(\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivelogsoftmaxwithloss">
<h3><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span><a class="headerlink" href="#adaptivelogsoftmaxwithloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveLogSoftmaxWithLoss</code><span class="sig-paren">(</span><em>in_features</em>, <em>n_classes</em>, <em>cutoffs</em>, <em>div_value=4.0</em>, <em>head_bias=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Efficient softmax approximation as described in
<a class="reference external" href="https://arxiv.org/abs/1609.04309">Efficient softmax approximation for GPUs</a> by Edouard Grave, Armand Joulin,
Moustapha Cissé, David Grangier, and Hervé Jégou.</p>
<p>Adaptive softmax is an approximate strategy for training models with large
output spaces. It is most effective when the label distribution is highly
imbalanced, for example in natural language modelling, where the word
frequency distribution approximately follows the <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s law</a>.</p>
<p>Adaptive softmax partitions the labels into several clusters, according to
their frequency. These clusters may contain different number of targets
each.
Additionally, clusters containig less frequent labels assign lower
dimensional embeddings to those labels, which speeds up the computation.
For each minibatch, only clusters for which at least one target is
present are evaluated.</p>
<p>The idea is that the clusters which are accessed frequently
(like the first one, containing most frequent labels), should also be cheap
to compute – that is, contain a small number of assigned labels.</p>
<p>We highly recommend taking a look at the original paper for more details.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">cutoffs</span></code> should be an ordered Sequence of integers sorted
in the increasing order.
It controls number of clusters and the partitioning of targets into
clusters. For example setting <code class="docutils literal notranslate"><span class="pre">cutoffs</span> <span class="pre">=</span> <span class="pre">[10,</span> <span class="pre">100,</span> <span class="pre">1000]</span></code>
means that first <cite>10</cite> targets will be assigned
to the ‘head’ of the adaptive softmax, targets <cite>11, 12, …, 100</cite> will be
assigned to the first cluster, and targets <cite>101, 102, …, 1000</cite> will be
assigned to the second cluster, while targets
<cite>1001, 1002, …, n_classes - 1</cite> will be assigned
to the last, third cluster</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">div_value</span></code> is used to compute the size of each additional cluster,
which is given as
<span class="math notranslate nohighlight">\(\left\lfloor\frac{in\_features}{div\_value^{idx}}\right\rfloor\)</span>,
where <span class="math notranslate nohighlight">\(idx\)</span> is the cluster index (with clusters
for less frequent words having larger indices,
and indices starting from <span class="math notranslate nohighlight">\(1\)</span>).</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">head_bias</span></code> if set to True, adds a bias term to the ‘head’ of the
adaptive softmax. See paper for details. Set to False in the official
implementation.</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Labels passed as inputs to this module should be sorted accoridng to
their frequency. This means that the most frequent label should be
represented by the index <cite>0</cite>, and the least frequent
label should be represented by the index <cite>n_classes - 1</cite>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module returns a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code>
and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields. See further documentation for details.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To compute log-probabilities for all classes, the <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>
method can be used.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of features in the input tensor</li>
<li><strong>n_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of classes in the dataset.</li>
<li><strong>cutoffs</strong> (<em>Sequence</em>) – Cutoffs used to assign targets to their buckets.</li>
<li><strong>div_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value used as an exponent to compute sizes
of the clusters. Default: 4.0</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>output</strong> is a Tensor of size <code class="docutils literal notranslate"><span class="pre">N</span></code> containing computed target
log probabilities for each example</li>
<li><strong>loss</strong> is a Scalar representing the computed negative
log likelihood loss</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code> and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>input: <span class="math notranslate nohighlight">\((N, in\_features)\)</span></li>
<li>target: <span class="math notranslate nohighlight">\((N)\)</span> where each value satisfies <span class="math notranslate nohighlight">\(0 &lt;= target[i] &lt;= n\_classes\)</span></li>
<li>output: <span class="math notranslate nohighlight">\((N)\)</span></li>
<li>loss: <code class="docutils literal notranslate"><span class="pre">Scalar</span></code></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes log probabilities for all <span class="math notranslate nohighlight">\(n\_classes\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">log-probabilities of for each class <span class="math notranslate nohighlight">\(c\)</span>
in range <span class="math notranslate nohighlight">\(0 &lt;= c &lt;= n\_classes\)</span>, where <span class="math notranslate nohighlight">\(n\_classes\)</span> is a
parameter passed to <code class="docutils literal notranslate"><span class="pre">AdaptiveLogSoftmaxWithLoss</span></code> constructor.</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, in\_features)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, n\_classes)\)</span></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>This is equivalent to <cite>self.log_pob(input).argmax(dim=1)</cite>,
but is more efficient in some cases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a class with the highest probability for each example</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, in\_features)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N)\)</span></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="normalization-layers">
<h2>Normalization layers<a class="headerlink" href="#normalization-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batchnorm1d">
<h3><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size).</p>
<p>By default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h3><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size).</p>
<p>By default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h3><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size).</p>
<p>By default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it’s common terminology to call this Volumetric Batch Normalization
or Spatio-temporal Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="groupnorm">
<h3><span class="hidden-section">GroupNorm</span><a class="headerlink" href="#groupnorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GroupNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GroupNorm</code><span class="sig-paren">(</span><em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#GroupNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The input channels are separated into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups, each containing
<code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels. The mean and standard-deviation are calculated
separately over the each group. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable
per-channel affine transform parameter vectorss of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of groups to separate the channels into</li>
<li><strong>num_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of channels expected in input</li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-channel affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, num\_channels, *)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, num\_channels, *)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 3 groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm1d">
<h3><span class="hidden-section">InstanceNorm1d</span><a class="headerlink" href="#instancenorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, L)\)</span> or <span class="math notranslate nohighlight">\(L\)</span> from input of size <span class="math notranslate nohighlight">\((N, L)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, L)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm2d">
<h3><span class="hidden-section">InstanceNorm2d</span><a class="headerlink" href="#instancenorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm3d">
<h3><span class="hidden-section">InstanceNorm3d</span><a class="headerlink" href="#instancenorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size C (where C is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="layernorm">
<h3><span class="hidden-section">LayerNorm</span><a class="headerlink" href="#layernorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LayerNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LayerNorm</code><span class="sig-paren">(</span><em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#LayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated separately over the last
certain number dimensions which have to be of the shape specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>.
<span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable affine transform parameters of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code>.</p>
</div>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>normalized_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em> or </em><em>torch.Size</em>) – <p>input shape from an expected input
of size</p>
<div class="math notranslate nohighlight">
\[[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1]
    \times \ldots \times \text{normalized_shape}[-1]]\]</div>
<p>If a single integer is used, it is treated as a singleton list, and this module will
normalize over the last dimension which is expected to be of that specific size.</p>
</li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>elementwise_affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-element affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last two dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last dimension of size 10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="localresponsenorm">
<h3><span class="hidden-section">LocalResponseNorm</span><a class="headerlink" href="#localresponsenorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LocalResponseNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LocalResponseNorm</code><span class="sig-paren">(</span><em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#LocalResponseNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LocalResponseNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed
of several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<div class="math notranslate nohighlight">
\[b_{c} = a_{c}\left(k + \frac{\alpha}{n}
\sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> – amount of neighbouring channels used for normalization</li>
<li><strong>alpha</strong> – multiplicative factor. Default: 0.0001</li>
<li><strong>beta</strong> – exponent. Default: 0.75</li>
<li><strong>k</strong> – additive factor. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, ...)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, ...)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lrn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LocalResponseNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_2d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_2d</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_4d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_4d</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="recurrent-layers">
<h2>Recurrent layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="rnn">
<h3><span class="hidden-section">RNN</span><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNN">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer Elman RNN with <cite>tanh</cite> or <cite>ReLU</cite> non-linearity to an
input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[h_t = \tanh(w_{ih} x_t + b_{ih}  +  w_{hh} h_{(t-1)} + b_{hh})\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is
the input at time <cite>t</cite>, and <span class="math notranslate nohighlight">\(h_{(t-1)}\)</span> is the hidden state of the
previous layer at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <cite>‘relu’</cite>, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two RNNs together to form a <cite>stacked RNN</cite>,
with the second RNN taking in outputs of the first RNN and
computing the final results. Default: 1</li>
<li><strong>nonlinearity</strong> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as <cite>(batch, seq, feature)</cite>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
RNN layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
or <a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features (<cite>h_k</cite>) from the last layer of the RNN,
for each <cite>k</cite>.  If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has
been given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the hidden state for <cite>k = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the k-th layer,
of shape <cite>(hidden_size * input_size)</cite> for <cite>k = 0</cite>. Otherwise, the shape is
<cite>(hidden_size * hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the k-th layer,
of shape <cite>(hidden_size * hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstm">
<h3><span class="hidden-section">LSTM</span><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTM">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input
sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\
o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t c_{(t-1)} + i_t g_t \\
h_t = o_t \tanh(c_t)
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(c_t\)</span> is the cell
state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the input at time <cite>t</cite>, <span class="math notranslate nohighlight">\(h_{(t-1)}\)</span>
is the hidden state of the previous layer at time <cite>t-1</cite> or the initial hidden
state at time <cite>0</cite>, and <span class="math notranslate nohighlight">\(i_t\)</span>, <span class="math notranslate nohighlight">\(f_t\)</span>, <span class="math notranslate nohighlight">\(g_t\)</span>,
<span class="math notranslate nohighlight">\(o_t\)</span> are the input, forget, cell, and output gates, respectively.
<span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two LSTMs together to form a <cite>stacked LSTM</cite>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional LSTM. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence.
The input can also be a packed variable length sequence.
See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a> or
<a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a> for details.</p>
</li>
<li><p class="first"><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.</p>
</li>
<li><p class="first"><strong>c_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial cell state for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: output, (h_n, c_n)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features <cite>(h_t)</cite> from the last layer of the LSTM,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code> and similarly for <em>c_n</em>.</p>
</li>
<li><p class="first"><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the cell state for <cite>t = seq_len</cite></p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
<cite>(W_ii|W_if|W_ig|W_io)</cite>, of shape <cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
<cite>(W_hi|W_hf|W_hg|W_ho)</cite>, of shape <cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
<cite>(b_ii|b_if|b_ig|b_io)</cite>, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
<cite>(b_hi|b_hf|b_hg|b_ho)</cite>, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gru">
<h3><span class="hidden-section">GRU</span><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\
h_t = (1 - z_t) n_t + z_t h_{(t-1)} \\
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the input
at time <cite>t</cite>, <span class="math notranslate nohighlight">\(h_{(t-1)}\)</span> is the hidden state of the previous layer
at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>, and <span class="math notranslate nohighlight">\(r_t\)</span>,
<span class="math notranslate nohighlight">\(z_t\)</span>, <span class="math notranslate nohighlight">\(n_t\)</span> are the reset, update, and new gates, respectively.
<span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two GRUs together to form a <cite>stacked GRU</cite>,
with the second GRU taking in outputs of the first GRU and
computing the final results. Default: 1</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
GRU layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional GRU. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features h_t from the last layer of the GRU,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.</p>
<p>Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite></p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
(W_ir|W_iz|W_in), of shape <cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
(W_hr|W_hz|W_hn), of shape <cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
(b_ir|b_iz|b_in), of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math notranslate nohighlight">\(\text{k}^{th}\)</span> layer
(b_hr|b_hz|b_hn), of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rnncell">
<h3><span class="hidden-section">RNNCell</span><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNNCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em>, <em>nonlinearity='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<div class="math notranslate nohighlight">
\[h' = \tanh(w_{ih} x + b_{ih}  +  w_{hh} h + b_{hh})\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <cite>‘relu’</cite>, then ReLU is used in place of tanh.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>nonlinearity</strong> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: h’</dt>
<dd><ul class="first last simple">
<li><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstmcell">
<h3><span class="hidden-section">LSTMCell</span><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTMCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A long short-term memory (LSTM) cell.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\
o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
c' = f * c + i * g \\
h' = o \tanh(c') \\
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</p>
</li>
<li><p class="first"><strong>h_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.</p>
</li>
<li><p class="first"><strong>c_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial cell state
for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: h_1, c_1</dt>
<dd><ul class="first last simple">
<li><strong>h_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
<li><strong>c_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next cell state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx, cx = rnn(input[i], (hx, cx))</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="grucell">
<h3><span class="hidden-section">GRUCell</span><a class="headerlink" href="#grucell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRUCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRUCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A gated recurrent unit (GRU) cell</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\
n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
h' = (1 - z) * n + z * h
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: h’</dt>
<dd><ul class="first last simple">
<li><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="linear-layers">
<h2>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear">
<h3><span class="hidden-section">Linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Linear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y = xA^T + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_features</strong> – size of each input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *, in\_features)\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of
additional dimensions</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *, out\_features)\)</span> where all but the last dimension
are the same shape as the input.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> – the learnable weights of the module of shape
<cite>(out_features x in_features)</cite></li>
<li><strong>bias</strong> – the learnable bias of the module of shape <cite>(out_features)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h3><span class="hidden-section">Bilinear</span><a class="headerlink" href="#bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Bilinear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>in1_features</em>, <em>in2_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a bilinear transformation to the incoming data:
<span class="math notranslate nohighlight">\(y = x_1 A x_2 + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in1_features</strong> – size of each first input sample</li>
<li><strong>in2_features</strong> – size of each second input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *, \text{in1_features})\)</span>, <span class="math notranslate nohighlight">\((N, *, \text{in2_features})\)</span>
where <span class="math notranslate nohighlight">\(*\)</span> means any number of additional dimensions. All but the last
dimension of the inputs should be the same.</li>
<li>Output: <span class="math notranslate nohighlight">\((N, *, \text{out_features})\)</span> where all but the last dimension
are the same shape as the input.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> – the learnable weights of the module of shape
<cite>(out_features x in1_features x in2_features)</cite></li>
<li><strong>bias</strong> – the learnable bias of the module of shape <cite>(out_features)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Bilinear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-layers">
<h2>Dropout layers<a class="headerlink" href="#dropout-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dropout">
<h3><span class="hidden-section">Dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution. The elements to zero are randomized on every forward call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature
detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</li>
<li><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h3><span class="hidden-section">Dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout2d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero-out are randomized on every forward call.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zero-ed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h3><span class="hidden-section">Dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout3d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero are randomized on every forward call.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv3d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="alphadropout">
<h3><span class="hidden-section">AlphaDropout</span><a class="headerlink" href="#alphadropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AlphaDropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AlphaDropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#AlphaDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AlphaDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Alpha Dropout over the input.</p>
<p>Alpha Dropout is a type of Dropout that maintains the self-normalizing
property.
For an input with zero mean and unit standard deviation, the output of
Alpha Dropout maintains the original mean and standard deviation of the
input.
Alpha Dropout goes hand-in-hand with SELU activation function, which ensures
that the outputs have zero mean and unit standard deviation.</p>
<p>During training, it randomly masks some of the elements of the input
tensor with probability <em>p</em> using samples from a bernoulli distribution.
The elements to masked are randomized on every forward call, and scaled
and shifted to maintain zero mean and unit standard deviation.</p>
<p>During evaluation the module simply computes an identity function.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – probability of an element to be dropped. Default: 0.5</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AlphaDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-layers">
<h2>Sparse layers<a class="headerlink" href="#sparse-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="embedding">
<h3><span class="hidden-section">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Embedding">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em>, <em>_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#Embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, will renormalize the embedding vectors to have a norm lesser than
this before extracting.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the max_norm option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape (num_embeddings, embedding_dim)</p>
</td>
</tr>
</tbody>
</table>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: LongTensor of arbitrary shape containing the indices to extract</li>
<li>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Keep in mind that only a limited number of optimizers support
sparse gradients: currently it’s <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SGD</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>),
<code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SparseAdam</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>) and <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.Adagrad</span></code> (<cite>CPU</cite>)</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">With <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> set, the embedding vector at
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> is initialized to all zeros. However, note that this
vector can be modified afterwards, e.g., using a customized
initialization method, and thus changing the vector used to pad the
output. The gradient for this vector from <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a>
is always zero.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[-0.0251, -1.6902,  0.7172],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [-0.3677, -2.7265, -0.1685]],</span>

<span class="go">        [[ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [ 0.4362, -0.4004,  0.9400],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 0.9124, -2.3616,  1.1151]]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.1535, -2.0309,  0.9315],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [-0.1655,  0.9897,  0.0635]]])</span>
</pre></div>
</div>
<dl class="classmethod">
<dt id="torch.nn.Embedding.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>embeddings</em>, <em>freeze=True</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Embedding.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates Embedding instance from given 2-dimensional FloatTensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embeddings</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – FloatTensor containing weights for the Embedding.
First dimension is being passed to Embedding as ‘num_embeddings’, second as ‘embedding_dim’.</li>
<li><strong>freeze</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the tensor does not get updated in the learning process.
Equivalent to <code class="docutils literal notranslate"><span class="pre">embedding.weight.requires_grad</span> <span class="pre">=</span> <span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. weight matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># FloatTensor containing pretrained weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get embeddings for index 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 4.0000,  5.1000,  6.3000]])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="embeddingbag">
<h3><span class="hidden-section">EmbeddingBag</span><a class="headerlink" href="#embeddingbag" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.EmbeddingBag">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">EmbeddingBag</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#EmbeddingBag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.EmbeddingBag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums or means of ‘bags’ of embeddings, without instantiating the
intermediate embeddings.</p>
<p>For bags of constant length, this class</p>
<blockquote>
<div><ul class="simple">
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;sum&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.sum(dim=1)</span></code>,</li>
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;mean&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.mean(dim=1)</span></code>,</li>
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.max(dim=1)</span></code>.</li>
</ul>
</div></blockquote>
<p>However, <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a> is much more time and memory efficient than using a chain of these
operations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, will renormalize the embedding vectors to have a norm lesser than
this before extracting.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the max_norm option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor. See
Notes for more details regarding sparse gradients. Note: this option is not
supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape <code class="docutils literal notranslate"><span class="pre">(num_embeddings</span> <span class="pre">x</span> <span class="pre">embedding_dim)</span></code></p>
</td>
</tr>
</tbody>
</table>
<p>Inputs: <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<blockquote>
<div><ul>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span></code>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <code class="docutils literal notranslate"><span class="pre">N</span></code>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <code class="docutils literal notranslate"><span class="pre">B</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</div></blockquote>
<p>Output shape: <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">embedding_dim</span></code></p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[-0.8861, -5.4350, -0.0523],</span>
<span class="go">        [ 1.1306, -2.5798, -1.0044]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h2>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cosinesimilarity">
<h3><span class="hidden-section">CosineSimilarity</span><a class="headerlink" href="#cosinesimilarity" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineSimilarity">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineSimilarity</code><span class="sig-paren">(</span><em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#CosineSimilarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineSimilarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, computed along dim.</p>
<div class="math notranslate nohighlight">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension where cosine similarity is computed. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math notranslate nohighlight">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite></li>
<li>Input2: <span class="math notranslate nohighlight">\((\ast_1, D, \ast_2)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math notranslate nohighlight">\((\ast_1, \ast_2)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CosineSimilarity</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pairwisedistance">
<h3><span class="hidden-section">PairwiseDistance</span><a class="headerlink" href="#pairwisedistance" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PairwiseDistance">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>p=2</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#PairwiseDistance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors <span class="math notranslate nohighlight">\(v_1\)</span>,:math:<cite>v_2</cite> using the p-norm:</p>
<div class="math notranslate nohighlight">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<em>real</em>) – the norm degree. Default: 2</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-6</li>
<li><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Determines whether or not to keep the batch dimension.
Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Input2: <span class="math notranslate nohighlight">\((N, D)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math notranslate nohighlight">\((N)\)</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then <span class="math notranslate nohighlight">\((N, 1)\)</span>.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pdist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PairwiseDistance</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="l1loss">
<h3><span class="hidden-section">L1Loss</span><a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.L1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean absolute value of the
element-wise difference between input <cite>x</cite> and target <cite>y</cite>:</p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left| x_n - y_n \right|,\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the constructor argument
<cite>size_average=False</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then
<span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mseloss">
<h3><span class="hidden-section">MSELoss</span><a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MSELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MSELoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MSELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean squared error between
<cite>n</cite> elements in the input <cite>x</cite> and target <cite>y</cite>.</p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>To get a batch of losses, a loss per batch element, set <cite>reduce</cite> to
<code class="docutils literal notranslate"><span class="pre">False</span></code>. These losses are not averaged and are not affected by
<cite>size_average</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="crossentropyloss">
<h3><span class="hidden-section">CrossEntropyLoss</span><a class="headerlink" href="#crossentropyloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CrossEntropyLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CrossEntropyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.LogSoftmax()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.NLLLoss()</span></code> in one single class.</p>
<p>It is useful when training a classification problem with <cite>C</cite> classes.
If provided, the optional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> should be a 1D <cite>Tensor</cite>
assigning weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>
<p>The <cite>input</cite> is expected to contain scores for each class.</p>
<p><cite>input</cite> has to be a Tensor of size either <span class="math notranslate nohighlight">\((minibatch, C)\)</span> or
<span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math notranslate nohighlight">\(K \geq 2\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>This criterion expects a class index (0 to <cite>C-1</cite>) as the
<cite>target</cite> for each value of a 1D tensor of size <cite>minibatch</cite></p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)\]</div>
<p>or in the case of the <cite>weight</cite> argument being specified:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)\]</div>
<p>The losses are averaged across observations for each minibatch.</p>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each class.
If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <cite>size_average</cite> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>C = number of classes</cite>, or</dt>
<dd><span class="math notranslate nohighlight">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span>
in the case of <cite>K</cite>-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Target: <span class="math notranslate nohighlight">\((N)\)</span> where each value is <span class="math notranslate nohighlight">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or</dt>
<dd><span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span> in the case of
K-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the same size</dt>
<dd>as the target: <span class="math notranslate nohighlight">\((N)\)</span>, or
<span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span> in the case
of K-dimensional loss.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss">
<h3><span class="hidden-section">NLLLoss</span><a class="headerlink" href="#nllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.NLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#NLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>If provided, the optional argument <cite>weight</cite> should be a 1D Tensor assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.</p>
<p>The input given through a forward call is expected to contain
log-probabilities of each class. <cite>input</cite> has to be a Tensor of size either
<span class="math notranslate nohighlight">\((minibatch, C)\)</span> or <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math notranslate nohighlight">\(K \geq 2\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The target that this loss expects is a class index
<cite>(0 to C-1, where C = number of classes)</cite></p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\},\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default),
then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp; \text{if}\;
    \text{size_average} = \text{True},\\
    \sum_{n=1}^N l_n,  &amp; \text{if}\;
    \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math notranslate nohighlight">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over
non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>C = number of classes</cite>, or</dt>
<dd><span class="math notranslate nohighlight">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span>
in the case of <cite>K</cite>-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Target: <span class="math notranslate nohighlight">\((N)\)</span> where each value is <span class="math notranslate nohighlight">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or</dt>
<dd><span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span> in the case of
K-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the same size</dt>
<dd>as the target: <span class="math notranslate nohighlight">\((N)\)</span>, or
<span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math notranslate nohighlight">\(K \geq 2\)</span> in the case
of K-dimensional loss.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2D loss example (used, for example, with image inputs)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C x height x width</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poissonnllloss">
<h3><span class="hidden-section">PoissonNLLLoss</span><a class="headerlink" href="#poissonnllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PoissonNLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PoissonNLLLoss</code><span class="sig-paren">(</span><em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#PoissonNLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PoissonNLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Negative log likelihood loss with Poisson distribution of target.</p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{target} \sim \mathrm{Poisson}(\text{input})\\\text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
                            + \log(\text{target!})\end{aligned}\end{align} \]</div>
<p>The last term can be omitted or approximated with Stirling formula. The
approximation is used for target values more than 1. For targets less or
equal to 1 zeros are added to the loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math notranslate nohighlight">\(\exp(\text{input}) - \text{target}*\text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> the loss is
<span class="math notranslate nohighlight">\(\text{input} - \text{target}*\log(\text{input}+\text{eps})\)</span>.</li>
<li><strong>full</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – <p>whether to compute full loss, i. e. to add the
Stirling approximation term</p>
<div class="math notranslate nohighlight">
\[\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).\]</div>
</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math notranslate nohighlight">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input</span> <span class="pre">==</span> <span class="pre">False</span></code>. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="kldivloss">
<h3><span class="hidden-section">KLDivLoss</span><a class="headerlink" href="#kldivloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.KLDivLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">KLDivLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#KLDivLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.</p>
<p>As with <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em>. However, unlike <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a>, <cite>input</cite> is not
restricted to a 2D Tensor, because the criterion is applied element-wise.
The targets are given as <em>probabilities</em> (i.e. without taking the logarithm).</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> set to <code class="docutils literal notranslate"><span class="pre">False</span></code>) loss can be described as:</p>
<div class="math notranslate nohighlight">
\[l(x,y) = L := \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right),\]</div>
<p>where the index <span class="math notranslate nohighlight">\(N\)</span> spans all dimensions of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <span class="math notranslate nohighlight">\(L\)</span> has the same
shape as <code class="docutils literal notranslate"><span class="pre">input</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (the default), then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>By default, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. However, if the field
<code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The default averaging means that the loss is actually <strong>not</strong> the
KL Divergence because the terms are already probability weighted.
A future release of PyTorch may move the default loss closer to the
mathematical definition.</p>
<p>To get the real KL Divergence, use <code class="docutils literal notranslate"><span class="pre">size_average=False</span></code>, and
then divide the output by the batch size.</p>
<p>Example:</p>
<div class="last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probs2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">log_probs1</span><span class="p">,</span> <span class="n">probs2</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="go">tensor(0.7142)</span>
</pre></div>
</div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
<li><dl class="first docutils">
<dt>output: scalar by default. If <cite>reduce</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then <span class="math notranslate nohighlight">\((N, *)\)</span>,</dt>
<dd>the same shape as the input</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="bceloss">
<h3><span class="hidden-section">BCELoss</span><a class="headerlink" href="#bceloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCELoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:</p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>y</cite> should be numbers
between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
“nbatch”.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N, *)</cite>, same shape as
input.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bcewithlogitsloss">
<h3><span class="hidden-section">BCEWithLogitsLoss</span><a class="headerlink" href="#bcewithlogitsloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCEWithLogitsLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCEWithLogitsLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This loss combines a <cite>Sigmoid</cite> layer and the <cite>BCELoss</cite> in one single
class. This version is more numerically stable than using a plain <cite>Sigmoid</cite>
followed by a <cite>BCELoss</cite> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ t_n \cdot \log \sigma(x_n)
+ (1 - t_n) \cdot \log (1 - \sigma(x_n)) \right],\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>t[i]</cite> should be numbers
between 0 and 1.</p>
<p>It’s possible to trade off recall and precision by adding weights to positive examples.
In this case the loss can be described as:</p>
<div class="math notranslate nohighlight">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ p_n t_n \cdot \log \sigma(x_n)
+ (1 - t_n) \cdot \log (1 - \sigma(x_n)) \right],\]</div>
<p>where <span class="math notranslate nohighlight">\(p_n\)</span> is the positive weight of class <span class="math notranslate nohighlight">\(n\)</span>.
<span class="math notranslate nohighlight">\(p_n &gt; 1\)</span> increases the recall, <span class="math notranslate nohighlight">\(p_n &lt; 1\)</span> increases the precision.</p>
<p>For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <cite>pos_weight</cite> for the class should be equal to <span class="math notranslate nohighlight">\(\frac{300}{100}=3\)</span>.
The loss would act as if the dataset contains math:<cite>3times 100=300</cite> positive examples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
“nbatch”.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
<li><strong>pos_weight</strong> – a weight of positive examples.
Must be a vector with length equal to the number of classes.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="marginrankingloss">
<h3><span class="hidden-section">MarginRankingLoss</span><a class="headerlink" href="#marginrankingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MarginRankingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MarginRankingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MarginRankingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <cite>x1</cite>, <cite>x2</cite>, two 1D mini-batch <cite>Tensor`s,
and a label 1D mini-batch tensor `y</cite> with values (<cite>1</cite> or <cite>-1</cite>).</p>
<p>If <cite>y == 1</cite> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <cite>y == -1</cite>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>0</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</li>
<li>Target: <span class="math notranslate nohighlight">\((N)\)</span></li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="hingeembeddingloss">
<h3><span class="hidden-section">HingeEmbeddingLoss</span><a class="headerlink" href="#hingeembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.HingeEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">HingeEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.HingeEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the loss given an input tensor <cite>x</cite> and a labels tensor <cite>y</cite>
containing values (<cite>1</cite> or <cite>-1</cite>).
This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as <cite>x</cite>, and is typically
used for learning nonlinear embeddings or semi-supervised learning.</p>
<p>The loss function for <span class="math notranslate nohighlight">\(n\)</span>-th sample in the mini-batch is</p>
<div class="math notranslate nohighlight">
\[\begin{split}l_n = \begin{cases}
    x_n, &amp; \text{if}\; y_n = 1,\\
    \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,
\end{cases}\end{split}\]</div>
<p>and the total loss functions is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(L = \{l_1,\dots,l_N\}^\top\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: Tensor of arbitrary shape. The sum operation operates over all the elements.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelmarginloss">
<h3><span class="hidden-section">MultiLabelMarginLoss</span><a class="headerlink" href="#multilabelmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>)
and output <cite>y</cite> (which is a 2D <cite>Tensor</cite> of target class indices).
For each sample in the mini-batch:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}\]</div>
<p>where <cite>i == 0</cite> to <cite>x.size(0)</cite>, <cite>j == 0</cite> to <cite>y.size(0)</cite>,
<span class="math notranslate nohighlight">\(y[j] \geq 0\)</span>, and <span class="math notranslate nohighlight">\(i \neq y[j]\)</span> for all <cite>i</cite> and <cite>j</cite>.</p>
<p><cite>y</cite> and <cite>x</cite> must have the same size.</p>
<p>The criterion only considers a contiguous block of non-negative targets that
starts at the front.</p>
<p>This allows for different samples to have variable amounts of target classes</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((C)\)</span> or <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite>
is the number of classes.</li>
<li>Target: <span class="math notranslate nohighlight">\((C)\)</span> or <span class="math notranslate nohighlight">\((N, C)\)</span>, same shape as the input.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="smoothl1loss">
<h3><span class="hidden-section">SmoothL1Loss</span><a class="headerlink" href="#smoothl1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SmoothL1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SmoothL1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SmoothL1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <cite>MSELoss</cite> and in some cases
prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).
Also known as the Huber loss:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{i}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}z_{i} =
\begin{cases}
0.5 (x_i - y_i)^2, &amp; \text{if } |x_i - y_i| &lt; 1 \\
|x_i - y_i| - 0.5, &amp; \text{otherwise }
\end{cases}\end{split}\]</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each
the sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then
<span class="math notranslate nohighlight">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="softmarginloss">
<h3><span class="hidden-section">SoftMarginLoss</span><a class="headerlink" href="#softmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SoftMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a two-class classification
logistic loss between input tensor <cite>x</cite> and target tensor <cite>y</cite> (containing 1 or
-1).</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: Tensor of arbitrary shape.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelsoftmarginloss">
<h3><span class="hidden-section">MultiLabelSoftMarginLoss</span><a class="headerlink" href="#multilabelsoftmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelSoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelSoftMarginLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelSoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <cite>x</cite> and target <cite>y</cite> of size <cite>(N, C)</cite>.
For each sample in the minibatch:</p>
<div class="math notranslate nohighlight">
\[loss(x, y) = - \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
                 + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)\]</div>
<p>where <cite>i == 0</cite> to <cite>x.nElement()-1</cite>, <cite>y[i]  in {0,1}</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite> is the number of classes.</li>
<li>Target: <span class="math notranslate nohighlight">\((N, C)\)</span>, same shape as the input.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cosineembeddingloss">
<h3><span class="hidden-section">CosineEmbeddingLoss</span><a class="headerlink" href="#cosineembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given input tensors
<span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> and a <cite>Tensor</cite> label <cite>y</cite> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.</p>
<p>The loss function for each sample is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{loss}(x, y) =
\begin{cases}
1 - \cos(x_1, x_2), &amp; \text{if } y == 1 \\
\max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y == -1
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Should be a number from <cite>-1</cite> to <cite>1</cite>, <cite>0</cite> to <cite>0.5</cite>
is suggested. If <cite>margin</cite> is missing, the default value is <cite>0</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="multimarginloss">
<h3><span class="hidden-section">MultiMarginLoss</span><a class="headerlink" href="#multimarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiMarginLoss</code><span class="sig-paren">(</span><em>p=1</em>, <em>margin=1</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class classification hinge
loss (margin-based loss) between input <cite>x</cite> (a 2D mini-batch <cite>Tensor</cite>) and
output <cite>y</cite> (which is a 1D tensor of target class indices,
<span class="math notranslate nohighlight">\(0 \leq y \leq \text{x.size}(1)\)</span>):</p>
<p>For each mini-batch sample, the loss in terms of the 1D input <cite>x</cite> and scalar
output <cite>y</cite> is:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}\]</div>
<p>where <cite>i == 0</cite> to <cite>x.size(0)</cite> and <span class="math notranslate nohighlight">\(i \neq y\)</span>.</p>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <cite>weight</cite> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<div class="math notranslate nohighlight">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>. <cite>1</cite> and <cite>2</cite> are the only
supported values</li>
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="tripletmarginloss">
<h3><span class="hidden-section">TripletMarginLoss</span><a class="headerlink" href="#tripletmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.TripletMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">TripletMarginLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#TripletMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors x1, x2, x3 and a margin with a value greater than 0.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite>: anchor, positive examples and negative
example respectively. The shapes of all input tensors should be
<span class="math notranslate nohighlight">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math notranslate nohighlight">
\[L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}\]</div>
<p>where <span class="math notranslate nohighlight">\(d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Default: <cite>1</cite>.</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The norm degree for pairwise distance. Default: <cite>2</cite>.</li>
<li><strong>swap</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The distance swap is described in detail in the paper
<cite>Learning shallow convolutional feature descriptors with triplet losses</cite> by
V. Balntas, E. Riba et al. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, D)\)</span> where <cite>D</cite> is the vector dimension.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="vision-layers">
<h2>Vision layers<a class="headerlink" href="#vision-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixelshuffle">
<h3><span class="hidden-section">PixelShuffle</span><a class="headerlink" href="#pixelshuffle" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PixelShuffle">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PixelShuffle</code><span class="sig-paren">(</span><em>upscale_factor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a Tensor of shape <span class="math notranslate nohighlight">\((*, r^2C, H, W)\)</span> to a
tensor of shape <span class="math notranslate nohighlight">\((C, rH, rW)\)</span>.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math notranslate nohighlight">\(1/r\)</span>.</p>
<p>Look at the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((N, C * \text{upscale_factor}^2, H, W)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, C, H * \text{upscale_factor}, W * \text{upscale_factor})\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsample">
<h3><span class="hidden-section">Upsample</span><a class="headerlink" href="#upsample" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Upsample">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Upsample</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#Upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form
<cite>minibatch x channels x [optional depth] x [optional height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear
for 3D, 4D and 5D input Tensor, respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>([optional D_out], [optional H_out], W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<em>int / tuple of python:ints</em><em>, </em><em>optional</em>) – the multiplier for the image height / width / depth</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – the upsampling algorithm: one of <cite>nearest</cite>, <cite>linear</cite>, <cite>bilinear</cite> and <cite>trilinear</cite>.
Default: <cite>nearest</cite></li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, W_{in})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span> or <span class="math notranslate nohighlight">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, W_{out})\)</span>, <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math notranslate nohighlight">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}D_{out} = \left\lfloor D_{in} \times \text{scale_factor} \right\rfloor \text{ or size}[-3]\\H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor \text{ or size}[-2]\\W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor \text{ or size}[-1]\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See below for concrete examples on how this affects the outputs.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  2.5000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  3.5000],</span>
<span class="go">          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Try scaling the same data in a larger tensor</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span>
<span class="go">tensor([[[[ 1.,  2.,  0.],</span>
<span class="go">          [ 3.,  4.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are the same with the small input (except at boundary)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],</span>
<span class="go">          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],</span>
<span class="go">          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are now changed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],</span>
<span class="go">          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],</span>
<span class="go">          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],</span>
<span class="go">          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],</span>
<span class="go">          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingnearest2d">
<h3><span class="hidden-section">UpsamplingNearest2d</span><a class="headerlink" href="#upsamplingnearest2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingNearest2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingNearest2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>(H_out, W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the multiplier for the image height or width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\\W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingbilinear2d">
<h3><span class="hidden-section">UpsamplingBilinear2d</span><a class="headerlink" href="#upsamplingbilinear2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingBilinear2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingBilinear2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>(H_out, W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the multiplier for the image height or width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>. It is
equivalent to <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math notranslate nohighlight">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math notranslate nohighlight">\((N, C, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor H_{in} \times \text{scale_factor} \right\rfloor\\W_{out} = \left\lfloor W_{in} \times \text{scale_factor} \right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-layers-multi-gpu-distributed">
<h2>DataParallel layers (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-layers-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataparallel">
<h3><span class="hidden-section">DataParallel</span><a class="headerlink" href="#dataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.DataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">DataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#DataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. In the forward pass, the module is replicated on each device,
and each replica handles a portion of the input. During the backwards
pass, gradients from each replica are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used.</p>
<p>See also: <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel EXCEPT Tensors. All tensors will be scattered on dim
specified (default 0). Primitive types will be broadcasted, but all
other types will be a shallow copy and can be corrupted if written to in
the model’s forward pass.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
will be invoked <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> times, each with inputs located on
a particular device. Particularly, the hooks are only guaranteed to be
executed in correct order with respect to operations on corresponding
devices. For example, it is not guaranteed that hooks set via
<a class="reference internal" href="#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_forward_pre_hook()</span></code></a> be executed before
<cite>all</cite> <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> calls, but
that each such hook be executed before the corresponding
<a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> call of that device.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> returns a scalar (i.e., 0-dimensional tensor) in
<code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code>, this wrapper will return a vector of length equal to
number of devices used in data parallelism, containing the result from
each device.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is a subtlety in using the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <a class="reference internal" href="notes/faq.html#pack-rnn-unpack-with-data-parallelism"><span class="std std-ref">My recurrent network doesn’t work with data parallelism</span></a> section in FAQ for
details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> – module to be parallelized</li>
<li><strong>device_ids</strong> – CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> – device location of output (default: device_ids[0])</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="distributeddataparallel">
<h3><span class="hidden-section">DistributedDataParallel</span><a class="headerlink" href="#distributeddataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>broadcast_buffers=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/distributed.html#DistributedDataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine and each device, and
each such replica handles a portion of the input. During the backwards
pass, gradients from each node are averaged.</p>
<p>The batch size should be larger than the number of GPUs used locally. It
should also be an integer multiple of the number of GPUs so that each chunk
is the same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a class="reference internal" href="distributed.html#distributed-basics"><span class="std std-ref">Basics</span></a> and <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a>.
The same constraints on input as in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> apply.</p>
<p>Creation of this class requires the distributed package to be already
initialized in the process group mode
(see <a class="reference internal" href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code></a>).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module works only with the <code class="docutils literal notranslate"><span class="pre">nccl</span></code> and <code class="docutils literal notranslate"><span class="pre">gloo</span></code> backends.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different processes might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.
Same applies to buffers.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all buffers and gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module doesn’t work with <a class="reference internal" href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you plan on using this module with a <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend or a <code class="docutils literal notranslate"><span class="pre">gloo</span></code>
backend (that uses Infiniband), together with a DataLoader that uses
multiple workers, please change the multiprocessing start method to
<code class="docutils literal notranslate"><span class="pre">forkserver</span></code> (Python 3 only) or <code class="docutils literal notranslate"><span class="pre">spawn</span></code>. Unfortunately
Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will
likely experience deadlocks if you don’t change this setting.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Parameters are never broadcast between processes. The module performs
an all-reduce step on gradients and assumes that they will be modified
by the optimizer in all processes in the same way. Buffers
(e.g. BatchNorm stats) are broadcast from the module in process of rank
0, to all other replicas in the system in every iteration.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
won’t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> – module to be parallelized</li>
<li><strong>device_ids</strong> – CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> – device location of output (default: device_ids[0])</li>
<li><strong>broadcast_buffers</strong> – flag that enables syncing (broadcasting) buffers of
the module at beginning of the forward function.
(default: True)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="clip-grad-norm">
<h3><span class="hidden-section">clip_grad_norm_</span><a class="headerlink" href="#clip-grad-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_norm_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_norm_</code><span class="sig-paren">(</span><em>parameters</em>, <em>max_norm</em>, <em>norm_type=2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – max norm of the gradients</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Total norm of the parameters (viewed as a single vector).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="clip-grad-value">
<h3><span class="hidden-section">clip_grad_value_</span><a class="headerlink" href="#clip-grad-value" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_value_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_value_</code><span class="sig-paren">(</span><em>parameters</em>, <em>clip_value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/clip_grad.html#clip_grad_value_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.clip_grad_value_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient of an iterable of parameters at specified value.</p>
<p>Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</li>
<li><strong>clip_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximum allowed value of the gradients
The gradients are clipped in the range [-clip_value, clip_value]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="parameters-to-vector">
<h3><span class="hidden-section">parameters_to_vector</span><a class="headerlink" href="#parameters-to-vector" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.parameters_to_vector">
<code class="descclassname">torch.nn.utils.</code><code class="descname">parameters_to_vector</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.parameters_to_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert parameters to one vector</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The parameters represented by a single vector</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="vector-to-parameters">
<h3><span class="hidden-section">vector_to_parameters</span><a class="headerlink" href="#vector-to-parameters" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.vector_to_parameters">
<code class="descclassname">torch.nn.utils.</code><code class="descname">vector_to_parameters</code><span class="sig-paren">(</span><em>vec</em>, <em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.vector_to_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert one vector to the parameters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vec</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a single vector represents the parameters of a model.</li>
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="weight-norm">
<h3><span class="hidden-section">weight_norm</span><a class="headerlink" href="#weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies weight normalization to a parameter in the given module.</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}\]</div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <cite>name</cite> (e.g. “weight”) with two parameters: one specifying the magnitude
(e.g. “weight_g”) and one specifying the direction (e.g. “weight_v”).
Weight normalization is implemented via a hook that recomputes the weight
tensor from the magnitude and direction before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>
call.</p>
<p>By default, with <cite>dim=0</cite>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<cite>dim=None</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension over which to compute the norm</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original module with the weight norm hook</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="go">Linear (20 -&gt; 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-weight-norm">
<h3><span class="hidden-section">remove_weight_norm</span><a class="headerlink" href="#remove-weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.remove_weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#remove_weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the weight normalization reparameterization from a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="spectral-norm">
<h3><span class="hidden-section">spectral_norm</span><a class="headerlink" href="#spectral-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>n_power_iterations=1</em>, <em>eps=1e-12</em>, <em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/spectral_norm.html#spectral_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies spectral normalization to a parameter in the given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{W} &amp;= \dfrac{\mathbf{W}}{\sigma(\mathbf{W})} \\
\sigma(\mathbf{W}) &amp;= \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}\end{split}\]</div>
<p>Spectral normalization stabilizes the training of discriminators (critics)
in Generaive Adversarial Networks (GANs) by rescaling the weight tensor
with spectral norm <span class="math notranslate nohighlight">\(\sigma\)</span> of the weight matrix calculated using
power iteration method. If the dimension of the weight tensor is greater
than 2, it is reshaped to 2D in power iteration method to get spectral
norm. This is implemented via a hook that calculates spectral norm and
rescales weight before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> call.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1802.05957">Spectral Normalization for Generative Adversarial Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
<li><strong>n_power_iterations</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – number of power iterations to
calculate spectal norm</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – epsilon for numerical stability in
calculating norms</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension corresponding to number of outputs,
the default is 0, except for modules that are instances of
ConvTranspose1/2/3d, when it is 1</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original module with the spectal norm hook</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="go">Linear (20 -&gt; 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_u</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-spectral-norm">
<h3><span class="hidden-section">remove_spectral_norm</span><a class="headerlink" href="#remove-spectral-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.remove_spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.remove_spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the spectral normalization reparameterization from a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_spectral_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="packedsequence">
<h3><span class="hidden-section">PackedSequence</span><a class="headerlink" href="#packedsequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.PackedSequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">PackedSequence</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#PackedSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.PackedSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds the data and list of <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_sizes</span></code> of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Instances of this class should never be created manually. They are meant
to be instantiated by functions like <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p class="last">Batch sizes represent the number elements at each sequence step in
the batch, not the varying sequence lengths passed to
<a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.  For instance, given data  <code class="docutils literal notranslate"><span class="pre">abc</span></code> and <cite>x</cite>
the <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> would contain data <code class="docutils literal notranslate"><span class="pre">axbc</span></code> with
<code class="docutils literal notranslate"><span class="pre">batch_sizes=[2,1,1]</span></code>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor containing packed sequence</li>
<li><strong>batch_sizes</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor of integers holding
information about the batch size at each sequence step</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-padded-sequence">
<h3><span class="hidden-section">pack_padded_sequence</span><a class="headerlink" href="#pack-padded-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_padded_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_padded_sequence</code><span class="sig-paren">(</span><em>input</em>, <em>lengths</em>, <em>batch_first=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pack_padded_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pack_padded_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a Tensor containing padded sequences of variable length.</p>
<p>Input can be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> where <cite>T</cite> is the length of the longest sequence
(equal to <code class="docutils literal notranslate"><span class="pre">lengths[0]</span></code>), <cite>B</cite> is the batch size, and <cite>*</cite> is any number of
dimensions (including 0). If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> inputs are
expected.</p>
<p>The sequences should be sorted by length in a decreasing order, i.e.
<code class="docutils literal notranslate"><span class="pre">input[:,0]</span></code> should be the longest sequence, and <code class="docutils literal notranslate"><span class="pre">input[:,B-1]</span></code> the
shortest one.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function accepts any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Tensor can be retrieved from
a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object by accessing its <code class="docutils literal notranslate"><span class="pre">.data</span></code> attribute.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – padded batch of variable length sequences.</li>
<li><strong>lengths</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – list of sequences lengths of each batch element.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input is expected in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-packed-sequence">
<h3><span class="hidden-section">pad_packed_sequence</span><a class="headerlink" href="#pad-packed-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_packed_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_packed_sequence</code><span class="sig-paren">(</span><em>sequence</em>, <em>batch_first=False</em>, <em>padding_value=0.0</em>, <em>total_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pad_packed_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pad_packed_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>The returned Tensor’s data will be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>T</cite> is the length
of the longest sequence and <cite>B</cite> is the batch size. If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True,
the data will be transposed into <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is useful to implement the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <a class="reference internal" href="notes/faq.html#pack-rnn-unpack-with-data-parallelism"><span class="std std-ref">this FAQ section</span></a> for
details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequence</strong> (<em>PackedSequence</em>) – batch to pad</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</li>
<li><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – values for padded elements.</li>
<li><strong>total_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if not <code class="docutils literal notranslate"><span class="pre">None</span></code>, the output will be padded to
have length <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code>. This method will throw <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>
if <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is less than the max sequence length in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">sequence</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple of Tensor containing the padded sequence, and a Tensor
containing the list of lengths of each sequence in the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-sequence">
<h3><span class="hidden-section">pad_sequence</span><a class="headerlink" href="#pad-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_sequence</code><span class="sig-paren">(</span><em>sequences</em>, <em>batch_first=False</em>, <em>padding_value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pad_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pad_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad a list of variable length Tensors with zero</p>
<p><code class="docutils literal notranslate"><span class="pre">pad_sequence</span></code> stacks a list of Tensors along a new dimension,
and pads them to equal length. For example, if the input is list of
sequences with size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code> and if batch_first is False, and <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>
otherwise.</p>
<p><cite>B</cite> is batch size. It is equal to the number of elements in <code class="docutils literal notranslate"><span class="pre">sequences</span></code>.
<cite>T</cite> is length of the longest sequence.
<cite>L</cite> is length of the sequence.
<cite>*</cite> is any number of trailing dimensions, including none.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pad_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([25, 3, 300])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<dl class="last docutils">
<dt>This function returns a Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> or <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> where <cite>T</cite> is the</dt>
<dd>length of the longest sequence.</dd>
<dt>Function assumes trailing dimensions and type of all the Tensors</dt>
<dd>in sequences are same.</dd>
</dl>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – list of variable length sequences.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> if True, or in
<code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</li>
<li><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value for padded elements. Default: 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> if batch_first is False
Tensor of size <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-sequence">
<h3><span class="hidden-section">pack_sequence</span><a class="headerlink" href="#pack-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_sequence</code><span class="sig-paren">(</span><em>sequences</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pack_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pack_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a list of variable length Tensors</p>
<p><code class="docutils literal notranslate"><span class="pre">sequences</span></code> should be a list of Tensors of size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>L</cite> is
the length of a sequence and <cite>*</cite> is any number of trailing dimensions,
including zero. They should be sorted in the order of decreasing length.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pack_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pack_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
<span class="go">PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – A list of sequences of decreasing length.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-functional">
<h1>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convolution-functions">
<h2>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id20">
<h3><span class="hidden-section">conv1d</span><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iW\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\(out\_channels \times \frac{in\_channels}{groups} \times kW\)</span></li>
<li><strong>bias</strong> – optional bias of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or
a one-element tuple <cite>(sW,)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a one-element tuple <cite>(padW,)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a one-element tuple <cite>(dW,)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id21">
<h3><span class="hidden-section">conv2d</span><a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iH \times iW\)</span>)</li>
<li><strong>weight</strong> – filters of shape (<span class="math notranslate nohighlight">\(out\_channels \times \frac{in\_channels}{groups} \times kH \times kW\)</span>)</li>
<li><strong>bias</strong> – optional bias tensor of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dH, dW)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by the
number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id22">
<h3><span class="hidden-section">conv3d</span><a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iT \times iH \times iW\)</span>)</li>
<li><strong>weight</strong> – filters of shape (<span class="math notranslate nohighlight">\(out\_channels \times \frac{in\_channels}{groups} \times kT \times kH \times kW\)</span>)</li>
<li><strong>bias</strong> – optional bias tensor of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h3><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iW\)</span>)</li>
<li><strong>weight</strong> – filters of shape (<span class="math notranslate nohighlight">\(in\_channels \times \frac{out\_channels}{groups} \times kW\)</span>)</li>
<li><strong>bias</strong> – optional bias of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sW,)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padW,)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padW)</span></code>. Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dW,)</span></code>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h3><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iH \times iW\)</span>)</li>
<li><strong>weight</strong> – filters of shape (<span class="math notranslate nohighlight">\(in\_channels \times \frac{out\_channels}{groups} \times kH \times kW\)</span>)</li>
<li><strong>bias</strong> – optional bias of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sH,</span> <span class="pre">sW)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padH,</span> <span class="pre">padW)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padH,</span> <span class="pre">out_padW)</span></code>.
Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dH,</span> <span class="pre">dW)</span></code>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h3><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iT \times iH \times iW\)</span>)</li>
<li><strong>weight</strong> – filters of shape (<span class="math notranslate nohighlight">\(in\_channels \times \frac{out\_channels}{groups} \times kT \times kH \times kW\)</span>)</li>
<li><strong>bias</strong> – optional bias of shape (<span class="math notranslate nohighlight">\(out\_channels\)</span>). Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sT,</span> <span class="pre">sH,</span> <span class="pre">sW)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padT,</span> <span class="pre">padH,</span> <span class="pre">padW)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(out_padT,</span> <span class="pre">out_padH,</span> <span class="pre">out_padW)</span></code>. Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(in\_channels\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id23">
<h3><span class="hidden-section">unfold</span><a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.unfold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">unfold</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#unfold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from an batched input tensor.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Unfold</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="id24">
<h3><span class="hidden-section">fold</span><a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.fold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">fold</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Fold</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h2>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="avg-pool1d">
<h3><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iW\)</span>)</li>
<li><strong>kernel_size</strong> – the size of the window. Can be a single number or a
tuple <cite>(kW,)</cite></li>
<li><strong>stride</strong> – the stride of the window. Can be a single number or a tuple
<cite>(sW,)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padW,)</cite>. Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Example::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h3><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in <span class="math notranslate nohighlight">\(kH \times kW\)</span> regions by step size
<span class="math notranslate nohighlight">\(sH \times sW\)</span> steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iH \times iW\)</span>)</li>
<li><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple (<span class="math notranslate nohighlight">\(kH \times kW\)</span>)</li>
<li><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h3><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in <span class="math notranslate nohighlight">\(kT \times kH \times kW\)</span> regions by step
size <span class="math notranslate nohighlight">\(sT \times sH \times sW\)</span> steps. The number of output features is equal to
<span class="math notranslate nohighlight">\(\lfloor\frac{\text{input planes}}{sT}\rfloor\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor (<span class="math notranslate nohighlight">\(minibatch \times in\_channels \times iT \times iH \times iW\)</span>)</li>
<li><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple (<span class="math notranslate nohighlight">\(kT \times kH \times kW\)</span>)</li>
<li><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>, Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h3><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool2d">
<h3><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool3d">
<h3><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h3><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h3><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h3><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool1d">
<h3><span class="hidden-section">lp_pool1d</span><a class="headerlink" href="#lp-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h3><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h3><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h3><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool3d">
<h3><span class="hidden-section">adaptive_max_pool3d</span><a class="headerlink" href="#adaptive-max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h3><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h3><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool3d">
<h3><span class="hidden-section">adaptive_avg_pool3d</span><a class="headerlink" href="#adaptive-avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h2>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id25">
<h3><span class="hidden-section">threshold</span><a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>See <a class="reference internal" href="#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Threshold</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.threshold_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold_</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">threshold()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id26">
<h3><span class="hidden-section">relu</span><a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise. See
<a class="reference internal" href="#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu_</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id27">
<h3><span class="hidden-section">hardtanh</span><a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise. See <a class="reference internal" href="#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardtanh</span></code></a> for more
details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.hardtanh_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh_</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardtanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id28">
<h3><span class="hidden-section">relu6</span><a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math notranslate nohighlight">\(\text{ReLU6}(x) = \min(\max(0,x), 6)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU6</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id29">
<h3><span class="hidden-section">elu</span><a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#elu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.elu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu_</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.elu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id30">
<h3><span class="hidden-section">selu</span><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.selu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\)</span>,
with <span class="math notranslate nohighlight">\(\alpha=1.6732632423543772848170429916717\)</span> and
<span class="math notranslate nohighlight">\(scale=1.0507009873554804934193349852946\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">SELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="leaky-relu">
<h3><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#leaky_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative_slope} * \min(0, x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.leaky_relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu_</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.leaky_relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id31">
<h3><span class="hidden-section">prelu</span><a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math notranslate nohighlight">\(\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)\)</span> where weight is a
learnable parameter.</p>
<p>See <a class="reference internal" href="#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">PReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id32">
<h3><span class="hidden-section">rrelu</span><a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#rrelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized leaky ReLU.</p>
<p>See <a class="reference internal" href="#torch.nn.RReLU" title="torch.nn.RReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">RReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.rrelu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu_</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.rrelu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code class="xref py py-func docutils literal notranslate"><span class="pre">rrelu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="glu">
<h3><span class="hidden-section">glu</span><a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.glu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">glu</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.glu" title="Permalink to this definition">¶</a></dt>
<dd><p>The gated linear unit. Computes:</p>
<div class="math notranslate nohighlight">
\[H = A \times \sigma(B)\]</div>
<p>where <cite>input</cite> is split in half along <cite>dim</cite> to form <cite>A</cite> and <cite>B</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input tensor</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension on which to split the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id33">
<h3><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math notranslate nohighlight">\(\text{LogSigmoid}(x) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id34">
<h3><span class="hidden-section">hardshrink</span><a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise</p>
<p>See <a class="reference internal" href="#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id35">
<h3><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math notranslate nohighlight">\(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanhshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id36">
<h3><span class="hidden-section">softsign</span><a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math notranslate nohighlight">\(\text{SoftSign}(x) = \frac{x}{1 + |x|}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softsign</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id37">
<h3><span class="hidden-section">softplus</span><a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id38">
<h3><span class="hidden-section">softmin</span><a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmin function.</p>
<p>Note that <span class="math notranslate nohighlight">\(\text{Softmin}(x) = \text{Softmax}(-x)\)</span>. See softmax definition for mathematical formula.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmin</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmin will be computed (so every slice
along dim will sum to 1).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id39">
<h3><span class="hidden-section">softmax</span><a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax function.</p>
<p>Softmax is defined as:</p>
<p><span class="math notranslate nohighlight">\(\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)</span></p>
<p>It is applied to all slices along dim, and will re-scale them so that the elements
lie in the range <cite>(0, 1)</cite> and sum to 1.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use log_softmax instead (it’s faster and has better numerical properties).</p>
</div>
</dd></dl>

</div>
<div class="section" id="id40">
<h3><span class="hidden-section">softshrink</span><a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>See <a class="reference internal" href="#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="gumbel-softmax">
<h3><span class="hidden-section">gumbel_softmax</span><a class="headerlink" href="#gumbel-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.gumbel_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">gumbel_softmax</code><span class="sig-paren">(</span><em>logits</em>, <em>tau=1</em>, <em>hard=False</em>, <em>eps=1e-10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#gumbel_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.gumbel_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample from the Gumbel-Softmax distribution and optionally discretize.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>logits</strong> – <cite>[batch_size, num_features]</cite> unnormalized log probabilities</li>
<li><strong>tau</strong> – non-negative scalar temperature</li>
<li><strong>hard</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned samples will be discretized as one-hot vectors,
but will be differentiated as if it is the soft sample in autograd</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Sampled tensor of shape <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">num_features</span></code> from the Gumbel-Softmax distribution.
If <code class="docutils literal notranslate"><span class="pre">hard=True</span></code>, the returned samples will be one-hot, otherwise they will
be probability distributions that sum to 1 across features</p>
</td>
</tr>
</tbody>
</table>
<p>Constraints:</p>
<ul class="simple">
<li>Currently only work on 2D input <code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code> tensor of shape <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">num_features</span></code></li>
</ul>
<p>Based on
<a class="reference external" href="https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb">https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb</a> ,
(MIT license)</p>
</dd></dl>

</div>
<div class="section" id="log-softmax">
<h3><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax followed by a logarithm.</p>
<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.</p>
<p>See <a class="reference internal" href="#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which log_softmax will be computed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id41">
<h3><span class="hidden-section">tanh</span><a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math notranslate nohighlight">\(\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id42">
<h3><span class="hidden-section">sigmoid</span><a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math notranslate nohighlight">\(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h2>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batch-norm">
<h3><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#batch_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization for each channel across a batch of data.</p>
<p>See <a class="reference internal" href="#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="instance-norm">
<h3><span class="hidden-section">instance_norm</span><a class="headerlink" href="#instance-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.instance_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">instance_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean=None</em>, <em>running_var=None</em>, <em>weight=None</em>, <em>bias=None</em>, <em>use_input_stats=True</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#instance_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.instance_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization for each channel in each data sample in a
batch.</p>
<p>See <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="layer-norm">
<h3><span class="hidden-section">layer_norm</span><a class="headerlink" href="#layer-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.layer_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">layer_norm</code><span class="sig-paren">(</span><em>input</em>, <em>normalized_shape</em>, <em>weight=None</em>, <em>bias=None</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#layer_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.layer_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization for last certain number of dimensions.</p>
<p>See <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="local-response-norm">
<h3><span class="hidden-section">local_response_norm</span><a class="headerlink" href="#local-response-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.local_response_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">local_response_norm</code><span class="sig-paren">(</span><em>input</em>, <em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#local_response_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.local_response_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed of
several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<p>See <a class="reference internal" href="#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocalResponseNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="normalize">
<h3><span class="hidden-section">normalize</span><a class="headerlink" href="#normalize" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.normalize">
<code class="descclassname">torch.nn.functional.</code><code class="descname">normalize</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em>, <em>dim=1</em>, <em>eps=1e-12</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs <span class="math notranslate nohighlight">\(L_p\)</span> normalization of inputs over specified dimension.</p>
<p>Does:</p>
<div class="math notranslate nohighlight">
\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}\]</div>
<p>for each subtensor v over dimension dim of input. Each subtensor is
flattened into a vector, i.e. <span class="math notranslate nohighlight">\(\lVert v \rVert_p\)</span> is not a matrix
norm.</p>
<p>With default arguments normalizes over the second dimension with Euclidean
norm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of any shape</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the exponent value in the norm formulation. Default: 2</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – small value to avoid division by zero. Default: 1e-12</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h2>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id43">
<h3><span class="hidden-section">linear</span><a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y = xA^T + b\)</span>.</p>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: <span class="math notranslate nohighlight">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</li>
<li>Weight: <span class="math notranslate nohighlight">\((out\_features, in\_features)\)</span></li>
<li>Bias: <span class="math notranslate nohighlight">\((out\_features)\)</span></li>
<li>Output: <span class="math notranslate nohighlight">\((N, *, out\_features)\)</span></li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="id44">
<h3><span class="hidden-section">bilinear</span><a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">bilinear</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.bilinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h2>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id45">
<h3><span class="hidden-section">dropout</span><a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="alpha-dropout">
<h3><span class="hidden-section">alpha_dropout</span><a class="headerlink" href="#alpha-dropout" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.alpha_dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">alpha_dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#alpha_dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.alpha_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies alpha dropout to the input.</p>
<p>See <a class="reference internal" href="#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlphaDropout</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="id46">
<h3><span class="hidden-section">dropout2d</span><a class="headerlink" href="#id46" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout2d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout2d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id47">
<h3><span class="hidden-section">dropout3d</span><a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout3d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout3d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="sparse-functions">
<h2>Sparse functions<a class="headerlink" href="#sparse-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id48">
<h3><span class="hidden-section">embedding</span><a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p>
<p>This module is often used to retrieve word embeddings using indices.
The input to the module is a list of indices, and the embedding matrix,
and the output is the corresponding word embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix
Number of rows should correspond to the maximum possible index + 1,
number of columns is the embedding size</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, will renormalize the embedding vectors to have a norm lesser than
this before extracting. Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the max_norm option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: LongTensor of arbitrary shape containing the indices to extract</li>
<li><dl class="first docutils">
<dt>Weight: Embedding matrix of floating point type with shape <cite>(V, embedding_dim)</cite>,</dt>
<dd>where V = maximum index + 1 and embedding_dim = the embedding size</dd>
</dl>
</li>
<li>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># an embedding matrix containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="go">tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.4161,  0.2419,  0.7383]],</span>

<span class="go">        [[ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.0237,  0.7794,  0.0528],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.5609,  0.5384,  0.8720],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.6262,  0.2438,  0.7471]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="embedding-bag">
<h3><span class="hidden-section">embedding_bag</span><a class="headerlink" href="#embedding-bag" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding_bag">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding_bag</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>offsets=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding_bag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding_bag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums or means of ‘bags’ of embeddings, without instantiating the
intermediate embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.EmbeddingBag</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<em>LongTensor</em>) – Tensor containing bags of indices into the embedding matrix</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix
Number of rows should correspond to the maximum possible index + 1,
number of columns is the embedding size</li>
<li><strong>offsets</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – Only used when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D. <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> determines
the starting index position of each bag (sequence) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, will renormalize the embedding vectors to have a norm lesser than
this before extracting. Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The <code class="docutils literal notranslate"><span class="pre">p</span></code> in the <code class="docutils literal notranslate"><span class="pre">p</span></code>-norm to compute for the max_norm option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Shape:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<ul>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span></code>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <code class="docutils literal notranslate"><span class="pre">N</span></code>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <code class="docutils literal notranslate"><span class="pre">B</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> (Tensor): the learnable weights of the module of
shape <code class="docutils literal notranslate"><span class="pre">(num_embeddings</span> <span class="pre">x</span> <span class="pre">embedding_dim)</span></code></p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>: aggregated embedding values of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">embedding_dim</span></code></p>
</li>
</ul>
</div></blockquote>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="go">        [ 0.5893,  0.4386,  0.5882]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="id49">
<h2>Distance functions<a class="headerlink" href="#id49" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pairwise-distance">
<h3><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pairwise_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.PairwiseDistance</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="cosine-similarity">
<h3><span class="hidden-section">cosine_similarity</span><a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_similarity">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cosine_similarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math notranslate nohighlight">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x1</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First input.</li>
<li><strong>x2</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second input (of size matching x1).</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of vectors. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math notranslate nohighlight">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite>.</li>
<li>Output: <span class="math notranslate nohighlight">\((\ast_1, \ast_2)\)</span> where 1 is at position <cite>dim</cite>.</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="id50">
<h2>Loss functions<a class="headerlink" href="#id50" title="Permalink to this headline">¶</a></h2>
<div class="section" id="binary-cross-entropy">
<h3><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output.</p>
<p>See <a class="reference internal" href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCELoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poisson-nll-loss">
<h3><span class="hidden-section">poisson_nll_loss</span><a class="headerlink" href="#poisson-nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.poisson_nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">poisson_nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#poisson_nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.poisson_nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Poisson negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoissonNLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – expectation of underlying Poisson distribution.</li>
<li><strong>target</strong> – random sample <span class="math notranslate nohighlight">\(target \sim \text{Poisson}(input)\)</span>.</li>
<li><strong>log_input</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math notranslate nohighlight">\(\exp(\text{input}) - \text{target} * \text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> then loss is
<span class="math notranslate nohighlight">\(\text{input} - \text{target} * \log(\text{input}+\text{eps})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>full</strong> – whether to compute full loss, i. e. to add the Stirling
approximation term. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
<span class="math notranslate nohighlight">\(\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})\)</span>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math notranslate nohighlight">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input`=``False`</span></code>. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cosine-embedding-loss">
<h3><span class="hidden-section">cosine_embedding_loss</span><a class="headerlink" href="#cosine-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_embedding_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#cosine_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h3><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in a single
function.</p>
<p>See <a class="reference internal" href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math notranslate nohighlight">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K &gt; 1\)</span>
in the case of K-dimensional loss.</li>
<li><strong>target</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math notranslate nohighlight">\((N)\)</span> where each value is <span class="math notranslate nohighlight">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K \geq 1\)</span> for
K-dimensional loss.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hinge-embedding-loss">
<h3><span class="hidden-section">hinge_embedding_loss</span><a class="headerlink" href="#hinge-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hinge_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hinge_embedding_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hinge_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hinge_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">HingeEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h3><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#kl_div"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss.</p>
<p>See <a class="reference internal" href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="l1-loss">
<h3><span class="hidden-section">l1_loss</span><a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that takes the mean element-wise absolute value difference.</p>
<p>See <a class="reference internal" href="#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="mse-loss">
<h3><span class="hidden-section">mse_loss</span><a class="headerlink" href="#mse-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.mse_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#mse_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the element-wise mean squared error.</p>
<p>See <a class="reference internal" href="#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="margin-ranking-loss">
<h3><span class="hidden-section">margin_ranking_loss</span><a class="headerlink" href="#margin-ranking-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.margin_ranking_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">margin_ranking_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#margin_ranking_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.margin_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginRankingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-margin-loss">
<h3><span class="hidden-section">multilabel_margin_loss</span><a class="headerlink" href="#multilabel-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-soft-margin-loss">
<h3><span class="hidden-section">multilabel_soft_margin_loss</span><a class="headerlink" href="#multilabel-soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelSoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multi-margin-loss">
<h3><span class="hidden-section">multi_margin_loss</span><a class="headerlink" href="#multi-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multi_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multi_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>p=1</em>, <em>margin=1</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#multi_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multi_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</dt>
<dd>reduce=None, reduction=’elementwise_mean’) -&gt; Tensor</dd>
</dl>
<p>See <a class="reference internal" href="#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="nll-loss">
<h3><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math notranslate nohighlight">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K &gt; 1\)</span>
in the case of K-dimensional loss.</li>
<li><strong>target</strong> – <span class="math notranslate nohighlight">\((N)\)</span> where each value is <span class="math notranslate nohighlight">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K \geq 1\)</span> for
K-dimensional loss.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy-with-logits">
<h3><span class="hidden-section">binary_cross_entropy_with_logits</span><a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy_with_logits">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy_with_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures Binary Cross Entropy between target and output
logits.</p>
<p>See <a class="reference internal" href="#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCEWithLogitsLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘elementwise_mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘elementwise_mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘elementwise_mean’</li>
<li><strong>pos_weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a weight of positive examples.
Must be a vector with length equal to the number of classes.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h3><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#smooth_l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.</p>
<p>See <a class="reference internal" href="#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="soft-margin-loss">
<h3><span class="hidden-section">soft_margin_loss</span><a class="headerlink" href="#soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="triplet-margin-loss">
<h3><span class="hidden-section">triplet_margin_loss</span><a class="headerlink" href="#triplet-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.triplet_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">triplet_margin_loss</code><span class="sig-paren">(</span><em>anchor</em>, <em>positive</em>, <em>negative</em>, <em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='elementwise_mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#triplet_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.triplet_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">TripletMarginLoss</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h2>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixel-shuffle">
<h3><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><em>input</em>, <em>upscale_factor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pixel_shuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math notranslate nohighlight">\([*, C*r^2, H, W]\)</span> to a
tensor of shape <span class="math notranslate nohighlight">\([C, H*r, W*r]\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Input</li>
<li><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h3><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<dl class="docutils">
<dt><cite>Nd</cite> constant padding:  The number of dimensions to pad is</dt>
<dd><span class="math notranslate nohighlight">\(\left\lfloor\frac{len(padding)}{2}\right\rfloor\)</span> and the dimensions that get padded begins with the
last dimension and moves forward. See below for examples.</dd>
<dt><cite>1D</cite>, <cite>2D</cite> and <cite>3D</cite> “reflect” / “replicate” padding:</dt>
<dd><dl class="first last docutils">
<dt>for 1D:</dt>
<dd>3D input tensor with padding of the form <cite>(padLeft, padRight)</cite></dd>
<dt>for 2D:</dt>
<dd>4D input tensor with padding of the form <cite>(padLeft, padRight, padTop, padBottom)</cite>.</dd>
<dt>for 3D:</dt>
<dd>5D input tensor with padding of the form
<cite>(padLeft, padRight, padTop, padBottom, padFront, padBack)</cite>. No “reflect” implementation.</dd>
</dl>
</dd>
</dl>
<p>See <a class="reference internal" href="#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConstantPad2d</span></code></a>, <a class="reference internal" href="#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReflectionPad2d</span></code></a>, and
<a class="reference internal" href="#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReplicationPad2d</span></code></a> for concrete examples on how each of the
padding modes works.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <cite>Nd</cite> tensor</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – m-elem tuple, where <span class="math notranslate nohighlight">\(\frac{m}{2} \leq\)</span> input dimensions and <span class="math notranslate nohighlight">\(m\)</span> is even.</li>
<li><strong>mode</strong> – ‘constant’, ‘reflect’ or ‘replicate’. Default: ‘constant’</li>
<li><strong>value</strong> – fill value for ‘constant’ padding. Default: 0</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># pad last dim by 1 on each side</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p1d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># effectively zero padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p2d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 8, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p3d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p3d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 9, 7, 3])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="interpolate">
<h3><span class="hidden-section">interpolate</span><a class="headerlink" href="#interpolate" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.interpolate">
<code class="descclassname">torch.nn.functional.</code><code class="descname">interpolate</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#interpolate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<p>The algorithm used for interpolation is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric sampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</li>
<li><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. Default: ‘nearest’</li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
</dd></dl>

</div>
<div class="section" id="id51">
<h3><span class="hidden-section">upsample</span><a class="headerlink" href="#id51" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...)</span></code>.</p>
</div>
<p>The algorithm used for upsampling is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric upsampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for upsampling are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</li>
<li><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’. Default: ‘nearest’</li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-nearest">
<h3><span class="hidden-section">upsample_nearest</span><a class="headerlink" href="#upsample-nearest" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_nearest">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_nearest</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_nearest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_nearest" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using nearest neighbours’ pixel values.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='nearest')</span></code>.</p>
</div>
<p>Currently spatial and volumetric upsampling are supported (i.e. expected
inputs are 4 or 5 dimensional).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatia
size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="upsample-bilinear">
<h3><span class="hidden-section">upsample_bilinear</span><a class="headerlink" href="#upsample-bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_bilinear</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using bilinear upsampling.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with
<code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<p>Expected inputs are spatial (4 dimensional). Use <cite>upsample_trilinear</cite> fo
volumetric (5 dimensional) inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – multiplier for spatial size</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="grid-sample">
<h3><span class="hidden-section">grid_sample</span><a class="headerlink" href="#grid-sample" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.grid_sample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">grid_sample</code><span class="sig-paren">(</span><em>input</em>, <em>grid</em>, <em>mode='bilinear'</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#grid_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.grid_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and a flow-field <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>, computes the
<cite>output</cite> using input pixel locations from the grid.</p>
<p>Uses bilinear interpolation to sample the input pixels.
Currently, only spatial (4 dimensional) and volumetric (5 dimensional)
inputs are supported.</p>
<p>For each output location, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has <cite>x</cite>, <cite>y</cite>
input pixel locations which are used to compute output.
In the case of 5D inputs, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has <cite>x</cite>, <cite>y</cite>, <cite>z</cite> pixel locations.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To avoid confusion in notation, let’s note that <cite>x</cite> corresponds to the <cite>width</cite> dimension <cite>IW</cite>,
<cite>y</cite> corresponds to the height dimension <cite>IH</cite> and <cite>z</cite> corresponds to the <cite>depth</cite> dimension <cite>ID</cite>.</p>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has values in the range of <cite>[-1, 1]</cite>. This is because the
pixel locations are normalized by the input height and width.</p>
<p>For example, values: x: -1, y: -1 is the left-top pixel of the input, and
values: x: 1, y: 1 is the right-bottom pixel of the input.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has values outside the range of <cite>[-1, 1]</cite>, those locations
are handled as defined by <cite>padding_mode</cite>. Options are <cite>zeros</cite> or <cite>border</cite>,
defining those locations to use 0 or image border values as contribution
to the bilinear interpolation.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is used in building Spatial Transformer Networks</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input batch (N x C x IH x IW) or (N x C x ID x IH x IW)</li>
<li><strong>grid</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – flow-field of size (N x OH x OW x 2) or (N x OD x OH x OW x 3)</li>
<li><strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – padding mode for outside grid values
‘zeros’ | ‘border’. Default: ‘zeros’</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="affine-grid">
<h3><span class="hidden-section">affine_grid</span><a class="headerlink" href="#affine-grid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.affine_grid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">affine_grid</code><span class="sig-paren">(</span><em>theta</em>, <em>size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#affine_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.affine_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a 2d flow field, given a batch of affine matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">theta</span></code>
Generally used in conjunction with <a class="reference internal" href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid_sample()</span></code></a> to
implement Spatial Transformer Networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input batch of affine matrices (<span class="math notranslate nohighlight">\(N \times 2 \times 3\)</span>)</li>
<li><strong>size</strong> (<em>torch.Size</em>) – the target output image size (<span class="math notranslate nohighlight">\(N \times C \times H \times W\)</span>)
Example: torch.Size((32, 3, 24, 24))</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor of size (<span class="math notranslate nohighlight">\(N \times H \times W \times 2\)</span>)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-functions-multi-gpu-distributed">
<h2>DataParallel functions (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-functions-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-parallel">
<h3><span class="hidden-section">data_parallel</span><a class="headerlink" href="#data-parallel" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.parallel.data_parallel">
<code class="descclassname">torch.nn.parallel.</code><code class="descname">data_parallel</code><span class="sig-paren">(</span><em>module</em>, <em>inputs</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>module_kwargs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#data_parallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.data_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p>
<p>This is the functional version of the DataParallel module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> – the module to evaluate in parallel</li>
<li><strong>inputs</strong> – inputs to the module</li>
<li><strong>device_ids</strong> – GPU ids on which to replicate module</li>
<li><strong>output_device</strong> – GPU location of the output  Use -1 to indicate the CPU.
(default: device_ids[0])</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a Tensor containing the result of module(input) located on
output_device</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-init">
<h1>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="torch.nn.init.calculate_gain">
<code class="descclassname">torch.nn.init.</code><code class="descname">calculate_gain</code><span class="sig-paren">(</span><em>nonlinearity</em>, <em>param=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#calculate_gain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.calculate_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the recommended gain value for the given nonlinearity function.
The values are as follows:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="75%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">nonlinearity</th>
<th class="head">gain</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Linear / Identity</td>
<td><span class="math notranslate nohighlight">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>Conv{1,2,3}D</td>
<td><span class="math notranslate nohighlight">\(1\)</span></td>
</tr>
<tr class="row-even"><td>Sigmoid</td>
<td><span class="math notranslate nohighlight">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>Tanh</td>
<td><span class="math notranslate nohighlight">\(\frac{5}{3}\)</span></td>
</tr>
<tr class="row-even"><td>ReLU</td>
<td><span class="math notranslate nohighlight">\(\sqrt{2}\)</span></td>
</tr>
<tr class="row-odd"><td>Leaky Relu</td>
<td><span class="math notranslate nohighlight">\(\sqrt{\frac{2}{1 + \text{negative_slope}^2}}\)</span></td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name)</li>
<li><strong>param</strong> – optional parameter for the non-linear function</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gain</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>b=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the uniform
distribution <span class="math notranslate nohighlight">\(\mathcal{U}(a, b)\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the lower bound of the uniform distribution</li>
<li><strong>b</strong> – the upper bound of the uniform distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>mean=0</em>, <em>std=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the normal
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\text{mean}, \text{std})\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>mean</strong> – the mean of the normal distribution</li>
<li><strong>std</strong> – the standard deviation of the normal distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.constant_">
<code class="descclassname">torch.nn.init.</code><code class="descname">constant_</code><span class="sig-paren">(</span><em>tensor</em>, <em>val</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#constant_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.constant_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with the value <span class="math notranslate nohighlight">\(\text{val}\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>val</strong> – the value to fill the tensor with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.eye_">
<code class="descclassname">torch.nn.init.</code><code class="descname">eye_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#eye_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.eye_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2-dimensional input <cite>Tensor</cite> with the identity
matrix. Preserves the identity of the inputs in <cite>Linear</cite> layers, where as
many inputs are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> – a 2-dimensional <cite>torch.Tensor</cite></td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">eye_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.dirac_">
<code class="descclassname">torch.nn.init.</code><code class="descname">dirac_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#dirac_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.dirac_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the {3, 4, 5}-dimensional input <cite>Tensor</cite> with the Dirac
delta function. Preserves the identity of the inputs in <cite>Convolutional</cite>
layers, where as many input channels are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> – a {3, 4, 5}-dimensional <cite>torch.Tensor</cite></td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">dirac_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Understanding the difficulty of training deep feedforward
neural networks” - Glorot, X. &amp; Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-a, a)\)</span> where</p>
<div class="math notranslate nohighlight">
\[a = \text{gain} \times \sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}}\]</div>
<p>Also known as Glorot initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>gain</strong> – an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Understanding the difficulty of training deep feedforward
neural networks” - Glorot, X. &amp; Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled from
<span class="math notranslate nohighlight">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math notranslate nohighlight">
\[\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan_in} + \text{fan_out}}}\]</div>
<p>Also known as Glorot initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>gain</strong> – an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification” - He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\text{bound}, \text{bound})\)</span> where</p>
<div class="math notranslate nohighlight">
\[\text{bound} = \sqrt{\frac{6}{(1 + a^2) \times \text{fan_in}}}\]</div>
<p>Also known as He initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> – either ‘fan_in’ (default) or ‘fan_out’. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with ‘relu’ or ‘leaky_relu’ (default).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification” - He, K. et al. (2015), using a
normal distribution. The resulting tensor will have values sampled from
<span class="math notranslate nohighlight">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math notranslate nohighlight">
\[\text{std} = \sqrt{\frac{2}{(1 + a^2) \times \text{fan_in}}}\]</div>
<p>Also known as He initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> – either ‘fan_in’ (default) or ‘fan_out’. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with ‘relu’ or ‘leaky_relu’ (default).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.orthogonal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">orthogonal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#orthogonal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.orthogonal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with a (semi) orthogonal matrix, as
described in “Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks” - Saxe, A. et al. (2013). The input tensor must have
at least 2 dimensions, and for tensors with more than 2 dimensions the
trailing dimensions are flattened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite>, where <span class="math notranslate nohighlight">\(n \geq 2\)</span></li>
<li><strong>gain</strong> – optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.sparse_">
<code class="descclassname">torch.nn.init.</code><code class="descname">sparse_</code><span class="sig-paren">(</span><em>tensor</em>, <em>sparsity</em>, <em>std=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#sparse_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.sparse_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2D input <cite>Tensor</cite> as a sparse matrix, where the
non-zero elements will be drawn from the normal distribution
<span class="math notranslate nohighlight">\(\mathcal{N}(0, 0.01)\)</span>, as described in “Deep learning via
Hessian-free optimization” - Martens, J. (2010).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>sparsity</strong> – The fraction of elements in each column to be set to zero</li>
<li><strong>std</strong> – the standard deviation of the normal distribution used to generate
the non-zero values</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">sparse_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optim.html" class="btn btn-neutral float-right" title="torch.optim" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="storage.html" class="btn btn-neutral" title="torch.Storage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
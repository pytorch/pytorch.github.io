


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.padding &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href="http://pytorch.org/docs/versions.html"> 1.0.0 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.padding</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.padding</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">.module</span> <span class="k">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="k">import</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_quadruple</span><span class="p">,</span> <span class="n">_ntuple</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="k">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..._jit_internal</span> <span class="k">import</span> <span class="n">weak_module</span><span class="p">,</span> <span class="n">weak_script_method</span>


<span class="c1"># TODO: grad_output size asserts in THNN</span>


<span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">_ConstantPadNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConstantPadNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@weak_script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;padding=</span><span class="si">{}</span><span class="s1">, value=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>


<div class="viewcode-block" id="ConstantPad1d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConstantPad1d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ConstantPad1d</span><span class="p">(</span><span class="n">_ConstantPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor boundaries with a constant value.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in both boundaries. If a 2-`tuple`, uses</span>
<span class="sd">            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, W_{out})` where</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 2, 4)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],</span>
<span class="sd">                 [-1.3287,  1.8966,  0.1466, -0.2771]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,</span>
<span class="sd">                   3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,</span>
<span class="sd">                   3.5000]]])</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[ 1.6616,  1.4523, -1.1255],</span>
<span class="sd">                 [-3.6372,  0.1182, -1.8652]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5)</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConstantPad1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConstantPad2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConstantPad2d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ConstantPad2d</span><span class="p">(</span><span class="n">_ConstantPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor boundaries with a constant value.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,</span>
<span class="sd">            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, H_{out}, W_{out})` where</span>
<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 2, 2)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[ 1.6585,  0.4320],</span>
<span class="sd">                 [-0.8701, -0.4649]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],</span>
<span class="sd">                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConstantPad2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_quadruple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConstantPad3d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ConstantPad3d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ConstantPad3d</span><span class="p">(</span><span class="n">_ConstantPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor boundaries with a constant value.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 6-`tuple`, uses</span>
<span class="sd">            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`,</span>
<span class="sd">            :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`,</span>
<span class="sd">            :math:`\text{padding\_front}`, :math:`\text{padding\_back}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` where</span>
<span class="sd">          :math:`D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}`</span>
<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConstantPad3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_ntuple</span><span class="p">(</span><span class="mi">6</span><span class="p">)(</span><span class="n">padding</span><span class="p">)</span></div>


<span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">_ReflectionPadNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span>

    <span class="nd">@weak_script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>


<div class="viewcode-block" id="ReflectionPad1d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ReflectionPad1d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ReflectionPad1d</span><span class="p">(</span><span class="n">_ReflectionPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor using the reflection of the input boundary.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 2-`tuple`, uses</span>
<span class="sd">            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, W_{out})` where</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReflectionPad1d(2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[0., 1., 2., 3.],</span>
<span class="sd">                 [4., 5., 6., 7.]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],</span>
<span class="sd">                 [6., 5., 4., 5., 6., 7., 6., 5.]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],</span>
<span class="sd">                 [6., 5., 4., 5., 6., 7., 6., 5.]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1))</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],</span>
<span class="sd">                 [7., 6., 5., 4., 5., 6., 7., 6.]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReflectionPad1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReflectionPad2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ReflectionPad2d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ReflectionPad2d</span><span class="p">(</span><span class="n">_ReflectionPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor using the reflection of the input boundary.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,</span>
<span class="sd">            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, H_{out}, W_{out})` where</span>

<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReflectionPad2d(2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[[0., 1., 2.],</span>
<span class="sd">                  [3., 4., 5.],</span>
<span class="sd">                  [6., 7., 8.]]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[8., 7., 6., 7., 8., 7., 6.],</span>
<span class="sd">                  [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="sd">                  [2., 1., 0., 1., 2., 1., 0.],</span>
<span class="sd">                  [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="sd">                  [8., 7., 6., 7., 8., 7., 6.],</span>
<span class="sd">                  [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="sd">                  [2., 1., 0., 1., 2., 1., 0.]]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0))</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[7., 6., 7., 8., 7.],</span>
<span class="sd">                  [4., 3., 4., 5., 4.],</span>
<span class="sd">                  [1., 0., 1., 2., 1.],</span>
<span class="sd">                  [4., 3., 4., 5., 4.],</span>
<span class="sd">                  [7., 6., 7., 8., 7.]]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReflectionPad2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_quadruple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">_ReplicationPadNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span>

    <span class="nd">@weak_script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;replicate&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>


<div class="viewcode-block" id="ReplicationPad1d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ReplicationPad1d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ReplicationPad1d</span><span class="p">(</span><span class="n">_ReplicationPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor using replication of the input boundary.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 2-`tuple`, uses</span>
<span class="sd">            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, W_{out})` where</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad1d(2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[0., 1., 2., 3.],</span>
<span class="sd">                 [4., 5., 6., 7.]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],</span>
<span class="sd">                 [4., 4., 4., 5., 6., 7., 7., 7.]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1))</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],</span>
<span class="sd">                 [4., 4., 4., 4., 5., 6., 7., 7.]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReplicationPad1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReplicationPad2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ReplicationPad2d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ReplicationPad2d</span><span class="p">(</span><span class="n">_ReplicationPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor using replication of the input boundary.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,</span>
<span class="sd">            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, H_{out}, W_{out})` where</span>
<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad2d(2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[[0., 1., 2.],</span>
<span class="sd">                  [3., 4., 5.],</span>
<span class="sd">                  [6., 7., 8.]]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[0., 0., 0., 1., 2., 2., 2.],</span>
<span class="sd">                  [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="sd">                  [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="sd">                  [3., 3., 3., 4., 5., 5., 5.],</span>
<span class="sd">                  [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="sd">                  [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="sd">                  [6., 6., 6., 7., 8., 8., 8.]]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0))</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[0., 0., 1., 2., 2.],</span>
<span class="sd">                  [0., 0., 1., 2., 2.],</span>
<span class="sd">                  [0., 0., 1., 2., 2.],</span>
<span class="sd">                  [3., 3., 4., 5., 5.],</span>
<span class="sd">                  [6., 6., 7., 8., 8.]]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReplicationPad2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_quadruple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReplicationPad3d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ReplicationPad3d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ReplicationPad3d</span><span class="p">(</span><span class="n">_ReplicationPadNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor using replication of the input boundary.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 6-`tuple`, uses</span>
<span class="sd">            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`,</span>
<span class="sd">            :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`,</span>
<span class="sd">            :math:`\text{padding\_front}`, :math:`\text{padding\_back}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` where</span>
<span class="sd">          :math:`D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}`</span>
<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad3d(3)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReplicationPad3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">_ntuple</span><span class="p">(</span><span class="mi">6</span><span class="p">)(</span><span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="ZeroPad2d"><a class="viewcode-back" href="../../../../nn.html#torch.nn.ZeroPad2d">[docs]</a><span class="nd">@weak_module</span>
<span class="k">class</span> <span class="nc">ZeroPad2d</span><span class="p">(</span><span class="n">ConstantPad2d</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads the input tensor boundaries with zero.</span>

<span class="sd">    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding (int, tuple): the size of the padding. If is `int`, uses the same</span>
<span class="sd">            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,</span>
<span class="sd">            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C, H_{out}, W_{out})` where</span>
<span class="sd">          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`</span>
<span class="sd">          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ZeroPad2d(2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 1, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([[[[-0.1678, -0.4418,  1.9466],</span>
<span class="sd">                  [ 0.9604, -0.4219, -0.5241],</span>
<span class="sd">                  [-0.9162, -0.5436, -0.6446]]]])</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
<span class="sd">        &gt;&gt;&gt; # using different paddings for different sides</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0))</span>
<span class="sd">        &gt;&gt;&gt; m(input)</span>
<span class="sd">        tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="sd">                  [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],</span>
<span class="sd">                  [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],</span>
<span class="sd">                  [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ZeroPad2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.utils.data.dataloader &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='http://pytorch.org/docs/versions.html'>1.1.0 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard (experimental)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../__config__.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.utils.data.dataloader</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.utils.data.dataloader</h1><div class="highlight"><pre>
<span></span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Definition of the DataLoader and it&#39;s iterator _DataLoaderIter classes.</span>

<span class="sd">To support these two classes, in `./_utils` we define many utility methods and</span>
<span class="sd">functions to be run in multiprocessing. E.g., the data loading worker loop is</span>
<span class="sd">in `./_utils/worker.py`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">multiprocessing</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">SequentialSampler</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">BatchSampler</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_utils</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">torch._six</span> <span class="k">import</span> <span class="n">queue</span>


<span class="c1"># This function used to be defined in this file. However, it was moved to</span>
<span class="c1"># _utils/collate.py. Although it is rather hard to access this from user land</span>
<span class="c1"># (one has to explicitly directly `import torch.utils.data.dataloader`), there</span>
<span class="c1"># probably is user code out there using it. This aliasing maintains BC in this</span>
<span class="c1"># aspect.</span>
<span class="n">default_collate</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">collate</span><span class="o">.</span><span class="n">default_collate</span>


<div class="viewcode-block" id="DataLoader"><a class="viewcode-back" href="../../../../data.html#torch.utils.data.DataLoader">[docs]</a><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Data loader. Combines a dataset and a sampler, and provides</span>
<span class="sd">    single- or multi-process iterators over the dataset.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        dataset (Dataset): dataset from which to load the data.</span>
<span class="sd">        batch_size (int, optional): how many samples per batch to load</span>
<span class="sd">            (default: ``1``).</span>
<span class="sd">        shuffle (bool, optional): set to ``True`` to have the data reshuffled</span>
<span class="sd">            at every epoch (default: ``False``).</span>
<span class="sd">        sampler (Sampler, optional): defines the strategy to draw samples from</span>
<span class="sd">            the dataset. If specified, ``shuffle`` must be False.</span>
<span class="sd">        batch_sampler (Sampler, optional): like sampler, but returns a batch of</span>
<span class="sd">            indices at a time. Mutually exclusive with :attr:`batch_size`,</span>
<span class="sd">            :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.</span>
<span class="sd">        num_workers (int, optional): how many subprocesses to use for data</span>
<span class="sd">            loading. 0 means that the data will be loaded in the main process.</span>
<span class="sd">            (default: ``0``)</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a mini-batch.</span>
<span class="sd">        pin_memory (bool, optional): If ``True``, the data loader will copy tensors</span>
<span class="sd">            into CUDA pinned memory before returning them.  If your data elements</span>
<span class="sd">            are a custom type, or your ``collate_fn`` returns a batch that is a custom type</span>
<span class="sd">            see the example below.</span>
<span class="sd">        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,</span>
<span class="sd">            if the dataset size is not divisible by the batch size. If ``False`` and</span>
<span class="sd">            the size of dataset is not divisible by the batch size, then the last batch</span>
<span class="sd">            will be smaller. (default: ``False``)</span>
<span class="sd">        timeout (numeric, optional): if positive, the timeout value for collecting a batch</span>
<span class="sd">            from workers. Should always be non-negative. (default: ``0``)</span>
<span class="sd">        worker_init_fn (callable, optional): If not ``None``, this will be called on each</span>
<span class="sd">            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as</span>
<span class="sd">            input, after seeding and before data loading. (default: ``None``)</span>

<span class="sd">    .. note:: When ``num_workers != 0``, the corresponding worker processes are created each time</span>
<span class="sd">              iterator for the DataLoader is obtained (as in when you call</span>
<span class="sd">              ``enumerate(dataloader,0)``).</span>
<span class="sd">              At this point, the dataset, ``collate_fn`` and ``worker_init_fn`` are passed to each</span>
<span class="sd">              worker, where they are used to access and initialize data based on the indices</span>
<span class="sd">              queued up from the main process. This means that dataset access together with</span>
<span class="sd">              its internal IO, transforms and collation runs in the worker, while any</span>
<span class="sd">              shuffle randomization is done in the main process which guides loading by assigning</span>
<span class="sd">              indices to load. Workers are shut down once the end of the iteration is reached.</span>

<span class="sd">              Since workers rely on Python multiprocessing, worker launch behavior is different</span>
<span class="sd">              on Windows compared to Unix. On Unix fork() is used as the default</span>
<span class="sd">              muliprocessing start method, so child workers typically can access the dataset and</span>
<span class="sd">              Python argument functions directly through the cloned address space. On Windows, another</span>
<span class="sd">              interpreter is launched which runs your main script, followed by the internal</span>
<span class="sd">              worker function that receives the dataset, collate_fn and other arguments</span>
<span class="sd">              through Pickle serialization.</span>

<span class="sd">              This separate serialization means that you should take two steps to ensure you</span>
<span class="sd">              are compatible with Windows while using workers</span>
<span class="sd">              (this also works equally well on Unix):</span>

<span class="sd">              - Wrap most of you main script&#39;s code within ``if __name__ == &#39;__main__&#39;:`` block,</span>
<span class="sd">                to make sure it doesn&#39;t run again (most likely generating error) when each worker</span>
<span class="sd">                process is launched. You can place your dataset and DataLoader instance creation</span>
<span class="sd">                logic here, as it doesn&#39;t need to be re-executed in workers.</span>
<span class="sd">              - Make sure that ``collate_fn``, ``worker_init_fn`` or any custom dataset code</span>
<span class="sd">                is declared as a top level def, outside of that ``__main__`` check. This ensures</span>
<span class="sd">                they are available in workers as well</span>
<span class="sd">                (this is needed since functions are pickled as references only, not bytecode).</span>

<span class="sd">              By default, each worker will have its PyTorch seed set to</span>
<span class="sd">              ``base_seed + worker_id``, where ``base_seed`` is a long generated</span>
<span class="sd">              by main process using its RNG. However, seeds for other libraies</span>
<span class="sd">              may be duplicated upon initializing workers (w.g., NumPy), causing</span>
<span class="sd">              each worker to return identical random numbers. (See</span>
<span class="sd">              :ref:`dataloader-workers-random-seed` section in FAQ.) You may</span>
<span class="sd">              use :func:`torch.initial_seed()` to access the PyTorch seed for</span>
<span class="sd">              each worker in :attr:`worker_init_fn`, and use it to set other</span>
<span class="sd">              seeds before data loading.</span>

<span class="sd">    .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an</span>
<span class="sd">                 unpicklable object, e.g., a lambda function.</span>

<span class="sd">    The default memory pinning logic only recognizes Tensors and maps and iterables</span>
<span class="sd">    containg Tensors.  By default, if the pinning logic sees a batch that is a custom type</span>
<span class="sd">    (which will occur if you have a ``collate_fn`` that returns a custom batch type),</span>
<span class="sd">    or if each element of your batch is a custom type, the pinning logic will not</span>
<span class="sd">    recognize them, and it will return that batch (or those elements)</span>
<span class="sd">    without pinning the memory.  To enable memory pinning for custom batch or data types,</span>
<span class="sd">    define a ``pin_memory`` method on your custom type(s).</span>

<span class="sd">    Example::</span>

<span class="sd">        class SimpleCustomBatch:</span>
<span class="sd">            def __init__(self, data):</span>
<span class="sd">                transposed_data = list(zip(*data))</span>
<span class="sd">                self.inp = torch.stack(transposed_data[0], 0)</span>
<span class="sd">                self.tgt = torch.stack(transposed_data[1], 0)</span>

<span class="sd">            def pin_memory(self):</span>
<span class="sd">                self.inp = self.inp.pin_memory()</span>
<span class="sd">                self.tgt = self.tgt.pin_memory()</span>
<span class="sd">                return self</span>

<span class="sd">        def collate_wrapper(batch):</span>
<span class="sd">            return SimpleCustomBatch(batch)</span>

<span class="sd">        inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)</span>
<span class="sd">        tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)</span>
<span class="sd">        dataset = TensorDataset(inps, tgts)</span>

<span class="sd">        loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,</span>
<span class="sd">                            pin_memory=True)</span>

<span class="sd">        for batch_ndx, sample in enumerate(loader):</span>
<span class="sd">            print(sample.inp.is_pinned())</span>
<span class="sd">            print(sample.tgt.is_pinned())</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">batch_sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">default_collate</span><span class="p">,</span>
                 <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span> <span class="o">=</span> <span class="n">worker_init_fn</span>

        <span class="k">if</span> <span class="n">timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;timeout option should be non-negative&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">shuffle</span> <span class="ow">or</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">drop_last</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;batch_sampler option is mutually exclusive &#39;</span>
                                 <span class="s1">&#39;with batch_size, shuffle, sampler, and &#39;</span>
                                 <span class="s1">&#39;drop_last&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;sampler option is mutually exclusive with &#39;</span>
                             <span class="s1">&#39;shuffle&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_workers option cannot be negative; &#39;</span>
                             <span class="s1">&#39;use num_workers=0 to disable multiprocessing.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
            <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">BatchSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">batch_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="ow">and</span> <span class="n">attr</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="s1">&#39;sampler&#39;</span><span class="p">,</span> <span class="s1">&#39;drop_last&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> attribute should not be set after </span><span class="si">{}</span><span class="s1"> is &#39;</span>
                             <span class="s1">&#39;initialized&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_DataLoaderIter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_DataLoaderIter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Iterates once over the DataLoader&#39;s dataset, as specified by the sampler&quot;&quot;&quot;</span>

    <span class="c1"># NOTE [ Data Loader Multiprocessing Shutdown Logic ]</span>
    <span class="c1">#</span>
    <span class="c1"># Preliminary:</span>
    <span class="c1">#</span>
    <span class="c1"># Our data model looks like this (queues are indicated with curly brackets):</span>
    <span class="c1">#</span>
    <span class="c1">#                main process                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {index_queue}                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#              worker processes                            ||     DATA</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#            {worker_result_queue}                         ||     FLOW</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#      pin_memory_thread of main process                   ||   DIRECTION</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {data_queue}                               ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#                data output                               \/</span>
    <span class="c1">#</span>
    <span class="c1"># P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if</span>
    <span class="c1">#      `pin_memory=False`.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Terminating multiprocessing logic requires very careful design. In</span>
    <span class="c1"># particular, we need to make sure that</span>
    <span class="c1">#</span>
    <span class="c1">#   1. The iterator gracefully exits the workers when its last reference is</span>
    <span class="c1">#      gone or it is depleted.</span>
    <span class="c1">#</span>
    <span class="c1">#      In this case, the workers should be gracefully exited because the</span>
    <span class="c1">#      main process may still need to continue to run, and we want cleaning</span>
    <span class="c1">#      up code in the workers to be executed (e.g., releasing GPU memory).</span>
    <span class="c1">#      Naturally, we implement the shutdown logic in `__del__` of</span>
    <span class="c1">#      DataLoaderIterator.</span>
    <span class="c1">#</span>
    <span class="c1">#      We delay the discussion on the logic in this case until later.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. The iterator exits the workers when the loader process and/or worker</span>
    <span class="c1">#      processes exits normally or with error.</span>
    <span class="c1">#</span>
    <span class="c1">#      We set all workers and `pin_memory_thread` to have `daemon=True`.</span>
    <span class="c1">#</span>
    <span class="c1">#      You may ask, why can&#39;t we make the workers non-daemonic, and</span>
    <span class="c1">#      gracefully exit using the same logic as we have in `__del__` when the</span>
    <span class="c1">#      iterator gets deleted (see 1 above)?</span>
    <span class="c1">#</span>
    <span class="c1">#      First of all, `__del__` is **not** guaranteed to be called when</span>
    <span class="c1">#      interpreter exits. Even if it is called, by the time it executes,</span>
    <span class="c1">#      many Python core library resources may alreay be freed, and even</span>
    <span class="c1">#      simple things like acquiring an internal lock of a queue may hang.</span>
    <span class="c1">#      Therefore, in this case, we actually need to prevent `__del__` from</span>
    <span class="c1">#      being executed, and rely on the automatic termination of daemonic</span>
    <span class="c1">#      children. Thus, we register an `atexit` hook that sets a global flag</span>
    <span class="c1">#      `_utils.python_exit_status`. Since `atexit` hooks are executed in the</span>
    <span class="c1">#      reverse order of registration, we are guaranteed that this flag is</span>
    <span class="c1">#      set before library resources we use are freed. (Hooks freeing those</span>
    <span class="c1">#      resources are registered at importing the Python core libraries at</span>
    <span class="c1">#      the top of this file.) So in `__del__`, we check if</span>
    <span class="c1">#      `_utils.python_exit_status` is set or `None` (freed), and perform</span>
    <span class="c1">#      no-op if so.</span>
    <span class="c1">#</span>
    <span class="c1">#      Another problem with `__del__` is also related to the library cleanup</span>
    <span class="c1">#      calls. When a process ends, it shuts the all its daemonic children</span>
    <span class="c1">#      down with a SIGTERM (instead of joining them without a timeout).</span>
    <span class="c1">#      Simiarly for threads, but by a different mechanism. This fact,</span>
    <span class="c1">#      together with a few implementation details of multiprocessing, forces</span>
    <span class="c1">#      us to make workers daemonic. All of our problems arise when a</span>
    <span class="c1">#      DataLoader is used in a subprocess, and are caused by multiprocessing</span>
    <span class="c1">#      code which looks more or less like this:</span>
    <span class="c1">#</span>
    <span class="c1">#          try:</span>
    <span class="c1">#              your_function_using_a_dataloader()</span>
    <span class="c1">#          finally:</span>
    <span class="c1">#              multiprocessing.util._exit_function()</span>
    <span class="c1">#</span>
    <span class="c1">#      The joining/termination mentioned above happens inside</span>
    <span class="c1">#      `_exit_function()`. Now, if `your_function_using_a_dataloader()`</span>
    <span class="c1">#      throws, the stack trace stored in the exception will prevent the</span>
    <span class="c1">#      frame which uses `DataLoaderIter` to be freed. If the frame has any</span>
    <span class="c1">#      reference to the `DataLoaderIter` (e.g., in a method of the iter),</span>
    <span class="c1">#      its  `__del__`, which starts the shutdown procedure, will not be</span>
    <span class="c1">#      called. That, in turn, means that workers aren&#39;t notified. Attempting</span>
    <span class="c1">#      to join in `_exit_function` will then result in a hang.</span>
    <span class="c1">#</span>
    <span class="c1">#      For context, `_exit_function` is also registered as an `atexit` call.</span>
    <span class="c1">#      So it is unclear to me (@ssnl) why this is needed in a finally block.</span>
    <span class="c1">#      The code dates back to 2008 and there is no comment on the original</span>
    <span class="c1">#      PEP 371 or patch https://bugs.python.org/issue3050 (containing both</span>
    <span class="c1">#      the finally block and the `atexit` registration) that explains this.</span>
    <span class="c1">#</span>
    <span class="c1">#      Another choice is to just shutdown workers with logic in 1 above</span>
    <span class="c1">#      whenever we see an error in `next`. This isn&#39;t ideal because</span>
    <span class="c1">#        a. It prevents users from using try-catch to resume data loading.</span>
    <span class="c1">#        b. It doesn&#39;t prevent hanging if users have references to the</span>
    <span class="c1">#           iterator.</span>
    <span class="c1">#</span>
    <span class="c1">#   3. All processes exit if any of them die unexpectedly by fatal signals.</span>
    <span class="c1">#</span>
    <span class="c1">#      As shown above, the workers are set as daemonic children of the main</span>
    <span class="c1">#      process. However, automatic cleaning-up of such child processes only</span>
    <span class="c1">#      happens if the parent process exits gracefully (e.g., not via fatal</span>
    <span class="c1">#      signals like SIGKILL). So we must ensure that each process will exit</span>
    <span class="c1">#      even the process that should send/receive data to/from it were</span>
    <span class="c1">#      killed, i.e.,</span>
    <span class="c1">#</span>
    <span class="c1">#        a. A process won&#39;t hang when getting from a queue.</span>
    <span class="c1">#</span>
    <span class="c1">#           Even with carefully designed data dependencies (i.e., a `put()`</span>
    <span class="c1">#           always corresponding to a `get()`), hanging on `get()` can still</span>
    <span class="c1">#           happen when data in queue is corrupted (e.g., due to</span>
    <span class="c1">#           `cancel_join_thread` or unexpected exit).</span>
    <span class="c1">#</span>
    <span class="c1">#           For child exit, we set a timeout whenever we try to get data</span>
    <span class="c1">#           from `data_queue`, and check the workers&#39; status on each timeout</span>
    <span class="c1">#           and error.</span>
    <span class="c1">#           See `_DataLoaderiter._get_batch()` and</span>
    <span class="c1">#           `_DataLoaderiter._try_get_batch()` for details.</span>
    <span class="c1">#</span>
    <span class="c1">#           Additionally, for child exit on non-Windows platforms, we also</span>
    <span class="c1">#           register a SIGCHLD handler (which is supported on Windows) on</span>
    <span class="c1">#           the main process, which checks if any of the workers fail in the</span>
    <span class="c1">#           (Python) handler. This is more efficient and faster in detecting</span>
    <span class="c1">#           worker failures, compared to only using the above mechanism.</span>
    <span class="c1">#           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.</span>
    <span class="c1">#</span>
    <span class="c1">#           For `.get()` calls where the sender(s) is not the workers, we</span>
    <span class="c1">#           guard them with timeouts, and check the status of the sender</span>
    <span class="c1">#           when timeout happens:</span>
    <span class="c1">#             + in the workers, the `_utils.worker.ManagerWatchdog` class</span>
    <span class="c1">#               checks the status of the main process.</span>
    <span class="c1">#             + if `pin_memory=True`, when getting from `pin_memory_thread`,</span>
    <span class="c1">#               check `pin_memory_thread` status periodically until `.get()`</span>
    <span class="c1">#               returns or see that `pin_memory_thread` died.</span>
    <span class="c1">#</span>
    <span class="c1">#        b. A process won&#39;t hang when putting into a queue;</span>
    <span class="c1">#</span>
    <span class="c1">#           We use `mp.Queue` which has a separate background thread to put</span>
    <span class="c1">#           objects from an unbounded buffer array. The background thread is</span>
    <span class="c1">#           daemonic and usually automatically joined when the process</span>
    <span class="c1">#           exits.</span>
    <span class="c1">#</span>
    <span class="c1">#           However, in case that the receiver has ended abruptly while</span>
    <span class="c1">#           reading from the pipe, the join will hang forever. Therefore,</span>
    <span class="c1">#           for both `worker_result_queue` (worker -&gt; main process/pin_memory_thread)</span>
    <span class="c1">#           and each `index_queue` (main process -&gt; worker), we use</span>
    <span class="c1">#           `q.cancel_join_thread()` in sender process before any `q.put` to</span>
    <span class="c1">#           prevent this automatic join.</span>
    <span class="c1">#</span>
    <span class="c1">#           Moreover, having all queues called `cancel_join_thread` makes</span>
    <span class="c1">#           implementing graceful shutdown logic in `__del__` much easier.</span>
    <span class="c1">#           It won&#39;t need to get from any queue, which would also need to be</span>
    <span class="c1">#           guarded by periodic status checks.</span>
    <span class="c1">#</span>
    <span class="c1">#           Note that this may leave corrupted data in the queue, but we</span>
    <span class="c1">#           don&#39;t care about the data anyways once we are shutting down.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Now let&#39;s get back to 1:</span>
    <span class="c1">#   how we gracefully exit the workers when the last reference to the</span>
    <span class="c1">#   iterator is gone.</span>
    <span class="c1">#</span>
    <span class="c1"># To achieve this, we implement the following logic along with the design</span>
    <span class="c1"># choices mentioned above:</span>
    <span class="c1">#</span>
    <span class="c1"># [worker processes]</span>
    <span class="c1">#   While loader process is alive:</span>
    <span class="c1">#     Get from index_queue.</span>
    <span class="c1">#       If got a `None`, exit.</span>
    <span class="c1">#       If get anything else,</span>
    <span class="c1">#          Check `done_event`.</span>
    <span class="c1">#            If set, continue to next iteration</span>
    <span class="c1">#                    i.e., keep getting until see the `None`, then exit.</span>
    <span class="c1">#            Otherwise, process data.</span>
    <span class="c1">#       If timed out,</span>
    <span class="c1">#          No matter `done_event` is set (still need to see `None`) or not,</span>
    <span class="c1">#          must continue to next iteration .</span>
    <span class="c1">#</span>
    <span class="c1"># [pin_memory_thread]</span>
    <span class="c1">#   # No need to check main thread. If this thread is alive, the main loader</span>
    <span class="c1">#   # thread must be alive, because this thread is set as daemonic.</span>
    <span class="c1">#   While True:</span>
    <span class="c1">#     Get from index_queue.</span>
    <span class="c1">#       If got a `None`, exit.</span>
    <span class="c1">#       If get anything else,</span>
    <span class="c1">#          Check `done_event`.</span>
    <span class="c1">#            If set, continue to next iteration</span>
    <span class="c1">#                    i.e., keep getting until see the `None`, then exit.</span>
    <span class="c1">#            Otherwise, process data.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: we don&#39;t check the status of the main thread because</span>
    <span class="c1">#           1. if the process is killed by fatal signal, `pin_memory_thread`</span>
    <span class="c1">#              ends.</span>
    <span class="c1">#           2. in other cases, either the cleaning-up in __del__ or the</span>
    <span class="c1">#              automatic exit of daemonic thread will take care of it.</span>
    <span class="c1">#              This won&#39;t busy-wait either because `.get(timeout)` does not</span>
    <span class="c1">#              busy-wait.</span>
    <span class="c1">#</span>
    <span class="c1"># [main process]</span>
    <span class="c1">#   In the DataLoader Iter&#39;s `__del__`</span>
    <span class="c1">#     a. Set `done_event` (shared with `pin_memory_thread` and workers).</span>
    <span class="c1">#</span>
    <span class="c1">#        Note: from here on, the workers &amp; `pin_memory_thread` may exit at</span>
    <span class="c1">#              any time after they receive `None`.</span>
    <span class="c1">#</span>
    <span class="c1">#     b. Exit `pin_memory_thread`</span>
    <span class="c1">#          i.   Put `None` in `worker_result_queue`.</span>
    <span class="c1">#          ii.  Join the `pin_memory_thread`.</span>
    <span class="c1">#</span>
    <span class="c1">#     c. Exit the workers.</span>
    <span class="c1">#          i.   Put `None` in each worker&#39;s `index_queue`.</span>
    <span class="c1">#          ii.  Join the workers.</span>
    <span class="c1">#</span>
    <span class="c1">#        NOTE: This has to be after (b) because it may leave corrupted data</span>
    <span class="c1">#              in `worker_result_queue`, which `pin_memory_thread` reads</span>
    <span class="c1">#              from.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)</span>
    <span class="c1">#         can be omitted</span>
    <span class="c1">#</span>
    <span class="c1"># NB: `done_event`s isn&#39;t strictly needed. E.g., we can just check for</span>
    <span class="c1">#     `None` from `index_queue`, but it allows us to skip wasting resources</span>
    <span class="c1">#     processing indices already in `index_queue` if we are already shutting</span>
    <span class="c1">#     down.</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">batch_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">timeout</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span>

        <span class="n">base_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">worker_init_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">):</span>
                <span class="n">index_queue</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
                <span class="n">index_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_worker_loop</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">index_queue</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">base_seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
                <span class="n">w</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># NB: Process.start() actually take some time as it needs to</span>
                <span class="c1">#     start a process and pass the arguments over via a pipe.</span>
                <span class="c1">#     Therefore, we only add a worker to self.workers list after</span>
                <span class="c1">#     it started, so that we do not call .join() if program dies</span>
                <span class="c1">#     before it starts, and __del__ tries to join but will get:</span>
                <span class="c1">#     AssertionError: can only join a started process.</span>
                <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index_queue</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
                <span class="n">pin_memory_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">_pin_memory_loop</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="p">,</span>
                          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="p">))</span>
                <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="c1"># Similar to workers (see comment above), we only register</span>
                <span class="c1"># pin_memory_thread once it is started.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span> <span class="o">=</span> <span class="n">pin_memory_thread</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span>

            <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_set_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">pid</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">))</span>
            <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_set_SIGCHLD_handler</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># prime the prefetch loop</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_put_indices</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_try_get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">):</span>
        <span class="c1"># Tries to fetch data from `data_queue` for a given timeout. This can</span>
        <span class="c1"># also be used as inner loop of fetching without timeout, with the</span>
        <span class="c1"># sender status as the loop condition.</span>
        <span class="c1">#</span>
        <span class="c1"># This raises a `RuntimeError` if any worker died expectedly. This error</span>
        <span class="c1"># can come from either the SIGCHLD handler in `_utils/signal_handling.py`</span>
        <span class="c1"># (only for non-Windows platforms), or the manual check below on errors</span>
        <span class="c1"># and timeouts.</span>
        <span class="c1">#</span>
        <span class="c1"># Returns a 2-tuple:</span>
        <span class="c1">#   (bool: whether successfully get data, any: data if successful else None)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># At timeout and error, we manually check whether any worker has</span>
            <span class="c1"># failed. Note that this is the only mechanism for Windows to detect</span>
            <span class="c1"># worker failures.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">is_alive</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">):</span>
                <span class="n">pids_str</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">pid</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">w</span><span class="o">.</span><span class="n">is_alive</span><span class="p">())</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;DataLoader worker (pid(s) </span><span class="si">{}</span><span class="s1">) exited unexpectedly&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pids_str</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">_get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Fetches data from `self.data_queue`.</span>
        <span class="c1">#</span>
        <span class="c1"># We check workers&#39; status every `MP_STATUS_CHECK_INTERVAL` seconds,</span>
        <span class="c1"># which we achieve by running `self._try_get_batch(timeout=MP_STATUS_CHECK_INTERVAL)`</span>
        <span class="c1"># in a loop. This is the only mechanism to detect worker failures for</span>
        <span class="c1"># Windows. For other platforms, a SIGCHLD handler is also used for</span>
        <span class="c1"># worker failure detection.</span>
        <span class="c1">#</span>
        <span class="c1"># If `pin_memory=True`, we also need check if `pin_memory_thread` had</span>
        <span class="c1"># died at timeouts.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_batch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;DataLoader timed out after </span><span class="si">{}</span><span class="s1"> seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_batch</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># while condition is false, i.e., pin_memory_thread died.</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Pin memory thread exited unexpectedly&#39;</span><span class="p">)</span>
            <span class="c1"># In this case, `self.data_queue` is a `queue.Queue`,. But we don&#39;t</span>
            <span class="c1"># need to call `.task_done()` because we don&#39;t use `.join()`.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">success</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_try_get_batch</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># same-process loading</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">pin_memory_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">batch</span>

        <span class="c1"># check if the next sample has already been generated</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_next_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>

        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span><span class="p">:</span>
                <span class="c1"># store out-of-order samples</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span>
                <span class="k">continue</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_next_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="nb">next</span> <span class="o">=</span> <span class="fm">__next__</span>  <span class="c1"># Python 2 compatibility</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_put_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span><span class="p">]</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span><span class="p">,</span> <span class="n">indices</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_process_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_put_indices</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">_utils</span><span class="o">.</span><span class="n">ExceptionWrapper</span><span class="p">):</span>
            <span class="c1"># make multiline KeyError msg readable by working around</span>
            <span class="c1"># a python bug https://bugs.python.org/issue2651</span>
            <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">exc_type</span> <span class="o">==</span> <span class="ne">KeyError</span> <span class="ow">and</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">exc_msg</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;KeyError:&quot;</span> <span class="o">+</span> <span class="n">batch</span><span class="o">.</span><span class="n">exc_msg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">batch</span><span class="o">.</span><span class="n">exc_type</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">exc_msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: add limited pickling support for sharing an iterator</span>
        <span class="c1"># across multiple threads for HOGWILD.</span>
        <span class="c1"># Probably the best way to do this is by moving the sample pushing</span>
        <span class="c1"># to a separate thread and then just sharing the data queue</span>
        <span class="c1"># but signalling the end is tricky without a non-blocking API</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;_DataLoaderIter cannot be pickled&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shutdown_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on</span>
        <span class="c1"># the logic of this function.</span>
        <span class="n">python_exit_status</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">python_exit_status</span>
        <span class="k">if</span> <span class="n">python_exit_status</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="n">python_exit_status</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># See (2) of the note. If Python is shutting down, do no-op.</span>
            <span class="k">return</span>
        <span class="c1"># Normal exit when last reference is gone / iterator is depleted.</span>
        <span class="c1"># See (1) and the second half of the note.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

                <span class="c1"># Exit `pin_memory_thread` first because exiting workers may leave</span>
                <span class="c1"># corrupted data in `worker_result_queue` which `pin_memory_thread`</span>
                <span class="c1"># reads from.</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;pin_memory_thread&#39;</span><span class="p">):</span>
                    <span class="c1"># Use hasattr in case error happens before we set the attribute.</span>
                    <span class="c1"># First time do `worker_result_queue.put` in this process.</span>

                    <span class="c1"># `cancel_join_thread` in case that `pin_memory_thread` exited.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
                    <span class="c1"># Indicate that no more data will be put on this queue by the</span>
                    <span class="c1"># current process. This **must** be called after</span>
                    <span class="c1"># `pin_memory_thread` is joined because that thread shares the</span>
                    <span class="c1"># same pipe handles with this loader thread. If the handle is</span>
                    <span class="c1"># closed, Py3 will error in this case, but Py2 will just time</span>
                    <span class="c1"># out even if there is data in the queue.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

                <span class="c1"># Exit workers now.</span>
                <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="p">:</span>
                    <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                    <span class="c1"># Indicate that no more data will be put on this queue by the</span>
                    <span class="c1"># current process.</span>
                    <span class="n">q</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">:</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="c1"># Even though all this function does is putting into queues that</span>
                <span class="c1"># we have called `cancel_join_thread` on, weird things can</span>
                <span class="c1"># happen when a worker is killed by a signal, e.g., hanging in</span>
                <span class="c1"># `Event.set()`. So we need to guard this with SIGCHLD handler,</span>
                <span class="c1"># and remove pids from the C side data structure only at the</span>
                <span class="c1"># end.</span>
                <span class="c1">#</span>
                <span class="c1"># FIXME: Unfortunately, for Windows, we are missing a worker</span>
                <span class="c1">#        error detection mechanism here in this function, as it</span>
                <span class="c1">#        doesn&#39;t provide a SIGCHLD handler.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span><span class="p">:</span>
                    <span class="n">_utils</span><span class="o">.</span><span class="n">signal_handling</span><span class="o">.</span><span class="n">_remove_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../../_static/doctools.js"></script>
         <script type="text/javascript" src="../../../../_static/language_data.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
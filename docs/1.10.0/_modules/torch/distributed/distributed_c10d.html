


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.distributed_c10d &mdash; PyTorch 1.10.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />



  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->



  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.10.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.distributed_c10d</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.distributed_c10d</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AllreduceCoalescedOptions</span><span class="p">,</span>
    <span class="n">AllreduceOptions</span><span class="p">,</span>
    <span class="n">AllToAllOptions</span><span class="p">,</span>
    <span class="n">BarrierOptions</span><span class="p">,</span>
    <span class="n">BroadcastOptions</span><span class="p">,</span>
    <span class="n">GatherOptions</span><span class="p">,</span>
    <span class="n">PrefixStore</span><span class="p">,</span>
    <span class="n">ProcessGroup</span><span class="p">,</span>
    <span class="n">ReduceOp</span><span class="p">,</span>
    <span class="n">ReduceOptions</span><span class="p">,</span>
    <span class="n">ReduceScatterOptions</span><span class="p">,</span>
    <span class="n">ScatterOptions</span><span class="p">,</span>
    <span class="n">Store</span><span class="p">,</span>
    <span class="n">_DistributedDebugLevel</span><span class="p">,</span>
    <span class="n">_get_debug_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch._six</span> <span class="kn">import</span> <span class="n">string_classes</span>

<span class="kn">from</span> <span class="nn">.constants</span> <span class="kn">import</span> <span class="n">default_pg_timeout</span>
<span class="kn">from</span> <span class="nn">.rendezvous</span> <span class="kn">import</span> <span class="n">register_rendezvous_handler</span><span class="p">,</span> <span class="n">rendezvous</span>  <span class="c1"># noqa: F401</span>


<span class="c1"># This module is wildcard imported from torch.distributed.</span>
<span class="c1"># TODO: specify __all__</span>


<span class="n">_MPI_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_NCCL_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_GLOO_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">_pickler</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">Pickler</span>
<span class="n">_unpickler</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">Unpickler</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupMPI</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_MPI_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupNCCL</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_NCCL_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupGloo</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">_ProcessGroupWrapper</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_GLOO_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">PG_WRAPPER_STORE_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;pg_wrapper&quot;</span>


<span class="c1"># Some reduce ops are not supported by complex numbers and will result in an error.</span>
<span class="c1"># We currently provide complex support to the distributed API by viewing</span>
<span class="c1"># complex tensors as real (torch.view_as_real), meaning that calling</span>
<span class="c1"># these unsupported ops will return garbage values rather than error out.</span>
<span class="c1"># (e.g. max(2+3i, 3+2i) = 3+3i)</span>
<span class="c1"># We&#39;d like calls to unsupported ops to error out accordingly,</span>
<span class="c1"># rather than returning garbage values.</span>
<span class="k">def</span> <span class="nf">supports_complex</span><span class="p">(</span><span class="n">reduceOp</span><span class="p">:</span> <span class="n">ReduceOp</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">denyList</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">MIN</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCT</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BAND</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BOR</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BXOR</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">reduceOp</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">denyList</span>


<div class="viewcode-block" id="Backend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.Backend">[docs]</a><span class="k">class</span> <span class="nc">Backend</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An enum-like class of available backends: GLOO, NCCL, MPI, and other registered</span>
<span class="sd">    backends.</span>

<span class="sd">    The values of this class are lowercase strings, e.g., ``&quot;gloo&quot;``. They can</span>
<span class="sd">    be accessed as attributes, e.g., ``Backend.NCCL``.</span>

<span class="sd">    This class can be directly called to parse the string, e.g.,</span>
<span class="sd">    ``Backend(backend_str)`` will check if ``backend_str`` is valid, and</span>
<span class="sd">    return the parsed lowercase string if so. It also accepts uppercase strings,</span>
<span class="sd">    e.g., ``Backend(&quot;GLOO&quot;)`` returns ``&quot;gloo&quot;``.</span>

<span class="sd">    .. note:: The entry ``Backend.UNDEFINED`` is present but only used as</span>
<span class="sd">              initial value of some fields. Users should neither use it directly</span>
<span class="sd">              nor assume its existence.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">UNDEFINED</span> <span class="o">=</span> <span class="s2">&quot;undefined&quot;</span>
    <span class="n">GLOO</span> <span class="o">=</span> <span class="s2">&quot;gloo&quot;</span>
    <span class="n">NCCL</span> <span class="o">=</span> <span class="s2">&quot;nccl&quot;</span>
    <span class="n">MPI</span> <span class="o">=</span> <span class="s2">&quot;mpi&quot;</span>
    <span class="n">TCP</span> <span class="o">=</span> <span class="s2">&quot;tcp&quot;</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">string_classes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Backend name must be a string, but got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">TCP</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;TCP backend has been deprecated. Please use &quot;</span>
                <span class="s2">&quot;Gloo or MPI backend for collective operations &quot;</span>
                <span class="s2">&quot;on CPU tensors.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">value</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid backend: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span> <span class="ow">and</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span> <span class="ow">and</span> <span class="n">value</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">name</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_backend</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a new backend.</span>

<span class="sd">        This class method is used by 3rd party cpp extension to register new backend.</span>

<span class="sd">        Args:</span>
<span class="sd">            name (str): Backend name matching with the one in `init_process_group()`.</span>
<span class="sd">            func (function): Function handler that instantiates the backend.</span>
<span class="sd">                             The function should be implemented in the backend cpp extension</span>
<span class="sd">                             and takes four arguments, including prefix_store, rank,</span>
<span class="sd">                             world_size, and timeout.</span>

<span class="sd">        .. note:: This support of 3rd party backend is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">func</span><span class="p">)</span></div>


<span class="c1"># `_backend`, `dist_backend`, and `reduce_op` are here to maintain backward</span>
<span class="c1"># compatibility with pre-c10d distributed package.</span>
<span class="c1"># TODO: remove them when users are ready to take a hard dependency on PyTorch 1.</span>
<span class="n">_backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
<span class="n">dist_backend</span> <span class="o">=</span> <span class="n">Backend</span>


<span class="k">class</span> <span class="nc">_reduce_op</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,</span>
<span class="sd">    ``MIN``, and ``MAX``.</span>

<span class="sd">    :class:`~torch.distributed.ReduceOp` is recommended to use instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># __members__ is a dict storing key-value pairs for enum classes</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">__members__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__members__</span> <span class="o">=</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">__members__</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;torch.distributed.reduce_op is deprecated, please use &quot;</span>
            <span class="s2">&quot;torch.distributed.ReduceOp instead&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>


<span class="n">reduce_op</span> <span class="o">=</span> <span class="n">_reduce_op</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">group</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="c1"># Points to the default PG once initialized.</span>
    <span class="n">WORLD</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">GroupMember</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="c1"># Alias to group.WORLD for backward compatibility</span>
    <span class="n">WORLD</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">NON_GROUP_MEMBER</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="c1"># Cached process groups</span>
<span class="c1"># For NCCL and GLOO pg, it is a map from ProcessGroup to (Backend, Store)</span>
<span class="c1"># For MPI pg, it is a map from ProcessGroup to (Backend, None)</span>
<span class="n">_pg_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Store</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># Process group&#39;s names, map from ProcessGroup to str</span>
<span class="n">_pg_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># Process group&#39;s global rank to local rank mapping</span>
<span class="n">_pg_group_ranks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Default process group state</span>
<span class="n">_default_pg_init_method</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Process group count for default naming</span>
<span class="n">_group_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">STORE_BASED_BARRIER_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;store_based_barrier_key&quot;</span>


<span class="k">def</span> <span class="nf">_store_based_barrier</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Barrier based on store which is used for synchronizing processes after</span>
<span class="sd">    ``init_process_group`` or ``new_group``. Intended to be used only with</span>
<span class="sd">    those two methods and is not a generic alternative to ``barrier()``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">store_key</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">STORE_BASED_BARRIER_PREFIX</span><span class="p">,</span> <span class="n">_group_count</span><span class="p">)</span>
    <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Added key: </span><span class="si">{}</span><span class="s2"> to store for rank: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>

    <span class="c1"># Now wait for all workers to check in with the store.</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="c1"># Use &#39;add&#39; instead of &#39;get&#39; since for some store implementations &#39;add&#39;</span>
    <span class="c1"># doesn&#39;t work well with &#39;get&#39;. Ideally the store implementations should</span>
    <span class="c1"># be fixed, but for backward compatiblity reasons it is risky to change</span>
    <span class="c1"># the store implementations. Once, we completely migrate away from these</span>
    <span class="c1"># legacy stores, we can use &#39;get&#39; here instead.</span>
    <span class="n">worker_count</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">log_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">while</span> <span class="n">worker_count</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">worker_count</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Print status periodically to keep track.</span>
        <span class="k">if</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">log_time</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Waiting in store based barrier to initialize process group for &quot;</span>
                <span class="s2">&quot;rank: </span><span class="si">{}</span><span class="s2">, key: </span><span class="si">{}</span><span class="s2"> (world_size=</span><span class="si">{}</span><span class="s2">, worker_count=</span><span class="si">{}</span><span class="s2">, timeout=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">rank</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">worker_count</span><span class="p">,</span> <span class="n">timeout</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">log_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">timeout</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Timed out initializing process group in store based barrier on &quot;</span>
                <span class="s2">&quot;rank: </span><span class="si">{}</span><span class="s2">, for key: </span><span class="si">{}</span><span class="s2"> (world_size=</span><span class="si">{}</span><span class="s2">, worker_count=</span><span class="si">{}</span><span class="s2">, timeout=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">rank</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">worker_count</span><span class="p">,</span> <span class="n">timeout</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: Completed store-based barrier for key:</span><span class="si">{</span><span class="n">store_key</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> nodes.&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that checks if the current process&#39;s rank is not in a given group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span>


<span class="k">def</span> <span class="nf">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that gets a given group&#39;s local rank in the group from a given global</span>
<span class="sd">    rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;group.WORLD does not have local rank to global &quot;</span> <span class="s2">&quot;rank mapping&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_pg_group_ranks</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The given group does not exist&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">group_rank</span> <span class="o">=</span> <span class="n">_pg_group_ranks</span><span class="p">[</span><span class="n">group</span><span class="p">][</span><span class="n">rank</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The global rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> is not part of the group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
    <span class="k">return</span> <span class="n">group_rank</span>


<span class="k">def</span> <span class="nf">_get_global_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that gets a given group&#39;s global rank from a given local rank in the</span>
<span class="sd">    group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;group.WORLD does not have local rank to global &quot;</span> <span class="s2">&quot;rank mapping&quot;</span>
        <span class="p">)</span>
    <span class="n">group_rank_map</span> <span class="o">=</span> <span class="n">_pg_group_ranks</span><span class="p">[</span><span class="n">group</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">grp_rank</span> <span class="ow">in</span> <span class="n">group_rank_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">grp_rank</span> <span class="o">==</span> <span class="n">group_rank</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">rank</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The group rank is not part of the group&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_group_size</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that gets a given group&#39;s world size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_check_single_tensor</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the parameter ``param_name`` is a single tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument. Expected parameter `</span><span class="si">{}</span><span class="s2">` &quot;</span>
            <span class="s2">&quot;to be of type torch.Tensor.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_tensor_list</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the parameter ``param_name`` is a list of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument. Expected parameter `</span><span class="si">{}</span><span class="s2">` &quot;</span>
            <span class="s2">&quot;to be of type List[torch.Tensor].&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_op</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the ``op`` is either isend or irecv.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">isend</span><span class="p">,</span> <span class="n">irecv</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid ``op``. Expected ``op`` &quot;</span>
            <span class="s2">&quot;to be of type ``torch.distributed.isend`` or &quot;</span>
            <span class="s2">&quot;``torch.distributed.irecv``.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_p2p_op_list</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the ``p2p_op_list`` is a list of P2POp instances and</span>
<span class="sd">    all ops use the same backend.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">p2p_op</span><span class="p">,</span> <span class="n">P2POp</span><span class="p">)</span> <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid ``p2p_op_list``. Each op is expected to &quot;</span>
            <span class="s2">&quot;to be of type ``torch.distributed.P2POp``.&quot;</span>
        <span class="p">)</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">backend</span> <span class="o">==</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">p2p_op</span><span class="o">.</span><span class="n">group</span><span class="p">)</span> <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All groups need to use the same backend.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="is_mpi_available"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_mpi_available">[docs]</a><span class="k">def</span> <span class="nf">is_mpi_available</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the MPI backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_MPI_AVAILABLE</span></div>


<div class="viewcode-block" id="is_nccl_available"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_nccl_available">[docs]</a><span class="k">def</span> <span class="nf">is_nccl_available</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the NCCL backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_NCCL_AVAILABLE</span></div>


<span class="k">def</span> <span class="nf">is_gloo_available</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the Gloo backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_GLOO_AVAILABLE</span>


<div class="viewcode-block" id="is_initialized"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_initialized">[docs]</a><span class="k">def</span> <span class="nf">is_initialized</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checking if the default process group has been initialized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="is_torchelastic_launched"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_torchelastic_launched">[docs]</a><span class="k">def</span> <span class="nf">is_torchelastic_launched</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether this process was launched with ``torch.distributed.elastic``</span>
<span class="sd">    (aka torchelastic). The existence of ``TORCHELASTIC_RUN_ID`` environment</span>
<span class="sd">    variable is used as a proxy to determine whether the current process</span>
<span class="sd">    was launched with torchelastic. This is a reasonable proxy since</span>
<span class="sd">    ``TORCHELASTIC_RUN_ID`` maps to the rendezvous id which is always a</span>
<span class="sd">    non-null value indicating the job id for peer discovery purposes..</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TORCHELASTIC_RUN_ID&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>


<span class="k">def</span> <span class="nf">_get_default_group</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Getting the default process group created by init_process_group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Default process group has not been initialized, &quot;</span>
            <span class="s2">&quot;please make sure to call init_process_group.&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span>


<span class="k">def</span> <span class="nf">_get_default_store</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Getting the default store created by init_process_group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Default process group has not been initialized, &quot;</span>
            <span class="s2">&quot;please make sure to call init_process_group.&quot;</span>
        <span class="p">)</span>
    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">default_store</span> <span class="o">=</span> <span class="n">_pg_map</span><span class="p">[</span><span class="n">default_pg</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">default_store</span>


<span class="k">def</span> <span class="nf">_update_default_pg</span><span class="p">(</span><span class="n">pg</span><span class="p">):</span>
    <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span> <span class="o">=</span> <span class="n">pg</span>


<div class="viewcode-block" id="get_backend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_backend">[docs]</a><span class="k">def</span> <span class="nf">get_backend</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the backend of the given process group.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. The</span>
<span class="sd">            default is the general main process group. If another specific group</span>
<span class="sd">            is specified, the calling process must be part of :attr:`group`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The backend of the given process group as a lower case string.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">pg</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid process group specified&quot;</span><span class="p">)</span>
    <span class="n">pg_store</span> <span class="o">=</span> <span class="n">_pg_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pg_store</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">pg_store</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="init_process_group"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.init_process_group">[docs]</a><span class="k">def</span> <span class="nf">init_process_group</span><span class="p">(</span>
    <span class="n">backend</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">store</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the default distributed process group, and this will also</span>
<span class="sd">    initialize the distributed package.</span>

<span class="sd">    There are 2 main ways to initialize a process group:</span>
<span class="sd">        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.</span>
<span class="sd">        2. Specify ``init_method`` (a URL string) which indicates where/how</span>
<span class="sd">           to discover peers. Optionally specify ``rank`` and ``world_size``,</span>
<span class="sd">           or encode all required parameters in the URL and omit them.</span>

<span class="sd">    If neither is specified, ``init_method`` is assumed to be &quot;env://&quot;.</span>


<span class="sd">    Args:</span>
<span class="sd">        backend (str or Backend): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values include ``mpi``, ``gloo``,</span>
<span class="sd">            and ``nccl``. This field should be given as a lowercase string</span>
<span class="sd">            (e.g., ``&quot;gloo&quot;``), which can also be accessed via</span>
<span class="sd">            :class:`Backend` attributes (e.g., ``Backend.GLOO``). If using</span>
<span class="sd">            multiple processes per machine with ``nccl`` backend, each process</span>
<span class="sd">            must have exclusive access to every GPU it uses, as sharing GPUs</span>
<span class="sd">            between processes can result in deadlocks.</span>
<span class="sd">        init_method (str, optional): URL specifying how to initialize the</span>
<span class="sd">                                     process group. Default is &quot;env://&quot; if no</span>
<span class="sd">                                     ``init_method`` or ``store`` is specified.</span>
<span class="sd">                                     Mutually exclusive with ``store``.</span>
<span class="sd">        world_size (int, optional): Number of processes participating in</span>
<span class="sd">                                    the job. Required if ``store`` is specified.</span>
<span class="sd">        rank (int, optional): Rank of the current process (it should be a</span>
<span class="sd">                              number between 0 and ``world_size``-1).</span>
<span class="sd">                              Required if ``store`` is specified.</span>
<span class="sd">        store(Store, optional): Key/value store accessible to all workers, used</span>
<span class="sd">                                to exchange connection/address information.</span>
<span class="sd">                                Mutually exclusive with ``init_method``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">        group_name (str, optional, deprecated): Group name.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. As of now, the only</span>
<span class="sd">            options we support is ``ProcessGroupNCCL.Options`` for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            the nccl backend can pick up high priority cuda streams when</span>
<span class="sd">            there&#39;re compute kernels waiting.</span>

<span class="sd">    .. note:: To enable ``backend == Backend.MPI``, PyTorch needs to be built from source</span>
<span class="sd">        on a system that supports MPI.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_pg_group_ranks</span>
    <span class="k">global</span> <span class="n">_backend</span>
    <span class="k">global</span> <span class="n">_default_pg_init_method</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected timeout argument to be of type&quot;</span> <span class="s2">&quot;datetime.timedelta&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize the default process group &quot;</span> <span class="s2">&quot;twice!&quot;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="p">(</span><span class="n">store</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">init_method</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Cannot specify both init_method and store.&quot;</span>

    <span class="k">if</span> <span class="n">store</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;world_size must be positive if using store&quot;</span>
        <span class="k">assert</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;rank must be non-negative if using store&quot;</span>
    <span class="k">elif</span> <span class="n">init_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init_method</span> <span class="o">=</span> <span class="s2">&quot;env://&quot;</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">world_size</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;For MPI backend, world_size (</span><span class="si">{}</span><span class="s2">) and rank (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                <span class="s2">&quot;are ignored since they are assigned by the &quot;</span>
                <span class="s2">&quot;MPI runtime.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[],</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
        <span class="p">)</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="n">default_pg</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># backward compatible API</span>
        <span class="k">if</span> <span class="n">store</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rendezvous_iterator</span> <span class="o">=</span> <span class="n">rendezvous</span><span class="p">(</span>
                <span class="n">init_method</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
            <span class="p">)</span>
            <span class="n">store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">rendezvous_iterator</span><span class="p">)</span>
            <span class="n">store</span><span class="o">.</span><span class="n">set_timeout</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>

            <span class="c1"># Use a PrefixStore to avoid accidental overrides of keys used by</span>
            <span class="c1"># different systems (e.g. RPC) in case the store is multi-tenant.</span>
            <span class="n">store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="s2">&quot;default_pg&quot;</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>

        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
            <span class="n">world_size</span><span class="p">,</span>
            <span class="n">rank</span><span class="p">,</span>
            <span class="p">[],</span>
            <span class="n">backend</span><span class="p">,</span>
            <span class="n">store</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
            <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="n">default_pg</span><span class="p">)</span>

    <span class="n">_pg_group_ranks</span><span class="p">[</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="o">.</span><span class="n">size</span><span class="p">())}</span>  <span class="c1"># type: ignore[attr-defined, index]</span>
    <span class="n">_backend</span> <span class="o">=</span> <span class="n">_pg_map</span><span class="p">[</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># type: ignore[index]</span>
    <span class="n">_default_pg_init_method</span> <span class="o">=</span> <span class="n">init_method</span>

    <span class="c1"># barrier at the end to ensure that once we return from this method, all</span>
    <span class="c1"># process groups including global variables are updated correctly on all</span>
    <span class="c1"># ranks.</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
        <span class="c1"># MPI backend doesn&#39;t use store.</span>
        <span class="n">barrier</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use store based barrier here since barrier() used a bunch of</span>
        <span class="c1"># default devices and messes up NCCL internal state.</span>
        <span class="n">_store_based_barrier</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="c1"># Set sequence numbers for gloo and nccl process groups.</span>
        <span class="k">if</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">default_pg</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">]:</span>
            <span class="n">default_pg</span><span class="o">.</span><span class="n">_set_sequence_number_for_group</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_new_process_group_helper</span><span class="p">(</span>
    <span class="n">world_size</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">,</span>
    <span class="n">group_ranks</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">,</span>
    <span class="n">store</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a new distributed process group.</span>

<span class="sd">    This function must be called by ALL processes in the global group, even if</span>
<span class="sd">    the calling process is not part of the newly created group. In that case,</span>
<span class="sd">    this function returns GroupMember.NON_GROUP_MEMBER.</span>

<span class="sd">    This function is called with ``group_ranks == []`` for the default group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_pg_map</span>
    <span class="k">global</span> <span class="n">_group_count</span>
    <span class="k">global</span> <span class="n">_pg_names</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">group_name</span><span class="p">:</span>
        <span class="n">group_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">_group_count</span><span class="p">)</span>
        <span class="n">_group_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">group_name</span> <span class="ow">in</span> <span class="n">_pg_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;The specified group name has already been &quot;</span>
            <span class="s2">&quot;created, please use a different group name&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected timeout argument to be of type&quot;</span> <span class="s2">&quot;datetime.timedelta&quot;</span>
        <span class="p">)</span>

    <span class="c1"># The list of group ranks is empty if we&#39;re creating the default group.</span>
    <span class="n">is_default_group</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_ranks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">pg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ProcessGroupGloo</span><span class="p">,</span> <span class="n">ProcessGroupMPI</span><span class="p">,</span> <span class="n">ProcessGroupNCCL</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_mpi_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Distributed package doesn&#39;t have MPI built in.&quot;</span>
                <span class="s2">&quot; MPI is only included if you build PyTorch from&quot;</span>
                <span class="s2">&quot; source on a host that has MPI installed.&quot;</span>
            <span class="p">)</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">ProcessGroupMPI</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">group_ranks</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">pg</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span>
        <span class="n">_pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">_pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If this is a subgroup (which means group_ranks is specified),</span>
        <span class="c1"># we check if the current process is a member of the new group.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_default_group</span><span class="p">:</span>
            <span class="n">global_rank</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">global_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group_ranks</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span>

        <span class="c1"># Use the group name as prefix in the default store, such that</span>
        <span class="c1"># a single store can be reused by multiple groups.</span>
        <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="n">group_name</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pg_options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;GLOO options not supported&quot;</span><span class="p">)</span>
            <span class="n">pg</span> <span class="o">=</span> <span class="n">ProcessGroupGloo</span><span class="p">(</span><span class="n">prefix_store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
            <span class="c1"># In debug mode and if GLOO is available, wrap in a wrapper PG that</span>
            <span class="c1"># enables enhanced collective checking for debugability.</span>
            <span class="k">if</span> <span class="n">_get_debug_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">_DistributedDebugLevel</span><span class="o">.</span><span class="n">DETAIL</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_GLOO_AVAILABLE</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sd">&quot;&quot;&quot;TORCH_DISTRIBUTED_DEBUG was set to DETAIL, but</span>
<span class="sd">                                GLOO is not available. Build with Gloo to</span>
<span class="sd">                                create a wrapper process group in debug mode</span>
<span class="sd">                                to aid collective desynchronization debugging.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pg</span> <span class="o">=</span> <span class="n">_create_process_group_wrapper</span><span class="p">(</span>
                        <span class="n">wrapped_pg</span><span class="o">=</span><span class="n">pg</span><span class="p">,</span>
                        <span class="n">store_prefix</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
                        <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="n">_pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
            <span class="n">_pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_nccl_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Distributed package doesn&#39;t have NCCL &quot;</span> <span class="s2">&quot;built in&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">pg_options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">pg_options</span><span class="p">,</span> <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">Options</span>
                <span class="p">),</span> <span class="s2">&quot;Expected pg_options argument to be of type ProcessGroupNCCL.Options&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># default pg_options for NCCL</span>
                <span class="n">pg_options</span> <span class="o">=</span> <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
                <span class="n">pg_options</span><span class="o">.</span><span class="n">is_high_priority_stream</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">pg_options</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">=</span> <span class="n">timeout</span>

            <span class="n">pg</span> <span class="o">=</span> <span class="n">ProcessGroupNCCL</span><span class="p">(</span><span class="n">prefix_store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">pg_options</span><span class="p">)</span>
            <span class="c1"># In debug mode and if GLOO is available, wrap in a wrapper PG that</span>
            <span class="c1"># enables enhanced collective checking for debugability.</span>
            <span class="k">if</span> <span class="n">_get_debug_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">_DistributedDebugLevel</span><span class="o">.</span><span class="n">DETAIL</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_GLOO_AVAILABLE</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sd">&quot;&quot;&quot;TORCH_DISTRIBUTED_DEBUG was set to DETAIL, but</span>
<span class="sd">                                GLOO is not available. Build with Gloo to</span>
<span class="sd">                                create a wrapper process group in debug mode</span>
<span class="sd">                                to aid collective desynchronization debugging.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pg</span> <span class="o">=</span> <span class="n">_create_process_group_wrapper</span><span class="p">(</span>
                        <span class="n">wrapped_pg</span><span class="o">=</span><span class="n">pg</span><span class="p">,</span>
                        <span class="n">store_prefix</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
                        <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="n">_pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
            <span class="n">_pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pg</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">upper</span><span class="p">())(</span>
                <span class="n">prefix_store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span>
            <span class="p">)</span>
            <span class="n">_pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
            <span class="n">_pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>

    <span class="k">return</span> <span class="n">pg</span>


<span class="k">def</span> <span class="nf">destroy_process_group</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Destroy a given process group, and deinitialize the distributed package</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to be destroyed, if</span>
<span class="sd">                                        group.WORLD is given, all process</span>
<span class="sd">                                        groups including the default one will</span>
<span class="sd">                                        be destroyed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_pg_map</span>
    <span class="k">global</span> <span class="n">_pg_names</span>
    <span class="k">global</span> <span class="n">_pg_group_ranks</span>
    <span class="k">global</span> <span class="n">_default_pg_init_method</span>
    <span class="k">global</span> <span class="n">_group_count</span>

    <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">assert</span> <span class="n">pg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_pg_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid process group specified&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">_default_pg_init_method</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">_pg_map</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_pg_names</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_pg_group_ranks</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># when process group doesn&#39;t have an explicit name (only WORLD (default)</span>
        <span class="c1"># process group can have an explicit name), we use global _group_counter</span>
        <span class="c1"># to generate the name. We need to reset the counter on destruction to</span>
        <span class="c1"># allow consistent value to be generated when we re-create process</span>
        <span class="c1"># groups after some trainers recover from failure</span>
        <span class="c1">#</span>
        <span class="c1"># We only reset this when WORLD is being destroyed because if this</span>
        <span class="c1"># process group is in good state, we aren&#39;t dealing with failures.</span>
        <span class="n">_group_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">_pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">_pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">_pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>


<div class="viewcode-block" id="get_rank"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_rank">[docs]</a><span class="k">def</span> <span class="nf">get_rank</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the rank of the current process in the provided ``group`` or the</span>
<span class="sd">    default group if none was provided.</span>

<span class="sd">    Rank is a unique identifier assigned to each process within a distributed</span>
<span class="sd">    process group. They are always consecutive integers ranging from 0 to</span>
<span class="sd">    ``world_size``.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The rank of the process group</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span></div>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_world_size">[docs]</a><span class="k">def</span> <span class="nf">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the number of processes in the current process group</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The world size of the process group</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">return</span> <span class="n">_get_group_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="isend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.isend">[docs]</a><span class="k">def</span> <span class="nf">isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sends a tensor asynchronously.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Modifying ``tensor`` before the request completes causes undefined</span>
<span class="sd">        behavior.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with remote recv</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">        None, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">dst</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">group</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_dst_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span></div>


<div class="viewcode-block" id="irecv"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.irecv">[docs]</a><span class="k">def</span> <span class="nf">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Receives a tensor asynchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match recv with remote send</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">        None, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv_anysource</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">tag</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_src_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span></div>


<div class="viewcode-block" id="send"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.send">[docs]</a><span class="k">def</span> <span class="nf">send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sends a tensor synchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with remote recv</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">default_pg</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">dst</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_dst_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="recv"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.recv">[docs]</a><span class="k">def</span> <span class="nf">recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Receives a tensor synchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match recv with remote send</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sender rank</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv_anysource</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">tag</span><span class="p">)</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="n">src_rank</span> <span class="o">=</span> <span class="n">work</span><span class="o">.</span><span class="n">_source_rank</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">src_rank</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_get_global_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src_rank</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
            <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_src_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">src</span></div>


<span class="k">class</span> <span class="nc">P2POp</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to build point-to-point operations for ``batch_isend_irecv``.</span>

<span class="sd">    This class builds the type of P2P operation, communication buffer, peer rank,</span>
<span class="sd">    Process Group group, and tag. Instances of this class will be passed to</span>
<span class="sd">    ``batch_isend_irecv`` for point-to-point communications.</span>

<span class="sd">    Args:</span>
<span class="sd">        op (callable): A function to send data to or receive data from a peer process.</span>
<span class="sd">            The type of ``op`` is either ``torch.distributed.isend`` or</span>
<span class="sd">            ``torch.distributed.irecv``.</span>
<span class="sd">        tensor (Tensor): Tensor to send or receive.</span>
<span class="sd">        peer (int): Destination or source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with recv.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">peer</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peer</span> <span class="o">=</span> <span class="n">peer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tag</span> <span class="o">=</span> <span class="n">tag</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">peer</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">_check_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
        <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_batch_p2p_manager</span><span class="p">(</span><span class="n">backend</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">_group_start</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">_group_end</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">batch_isend_irecv</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Send or Receive a batch of tensors asynchronously and return a list of requests.</span>

<span class="sd">    Process each of the operations in p2p_op_list and return the corresponding</span>
<span class="sd">    requests. NCCL and Gloo backend are currently supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        p2p_op_list: A list of point-to-point operations(type of each operator is</span>
<span class="sd">            ``torch.distributed.P2POp``). The order of the isend/irecv in the list</span>
<span class="sd">            matters and it needs to match with corresponding isend/irecv on the</span>
<span class="sd">            remote end.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of distributed request objects returned by calling the corresponding</span>
<span class="sd">        op in the op_list.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; send_tensor = torch.arange(2) + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; recv_tensor = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)</span>
<span class="sd">        &gt;&gt;&gt; recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank + 1)%world_size)</span>
<span class="sd">        &gt;&gt;&gt; reqs = batch_isend_irecv([send_op, recv_op])</span>
<span class="sd">        &gt;&gt;&gt; for req in reqs:</span>
<span class="sd">        &gt;&gt;&gt;     req.wait()</span>
<span class="sd">        &gt;&gt;&gt; recv_tensor</span>
<span class="sd">        tensor([2, 3])     # Rank 0</span>
<span class="sd">        tensor([0, 1])     # Rank 1</span>

<span class="sd">    .. note:: Note that when this API is used with the NCCL PG backend, users must set</span>
<span class="sd">        the current GPU device with `torch.cuda.set_device`, otherwise it will</span>
<span class="sd">        lead to unexpected hang issues.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_p2p_op_list</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">)</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
    <span class="n">reqs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">_batch_p2p_manager</span><span class="p">(</span><span class="n">backend</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span><span class="p">:</span>
            <span class="n">op</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">op</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">tensor</span>
            <span class="n">peer</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">peer</span>
            <span class="n">curr_group</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">group</span>
            <span class="n">tag</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">tag</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">peer</span><span class="p">,</span> <span class="n">curr_group</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">ret</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">reqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reqs</span>


<div class="viewcode-block" id="broadcast_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast_multigpu">[docs]</a><span class="k">def</span> <span class="nf">broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">src_tensor</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts the tensor to the whole group with multiple GPU tensors</span>
<span class="sd">    per node.</span>

<span class="sd">    ``tensor`` must have the same number of elements in all the GPUs from</span>
<span class="sd">    all processes participating in the collective. each tensor in the list must</span>
<span class="sd">    be on a different GPU</span>

<span class="sd">    Only nccl and gloo backend are currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): Tensors that participate in the collective</span>
<span class="sd">            operation. If ``src`` is the rank, then the specified ``src_tensor``</span>
<span class="sd">            element of ``tensor_list`` (``tensor_list[src_tensor]``) will be</span>
<span class="sd">            broadcast to all other tensors (on different GPUs) in the src process</span>
<span class="sd">            and all tensors in ``tensor_list`` of other non-src processes.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        src_tensor (int, optional): Source tensor rank within ``tensor_list``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BroadcastOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="n">src_tensor</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast">[docs]</a><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts the tensor to the whole group.</span>

<span class="sd">    ``tensor`` must have the same number of elements in all processes</span>
<span class="sd">    participating in the collective.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Data to be sent if ``src`` is the rank of current</span>
<span class="sd">            process, and tensor to be used to save received data otherwise.</span>
<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BroadcastOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="all_reduce_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result. This function reduces a number of tensors on every node,</span>
<span class="sd">    while each tensor resides on different GPUs.</span>
<span class="sd">    Therefore, the input tensor in the tensor list needs to be GPU tensors.</span>
<span class="sd">    Also, each tensor in the tensor list needs to reside on a different GPU.</span>

<span class="sd">    After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise</span>
<span class="sd">    identical in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Only nccl and gloo backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): List of input and output tensors of</span>
<span class="sd">            the collective. The function operates in-place and requires that</span>
<span class="sd">            each tensor to be a GPU tensor on different GPUs.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If</span>
<span class="sd">            ``None``, the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span>
    <span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="all_reduce"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_reduce">[docs]</a><span class="k">def</span> <span class="nf">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result.</span>

<span class="sd">    After the call ``tensor`` is going to be bitwise identical in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 type.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1, 2]) # Rank 0</span>
<span class="sd">        tensor([3, 4]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([4, 6]) # Rank 0</span>
<span class="sd">        tensor([4, 6]) # Rank 1</span>

<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1.+1.j, 2.+2.j]) # Rank 0</span>
<span class="sd">        tensor([3.+3.j, 4.+4.j]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([4.+4.j, 6.+6.j]) # Rank 0</span>
<span class="sd">        tensor([4.+4.j, 6.+6.j]) # Rank 1</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">supports_complex</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all_reduce does not support </span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2"> on complex tensors&quot;</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">all_reduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    WARNING: at this time individual shape checking is not implemented across nodes.</span>
<span class="sd">    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the</span>
<span class="sd">    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce</span>
<span class="sd">    operation will proceed without complaint and return erroneous outputs. This lack</span>
<span class="sd">    of shape checking results in significant performance improvements but users of this</span>
<span class="sd">    function should take extra care to ensure that each node passes in tensors whose</span>
<span class="sd">    shapes match across nodes.</span>

<span class="sd">    Reduces each tensor in tensors (residing on the same device) across all machines</span>
<span class="sd">    in such a way that all get the final result.</span>

<span class="sd">    After the call each tensor in tensors is going to bitwise identical</span>
<span class="sd">    in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (List[Tensor]): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (Optional[ReduceOp]): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp`` enum. Specifies an operation used for</span>
<span class="sd">            element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (Optional[bool]): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">])</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">supports_complex</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all_reduce does not support </span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2"> on complex tensors&quot;</span><span class="p">)</span>

    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceCoalescedOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<div class="viewcode-block" id="reduce_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">reduce_multigpu</span><span class="p">(</span>
    <span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dst_tensor</span><span class="o">=</span><span class="mi">0</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data on multiple GPUs across all machines. Each tensor</span>
<span class="sd">    in ``tensor_list`` should reside on a separate GPU</span>

<span class="sd">    Only the GPU of ``tensor_list[dst_tensor]`` on the process with rank ``dst``</span>
<span class="sd">    is going to receive the final result.</span>

<span class="sd">    Only nccl backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): Input and output GPU tensors of the</span>
<span class="sd">            collective. The function operates in-place.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        dst_tensor (int, optional): Destination tensor rank within</span>
<span class="sd">                                    ``tensor_list``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, otherwise</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="n">dst_tensor</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce">[docs]</a><span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines.</span>

<span class="sd">    Only the process with rank ``dst`` is going to receive the final result.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="all_gather_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_gather_multigpu</span><span class="p">(</span>
    <span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers tensors from the whole group in a list.</span>
<span class="sd">    Each tensor in ``tensor_list`` should reside on a separate GPU</span>

<span class="sd">    Only nccl backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_lists (List[List[Tensor]]): Output lists. It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for output</span>
<span class="sd">            of the collective, e.g. ``output_tensor_lists[i]`` contains the</span>
<span class="sd">            all_gather result that resides on the GPU of</span>
<span class="sd">            ``input_tensor_list[i]``.</span>

<span class="sd">            Note that each element of ``output_tensor_lists`` has the size of</span>
<span class="sd">            ``world_size * len(input_tensor_list)``, since the function all</span>
<span class="sd">            gathers the result from every single GPU in the group. To interpret</span>
<span class="sd">            each element of ``output_tensor_lists[i]``, note that</span>
<span class="sd">            ``input_tensor_list[j]`` of rank k will be appear in</span>
<span class="sd">            ``output_tensor_lists[i][k * world_size + j]``</span>

<span class="sd">            Also note that ``len(output_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``output_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(output_tensor_lists[i])``) need to be the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        input_tensor_list (List[Tensor]): List of tensors(on different GPUs) to</span>
<span class="sd">            be broadcast from current process.</span>
<span class="sd">            Note that ``len(input_tensor_list)`` needs to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">output_tensor_lists</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span>
    <span class="p">]</span>
    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">_pickler</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="n">byte_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteStorage</span><span class="o">.</span><span class="n">from_buffer</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="c1"># Do not replace `torch.ByteTensor` or `torch.LongTensor` with torch.tensor and specifying dtype.</span>
    <span class="c1"># Otherwise, it will casue 100X slowdown.</span>
    <span class="c1"># See: https://github.com/pytorch/pytorch/issues/65696</span>
    <span class="n">byte_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">byte_storage</span><span class="p">)</span>
    <span class="n">local_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">byte_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">byte_tensor</span><span class="p">,</span> <span class="n">local_size</span>


<span class="k">def</span> <span class="nf">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">):</span>
    <span class="n">buf</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()[:</span><span class="n">tensor_size</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_unpickler</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">buf</span><span class="p">))</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>


<div class="viewcode-block" id="all_gather_object"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather_object">[docs]</a><span class="k">def</span> <span class="nf">all_gather_object</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers picklable objects from the whole group into a list. Similar to</span>
<span class="sd">    :func:`all_gather`, but Python objects can be passed in. Note that the object</span>
<span class="sd">    must be picklable in order to be gathered.</span>

<span class="sd">    Args:</span>
<span class="sd">        object_list (list[Any]): Output list. It should be correctly sized as the</span>
<span class="sd">            size of the group for this collective and will contain the output.</span>
<span class="sd">        object (Any): Pickable Python object to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None. If the calling rank is part of this group, the output of the</span>
<span class="sd">        collective will be populated into the input ``object_list``. If the</span>
<span class="sd">        calling rank is not part of the group, the passed in ``object_list`` will</span>
<span class="sd">        be unmodified.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the :func:`all_gather`</span>
<span class="sd">        collective since it does not provide an ``async_op`` handle and thus</span>
<span class="sd">        will be a blocking call.</span>

<span class="sd">    .. note:: For NCCL-based processed groups, internal tensor representations</span>
<span class="sd">        of objects must be moved to the GPU device before communication takes</span>
<span class="sd">        place. In this case, the device used is given by</span>
<span class="sd">        ``torch.cuda.current_device()`` and it is the user&#39;s responsiblity to</span>
<span class="sd">        ensure that this is set so that each rank has an individual GPU, via</span>
<span class="sd">        ``torch.cuda.set_device()``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`all_gather_object` uses ``pickle`` module implicitly, which is</span>
<span class="sd">        known to be insecure. It is possible to construct malicious pickle data</span>
<span class="sd">        which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt; gather_objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; output = [None for _ in gather_objects]</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather_object(output, gather_objects[dist.get_rank()])</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">local_size</span> <span class="o">=</span> <span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_nccl_available</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">(),</span> <span class="n">ProcessGroupNCCL</span>
    <span class="p">):</span>
        <span class="c1"># See note about using torch.cuda.current_device() here in docstring.</span>
        <span class="c1"># We cannot simply use my_rank since rank == device is not necessarily</span>
        <span class="c1"># true.</span>
        <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
        <span class="n">local_size</span> <span class="o">=</span> <span class="n">local_size</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="c1"># Gather all local sizes. This is so that we can find the max size, and index</span>
    <span class="c1"># until the correct size when deserializing the tensors.</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="n">object_size_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">object_sizes_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Allgather tensor sizes</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">max_object_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[type-var]</span>
    <span class="c1"># Resize tensor to max size across all ranks.</span>
    <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_object_size</span><span class="p">)</span>
    <span class="n">coalesced_output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="n">max_object_size</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="c1"># Output tensors are nonoverlapping views of coalesced_output_tensor</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">coalesced_output_tensor</span><span class="p">[</span><span class="n">max_object_size</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">max_object_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="c1"># Deserialize outputs back to object.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">):</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">object_size_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">object_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_object"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.gather_object">[docs]</a><span class="k">def</span> <span class="nf">gather_object</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">object_gather_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers picklable objects from the whole group in a single process.</span>
<span class="sd">    Similar to :func:`gather`, but Python objects can be passed in. Note that the</span>
<span class="sd">    object must be picklable in order to be gathered.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Any): Input object. Must be picklable.</span>
<span class="sd">        object_gather_list (list[Any]): Output list. On the ``dst`` rank, it</span>
<span class="sd">            should be correctly sized as the size of the group for this</span>
<span class="sd">            collective and will contain the output. Must be ``None`` on non-dst</span>
<span class="sd">            ranks. (default is ``None``)</span>
<span class="sd">        dst (int, optional): Destination rank. (default is 0)</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None. On the ``dst`` rank, ``object_gather_list`` will contain the</span>
<span class="sd">        output of the collective.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the gather collective</span>
<span class="sd">        since it does not provide an async_op handle and thus will be a blocking</span>
<span class="sd">        call.</span>

<span class="sd">    .. note:: Note that this API is not supported when using the NCCL backend.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`gather_object` uses ``pickle`` module implicitly, which is</span>
<span class="sd">        known to be insecure. It is possible to construct malicious pickle data</span>
<span class="sd">        which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt; gather_objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; output = [None for _ in gather_objects]</span>
<span class="sd">        &gt;&gt;&gt; dist.gather_object(</span>
<span class="sd">                gather_objects[dist.get_rank()],</span>
<span class="sd">                output if dist.get_rank() == 0 else None,</span>
<span class="sd">                dst=0</span>
<span class="sd">            )</span>
<span class="sd">        &gt;&gt;&gt; # On rank 0</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="c1"># Ensure object_gather_list is specified appopriately.</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">object_gather_list</span><span class="p">)</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">local_size</span> <span class="o">=</span> <span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="n">group_backend</span> <span class="o">=</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="n">is_nccl_backend</span> <span class="o">=</span> <span class="n">group_backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span>
    <span class="k">if</span> <span class="n">is_nccl_backend</span><span class="p">:</span>
        <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
        <span class="n">local_size</span> <span class="o">=</span> <span class="n">local_size</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="c1"># Gather all local sizes. This is so that we can find the max size, and index</span>
    <span class="c1"># until the correct size when deserializing the tensors.</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="n">object_size_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">object_sizes_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Allgather tensor sizes. An all-gather is needed here despite this being a</span>
    <span class="c1"># gather, since each rank needs to broadcast a tensor of the same (maximal)</span>
    <span class="c1"># size.</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">max_object_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[type-var]</span>
    <span class="c1"># Resize tensor to max size across all ranks.</span>
    <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_object_size</span><span class="p">)</span>
    <span class="c1"># Avoid populating output tensors if the result won&#39;t be gathered on this rank.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">dst</span><span class="p">:</span>
        <span class="n">coalesced_output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">max_object_size</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
        <span class="p">)</span>
        <span class="c1"># Output tensors are nonoverlapping views of coalesced_output_tensor</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">coalesced_output_tensor</span><span class="p">[</span><span class="n">max_object_size</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">max_object_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="c1"># All ranks call gather with equal-sized tensors.</span>
    <span class="n">gather</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span>
        <span class="n">gather_list</span><span class="o">=</span><span class="n">output_tensors</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">dst</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dst</span><span class="o">=</span><span class="n">dst</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">dst</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">):</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">object_size_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">object_gather_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_object_list"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast_object_list">[docs]</a><span class="k">def</span> <span class="nf">broadcast_object_list</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts picklable objects in ``object_list`` to the whole group. Similar</span>
<span class="sd">    to :func:`broadcast`, but Python objects can be passed in.</span>
<span class="sd">    Note that all objects in ``object_list`` must be picklable in order to be</span>
<span class="sd">    broadcasted.</span>

<span class="sd">    Args:</span>
<span class="sd">        object_list (List[Any]): List of input objects to broadcast.</span>
<span class="sd">            Each object must be picklable. Only objects on the ``src`` rank will</span>
<span class="sd">            be broadcast, but each rank must provide lists of equal sizes.</span>
<span class="sd">        src (int): Source rank from which to broadcast ``object_list``.</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>
<span class="sd">        device (``torch.device``, optional): If not None, the objects are</span>
<span class="sd">            serialized and converted to tensors which are moved to the</span>
<span class="sd">            ``device`` before broadcasting. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``. If rank is part of the group, ``object_list`` will contain the</span>
<span class="sd">        broadcasted objects from ``src`` rank.</span>

<span class="sd">    .. note:: For NCCL-based processed groups, internal tensor representations</span>
<span class="sd">        of objects must be moved to the GPU device before communication takes</span>
<span class="sd">        place. In this case, the device used is given by</span>
<span class="sd">        ``torch.cuda.current_device()`` and it is the user&#39;s responsiblity to</span>
<span class="sd">        ensure that this is set so that each rank has an individual GPU, via</span>
<span class="sd">        ``torch.cuda.set_device()``.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the :func:`all_gather`</span>
<span class="sd">        collective since it does not provide an ``async_op`` handle and thus</span>
<span class="sd">        will be a blocking call.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`broadcast_object_list` uses ``pickle`` module implicitly, which</span>
<span class="sd">        is known to be insecure. It is possible to construct malicious pickle</span>
<span class="sd">        data which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; else:</span>
<span class="sd">        &gt;&gt;&gt;     objects = [None, None, None]</span>
<span class="sd">        &gt;&gt;&gt; # Assumes backend is not NCCL</span>
<span class="sd">        &gt;&gt;&gt; device = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dist.broadcast_object_list(objects, src=0, device=device)</span>
<span class="sd">        &gt;&gt;&gt; broadcast_objects</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="c1"># Serialize object_list elements to tensors on src rank.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">size_list</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">object_list</span><span class="p">])</span>
        <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">size_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">object_list</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="c1"># Current device selection.</span>
    <span class="c1"># To preserve backwards compatibility, ``device`` is default to ``None``</span>
    <span class="c1"># in which case we run current logic of device selection, i.e.</span>
    <span class="c1"># ``current_device`` is CUDA if backend is NCCL otherwise CPU device. In the</span>
    <span class="c1"># case it is not ``None`` we move the size and object tensors to be</span>
    <span class="c1"># broadcasted to this device.</span>
    <span class="n">group_backend</span> <span class="o">=</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="n">is_nccl_backend</span> <span class="o">=</span> <span class="n">group_backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_nccl_backend</span> <span class="ow">and</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;device type must be cuda for nccl backend&quot;</span><span class="p">)</span>
        <span class="n">current_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_nccl_backend</span><span class="p">:</span>
            <span class="c1"># See note about using torch.cuda.current_device() here in</span>
            <span class="c1"># docstring. We cannot simply use my_rank since rank == device is</span>
            <span class="c1"># not necessarily true.</span>
            <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">is_nccl_backend</span><span class="p">:</span>
        <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">object_sizes_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>

    <span class="c1"># Broadcast object sizes</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># Concatenate and broadcast serialized object tensors</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">object_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">object_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_nccl_backend</span><span class="p">:</span>
        <span class="n">object_tensor</span> <span class="o">=</span> <span class="n">object_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">object_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="c1"># Deserialize objects using their stored sizes.</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obj_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">):</span>
            <span class="n">obj_view</span> <span class="o">=</span> <span class="n">object_tensor</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">obj_size</span><span class="p">]</span>
            <span class="n">obj_view</span> <span class="o">=</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                <span class="n">obj_view</span> <span class="o">=</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">offset</span> <span class="o">+=</span> <span class="n">obj_size</span>
            <span class="n">object_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">obj_view</span><span class="p">,</span> <span class="n">obj_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_object_list"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.scatter_object_list">[docs]</a><span class="k">def</span> <span class="nf">scatter_object_list</span><span class="p">(</span>
    <span class="n">scatter_object_output_list</span><span class="p">,</span> <span class="n">scatter_object_input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters picklable objects in ``scatter_object_input_list`` to the whole</span>
<span class="sd">    group. Similar to :func:`scatter`, but Python objects can be passed in. On</span>
<span class="sd">    each rank, the scattered object will be stored as the first element of</span>
<span class="sd">    ``scatter_object_output_list``. Note that all objects in</span>
<span class="sd">    ``scatter_object_input_list`` must be picklable in order to be scattered.</span>

<span class="sd">    Args:</span>
<span class="sd">        scatter_object_output_list (List[Any]): Non-empty list whose first</span>
<span class="sd">            element will store the object scattered to this rank.</span>
<span class="sd">        scatter_object_input_list (List[Any]): List of input objects to scatter.</span>
<span class="sd">            Each object must be picklable. Only objects on the ``src`` rank will</span>
<span class="sd">            be scattered, and the argument can be ``None`` for non-src ranks.</span>
<span class="sd">        src (int): Source rank from which to scatter</span>
<span class="sd">            ``scatter_object_input_list``.</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``. If rank is part of the group, ``scatter_object_output_list``</span>
<span class="sd">        will have its first element set to the scattered object for this rank.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the scatter collective</span>
<span class="sd">        since it does not provide an ``async_op`` handle and thus will be a</span>
<span class="sd">        blocking call.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`scatter_object_list` uses ``pickle`` module implicitly, which</span>
<span class="sd">        is known to be insecure. It is possible to construct malicious pickle</span>
<span class="sd">        data which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; else:</span>
<span class="sd">        &gt;&gt;&gt;     # Can be any list on non-src ranks, elements are not used.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [None, None, None]</span>
<span class="sd">        &gt;&gt;&gt; output_list = [None]</span>
<span class="sd">        &gt;&gt;&gt; dist.scatter_object_list(output_list, objects, src=0)</span>
<span class="sd">        &gt;&gt;&gt; # Rank i gets objects[i]. For example, on rank 2:</span>
<span class="sd">        &gt;&gt;&gt; output_list</span>
<span class="sd">        [{1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scatter_object_output_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">scatter_object_output_list</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected argument scatter_object_output_list to be a list of size at least 1.&quot;</span>
        <span class="p">)</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor_sizes</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">scatter_object_input_list</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span>

    <span class="c1"># Src rank broadcasts the maximum tensor size. This is because all ranks are</span>
    <span class="c1"># expected to call into scatter() with equal-sized tensors.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">max_tensor_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">:</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_tensor_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># Scatter actual serialized objects</span>
    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">scatter</span><span class="p">(</span>
        <span class="n">output_tensor</span><span class="p">,</span>
        <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span> <span class="k">else</span> <span class="n">tensor_list</span><span class="p">,</span>
        <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Scatter per-object sizes to trim tensors when deserializing back to object</span>
    <span class="n">obj_tensor_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">scatter</span><span class="p">(</span>
        <span class="n">obj_tensor_size</span><span class="p">,</span>
        <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span> <span class="k">else</span> <span class="n">tensor_sizes</span><span class="p">,</span>
        <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Deserialize back to object</span>
    <span class="n">scatter_object_output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">obj_tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_gather"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather">[docs]</a><span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers tensors from the whole group in a list.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (list[Tensor]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 dtype.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1, 2]) # Rank 0</span>
<span class="sd">        tensor([3, 4]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather(tensor_list, tensor)</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([1, 2]), tensor([3, 4])] # Rank 0</span>
<span class="sd">        [tensor([1, 2]), tensor([3, 4])] # Rank 1</span>

<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.cfloat dtype.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1.+1.j, 2.+2.j]) # Rank 0</span>
<span class="sd">        tensor([3.+3.j, 4.+4.j]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather(tensor_list, tensor)</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0</span>
<span class="sd">        [tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span>
    <span class="p">]</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_all_gather_base</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Single tensor all gather. Gathers a single tensor from all ranks, and puts them in a single output tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor (Tensor): Output tensor. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        input_tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 dtype.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = torch.zeros(2, dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor</span>
<span class="sd">        [tensor([0, 0])] # Rank 0 and 1</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.arange(1, dtype=torch.int64) + 1 + rank</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1]) # Rank 0</span>
<span class="sd">        tensor([2]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather_base(output_tensor, tensor)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor</span>
<span class="sd">        tensor([1,2]) # Rank 0</span>
<span class="sd">        tensor([1,2]) # Rank 1</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `_all_gather_base` is experimental and subject to change.</span>
<span class="sd">        It is the caller&#39;s responsibility to ensure the output_tensor</span>
<span class="sd">        is correctly sized.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="s2">&quot;input_tensor&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="s2">&quot;output_tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">output_tensor</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">input_tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">_allgather_base</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_allgather_base</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">all_gather_coalesced</span><span class="p">(</span>
    <span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers input tensors from the whole group in a list in a coalesced manner.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_lists (list[list[Tensor]]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        input_tensor_list (list[Tensor]): Tensors to be broadcast from</span>
<span class="sd">            current process. At least one tensor has to be non empty.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Example:</span>
<span class="sd">        we have 2 process groups, 2 ranks.</span>
<span class="sd">        rank 0 passes:</span>
<span class="sd">            input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],</span>
<span class="sd">                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]</span>
<span class="sd">        rank 1 passes:</span>
<span class="sd">            input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],</span>
<span class="sd">                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]</span>
<span class="sd">        both rank 0 and 1 get:</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[1, 1], [1, 1]], [2], [3, 3]],</span>
<span class="sd">                [[3, 3], [3, 3]], [5], [1, 1]]].</span>

<span class="sd">    WARNING: at this time individual shape checking is not implemented across nodes.</span>
<span class="sd">    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the</span>
<span class="sd">    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the</span>
<span class="sd">    all_gather_coalesced operation will proceed without complaint and return</span>
<span class="sd">    erroneous outputs. This lack of shape checking results in significant</span>
<span class="sd">    performance improvements but users of this function should take extra care</span>
<span class="sd">    to ensure that each node passes in tensors whose shapes match across nodes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We only check basic compatibility with C++ params here, C++ code will</span>
    <span class="c1"># do shape and type checking.</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument: &quot;</span> <span class="s2">&quot;output_tensor_lists should be a list&quot;</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">output_tensor_list</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="s2">&quot;output_tensor_lists&quot;</span><span class="p">)</span>

    <span class="n">output_tensor_lists</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span>
    <span class="p">]</span>
    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather_coalesced</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather_coalesced</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">gather_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``gather_list`` must be specified on destination rank.&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">gather_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Argument ``gather_list`` must NOT be specified &quot;</span>
            <span class="s2">&quot;on non-destination ranks.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.gather">[docs]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers a list of tensors in a single process.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input tensor.</span>
<span class="sd">        gather_list (list[Tensor], optional): List of appropriately-sized</span>
<span class="sd">            tensors to use for gathered data (default is None, must be specified</span>
<span class="sd">            on the destination rank)</span>
<span class="sd">        dst (int, optional): Destination rank (default is 0)</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>

    <span class="c1"># Parameter ``gather_list`` may be left unspecified on non-dst ranks.</span>
    <span class="k">if</span> <span class="n">gather_list</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">gather_list</span><span class="p">,</span> <span class="s2">&quot;gather_list&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gather_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">)</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">gather_list</span><span class="p">]</span> <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">GatherOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="scatter"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.scatter">[docs]</a><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Each process will receive exactly one tensor and store its data in the</span>
<span class="sd">    ``tensor`` argument.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Output tensor.</span>
<span class="sd">        scatter_list (list[Tensor]): List of tensors to scatter (default is</span>
<span class="sd">            None, must be specified on the source rank)</span>
<span class="sd">        src (int): Source rank (default is 0)</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>

    <span class="c1"># Parameter ``scatter_list`` may be left unspecified on non-src ranks.</span>
    <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">scatter_list</span><span class="p">,</span> <span class="s2">&quot;scatter_list&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scatter_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="n">scatter_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">scatter_list</span>
    <span class="p">]</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">src</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``scatter_list`` must be specified &quot;</span> <span class="s2">&quot;on source rank.&quot;</span>
            <span class="p">)</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">scatter_list</span><span class="p">]</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``scatter_list`` must NOT be specified &quot;</span>
                <span class="s2">&quot;on non-source ranks.&quot;</span>
            <span class="p">)</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">_get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce_scatter_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_scatter_multigpu">[docs]</a><span class="k">def</span> <span class="nf">reduce_scatter_multigpu</span><span class="p">(</span>
    <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce and scatter a list of tensors to the whole group.  Only nccl backend</span>
<span class="sd">    is currently supported.</span>

<span class="sd">    Each tensor in ``output_tensor_list`` should reside on a separate GPU, as</span>
<span class="sd">    should each list of tensors in ``input_tensor_lists``.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_list (List[Tensor]): Output tensors (on different GPUs)</span>
<span class="sd">            to receive the result of the operation.</span>

<span class="sd">            Note that ``len(output_tensor_list)`` needs to be the same for all</span>
<span class="sd">            the distributed processes calling this function.</span>

<span class="sd">        input_tensor_lists (List[List[Tensor]]): Input lists.  It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for input of</span>
<span class="sd">            the collective, e.g. ``input_tensor_lists[i]`` contains the</span>
<span class="sd">            reduce_scatter input that resides on the GPU of</span>
<span class="sd">            ``output_tensor_list[i]``.</span>

<span class="sd">            Note that each element of ``input_tensor_lists`` has the size of</span>
<span class="sd">            ``world_size * len(output_tensor_list)``, since the function</span>
<span class="sd">            scatters the result from every single GPU in the group.  To</span>
<span class="sd">            interpret each element of ``input_tensor_lists[i]``, note that</span>
<span class="sd">            ``output_tensor_list[j]`` of rank k receives the reduce-scattered</span>
<span class="sd">            result from ``input_tensor_lists[i][k * world_size + j]``</span>

<span class="sd">            Also note that ``len(input_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``input_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(input_tensor_lists[i])``) need to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce_scatter"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_scatter">[docs]</a><span class="k">def</span> <span class="nf">reduce_scatter</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces, then scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Output tensor.</span>
<span class="sd">        input_list (list[Tensor]): List of tensors to reduce and scatter.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="s2">&quot;input_list&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">([</span><span class="n">output</span><span class="p">],</span> <span class="p">[</span><span class="n">input_list</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">([</span><span class="n">output</span><span class="p">],</span> <span class="p">[</span><span class="n">input_list</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_reduce_scatter_base</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces, then scatters a flattened tensor to all processes in a group.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Output tensor.</span>
<span class="sd">        input (Tensor): Input tensor that is of size output tensor size times world size</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">_reduce_scatter_base</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_reduce_scatter_base</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">all_to_all_single</span><span class="p">(</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="nb">input</span><span class="p">,</span>
    <span class="n">output_split_sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_split_sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Each process splits input tensor and then scatters the split list</span>
<span class="sd">    to all processes in a group. Then concatenate the received tensors from all</span>
<span class="sd">    the processes in the group and return single output tensor.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Gathered cancatenated output tensor.</span>
<span class="sd">        input (Tensor): Input tensor to scatter.</span>
<span class="sd">        output_split_sizes: (list[Int], optional): Output split sizes for dim 0</span>
<span class="sd">            if specified None or empty, dim 0 of ``output`` tensor must divide</span>
<span class="sd">            equally by ``world_size``.</span>
<span class="sd">        input_split_sizes: (list[Int], optional): Input split sizes for dim 0</span>
<span class="sd">            if specified None or empty, dim 0 of ``input`` tensor must divide</span>
<span class="sd">            equally by ``world_size``.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `all_to_all_single` is experimental and subject to change.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(4) + rank * 4</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3])     # Rank 0</span>
<span class="sd">        tensor([4, 5, 6, 7])     # Rank 1</span>
<span class="sd">        tensor([8, 9, 10, 11])   # Rank 2</span>
<span class="sd">        tensor([12, 13, 14, 15]) # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([0, 4, 8, 12])    # Rank 0</span>
<span class="sd">        tensor([1, 5, 9, 13])    # Rank 1</span>
<span class="sd">        tensor([2, 6, 10, 14])   # Rank 2</span>
<span class="sd">        tensor([3, 7, 11, 15])   # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Essentially, it is similar to following operation:</span>
<span class="sd">        &gt;&gt;&gt; scatter_list = list(input.chunk(world_size))</span>
<span class="sd">        &gt;&gt;&gt; gather_list  = list(output.chunk(world_size))</span>
<span class="sd">        &gt;&gt;&gt; for i in range(world_size):</span>
<span class="sd">        &gt;&gt;&gt;   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)</span>

<span class="sd">        &gt;&gt;&gt; # Another example with uneven split</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="sd">        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="sd">        tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="sd">        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input_splits</span>
<span class="sd">        [2, 2, 1, 1]                                                     # Rank 0</span>
<span class="sd">        [3, 2, 2, 2]                                                     # Rank 1</span>
<span class="sd">        [2, 1, 1, 1]                                                     # Rank 2</span>
<span class="sd">        [2, 2, 2, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output_splits</span>
<span class="sd">        [2, 3, 2, 2]                                                     # Rank 0</span>
<span class="sd">        [2, 2, 1, 2]                                                     # Rank 1</span>
<span class="sd">        [1, 2, 1, 2]                                                     # Rank 2</span>
<span class="sd">        [1, 2, 1, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = ...</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input, output_splits, input_splits)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0</span>
<span class="sd">        tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1</span>
<span class="sd">        tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2</span>
<span class="sd">        tensor([ 5, 17, 18, 24, 36])                                     # Rank 3</span>


<span class="sd">        &gt;&gt;&gt; # Another example with tensors of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0</span>
<span class="sd">        tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1</span>
<span class="sd">        tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2</span>
<span class="sd">        tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0</span>
<span class="sd">        tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1</span>
<span class="sd">        tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2</span>
<span class="sd">        tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllToAllOptions</span><span class="p">()</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="n">output_split_sizes</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">output_split_sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output_split_sizes</span>
    <span class="n">input_split_sizes</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">input_split_sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_split_sizes</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">alltoall_base</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_split_sizes</span><span class="p">,</span> <span class="n">input_split_sizes</span><span class="p">,</span> <span class="n">opts</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">alltoall_base</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_split_sizes</span><span class="p">,</span> <span class="n">input_split_sizes</span><span class="p">,</span> <span class="n">opts</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<div class="viewcode-block" id="all_to_all"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_to_all">[docs]</a><span class="k">def</span> <span class="nf">all_to_all</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Each process scatters list of input tensors to all processes in a group and</span>
<span class="sd">    return gathered list of tensors in output list.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_list (list[Tensor]): List of tensors to be gathered one</span>
<span class="sd">            per rank.</span>
<span class="sd">        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `all_to_all` is experimental and subject to change.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(4) + rank * 4</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0</span>
<span class="sd">        [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1</span>
<span class="sd">        [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2</span>
<span class="sd">        [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0</span>
<span class="sd">        [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1</span>
<span class="sd">        [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2</span>
<span class="sd">        [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Essentially, it is similar to following operation:</span>
<span class="sd">        &gt;&gt;&gt; scatter_list = input</span>
<span class="sd">        &gt;&gt;&gt; gather_list  = output</span>
<span class="sd">        &gt;&gt;&gt; for i in range(world_size):</span>
<span class="sd">        &gt;&gt;&gt;   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)</span>

<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="sd">        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="sd">        tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="sd">        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input_splits</span>
<span class="sd">        [2, 2, 1, 1]                                                     # Rank 0</span>
<span class="sd">        [3, 2, 2, 2]                                                     # Rank 1</span>
<span class="sd">        [2, 1, 1, 1]                                                     # Rank 2</span>
<span class="sd">        [2, 2, 2, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output_splits</span>
<span class="sd">        [2, 3, 2, 2]                                                     # Rank 0</span>
<span class="sd">        [2, 2, 1, 2]                                                     # Rank 1</span>
<span class="sd">        [1, 2, 1, 2]                                                     # Rank 2</span>
<span class="sd">        [1, 2, 1, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.split(input_splits))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0</span>
<span class="sd">        [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1</span>
<span class="sd">        [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2</span>
<span class="sd">        [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = ...</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0</span>
<span class="sd">        [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1</span>
<span class="sd">        [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2</span>
<span class="sd">        [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Another example with tensors of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0</span>
<span class="sd">        [tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1</span>
<span class="sd">        [tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2</span>
<span class="sd">        [tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0</span>
<span class="sd">        [tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1</span>
<span class="sd">        [tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2</span>
<span class="sd">        [tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllToAllOptions</span><span class="p">()</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="s2">&quot;output_tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">,</span> <span class="s2">&quot;input_tensor_list&quot;</span><span class="p">)</span>

    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>
    <span class="n">output_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">alltoall</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">alltoall</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.barrier">[docs]</a><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronizes all processes.</span>

<span class="sd">    This collective blocks processes until the whole group enters this function,</span>
<span class="sd">    if async_op is False, or if async work handle is called on wait().</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        device_ids ([int], optional): List of device/GPU ids.</span>
<span class="sd">                                      Valid only for NCCL backend.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BarrierOptions</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Function argument device_ids not supported &quot;</span>
                <span class="s2">&quot;for the selected backend </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">opts</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="n">device_ids</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid function argument: &quot;</span> <span class="s2">&quot;device_ids type should be List[int]&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="monitored_barrier"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.monitored_barrier">[docs]</a><span class="k">def</span> <span class="nf">monitored_barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wait_all_ranks</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronizes all processes similar to ``torch.distributed.barrier``, but takes</span>
<span class="sd">    a configurable timeout and is able to report ranks that did not pass this</span>
<span class="sd">    barrier within that timeout. Specifically, for non-zero ranks, will block</span>
<span class="sd">    until a send/recv is processed from rank 0. Rank 0 will block until all send</span>
<span class="sd">    /recv from other ranks are processed, and will report failures for ranks</span>
<span class="sd">    that failed to respond in time. Note that if one rank does not reach the</span>
<span class="sd">    monitored_barrier (for example due to a hang), all other ranks would fail</span>
<span class="sd">    in monitored_barrier.</span>

<span class="sd">    This collective will block all processes/ranks in the group, until the</span>
<span class="sd">    whole group exits the function successfully, making it useful for debugging</span>
<span class="sd">    and synchronizing. However, it can have a performance impact and should only</span>
<span class="sd">    be used for debugging or scenarios that require full synchronization points</span>
<span class="sd">    on the host-side. For debugging purposees, this barrier can be inserted</span>
<span class="sd">    before the application&#39;s collective calls to check if any ranks are</span>
<span class="sd">    desynchronized.</span>

<span class="sd">    .. note:: Note that this collective is only supported with the GLOO backend.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If</span>
<span class="sd">            ``None``, the default process group will be used.</span>
<span class="sd">        timeout (datetime.timedelta, optional): Timeout for monitored_barrier.</span>
<span class="sd">            If ``None``, the default process group timeout will be used.</span>
<span class="sd">        wait_all_ranks (bool, optional): Whether to collect all failed ranks or</span>
<span class="sd">            not. By default, this is ``False`` and ``monitored_barrier`` on rank 0</span>
<span class="sd">            will throw on the first failed rank it encounters in order to fail</span>
<span class="sd">            fast. By setting ``wait_all_ranks=True`` ``monitored_barrier`` will</span>
<span class="sd">            collect all failed ranks and throw an error containing information</span>
<span class="sd">            about all failed ranks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() != 1:</span>
<span class="sd">        &gt;&gt;&gt;     dist.monitored_barrier() # Raises exception indicating that</span>
<span class="sd">        &gt;&gt;&gt; # rank 1 did not call into monitored_barrier.</span>
<span class="sd">        &gt;&gt;&gt; # Example with wait_all_ranks=True</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     dist.monitored_barrier(wait_all_ranks=True) # Raises exception</span>
<span class="sd">        &gt;&gt;&gt; # indicating that ranks 1, 2, ... world_size - 1 did not call into</span>
<span class="sd">        &gt;&gt;&gt; # monitored_barrier.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Need to call rank not in group before using the group, otherwise</span>
    <span class="c1"># &quot;Invalid process group&quot; error is raised.</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;monitored_barrier is only implemented for GLOO backend.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">timeout</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">timeout</span> <span class="o">=</span> <span class="n">default_pg_timeout</span>

    <span class="n">group_to_use</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span> <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">group</span>
    <span class="k">return</span> <span class="n">group_to_use</span><span class="o">.</span><span class="n">monitored_barrier</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">wait_all_ranks</span><span class="o">=</span><span class="n">wait_all_ranks</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_create_process_group_wrapper</span><span class="p">(</span>
    <span class="n">wrapped_pg</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">,</span>
    <span class="n">store_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">Store</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="n">timedelta</span> <span class="o">=</span> <span class="n">default_pg_timeout</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Create a separate prefix store for the helper process group.</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">PG_WRAPPER_STORE_PREFIX</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">store_prefix</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
    <span class="n">helper_pg</span> <span class="o">=</span> <span class="n">ProcessGroupGloo</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
    <span class="c1"># Wrap the underlying pg with ProcessGroupWrapper.</span>
    <span class="n">wrapped_pg</span> <span class="o">=</span> <span class="n">_ProcessGroupWrapper</span><span class="p">(</span><span class="n">wrapped_pg</span><span class="p">,</span> <span class="n">helper_pg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_pg</span>


<div class="viewcode-block" id="new_group"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.new_group">[docs]</a><span class="k">def</span> <span class="nf">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new distributed group.</span>

<span class="sd">    This function requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group. Additionally, groups</span>
<span class="sd">    should be created in the same order in all processes.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        ranks (list[int]): List of ranks of group members. If ``None``, will be</span>
<span class="sd">            set to all ranks. Default is ``None``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">        backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">            By default uses the same backend as the global group. This field</span>
<span class="sd">            should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">            also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">            ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">            corresponding to the default process group will be used. Default is</span>
<span class="sd">            ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A handle of distributed group that can be given to collective calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">global</span> <span class="n">_pg_group_ranks</span>

    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="n">default_backend</span><span class="p">,</span> <span class="n">default_store</span> <span class="o">=</span> <span class="n">_pg_map</span><span class="p">[</span><span class="n">default_pg</span><span class="p">]</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
    <span class="n">global_world_size</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># Default to the same backend as the global process group</span>
    <span class="c1"># if the backend is not specified.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">backend</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">default_backend</span>

    <span class="c1"># checks the input ranks</span>
    <span class="k">if</span> <span class="n">ranks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
        <span class="n">group_world_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">group_world_size</span> <span class="o">&gt;</span> <span class="n">global_world_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;the new group&#39;s world size should be less or &quot;</span>
                <span class="s2">&quot;equal to the world size set by &quot;</span>
                <span class="s2">&quot;init_process_group&quot;</span>
            <span class="p">)</span>
        <span class="c1"># check ranks&#39; sanity</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="n">global_world_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The new group&#39;s rank should be within the &quot;</span>
                    <span class="s2">&quot;the world_size set by init_process_group&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">global_rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="n">group_rank</span> <span class="o">=</span> <span class="n">ranks</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">global_rank</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_rank</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">global_world_size</span><span class="p">))</span>
        <span class="n">group_world_size</span> <span class="o">=</span> <span class="n">global_world_size</span>
        <span class="n">group_rank</span> <span class="o">=</span> <span class="n">global_rank</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">pg</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
        <span class="n">group_world_size</span><span class="p">,</span>
        <span class="n">group_rank</span><span class="p">,</span>
        <span class="n">ranks</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">,</span>
        <span class="n">default_store</span><span class="p">,</span>
        <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Create the global rank to group rank mapping</span>
    <span class="n">_pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">global_rank</span><span class="p">:</span> <span class="n">group_rank</span> <span class="k">for</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">global_rank</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="c1"># barrier at the end to ensure that once we return from this method, all</span>
    <span class="c1"># process groups including global variables are updated correctly on all</span>
    <span class="c1"># ranks.</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
        <span class="c1"># MPI doesn&#39;t have store.</span>
        <span class="n">barrier</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use store based barrier here since barrier() used a bunch of</span>
        <span class="c1"># default devices and messes up NCCL internal state.</span>
        <span class="n">_store_based_barrier</span><span class="p">(</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">default_store</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="c1"># Set sequence numbers for gloo and nccl process groups.</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="o">!=</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span> <span class="ow">and</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span>
            <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">pg</span><span class="o">.</span><span class="n">_set_sequence_number_for_group</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">pg</span></div>


<span class="k">def</span> <span class="nf">new_subgroups</span><span class="p">(</span>
    <span class="n">group_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates GPU subgroups of equal size. By default, it creates intra-machine subgroups,</span>
<span class="sd">    where each of which contains all the ranks of a machine, based on the assumption</span>
<span class="sd">    that each machine has the same number of CUDA devices.</span>

<span class="sd">    This is a convenience API that calls ``new_group`` to generate multiple subgroups.</span>
<span class="sd">    It requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API only works when CUDA is available.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If ``group_size`` is passed in, the world size must be divisible by ``group_size``.</span>
<span class="sd">        If no ``group_size`` is passed in, and not all the machines have the same number</span>
<span class="sd">        of devices, the subgroup division will be different across nodes and can cause</span>
<span class="sd">        unexpected behaviors.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        group_size (int, optional): The size of each subgroup. If ``None``,</span>
<span class="sd">            the default subgroup size is equal to the number of devices on each machine,</span>
<span class="sd">            based on the assumption that each machine has exactly the same</span>
<span class="sd">            number of devices. Default is ``None``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">        backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">            By default uses the same backend as the global group. This field</span>
<span class="sd">            should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">            also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">            ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">            corresponding to the default process group will be used. Default is</span>
<span class="sd">            ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The subgroup containing the current rank, and all the subgroups used for cleanup.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Create intra-machine subgroups.</span>
<span class="sd">        &gt;&gt;&gt; cur_subgroup, subgroups = dist.new_subgroups()</span>
<span class="sd">        &gt;&gt;&gt; # Allreduce within the machine.</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.ones(1, device=rank) * rank</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, group=cur_subgroup)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([8])     # Assume 8 is the number of CUDA devices per machine.</span>
<span class="sd">        &gt;&gt;&gt; # Cleanup.</span>
<span class="sd">        &gt;&gt;&gt; for subgroup in subgroups:</span>
<span class="sd">        &gt;&gt;&gt;     dist.destroy_process_group(subgroup)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Subgroups can only be created when CUDA is available&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&lt;</span> <span class="n">group_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The arg &#39;group_size&#39; must not exceed the world size&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The world size must be divisible by &#39;group_size&#39;&quot;</span><span class="p">)</span>

    <span class="n">subgroups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">subgroup_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span><span class="p">):</span>
        <span class="n">start_rank</span> <span class="o">=</span> <span class="n">subgroup_id</span> <span class="o">*</span> <span class="n">group_size</span>
        <span class="n">end_rank</span> <span class="o">=</span> <span class="n">start_rank</span> <span class="o">+</span> <span class="n">group_size</span>
        <span class="n">ranks_in_subgroup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_rank</span><span class="p">,</span> <span class="n">end_rank</span><span class="p">))</span>
        <span class="n">subgroup</span> <span class="o">=</span> <span class="n">new_group</span><span class="p">(</span>
            <span class="n">ranks</span><span class="o">=</span><span class="n">ranks_in_subgroup</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">subgroups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subgroup</span><span class="p">)</span>

        <span class="n">rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks_in_subgroup</span><span class="p">:</span>
            <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="n">subgroup</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Rank </span><span class="si">{}</span><span class="s2"> is assigned to subgroup </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">ranks_in_subgroup</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">cur_subgroup</span><span class="p">,</span> <span class="n">subgroups</span>


<span class="k">def</span> <span class="nf">new_subgroups_by_enumeration</span><span class="p">(</span>
    <span class="n">ranks_per_subgroup_list</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates GPU subgroups by dividing the global world, where the division is specified by</span>
<span class="sd">    a nested list of ranks. The subgroups cannot have overlap, and some ranks may not have</span>
<span class="sd">    to be in any subgroup.</span>

<span class="sd">    This is a convenience API that calls ``new_group`` to generate multiple subgroups.</span>
<span class="sd">    It requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        ranks_per_subgroup_list (list[list[int]]): A nested list of ranks of</span>
<span class="sd">            group members.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">         backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">             build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">             By default uses the same backend as the global group. This field</span>
<span class="sd">             should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">             also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">             ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">             corresponding to the default process group will be used. Default is</span>
<span class="sd">             ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The subgroup containing the current rank, and all the subgroups used for cleanup.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Create two subgroups, where each has 2 processes.</span>
<span class="sd">        &gt;&gt;&gt; cur_subgroup, subgroups = dist.new_subgroups(ranks=[[0, 2], [1, 3]])</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.ones(1, device=rank) * rank</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, group=cur_subgroup)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([2])     # Subgroup 0: ranks 0 and 2</span>
<span class="sd">        tensor([4])     # Subgroup 1: ranks 1 and 3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Subgroups can only be created when CUDA is available&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ranks_per_subgroup_list</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks_per_subgroup_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The arg &#39;ranks_per_subgroup_list&#39; cannot be empty&quot;</span><span class="p">)</span>

    <span class="n">world_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">()</span>

    <span class="n">subgroups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Create a mapping from rank to subgroup to check if there is any subgroup overlap.</span>
    <span class="n">rank_to_ranks_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># type: ignore[var-annotated]</span>
    <span class="k">for</span> <span class="n">ranks</span> <span class="ow">in</span> <span class="n">ranks_per_subgroup_list</span><span class="p">:</span>
        <span class="n">subgroup</span> <span class="o">=</span> <span class="n">new_group</span><span class="p">(</span>
            <span class="n">ranks</span><span class="o">=</span><span class="n">ranks</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">subgroups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subgroup</span><span class="p">)</span>
        <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">rank_to_ranks_dict</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Rank </span><span class="si">{}</span><span class="s2"> has appeared in both subgroup </span><span class="si">{}</span><span class="s2"> and </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">rank</span><span class="p">,</span> <span class="n">rank_to_ranks_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">ranks</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">rank_to_ranks_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">ranks</span>
            <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">rank</span><span class="p">:</span>
                <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="n">subgroup</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Rank </span><span class="si">{}</span><span class="s2"> is assigned to subgroup </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">ranks</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">cur_subgroup</span><span class="p">,</span> <span class="n">subgroups</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.parallel.distributed &mdash; PyTorch 1.9.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.9.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.parallel.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.parallel.distributed</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">Function</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span>

<span class="n">RPC_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="kn">import</span> <span class="n">ReduceOp</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="kn">import</span> <span class="n">_get_default_group</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">RPC_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.rpc</span> <span class="kn">import</span> <span class="n">RRef</span>
<span class="kn">from</span> <span class="nn">torch._utils</span> <span class="kn">import</span> <span class="n">_get_device_index</span>

<span class="kn">from</span> <span class="nn">..modules</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">._functions</span> <span class="kn">import</span> <span class="n">_get_stream</span>
<span class="kn">from</span> <span class="nn">.scatter_gather</span> <span class="kn">import</span> <span class="n">scatter_kwargs</span><span class="p">,</span> <span class="n">gather</span><span class="p">,</span> <span class="n">is_namedtuple</span>


<span class="k">def</span> <span class="nf">_find_tensors</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively find all tensors contained in the specified object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">RPC_AVAILABLE</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">RRef</span><span class="p">):</span>
        <span class="c1"># If the current node is the owner of the RRef, unwrap it and try to</span>
        <span class="c1"># find Tensors.</span>
        <span class="c1"># TODO: Expand to remote RRefs.</span>
        <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_owner</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">_find_tensors</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">local_value</span><span class="p">())</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">obj</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">,</span> <span class="n">obj</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
    <span class="k">return</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">_dump_DDP_relevant_env_vars</span><span class="p">():</span>
    <span class="n">relevant_env_vars</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;RANK&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">,</span>
        <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GLOO_SOCKET_IFNAME&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GLOO_DEVICE_TRANSPORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SOCKET_IFNAME&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_BLOCKING_WAIT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG_SUBSYS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_DISABLE&quot;</span><span class="p">,</span>
        <span class="c1"># More NCCL env vars:</span>
        <span class="s2">&quot;NCCL_P2P_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_P2P_LEVEL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SHM_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SOCKET_NTHREADS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NSOCKS_PERTHREAD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_BUFFSIZE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NTHREADS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_RINGS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_MAX_NCHANNELS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_MIN_NCHANNELS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_CHECKS_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_CHECK_POINTERS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_LAUNCH_MODE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_HCA&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_TIMEOUT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_RETRY_CNT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_GID_INDEX&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_SL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_TC&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_AR_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_CUDA_SUPPORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NET_GDR_LEVEL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NET_GDR_READ&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SINGLE_RING_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_LL_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TREE_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_ALGO&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_PROTO&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IGNORE_CPU_AFFINITY&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG_FILE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_COLLNET_ENABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TOPO_FILE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TOPO_DUMP_FILE&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">formatted_output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">relevant_env_vars</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="k">if</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="k">else</span> <span class="s2">&quot;N/A&quot;</span>
        <span class="n">formatted_output</span> <span class="o">+=</span> <span class="s2">&quot;env:</span><span class="si">%s</span><span class="s2">=</span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">formatted_output</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DDPUnevenInputsConfig</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">ddp_join_enabled</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">ddp_join_divide_by_initial_world_size</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">ddp_join_throw_on_early_termination</span><span class="p">:</span> <span class="nb">bool</span>

<span class="c1"># Add a DDPSink to run various functions when backwards starts, such as</span>
<span class="c1"># queueing call back of out-most backward/graph task,</span>
<span class="c1"># this helps call back is fired after all gradients&#39; calculation</span>
<span class="c1"># is completed.</span>
<span class="k">class</span> <span class="nc">_DDPSink</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">reducer</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">reducer</span> <span class="o">=</span> <span class="n">reducer</span>
        <span class="k">return</span> <span class="n">inputs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_delay_all_reduce</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements distributed data parallelism that is based on</span>
<span class="sd">    ``torch.distributed`` package at the module level.</span>

<span class="sd">    This container parallelizes the application of the given module by</span>
<span class="sd">    splitting the input across the specified devices by chunking in the batch</span>
<span class="sd">    dimension. The module is replicated on each machine and each device, and</span>
<span class="sd">    each such replica handles a portion of the input. During the backwards</span>
<span class="sd">    pass, gradients from each node are averaged.</span>

<span class="sd">    The batch size should be larger than the number of GPUs used locally.</span>

<span class="sd">    See also: :ref:`distributed-basics` and :ref:`cuda-nn-ddp-instead`.</span>
<span class="sd">    The same constraints on input as in :class:`torch.nn.DataParallel` apply.</span>

<span class="sd">    Creation of this class requires that ``torch.distributed`` to be already</span>
<span class="sd">    initialized, by calling :func:`torch.distributed.init_process_group`.</span>

<span class="sd">    ``DistributedDataParallel`` is proven to be significantly faster than</span>
<span class="sd">    :class:`torch.nn.DataParallel` for single-node multi-GPU data</span>
<span class="sd">    parallel training.</span>

<span class="sd">    To use ``DistributedDataParallel`` on a host with N GPUs, you should spawn</span>
<span class="sd">    up ``N`` processes, ensuring that each process exclusively works on a single</span>
<span class="sd">    GPU from 0 to N-1. This can be done by either setting</span>
<span class="sd">    ``CUDA_VISIBLE_DEVICES`` for every process or by calling:</span>

<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(i)</span>

<span class="sd">    where i is from 0 to N-1. In each process, you should refer the following</span>
<span class="sd">    to construct this module:</span>

<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(</span>
<span class="sd">        &gt;&gt;&gt;     backend=&#39;nccl&#39;, world_size=N, init_method=&#39;...&#39;</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)</span>

<span class="sd">    In order to spawn up multiple processes per node, you can use either</span>
<span class="sd">    ``torch.distributed.launch`` or ``torch.multiprocessing.spawn``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Please refer to `PyTorch Distributed Overview &lt;https://pytorch.org/tutorials/beginner/dist_overview.html&gt;`__</span>
<span class="sd">        for a brief introduction to all features related to distributed training.</span>

<span class="sd">    .. note::</span>
<span class="sd">        ``DistributedDataParallel`` can be used in conjunction with</span>
<span class="sd">        :class:`torch.distributed.optim.ZeroRedundancyOptimizer` to reduce</span>
<span class="sd">        per-rank optimizer states memory footprint. Please refer to</span>
<span class="sd">        `ZeroRedundancyOptimizer recipe &lt;https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html&gt;`__</span>
<span class="sd">        for more details.</span>

<span class="sd">    .. note:: ``nccl`` backend is currently the fastest and highly recommended</span>
<span class="sd">        backend when using GPUs. This applies to both single-node and</span>
<span class="sd">        multi-node distributed training.</span>

<span class="sd">    .. note:: This module also supports mixed-precision distributed training.</span>
<span class="sd">        This means that your model can have different types of parameters such</span>
<span class="sd">        as mixed types of ``fp16`` and ``fp32``, the gradient reduction on these</span>
<span class="sd">        mixed types of parameters will just work fine.</span>

<span class="sd">    .. note:: If you use ``torch.save`` on one process to checkpoint the module,</span>
<span class="sd">        and ``torch.load`` on some other processes to recover it, make sure that</span>
<span class="sd">        ``map_location`` is configured properly for every process. Without</span>
<span class="sd">        ``map_location``, ``torch.load`` would recover the module to devices</span>
<span class="sd">        where the module was saved from.</span>

<span class="sd">    .. note:: When a model is trained on ``M`` nodes with ``batch=N``, the</span>
<span class="sd">        gradient will be ``M`` times smaller when compared to the same model</span>
<span class="sd">        trained on a single node with ``batch=M*N`` if the loss is summed (NOT</span>
<span class="sd">        averaged as usual) across instances in a batch (because the gradients</span>
<span class="sd">        between different nodes are averaged). You should take this into</span>
<span class="sd">        consideration when you want to obtain a mathematically equivalent</span>
<span class="sd">        training process compared to the local training counterpart. But in most</span>
<span class="sd">        cases, you can just treat a DistributedDataParallel wrapped model, a</span>
<span class="sd">        DataParallel wrapped model and an ordinary model on a single GPU as the</span>
<span class="sd">        same (E.g. using the same learning rate for equivalent batch size).</span>

<span class="sd">    .. note::</span>
<span class="sd">        Parameters are never broadcast between processes. The module performs</span>
<span class="sd">        an all-reduce step on gradients and assumes that they will be modified</span>
<span class="sd">        by the optimizer in all processes in the same way. Buffers</span>
<span class="sd">        (e.g. BatchNorm stats) are broadcast from the module in process of rank</span>
<span class="sd">        0, to all other replicas in the system in every iteration.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If you are using DistributedDataParallel in conjunction with the</span>
<span class="sd">        :ref:`distributed-rpc-framework`, you should always use</span>
<span class="sd">        :meth:`torch.distributed.autograd.backward` to compute gradients and</span>
<span class="sd">        :class:`torch.distributed.optim.DistributedOptimizer` for optimizing</span>
<span class="sd">        parameters.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; import torch.distributed.autograd as dist_autograd</span>
<span class="sd">            &gt;&gt;&gt; from torch.nn.parallel import DistributedDataParallel as DDP</span>
<span class="sd">            &gt;&gt;&gt; from torch import optim</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.optim import DistributedOptimizer</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.rpc import RRef</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; t1 = torch.rand((3, 3), requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; t2 = torch.rand((3, 3), requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; rref = rpc.remote(&quot;worker1&quot;, torch.add, args=(t1, t2))</span>
<span class="sd">            &gt;&gt;&gt; ddp_model = DDP(my_model)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Setup optimizer</span>
<span class="sd">            &gt;&gt;&gt; optimizer_params = [rref]</span>
<span class="sd">            &gt;&gt;&gt; for param in ddp_model.parameters():</span>
<span class="sd">            &gt;&gt;&gt;     optimizer_params.append(RRef(param))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; dist_optim = DistributedOptimizer(</span>
<span class="sd">            &gt;&gt;&gt;     optim.SGD,</span>
<span class="sd">            &gt;&gt;&gt;     optimizer_params,</span>
<span class="sd">            &gt;&gt;&gt;     lr=0.05,</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; with dist_autograd.context() as context_id:</span>
<span class="sd">            &gt;&gt;&gt;     pred = ddp_model(rref.to_here())</span>
<span class="sd">            &gt;&gt;&gt;     loss = loss_func(pred, loss)</span>
<span class="sd">            &gt;&gt;&gt;     dist_autograd.backward(context_id, loss)</span>
<span class="sd">            &gt;&gt;&gt;     dist_optim.step()</span>

<span class="sd">    .. note::</span>
<span class="sd">        To let a non-DDP model load a state dict from a DDP model,</span>
<span class="sd">        :meth:`~torch.nn.modules.utils.consume_prefix_in_state_dict_if_present`</span>
<span class="sd">        needs to be applied to strip the prefix &quot;module.&quot; in the DDP state dict before loading.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Constructor, forward method, and differentiation of the output (or a</span>
<span class="sd">        function of the output of this module) are distributed synchronization</span>
<span class="sd">        points. Take that into account in case different processes might be</span>
<span class="sd">        executing different code.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model by the</span>
<span class="sd">        time it is created. No parameters should be added nor removed later.</span>
<span class="sd">        Same applies to buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model of each</span>
<span class="sd">        distributed processes are in the same order. The module itself will</span>
<span class="sd">        conduct gradient ``allreduce`` following the reverse order of the</span>
<span class="sd">        registered parameters of the model. In other words, it is users&#39;</span>
<span class="sd">        responsibility to ensure that each distributed process has the exact</span>
<span class="sd">        same model and thus the exact same parameter registration order.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module allows parameters with non-rowmajor-contiguous strides.</span>
<span class="sd">        For example, your model may contain some parameters whose</span>
<span class="sd">        :class:`torch.memory_format` is ``torch.contiguous_format``</span>
<span class="sd">        and others whose format is ``torch.channels_last``.  However,</span>
<span class="sd">        corresponding parameters in different processes must have the</span>
<span class="sd">        same strides.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module doesn&#39;t work with :func:`torch.autograd.grad` (i.e. it will</span>
<span class="sd">        only work if gradients are to be accumulated in ``.grad`` attributes of</span>
<span class="sd">        parameters).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If you plan on using this module with a ``nccl`` backend or a ``gloo``</span>
<span class="sd">        backend (that uses Infiniband), together with a DataLoader that uses</span>
<span class="sd">        multiple workers, please change the multiprocessing start method to</span>
<span class="sd">        ``forkserver`` (Python 3 only) or ``spawn``. Unfortunately</span>
<span class="sd">        Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will</span>
<span class="sd">        likely experience deadlocks if you don&#39;t change this setting.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Forward and backward hooks defined on :attr:`module` and its submodules</span>
<span class="sd">        won&#39;t be invoked anymore, unless the hooks are initialized in the</span>
<span class="sd">        :meth:`forward` method.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        You should never try to change your model&#39;s parameters after wrapping</span>
<span class="sd">        up your model with ``DistributedDataParallel``. Because, when</span>
<span class="sd">        wrapping up your model with ``DistributedDataParallel``, the constructor</span>
<span class="sd">        of ``DistributedDataParallel`` will register the additional gradient</span>
<span class="sd">        reduction functions on all the parameters of the model itself at the</span>
<span class="sd">        time of construction. If you change the model&#39;s parameters afterwards,</span>
<span class="sd">        gradient redunction functions no longer match the correct set of</span>
<span class="sd">        parameters.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using ``DistributedDataParallel`` in conjunction with the</span>
<span class="sd">        :ref:`distributed-rpc-framework` is experimental and subject to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (Module): module to be parallelized</span>
<span class="sd">        device_ids (list of int or torch.device): CUDA devices.</span>
<span class="sd">                   1) For single-device modules, ``device_ids`` can</span>
<span class="sd">                   contain exactly one device id, which represents the only</span>
<span class="sd">                   CUDA device where the input module corresponding to this process resides.</span>
<span class="sd">                   Alternatively, ``device_ids`` can also be ``None``.</span>
<span class="sd">                   2) For multi-device modules and CPU modules,</span>
<span class="sd">                   ``device_ids`` must be ``None``.</span>

<span class="sd">                   When ``device_ids`` is ``None`` for both cases,</span>
<span class="sd">                   both the input data for the forward pass and the actual module</span>
<span class="sd">                   must be placed on the correct device.</span>
<span class="sd">                   (default: ``None``)</span>
<span class="sd">        output_device (int or torch.device): Device location of output for</span>
<span class="sd">                      single-device CUDA modules. For multi-device modules and</span>
<span class="sd">                      CPU modules, it must be ``None``, and the module itself</span>
<span class="sd">                      dictates the output location. (default: ``device_ids[0]``</span>
<span class="sd">                      for single-device modules)</span>
<span class="sd">        broadcast_buffers (bool): Flag that enables syncing (broadcasting)</span>
<span class="sd">                          buffers of the module at beginning of the ``forward``</span>
<span class="sd">                          function. (default: ``True``)</span>
<span class="sd">        process_group: The process group to be used for distributed data</span>
<span class="sd">                       all-reduction. If ``None``, the default process group, which</span>
<span class="sd">                       is created by :func:`torch.distributed.init_process_group`,</span>
<span class="sd">                       will be used. (default: ``None``)</span>
<span class="sd">        bucket_cap_mb: ``DistributedDataParallel`` will bucket parameters into</span>
<span class="sd">                       multiple buckets so that gradient reduction of each</span>
<span class="sd">                       bucket can potentially overlap with backward computation.</span>
<span class="sd">                       :attr:`bucket_cap_mb` controls the bucket size in</span>
<span class="sd">                       MegaBytes (MB). (default: 25)</span>
<span class="sd">        find_unused_parameters (bool): Traverse the autograd graph from all</span>
<span class="sd">                               tensors contained in the return value of the</span>
<span class="sd">                               wrapped module&#39;s ``forward`` function. Parameters</span>
<span class="sd">                               that don&#39;t receive gradients as part of this</span>
<span class="sd">                               graph are preemptively marked as being ready to</span>
<span class="sd">                               be reduced. Note that all ``forward`` outputs</span>
<span class="sd">                               that are derived from module parameters must</span>
<span class="sd">                               participate in calculating loss and later the</span>
<span class="sd">                               gradient computation. If they don&#39;t, this wrapper</span>
<span class="sd">                               will hang waiting for autograd to produce</span>
<span class="sd">                               gradients for those parameters. Any outputs</span>
<span class="sd">                               derived from module parameters that are otherwise</span>
<span class="sd">                               unused can be detached from the autograd graph</span>
<span class="sd">                               using ``torch.Tensor.detach``. (default: ``False``)</span>
<span class="sd">        check_reduction: This argument is deprecated.</span>
<span class="sd">        gradient_as_bucket_view (bool): When set to ``True``, gradients will be views</span>
<span class="sd">                      pointing to different offsets of ``allreduce`` communication</span>
<span class="sd">                      buckets. This can reduce peak memory usage, where the</span>
<span class="sd">                      saved memory size will be equal to the total gradients</span>
<span class="sd">                      size. Moreover, it avoids the overhead of copying between</span>
<span class="sd">                      gradients and ``allreduce`` communication buckets. When</span>
<span class="sd">                      gradients are views, ``detach_()`` cannot be called on the</span>
<span class="sd">                      gradients. If hitting such errors, please fix it by</span>
<span class="sd">                      referring to the :meth:`~torch.optim.Optimizer.zero_grad`</span>
<span class="sd">                      function in ``torch/optim/optimizer.py`` as a solution.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        module (Module): the module to be parallelized.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)</span>
<span class="sd">        &gt;&gt;&gt; net = torch.nn.parallel.DistributedDataParallel(model, pg)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">broadcast_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
        <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">check_reduction</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="nb">any</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())),</span> <span class="p">(</span>
            <span class="s2">&quot;DistributedDataParallel is not needed when a module &quot;</span>
            <span class="s2">&quot;doesn&#39;t have any parameter that requires a gradient.&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;device_ids can only be None or contain a single element.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_device_module</span> <span class="o">=</span> <span class="nb">len</span><span class="p">({</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()})</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">distinct_device_types</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">distinct_device_types</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;DistributedDataParallel&#39;s input module must be on &quot;</span>
                <span class="s2">&quot;the same type of devices, but input module parameters locate in </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">distinct_device_types</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">distinct_device_types</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># For backward compatibility.</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_device_module</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">or</span> <span class="n">output_device</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;DistributedDataParallel device_ids and output_device arguments &quot;</span>
                    <span class="s2">&quot;only work with single-device/multiple-device GPU modules or CPU modules, &quot;</span>
                    <span class="s2">&quot;but got device_ids </span><span class="si">{}</span><span class="s2">, output_device </span><span class="si">{}</span><span class="s2">, and module parameters </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">device_ids</span><span class="p">,</span>
                        <span class="n">output_device</span><span class="p">,</span>
                        <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_device_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">output_device</span> <span class="o">=</span> <span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">output_device</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span> <span class="o">=</span> <span class="n">broadcast_buffers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="o">=</span> <span class="n">find_unused_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span> <span class="o">=</span> <span class="n">_DDPUnevenInputsConfig</span><span class="p">(</span>
            <span class="n">ddp_join_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ddp_join_divide_by_initial_world_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ddp_join_throw_on_early_termination</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_as_bucket_view</span> <span class="o">=</span> <span class="n">gradient_as_bucket_view</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;_ddp_params_and_buffers_to_ignore&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">_ddp_params_and_buffers_to_ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">check_reduction</span><span class="p">:</span>
            <span class="c1"># This argument is no longer used since the reducer</span>
            <span class="c1"># will ensure reduction completes even if some parameters</span>
            <span class="c1"># do not receive gradients.</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `check_reduction` argument in `DistributedDataParallel` &quot;</span>
                <span class="s2">&quot;module is deprecated. Please avoid using it.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check that a module does not have Uninitialized parameters</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">UninitializedParameter</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Modules with uninitialized parameters can&#39;t be used with `DistributedDataParallel`. &quot;</span>
                    <span class="s2">&quot;Run a dummy forward pass to correctly initialize the modules&quot;</span>
                <span class="p">)</span>
        <span class="c1"># used for intra-node param sync and inter-node sync as wel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>

        <span class="c1"># reduction bucket size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">bucket_cap_mb</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="c1"># Whether to perform input tensor CPU to GPU copies on a side-stream</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_side_stream_for_tensor_copies</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PYTORCH_DDP_USE_SIDE_STREAM&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span>
        <span class="p">)</span>

        <span class="c1"># TODO(wayi@): Remove this field since SPMD is no longer supported,</span>
        <span class="c1"># and also remove all the relevant unnecessary loops.</span>
        <span class="c1"># Module replication within process (single-process multi device)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">]</span>
        <span class="c1"># Build parameters for reducer.</span>
        <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_params_for_reducer</span><span class="p">()</span>
        <span class="c1"># Verify model equivalence.</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_verify_model_across_ranks</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># Sync params and buffers. Ensures all DDP models start off at the same value.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_params_and_buffers</span><span class="p">(</span><span class="n">authoritative_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># In debug mode, build a mapping of parameter index -&gt; parameter.</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">_get_debug_mode</span><span class="p">()</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_DistributedDebugLevel</span><span class="o">.</span><span class="n">OFF</span><span class="p">:</span>
            <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_param_to_name_mapping</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Builds reducer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span><span class="p">,</span> <span class="n">param_to_name_mapping</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sync_params_and_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">module_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span><span class="p">:</span>
                <span class="n">module_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_states</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_broadcast_coalesced</span><span class="p">(</span>
                <span class="n">module_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">,</span> <span class="n">authoritative_rank</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_ddp_init_helper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span><span class="p">,</span> <span class="n">param_to_name_mapping</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization helper function that does the following:</span>
<span class="sd">        (1) bucketing the parameters for reductions</span>
<span class="sd">        (2) resetting the bucketing states</span>
<span class="sd">        (3) registering the grad hooks</span>
<span class="sd">        (4) Logging constructin-time DDP logging data</span>
<span class="sd">        (5) passing a handle of DDP to SyncBatchNorm Layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># The bucket size limit is specified in the constructor.</span>
        <span class="c1"># Additionally, we allow for a single small bucket for parameters</span>
        <span class="c1"># that are defined first, such that their gradients don&#39;t spill into</span>
        <span class="c1"># a much larger bucket, adding unnecessary latency after gradient</span>
        <span class="c1"># computation finishes. Experiments showed 1MB is a reasonable value.</span>
        <span class="n">bucket_indices</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_compute_bucket_assignment_by_size</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">_DEFAULT_FIRST_BUCKET_BYTES</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span><span class="p">],</span>
            <span class="n">expect_sparse_gradient</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># Note: reverse list of buckets because we want to approximate the</span>
        <span class="c1"># order in which their gradients are produced, and assume they</span>
        <span class="c1"># are used in the forward pass in the order they are defined.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Reducer</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">,</span>
            <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">bucket_indices</span><span class="p">)),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
            <span class="n">expect_sparse_gradient</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_as_bucket_view</span><span class="p">,</span>
            <span class="n">param_to_name_mapping</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">)</span>

        <span class="c1"># Set logging data that can be got during construction time.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_construction_data_and_log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="p">[]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">,</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># passing a handle to torch.nn.SyncBatchNorm layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_passing_sync_batchnorm_handle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_default_group</span><span class="p">()</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;process_group&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;reducer&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;logger&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">attrs</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># If serializable, then the process group should be the default one</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;require_forward_param_sync&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;require_backward_grad_sync&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_params_for_reducer</span><span class="p">()</span>
        <span class="c1"># In debug mode, build a mapping of parameter index -&gt; parameter.</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">_get_debug_mode</span><span class="p">()</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_DistributedDebugLevel</span><span class="o">.</span><span class="n">OFF</span><span class="p">:</span>
            <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_param_to_name_mapping</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Builds reducer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span><span class="p">,</span> <span class="n">param_to_name_mapping</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_params_for_reducer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Build tuple of (module, parameter) for all parameters that require grads.</span>
        <span class="n">modules_and_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span>
                <span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">replica</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">param</span>
                    <span class="c1"># Note that we access module.named_parameters instead of</span>
                    <span class="c1"># parameters(module). parameters(module) is only needed in the</span>
                    <span class="c1"># single-process multi device case, where it accesses replicated</span>
                    <span class="c1"># parameters through _former_parameters.</span>
                    <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="ow">and</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span>
                <span class="p">]</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">replica</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span>
        <span class="p">]</span>

        <span class="c1"># Deduplicate any parameters that might be shared across child modules.</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">modules_and_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># &quot;p not in memo&quot; is the deduplication check.</span>
            <span class="c1"># &quot;not memo.add(p)&quot; is always True, and it&#39;s only there to cause &quot;add(p)&quot; if needed.</span>
            <span class="p">[(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">replica_mps</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">replica_mps</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span>
        <span class="p">]</span>

        <span class="c1"># Build list of parameters.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">list</span><span class="p">(</span><span class="n">parameter</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">replica</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">replica</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span>
        <span class="p">]</span>

        <span class="c1"># Checks if a module will produce a sparse gradient.</span>
        <span class="k">def</span> <span class="nf">produces_sparse_gradient</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="n">sparse</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Build list of booleans indicating whether or not to expect sparse</span>
        <span class="c1"># gradients for the corresponding parameters.</span>
        <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">list</span><span class="p">(</span><span class="n">produces_sparse_gradient</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">replica</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">replica</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span>
        <span class="p">]</span>

        <span class="c1"># The following modules_params and modules_buffers are used for</span>
        <span class="c1"># param/buffer sync in _sync_params.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules_params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">))</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span>
        <span class="p">]</span>
        <span class="c1"># Collect buffers for modules, filtering out buffers that should be ignored.</span>
        <span class="n">named_module_buffers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()]</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span>
                <span class="n">buffer</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span> <span class="ow">in</span> <span class="n">module_buffers</span>
                <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">module_buffers</span> <span class="ow">in</span> <span class="n">named_module_buffers</span>
        <span class="p">]</span>

        <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span>

    <span class="k">def</span> <span class="nf">_build_param_to_name_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
        <span class="n">param_to_param_index</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="p">}</span>
        <span class="n">param_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">param_index_to_param_fqn</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="c1"># Bypass ignored parameters since those are not reduced by DDP</span>
                <span class="c1"># to begin with.</span>
                <span class="k">if</span> <span class="n">fqn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_set</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Param with name </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> found in module parameters, but not DDP parameters.&quot;</span>
                            <span class="s2">&quot; This indicates a bug in DDP, please report an issue to PyTorch.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">param_index</span> <span class="o">=</span> <span class="n">param_to_param_index</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                    <span class="n">param_index_to_param_fqn</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fqn</span>

        <span class="c1"># Ensure we covered all parameters</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_set</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_index_to_param_fqn</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Expected param to name mapping to cover all parameters, but&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; got conflicting lengths: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> vs &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_index_to_param_fqn</span><span class="p">)</span><span class="si">}</span><span class="s2">. This indicates a bug in DDP&quot;</span>
                    <span class="s2">&quot;, please report an issue to PyTorch.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">param_index_to_param_fqn</span>

    <span class="k">def</span> <span class="nf">_get_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a generator of module parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">model_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">ps</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_former_parameters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;_former_parameters&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">p</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span> <span class="k">if</span> <span class="n">recurse</span> <span class="k">else</span> <span class="p">[</span><span class="n">m</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">_check_default_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">!=</span> <span class="n">_get_default_group</span><span class="p">():</span>
                <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">pickle_not_supported</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;DDP Pickling/Unpickling are only supported &quot;</span>
                <span class="s2">&quot;when using DDP with the default process &quot;</span>
                <span class="s2">&quot;group. That is, when you have called &quot;</span>
                <span class="s2">&quot;init_process_group and have not passed &quot;</span>
                <span class="s2">&quot;process_group argument to DDP constructor&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel.no_sync"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">no_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to disable gradient synchronizations across DDP</span>
<span class="sd">        processes. Within this context, gradients will be accumulated on module</span>
<span class="sd">        variables, which will later be synchronized in the first</span>
<span class="sd">        forward-backward pass exiting the context.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; ddp = torch.nn.parallel.DistributedDataParallel(model, pg)</span>
<span class="sd">            &gt;&gt;&gt; with ddp.no_sync():</span>
<span class="sd">            &gt;&gt;&gt;   for input in inputs:</span>
<span class="sd">            &gt;&gt;&gt;     ddp(input).backward()  # no synchronization, accumulate grads</span>
<span class="sd">            &gt;&gt;&gt; ddp(another_input).backward()  # synchronize grads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_require_backward_grad_sync</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="n">old_require_backward_grad_sync</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;DistributedDataParallel.forward&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">save_thread_local_state</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_runtime_stats_and_log</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_forward</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_enabled</span><span class="p">:</span>
                <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">work</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_throw_on_early_termination</span><span class="p">:</span>
                    <span class="c1"># Active ranks schedule an allreduce with zeros, inactive</span>
                    <span class="c1"># ranks schedule them with 1. If the result != 0 it</span>
                    <span class="c1"># indicates at least one rank has terminated and we should</span>
                    <span class="c1"># throw.</span>
                    <span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">zeros</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
                    <span class="n">should_throw_stop_iteration</span> <span class="o">=</span> <span class="n">zeros</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">should_throw_stop_iteration</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;Detected at least one rank that exhausted inputs. Throwing across all ranks.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_forward_pass_work_handle</span><span class="p">(</span>
                        <span class="n">work</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_divide_by_initial_world_size</span><span class="p">,</span>
                    <span class="p">)</span>

            <span class="c1"># Calling _rebuild_buckets before forward compuation,</span>
            <span class="c1"># It may allocate new buckets before deallocating old buckets</span>
            <span class="c1"># inside _rebuild_buckets. To save peak memory usage,</span>
            <span class="c1"># call _rebuild_buckets before the peak memory usage increases</span>
            <span class="c1"># during forward computation.</span>
            <span class="c1"># This should be called only once during whole training period.</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_rebuild_buckets</span><span class="p">():</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Reducer buckets have been rebuilt in this iteration.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_params</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_enabled</span><span class="p">:</span>
                <span class="c1"># Notify joined ranks whether they should sync in backwards pass or not.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_check_global_requires_backward_grad_sync</span><span class="p">(</span><span class="n">is_joined_rank</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">:</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># We&#39;ll return the output object verbatim since it is a freeform</span>
                <span class="c1"># object. We need to find any tensors in this object, though,</span>
                <span class="c1"># because we need to figure out which parameters were used during</span>
                <span class="c1"># this forward pass, to ensure we short circuit reduction for any</span>
                <span class="c1"># unused parameters. Only if `find_unused_parameters` is set.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">:</span>
                    <span class="c1"># Do not need to populate this for static graph.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_backward</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">(</span><span class="n">output</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_backward</span><span class="p">([])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># TODO. Right now we add this sink for static_graph training only. once</span>
        <span class="c1"># this feature is stable, we will add this sink for all cases. E.g.</span>
        <span class="c1"># This sink can help capture more accuracte backward start time as well.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Need to grab list of tensors from user output in order to pass</span>
            <span class="c1"># to custom autograd function.</span>
            <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">treespec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">passthrough_tensor_list</span> <span class="o">=</span> <span class="n">_DDPSink</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">,</span>
                <span class="o">*</span><span class="n">output_tensor_list</span>
            <span class="p">)</span>
            <span class="c1"># Reconstruct output data structure.</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">passthrough_tensor_list</span><span class="p">,</span> <span class="n">treespec</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_recursive_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">target_gpu</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively moves input to the target_gpu.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">to_map</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">target_gpu</span><span class="p">):</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">obj</span><span class="p">,)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_side_stream_for_tensor_copies</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target_gpu</span><span class="p">),)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Perform CPU -&gt; GPU copies in a background stream. This code is</span>
                    <span class="c1"># motivated from similar logic in torch/nn/parallel/_functions.py</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">_get_stream</span><span class="p">(</span><span class="n">target_gpu</span><span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
                        <span class="n">output</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target_gpu</span><span class="p">)</span>
                    <span class="c1"># synchronize with the copy stream</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">target_gpu</span><span class="p">):</span>
                        <span class="n">current_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                        <span class="c1"># Sync the current stream with the copy stream</span>
                        <span class="n">current_stream</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
                        <span class="c1"># Ensure tensor memory is not reused until work on</span>
                        <span class="c1"># main stream is complete</span>
                        <span class="n">output</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
            <span class="k">if</span> <span class="n">is_namedtuple</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">to_map</span><span class="p">,</span> <span class="n">obj</span><span class="p">))]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">to_map</span><span class="p">,</span> <span class="n">obj</span><span class="p">)))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">to_map</span><span class="p">,</span> <span class="n">obj</span><span class="p">))]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">to_map</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()))]</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">obj</span><span class="p">]</span>

        <span class="c1"># Avoid reference cycle</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">to_map</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">to_map</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">to_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recursive_to</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span> <span class="k">if</span> <span class="n">inputs</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recursive_to</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">extend</span><span class="p">([()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))])</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">extend</span><span class="p">([{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">))])</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="c1"># When running in join mode, schedules an allreduce to match the one in the</span>
    <span class="c1"># forward pass to determine the no. of currently active processes and whether</span>
    <span class="c1"># all processes have joined.</span>
    <span class="k">def</span> <span class="nf">_schedule_shadow_all_reduce_for_fwd_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">all_active_procs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">all_active_procs</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_active_procs</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># When running in join mode, schedules an allreduce to notify joined ranks</span>
    <span class="c1"># of whether backwards pass synchronization will run this iteraton or not.</span>
    <span class="k">def</span> <span class="nf">_check_global_requires_backward_grad_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_joined_rank</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_joined_rank</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
            <span class="n">requires_sync_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">requires_sync_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">work</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
            <span class="n">requires_sync_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">work</span><span class="p">,</span> <span class="n">requires_sync_tensor</span>

    <span class="c1"># When running in join mode, checks and performs sync of module buffers if</span>
    <span class="c1"># the models have buffers that should be synchronized in the forward pass.</span>
    <span class="k">def</span> <span class="nf">_check_and_sync_module_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">will_sync_module_buffers</span><span class="p">():</span>
            <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_broadcast_coalesced</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">,</span> <span class="n">authoritative_rank</span>
            <span class="p">)</span>

    <span class="c1"># When running in join model, agrees upon a common rank and broadcast model</span>
    <span class="c1"># parameters to all other ranks.</span>
    <span class="k">def</span> <span class="nf">_sync_final_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_last_joiner</span><span class="p">):</span>
        <span class="c1"># Agree upon the process that will be the authoritative model copy.</span>
        <span class="c1"># The current rank is a candidate for being the authoritative copy if</span>
        <span class="c1"># is_last_joiner=True. We break ties via picking the larger rank.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="n">is_last_joiner</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_params_and_buffers</span><span class="p">(</span><span class="n">authoritative_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_authoritative_rank</span><span class="p">)</span>

    <span class="c1"># Schedule allreduce ops to match those scheduled in the reducer&#39;s backward</span>
    <span class="c1"># pass.</span>
    <span class="k">def</span> <span class="nf">_match_all_reduce_for_bwd_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">allreduce_work</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Schedule allreduce in the same order as Reducer schedules them, i.e.</span>
        <span class="c1"># the order of the buckets. Retrieving the bucket order from the reducer</span>
        <span class="c1"># ensures that we keep the same order in join mode, such as when bucket</span>
        <span class="c1"># order is rebuilt dynamically.</span>
        <span class="n">all_bucket_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">get_bucket_tensors</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">bucket_tensors</span> <span class="ow">in</span> <span class="n">all_bucket_tensors</span><span class="p">:</span>
            <span class="c1"># Joined processes contribute zero gradient. In the case that</span>
            <span class="c1"># divide_by_initial_world_size=True, we divide grads by the static</span>
            <span class="c1"># world size, if not, the dividing factor is reduced by the number</span>
            <span class="c1"># of joined processes.</span>
            <span class="n">zero_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">bucket_tensors</span><span class="p">]</span>
            <span class="n">work</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">zero_tensors</span><span class="p">)</span>
            <span class="n">allreduce_work</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">work</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">work</span> <span class="ow">in</span> <span class="n">allreduce_work</span><span class="p">:</span>
            <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

    <span class="c1"># Allreduces the used parameter mapping across ranks.</span>
    <span class="k">def</span> <span class="nf">_match_unused_params_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">locally_used_param_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_get_local_used_maps</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">locally_used_param_maps</span><span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel.join"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">join</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">divide_by_initial_world_size</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">enable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">throw_on_early_termination</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to be used in conjunction with an instance of</span>
<span class="sd">        :class:`torch.nn.parallel.DistributedDataParallel` to be</span>
<span class="sd">        able to train with uneven inputs across participating processes.</span>

<span class="sd">        This context manager will keep track of already-joined DDP processes,</span>
<span class="sd">        and &quot;shadow&quot; the forward and backward passes by inserting collective</span>
<span class="sd">        communication operations to match with the ones created by non-joined</span>
<span class="sd">        DDP processes. This will ensure each collective call has a corresponding</span>
<span class="sd">        call by already-joined DDP processes, preventing hangs or errors that</span>
<span class="sd">        would otherwise happen when training with uneven inputs across</span>
<span class="sd">        processes. Alternatively, if the flag ``throw_on_early_termination`` is</span>
<span class="sd">        specified to be ``True``, all trainers will throw an error once one rank</span>
<span class="sd">        runs out of inputs, allowing these errors to be caught and handled</span>
<span class="sd">        according to application logic.</span>

<span class="sd">        Once all DDP processes have joined, the context manager will broadcast</span>
<span class="sd">        the model corresponding to the last joined process to all processes to</span>
<span class="sd">        ensure the model is the same across all processes</span>
<span class="sd">        (which is guaranteed by DDP).</span>

<span class="sd">        To use this to enable training with uneven inputs across processes,</span>
<span class="sd">        simply wrap this context manager around your training loop. No further</span>
<span class="sd">        modifications to the model or data loading is required.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If the model or training loop this context manager is wrapped around</span>
<span class="sd">            has additional distributed collective operations, such as</span>
<span class="sd">            ``SyncBatchNorm`` in the model&#39;s forward pass, then the flag</span>
<span class="sd">            ``throw_on_early_termination`` must be enabled. This is because this</span>
<span class="sd">            context manager is not aware of non-DDP collective communication.</span>
<span class="sd">            This flag will cause all ranks to throw when any one rank</span>
<span class="sd">            exhausts inputs, allowing these errors to be caught and recovered</span>
<span class="sd">            from across all ranks.</span>

<span class="sd">        Args:</span>
<span class="sd">            divide_by_initial_world_size (bool): If ``True``, will divide</span>
<span class="sd">                gradients by the initial ``world_size`` DDP training was launched</span>
<span class="sd">                with. If ``False``, will compute the effective world size</span>
<span class="sd">                (number of ranks that have not depleted their inputs yet) and</span>
<span class="sd">                divide gradients by that during allreduce. Set</span>
<span class="sd">                ``divide_by_initial_world_size=True`` to ensure every input</span>
<span class="sd">                sample including the uneven inputs have equal weight in terms of</span>
<span class="sd">                how much they contribute to the global gradient. This is</span>
<span class="sd">                achieved by always dividing the gradient by the initial</span>
<span class="sd">                ``world_size`` even when we encounter uneven inputs. If you set</span>
<span class="sd">                this to ``False``, we divide the gradient by the remaining</span>
<span class="sd">                number of nodes. This ensures parity with training on a smaller</span>
<span class="sd">                ``world_size`` although it also means the uneven inputs would</span>
<span class="sd">                contribute more towards the global gradient. Typically, you</span>
<span class="sd">                would want to set this to ``True`` for cases where the last few</span>
<span class="sd">                inputs of your training job are uneven. In extreme cases, where</span>
<span class="sd">                there is a large discrepancy in the number of inputs, setting</span>
<span class="sd">                this to ``False`` might provide better results.</span>
<span class="sd">            enable (bool): Whether to enable uneven input detection or not. Pass</span>
<span class="sd">                in ``enable=False`` to disable in cases where you know that</span>
<span class="sd">                inputs are even across participating processes. Default is</span>
<span class="sd">                ``True``.</span>
<span class="sd">            throw_on_early_termination (bool): Whether to throw an error</span>
<span class="sd">                or continue training when at least one rank has exhausted</span>
<span class="sd">                inputs. If ``True``, will throw upon the first rank reaching end</span>
<span class="sd">                of data. If ``False``, will continue training with a smaller</span>
<span class="sd">                effective world size until all ranks are joined. Note that if</span>
<span class="sd">                this flag is specified, then the flag</span>
<span class="sd">                ``divide_by_initial_world_size`` would be ignored. Default</span>
<span class="sd">                is ``False``.</span>


<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt;  import torch</span>
<span class="sd">          &gt;&gt;&gt;  import torch.distributed as dist</span>
<span class="sd">          &gt;&gt;&gt;  import os</span>
<span class="sd">          &gt;&gt;&gt;  import torch.multiprocessing as mp</span>
<span class="sd">          &gt;&gt;&gt;  import torch.nn as nn</span>
<span class="sd">          &gt;&gt;&gt;  # On each spawned worker</span>
<span class="sd">          &gt;&gt;&gt;  def worker(rank):</span>
<span class="sd">          &gt;&gt;&gt;      dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=2)</span>
<span class="sd">          &gt;&gt;&gt;      torch.cuda.set_device(rank)</span>
<span class="sd">          &gt;&gt;&gt;      model = nn.Linear(1, 1, bias=False).to(rank)</span>
<span class="sd">          &gt;&gt;&gt;      model = torch.nn.parallel.DistributedDataParallel(</span>
<span class="sd">          &gt;&gt;&gt;          model, device_ids=[rank], output_device=rank</span>
<span class="sd">          &gt;&gt;&gt;      )</span>
<span class="sd">          &gt;&gt;&gt;      # Rank 1 gets one more input than rank 0.</span>
<span class="sd">          &gt;&gt;&gt;      inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]</span>
<span class="sd">          &gt;&gt;&gt;      with model.join():</span>
<span class="sd">          &gt;&gt;&gt;          for _ in range(5):</span>
<span class="sd">          &gt;&gt;&gt;              for inp in inputs:</span>
<span class="sd">          &gt;&gt;&gt;                  loss = model(inp).sum()</span>
<span class="sd">          &gt;&gt;&gt;                  loss.backward()</span>
<span class="sd">          &gt;&gt;&gt;  # Without the join() API, the below synchronization will hang</span>
<span class="sd">          &gt;&gt;&gt;  # blocking for rank 1&#39;s allreduce to complete.</span>
<span class="sd">          &gt;&gt;&gt;  torch.cuda.synchronize(device=rank)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Log uneven input API usage.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_uneven_input_join</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">has_error</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span> <span class="o">=</span> <span class="n">_DDPUnevenInputsConfig</span><span class="p">(</span>
                <span class="n">ddp_join_enabled</span><span class="o">=</span><span class="n">enable</span><span class="p">,</span>
                <span class="n">ddp_join_divide_by_initial_world_size</span><span class="o">=</span><span class="n">divide_by_initial_world_size</span><span class="p">,</span>
                <span class="n">ddp_join_throw_on_early_termination</span><span class="o">=</span><span class="n">throw_on_early_termination</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Set to skip any processing in the finally block.</span>
            <span class="n">has_error</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">raise</span> <span class="n">e</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Skip any processing to let the exception immediately be raised if</span>
            <span class="c1"># there was one.</span>
            <span class="k">if</span> <span class="n">enable</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_error</span><span class="p">:</span>
                <span class="n">all_procs_joined</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">is_last_joiner</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">WARN_THRESHOLD</span> <span class="o">=</span> <span class="mi">1000</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;once&quot;</span><span class="p">)</span>
                <span class="k">while</span> <span class="ow">not</span> <span class="n">all_procs_joined</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">WARN_THRESHOLD</span><span class="p">:</span>
                        <span class="n">my_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                            <span class="s2">&quot;Detected uneven input skew of greater &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;than </span><span class="si">{</span><span class="n">WARN_THRESHOLD</span><span class="si">}</span><span class="s2">. This means that rank </span><span class="si">{</span><span class="n">my_rank</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;has at least </span><span class="si">{</span><span class="n">WARN_THRESHOLD</span><span class="si">}</span><span class="s2"> fewer inputs than &quot;</span>
                            <span class="s2">&quot;other currently active ranks. This level of skew could &quot;</span>
                            <span class="s2">&quot;lead to performance degradation during training.&quot;</span>
                        <span class="p">)</span>
                    <span class="c1"># Schedules allreduce to match fwd pass allreduce in non-joined procs</span>
                    <span class="n">num_active_procs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_schedule_shadow_all_reduce_for_fwd_pass</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">num_active_procs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">all_procs_joined</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Some DDP process still needs to be joined.</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_throw_on_early_termination</span><span class="p">:</span>
                            <span class="c1"># Schedule allreduce telling active ranks to terminate</span>
                            <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
                            <span class="c1"># Raising StopIteration doesn&#39;t throw error in python 3.6</span>
                            <span class="c1"># and throws RuntimeError in 3.7+ (PEP 479), so just</span>
                            <span class="c1"># raise RuntimeError here.</span>
                            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="si">}</span><span class="s2"> exhausted all inputs.&quot;</span>
                            <span class="p">)</span>
                        <span class="k">if</span> <span class="n">is_last_joiner</span><span class="p">:</span>
                            <span class="n">is_last_joiner</span> <span class="o">=</span> <span class="kc">False</span>
                        <span class="c1"># It will rebuild buckets only once during training period</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_rebuild_buckets</span><span class="p">()</span>
                        <span class="c1"># Schedule a corresponding broadcast if we are syncing module</span>
                        <span class="c1"># buffers in the forward pass.</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_sync_module_buffers</span><span class="p">()</span>

                        <span class="p">(</span>
                            <span class="n">work</span><span class="p">,</span>
                            <span class="n">should_sync_backwards_tensor</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_global_requires_backward_grad_sync</span><span class="p">(</span>
                            <span class="n">is_joined_rank</span><span class="o">=</span><span class="kc">True</span>
                        <span class="p">)</span>
                        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
                        <span class="c1"># If nonzero, then we should sync in the bwd pass.</span>
                        <span class="n">should_sync_backwards</span> <span class="o">=</span> <span class="n">should_sync_backwards_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span>
                        <span class="c1"># Forward param sync is disabled in the next iteration</span>
                        <span class="c1"># if we are skipping grad sync this iteration. Hence, we</span>
                        <span class="c1"># set require_forward_param_sync appropriately here.</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="n">should_sync_backwards</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">should_sync_backwards</span><span class="p">:</span>
                            <span class="k">continue</span>
                        <span class="c1"># Schedules one allreduce per gradient bucket to match</span>
                        <span class="c1"># the backwards pass allreduce.</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_match_all_reduce_for_bwd_pass</span><span class="p">()</span>
                        <span class="c1"># Check if we need to allreduce locally unused params.</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_match_unused_params_allreduce</span><span class="p">()</span>
                        <span class="c1"># It will push rebuilt params only once during training period</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_push_all_rebuilt_params</span><span class="p">()</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># All procs joined. Agree on authoritative rank and broadcast the model.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_final_model</span><span class="p">(</span><span class="n">is_last_joiner</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistributedDataParallel.register_comm_hook"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">callable</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a communication hook which is an enhancement that provides a</span>
<span class="sd">        flexible hook to users where they can specify how DDP aggregates gradients</span>
<span class="sd">        across multiple workers.</span>

<span class="sd">        This hook would be very useful for researchers to try out new ideas. For</span>
<span class="sd">        example, this hook can be used to implement several algorithms like GossipGrad</span>
<span class="sd">        and gradient compression which involve different communication strategies for</span>
<span class="sd">        parameter syncs while running Distributed DataParallel training.</span>

<span class="sd">        Args:</span>
<span class="sd">            state (object): Passed to the hook to maintain any state information during the training process.</span>
<span class="sd">                            Examples include error feedback in gradient compression,</span>
<span class="sd">                            peers to communicate with next in GossipGrad, etc.</span>

<span class="sd">                            It is locally stored by each worker</span>
<span class="sd">                            and shared by all the gradient tensors on the worker.</span>
<span class="sd">            hook (callable): Averages gradient tensors across workers and defined as:</span>
<span class="sd">                             ``hook(state: object, bucket: dist.GradBucket) -&gt; torch.futures.Future``:</span>

<span class="sd">                             This function is called once the bucket is ready. The</span>
<span class="sd">                             hook can perform whatever processing is needed and return</span>
<span class="sd">                             a Future indicating completion of any async work (ex: allreduce).</span>
<span class="sd">                             If the hook doesn&#39;t perform any communication, it can also</span>
<span class="sd">                             just return a completed Future. The Future should hold the</span>
<span class="sd">                             new value of grad bucket&#39;s tensors. Once a bucket is ready,</span>
<span class="sd">                             c10d reducer would call this hook and use the tensors returned</span>
<span class="sd">                             by the Future and copy grads to individual parameters.</span>

<span class="sd">                             We also provide an API called ``get_future`` to retrieve a</span>
<span class="sd">                             Future associated with the completion of ``c10d.ProcessGroup.work``.</span>
<span class="sd">                             ``get_future`` is currently supported for MPI and also supported for most</span>
<span class="sd">                             operations on GLOO and MPI, except for peer to peer operations (send/recv).</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            Grad bucket&#39;s tensors will not be predivided by world_size. User is responsible</span>
<span class="sd">            to divide by the world_size in case of operations like allreduce.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook can only be registered once and should be registered</span>
<span class="sd">            before calling backward.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            The Future object that hook returns should contain a result that has the same</span>
<span class="sd">            shape with the tensors inside grad bucket.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook does not support single-process multiple-device mode.</span>
<span class="sd">            Gradbucket tensors should consist of only a single tensor.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            ``get_future`` API supports NCCL, and partially GLOO and MPI backends (no support</span>
<span class="sd">            for peer-to-peer operations like send/recv) and will return a ``torch._C.Future``</span>
<span class="sd">            which is an internal type and should be used with caution. It can still be used by</span>
<span class="sd">            ``register_comm_hook`` API, but it is subject to some subtle differences compared</span>
<span class="sd">            to ``torch.futures.Future``.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook is experimental and subject to change.</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a noop hook that returns the same tensors.</span>

<span class="sd">            &gt;&gt;&gt; def noop(state: object, bucket: dist.GradBucket): -&gt; torch.futures.Future</span>
<span class="sd">            &gt;&gt;&gt;     fut = torch.futures.Future()</span>
<span class="sd">            &gt;&gt;&gt;     fut.set_result(bucket.get_tensors())</span>
<span class="sd">            &gt;&gt;&gt;     return fut</span>

<span class="sd">            &gt;&gt;&gt; ddp.register_comm_hook(state = None, hook = noop)</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a Parallel SGD algorithm where gradients are encoded before</span>
<span class="sd">            allreduce, and then decoded after allreduce.</span>

<span class="sd">            &gt;&gt;&gt; def encode_and_decode(state: object, bucket: dist.GradBucket): -&gt; torch.futures.Future</span>
<span class="sd">            &gt;&gt;&gt;     tensors = [t / process_group.world_size for t in bucket.get_tensors()]</span>
<span class="sd">            &gt;&gt;&gt;     encoded_tensors = encode(tensors) # encode gradients</span>
<span class="sd">            &gt;&gt;&gt;     fut = process_group.allreduce(encoded_tensors).get_future()</span>
<span class="sd">            &gt;&gt;&gt;     # Define the then callback to decode.</span>
<span class="sd">            &gt;&gt;&gt;     def decode(fut):</span>
<span class="sd">            &gt;&gt;&gt;         decoded_tensors = decode(fut.value()) # decode gradients</span>
<span class="sd">            &gt;&gt;&gt;         return decoded_tensors</span>
<span class="sd">            &gt;&gt;&gt;     return fut.then(decode)</span>

<span class="sd">            &gt;&gt;&gt; ddp.register_comm_hook(state = None, hook = encode_and_decode)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_comm_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_comm_hook_name</span><span class="p">(</span><span class="n">hook</span><span class="o">.</span><span class="vm">__qualname__</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_register_builtin_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comm_hook_type</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a built-in communication hook that specifies how DDP</span>
<span class="sd">        aggregates gradients across multiple workers.</span>
<span class="sd">        The built-in hooks aim to provide efficient C++ implementations for certain hooks,</span>
<span class="sd">        which might not be as efficient if implemented in Python using a Python communication hook.</span>

<span class="sd">        Args:</span>
<span class="sd">            comm_hook_type (dist.BuiltinCommHookType): type of communication hook, such as</span>
<span class="sd">            ALLREDUCE, FP16_COMPRESS, etc.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook can only be registered once and should be registered</span>
<span class="sd">            before calling backward.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook does not support single-process multiple-device mode.</span>
<span class="sd">            Gradbucket tensors should consist of only a single tensor.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook is experimental and subject to change.</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a FP16 compression where gradients are</span>
<span class="sd">            compressed into 16-bit floating-point numbers before allreduce, and</span>
<span class="sd">            then decompressed after allreduce.</span>

<span class="sd">            &gt;&gt;&gt; ddp._register_builtin_comm_hook(dist.BuiltinCommHookType.FP16_COMPRESS)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_comm_hook_name</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">comm_hook_type</span><span class="p">))</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_register_builtin_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">,</span> <span class="n">comm_hook_type</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_distributed_broadcast_coalesced</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_broadcast_coalesced</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">authoritative_rank</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">will_sync_module_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_find_common_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_rank</span><span class="p">,</span> <span class="n">rank_cond</span><span class="p">):</span>
        <span class="c1"># -1 indicates that this rank is not under consideration to be the</span>
        <span class="c1"># common_rank</span>
        <span class="n">rank_to_use</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">input_rank</span> <span class="k">if</span> <span class="n">rank_cond</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">rank_to_use</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rank_to_use</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;BUG! Expected rank_cond to be true for at least one process.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">rank_to_use</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_sync_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># module buffer sync</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">will_sync_module_buffers</span><span class="p">():</span>
                <span class="c1"># Synchronize buffers across processes.</span>
                <span class="c1"># If we are running DDP with the join manager, we have to agree</span>
                <span class="c1"># upon a rank to sync module buffers from, since rank 0 may</span>
                <span class="c1"># already have been joined and have stale module buffers.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp_uneven_inputs_config</span><span class="o">.</span><span class="n">ddp_join_enabled</span><span class="p">:</span>
                    <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="kc">True</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># The process with rank 0 is considered the authoritative copy.</span>
                    <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_broadcast_coalesced</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">,</span>
                    <span class="n">authoritative_rank</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_passing_sync_batchnorm_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_copies</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">dev_idx</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">module_copies</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">!=</span> <span class="s2">&quot;cpu&quot;</span>
                    <span class="p">),</span> <span class="s2">&quot;SyncBatchNorm layers only work with GPU modules&quot;</span>

    <span class="k">def</span> <span class="nf">_check_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">hook</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Communication hook must be callable.&quot;</span><span class="p">)</span>

        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bucket&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">_empty</span>
            <span class="ow">and</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bucket&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Communication hook: bucket annotation should be dist.GradBucket.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">_empty</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span>
            <span class="ow">and</span> <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">Future</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Communication hook: return annotation should be torch.futures.Future or torch._C.Future.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_distributed_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_params_and_buffers_to_ignore_for_model</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span> <span class="n">params_and_buffers_to_ignore</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets parameters and buffers to be ignored by DDP. Expected format for</span>
<span class="sd">        parameters is the fully qualified name: {module_name}.{param_name}, and</span>
<span class="sd">        similarly, {module_name}.{buffer_name} for buffers. For example:</span>
<span class="sd">        params_to_ignore = []</span>
<span class="sd">        # NB: model here is vanilla PyTorch module, not yet wrapped with DDP.</span>
<span class="sd">        for module_name, module in model.named_modules():</span>
<span class="sd">            for param_name, param in module.named_parameters(recurse=False):</span>
<span class="sd">                if should_ignore(param):</span>
<span class="sd">                    # Create expected format</span>
<span class="sd">                    fqn = f&quot;{module_name}.{param_name}&quot;</span>
<span class="sd">                    params_to_ignore.append(fqn)</span>
<span class="sd">        torch.nn.parallel.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(</span>
<span class="sd">            model,</span>
<span class="sd">            params_to_ignore</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This is a workaround to set parameters and buffers DDP should ignore</span>
        <span class="c1"># during synchronization. It will be removed when the API is finalized</span>
        <span class="c1"># as part of addressing https://github.com/pytorch/pytorch/issues/43690.</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_ddp_params_and_buffers_to_ignore</span> <span class="o">=</span> <span class="n">params_and_buffers_to_ignore</span>

    <span class="k">def</span> <span class="nf">_get_ddp_logging_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This interface can be called after DistributedDataParallel() is</span>
<span class="sd">        constructed. It returns a dictionary of logging data. It could help</span>
<span class="sd">        for debugging and analysis. The loggind data includes DistributedDataParallel</span>
<span class="sd">        constructor input parameters, some internal states of DistributedDataParallel</span>
<span class="sd">        and performance metrics. Simply print the dictorinary and see what</span>
<span class="sd">        these metrics are.</span>
<span class="sd">        THis is a prototype interface and subject to change in the future.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ddp_logging_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_get_ddp_logging_data</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">ddp_logging_data</span><span class="o">.</span><span class="n">strs_map</span><span class="p">,</span> <span class="o">**</span><span class="n">ddp_logging_data</span><span class="o">.</span><span class="n">ints_map</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_set_ddp_runtime_logging_sample_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This interface allows users to set sample_rate of collecting</span>
<span class="sd">        runtime stats. The runtime stats will be recorded for the</span>
<span class="sd">        first 10 iterations, after 10 iteratons runtime stats will be</span>
<span class="sd">        recorded once every &quot;sample_rate&quot; training iterations. In</span>
<span class="sd">        default, runtime stats are recorded for the first 10 iterations,</span>
<span class="sd">        after 10 iterations runtime stats are recorded once every</span>
<span class="sd">        &quot;kDDPRuntimeLoggingSampleRate=100&quot; training iterations.</span>
<span class="sd">        This is a prototype interface and subject to change in the future.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_rate</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;DDP runtime logging sample rate should be equal or greater than 1&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_ddp_runtime_logging_sample_rate</span><span class="p">(</span><span class="n">sample_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_static_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Users can explicitly let DDP know the trained graph is static,</span>
<span class="sd">        when 1) the set of used and unused parameters will not change</span>
<span class="sd">        during the whole training loop; in this case, it does not matter</span>
<span class="sd">        whether users set find_unsued_parameters = true or not.</span>
<span class="sd">        2) how the graph is trained will not change during the whole training</span>
<span class="sd">        loop (meaning there is no control flow depending on iterations).</span>
<span class="sd">        When graph is set to be static, DDP will support cases that can not</span>
<span class="sd">        be supported in the past: 1) reentrant backwards</span>
<span class="sd">        2) activation checkpointing multiple times 3)</span>
<span class="sd">        activation checkpointing with find_unused_parameters = true.</span>
<span class="sd">        4) not all output tensors are used in loss calculation.</span>
<span class="sd">        5) there is model parameter that is outside of forward function.</span>
<span class="sd">        6) potentially improve performance when find_unsued_parameters = true</span>
<span class="sd">        or there are unused parameters, as DDP will not search graph in each</span>
<span class="sd">        iteraton to detect unused parameters when static_graph is set to be True.</span>

<span class="sd">        This API should be called after DistributedDataParallel construction, and</span>
<span class="sd">        before training loops starts. Also it should be called in the same way for</span>
<span class="sd">        all ranks. For example:</span>
<span class="sd">            ddp_model = DistributedDataParallel(model)</span>
<span class="sd">            ddp_model._set_static_graph()</span>
<span class="sd">            for i in range(n):</span>
<span class="sd">                .....</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed find_unused_parameters=true to DistributedDataParallel, &quot;</span>
                <span class="s2">&quot;`_set_static_graph` will detect unused parameters automatically, so &quot;</span>
                <span class="s2">&quot;you do not need to set find_unused_parameters=true, just be sure these &quot;</span>
                <span class="s2">&quot;unused parameters will not change during training loop while calling &quot;</span>
                <span class="s2">&quot;`_set_static_graph`.&quot;</span>
            <span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
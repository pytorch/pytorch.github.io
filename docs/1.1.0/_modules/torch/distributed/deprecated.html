


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.deprecated &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/deprecated.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  <a href='http://pytorch.org/docs/versions.html'>1.1.0 &#x25BC</a>
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard (experimental)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../__config__.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.deprecated</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.deprecated</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">torch.distributed.deprecated provides an MPI-like interface for exchanging tensor</span>
<span class="sd">data across multi-machine networks. It supports a few different backends</span>
<span class="sd">and initialization methods.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">atexit</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">torch._utils</span> <span class="k">import</span> <span class="n">_flatten_dense_tensors</span><span class="p">,</span> <span class="n">_unflatten_dense_tensors</span>


<span class="k">class</span> <span class="nc">dist_backend</span><span class="p">:</span>
    <span class="n">UNDEFINED</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">TCP</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">MPI</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">GLOO</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">NCCL</span> <span class="o">=</span> <span class="mi">3</span>


<span class="n">_INITIALIZED_PG</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_INITIALIZED_MW</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">_initialized</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
<span class="n">_scope</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_extend_scope</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="n">_scope</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)})</span>


<span class="k">def</span> <span class="nf">is_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_distributed</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">destroy_process_group</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Destroy the initialized distributed package</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_backend</span>
    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_destroy_process_group</span><span class="p">()</span>
    <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">is_initialized</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checking if the process group has been initialized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span>


<div class="viewcode-block" id="init_process_group"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.init_process_group">[docs]</a><span class="k">def</span> <span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initializes the distributed package.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        backend (str): Name of the backend to use. Depending on build-time configuration</span>
<span class="sd">            valid values include: ``tcp``, ``mpi``, ``gloo`` and ``nccl``.</span>
<span class="sd">        init_method (str, optional): URL specifying how to initialize the package.</span>
<span class="sd">        world_size (int, optional): Number of processes participating in the job.</span>
<span class="sd">        rank (int, optional): Rank of the current process.</span>
<span class="sd">        group_name (str, optional): Group name. See description of init methods.</span>

<span class="sd">    To enable ``backend == mpi``, PyTorch needs to built from source on a system that</span>
<span class="sd">    supports MPI. If you want to use Open MPI with CUDA-aware support, please use</span>
<span class="sd">    Open MPI major version 2 and above.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This method initializes CUDA context. Therefore, if multiple processes</span>
<span class="sd">        run on a single machine but use different GPUs, make sure to use</span>
<span class="sd">        :func:`torch.cuda.set_device` before this method to avoid unnecessarily</span>
<span class="sd">        creating context on the first visible device.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;world_size&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group_name&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;rank&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;got unexpected keyword arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch built without distributed support&quot;</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="k">if</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize torch.distributed.deprecated twice!&quot;</span><span class="p">)</span>

    <span class="c1"># Checking and assigning the distributed backend</span>
    <span class="k">global</span> <span class="n">_backend</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;tcp&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">TCP</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;mpi&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">MPI</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;gloo&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">GLOO</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid distributed backend name: &quot;</span> <span class="o">+</span> <span class="n">backend</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
                                      <span class="n">group_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="n">_INITIALIZED_PG</span>

    <span class="k">if</span> <span class="n">_backend</span> <span class="o">==</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">destroy_process_group</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_extension</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;distributed module initialization failed&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">init_master_worker</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ================================================================================</span>
<span class="s2">                                        WARNING</span>
<span class="s2">    ================================================================================</span>
<span class="s2">    Master-worker mode is still experimental. The API will change without</span>
<span class="s2">    notice and we do not guarantee full correctness and expected performance yet.</span>
<span class="s2">    We&#39;ll announce it once it&#39;s ready.</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;world_size&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group_name&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;rank&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;got unexpected keyword arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch built without distributed support&quot;</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="k">if</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize torch.distributed.deprecated twice!&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_master_worker</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
                                      <span class="n">group_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="n">_INITIALIZED_MW</span>
    <span class="kn">import</span> <span class="nn">torch.distributed.deprecated.collectives</span> <span class="k">as</span> <span class="nn">collectives</span>
    <span class="kn">import</span> <span class="nn">torch.distributed.deprecated.remote_types</span> <span class="k">as</span> <span class="nn">remote_types</span>
    <span class="n">_extend_scope</span><span class="p">(</span><span class="n">collectives</span><span class="p">)</span>
    <span class="n">_extend_scope</span><span class="p">(</span><span class="n">remote_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_extension</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;distributed module initialization failed&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">reduce_op</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">SUM</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">PRODUCT</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">MAX</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">MIN</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">group</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">WORLD</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_DistributedRequest</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span>

    <span class="k">def</span> <span class="nf">is_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_request_is_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_request_wait</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="p">)</span>


<div class="viewcode-block" id="get_rank"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.get_rank">[docs]</a><span class="k">def</span> <span class="nf">get_rank</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the rank of current process.</span>

<span class="sd">    Rank is a unique identifier assigned to each process within a distributed</span>
<span class="sd">    group. They are always consecutive integers ranging from ``0`` to</span>
<span class="sd">    ``world_size - 1`` (inclusive).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_get_rank</span><span class="p">()</span></div>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.get_world_size">[docs]</a><span class="k">def</span> <span class="nf">get_world_size</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the number of processes in the distributed group.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_get_num_processes</span><span class="p">()</span></div>


<div class="viewcode-block" id="isend"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.isend">[docs]</a><span class="k">def</span> <span class="nf">isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sends a tensor asynchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">_DistributedRequest</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">))</span></div>


<div class="viewcode-block" id="irecv"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.irecv">[docs]</a><span class="k">def</span> <span class="nf">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Receives a tensor asynchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int): Source rank.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">_DistributedRequest</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">))</span></div>


<div class="viewcode-block" id="send"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.send">[docs]</a><span class="k">def</span> <span class="nf">send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sends a tensor synchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span></div>


<div class="viewcode-block" id="recv"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.recv">[docs]</a><span class="k">def</span> <span class="nf">recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Receives a tensor synchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sender rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_recv_any_source</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.broadcast_multigpu">[docs]</a><span class="k">def</span> <span class="nf">broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Broadcasts the tensor to the whole group with multiple GPU tensors</span>
<span class="sd">    per node.</span>

<span class="sd">    :attr:`tensor` must have the same number of elements in all the GPUs from</span>
<span class="sd">    all processes participating in the collective. each tensor in the list must</span>
<span class="sd">    be on a different GPU.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): Tensors that participate in the collective</span>
<span class="sd">            operation. if ``src`` is the rank, then the first element of</span>
<span class="sd">            ``tensor_list`` (``tensor_list[0]``) will be broadcasted to all</span>
<span class="sd">            other tensors (on different GPUs) in the src process and all tensors</span>
<span class="sd">            in ``tensor_list`` of other non-src processes. You also need to make</span>
<span class="sd">            sure that ``len(tensor_list)`` is the same for all the distributed</span>
<span class="sd">            processes calling this function.</span>

<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.broadcast">[docs]</a><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Broadcasts the tensor to the whole group.</span>

<span class="sd">    :attr:`tensor` must have the same number of elements in all processes</span>
<span class="sd">    participating in the collective.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Data to be sent if :attr:`src` is the rank of</span>
<span class="sd">            current process, and tensor to be used to save received data</span>
<span class="sd">            otherwise.</span>
<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_reduce_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result. This function reduces a number of tensors on every node,</span>
<span class="sd">    while each tensor resides on a different GPU.</span>
<span class="sd">    Therefore, the input tensor in the tensor list needs to be GPU tensors.</span>
<span class="sd">    Also, each tensor in the tensor list needs to reside on a different GPU.</span>

<span class="sd">    After the call, all tensors in :attr:`tensor_list` will be bitwise identical</span>
<span class="sd">    in all processes.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): List of input and output tensors of</span>
<span class="sd">            the collective. The function operates in-place and requires that</span>
<span class="sd">            each tensor to be a GPU tensor on different GPUs.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_reduce"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_reduce">[docs]</a><span class="k">def</span> <span class="nf">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result.</span>

<span class="sd">    After the call :attr:`tensor` will be bitwise identical in all processes.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="reduce_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data on multiple GPUs across all machines. Each tensor</span>
<span class="sd">    in :attr:`tensor_list` should reside on a separate GPU.</span>

<span class="sd">    Only the GPU of ``tensor_list[0]`` on the process with rank :attr:`dst` is</span>
<span class="sd">    going to receive the final result.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): Input and output GPU tensors of the</span>
<span class="sd">            collective. The function operates in-place.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="reduce"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.reduce">[docs]</a><span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines.</span>

<span class="sd">    Only the process with rank :attr:`dst` is going to receive the final result.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_gather_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_gather_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_gather_multigpu</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span>
                        <span class="n">input_tensor_list</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers tensors from the whole group in a list.</span>
<span class="sd">    Each tensor in :attr:`input_tensor_list` should reside on a separate GPU.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`output_tensor_lists` and</span>
<span class="sd">      :attr:`input_tensor_list` should only contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        output_tensor_lists (List[List[Tensor]]): Output lists. It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for output of</span>
<span class="sd">            the collective.</span>
<span class="sd">            e.g. ``output_tensor_lists[i]`` contains the all_gather</span>
<span class="sd">            result that resides on the GPU of ``input_tensor_list[i]``.</span>
<span class="sd">            Note that each element of ``output_tensor_lists[i]`` has the size of</span>
<span class="sd">            ``world_size * len(input_tensor_list)``, since the function all</span>
<span class="sd">            gathers the result from every single GPU in the group. To interpret</span>
<span class="sd">            each element of ``output_tensor_list[i]``, note that</span>
<span class="sd">            ``input_tensor_list[j]`` of rank k will be appear in</span>
<span class="sd">            ``output_tensor_list[i][rank * world_size + j]``</span>
<span class="sd">            Also note that ``len(output_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``output_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(output_tensor_lists[i])``) need to be the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        input_tensor_list (List[Tensor]): List of tensors (on different GPUs) to</span>
<span class="sd">            be broadcast from current process.</span>
<span class="sd">            Note that ``len(input_tensor_list)`` needs to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="n">flatten_tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">output_tensor_list</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span><span class="p">:</span>
        <span class="n">flatten_tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_flatten_dense_tensors</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">))</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_gather_multigpu</span><span class="p">(</span><span class="n">flatten_tensor_list</span><span class="p">,</span>
                                             <span class="n">input_tensor_list</span><span class="p">,</span>
                                             <span class="n">group</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">flatten_tensor</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span>
                                                  <span class="n">flatten_tensor_list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span>
                                 <span class="n">_unflatten_dense_tensors</span><span class="p">(</span><span class="n">flatten_tensor</span><span class="p">,</span>
                                                          <span class="n">output_tensor_list</span><span class="p">)):</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="all_gather"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_gather">[docs]</a><span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers tensors from the whole group in a list.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (list[Tensor]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">_backend</span> <span class="o">!=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">all_gather_multigpu</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.gather">[docs]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers a list of tensors in a single process.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input tensor.</span>
<span class="sd">        dst (int): Destination rank. Required in all processes except the one that</span>
<span class="sd">            is receiveing the data.</span>
<span class="sd">        gather_list (list[Tensor]): List of appropriately-sized tensors to</span>
<span class="sd">            use for received data. Required only in the receiving process.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">dst</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;dst&#39;</span><span class="p">,</span> <span class="n">my_rank</span><span class="p">)</span>
    <span class="n">gather_list</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;gather_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">_group</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;got unexpected kwargs&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gather_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;gather_list is a required argument in gather destination&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_gather_recv</span><span class="p">(</span><span class="n">gather_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gather_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-empty gather_list can be given only to gather destination&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_gather_send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.scatter">[docs]</a><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Each process will receive exactly one tensor and store its data in the</span>
<span class="sd">    :attr:`tensor` argument.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Output tensor.</span>
<span class="sd">        src (int): Source rank. Required in all processes except the one that</span>
<span class="sd">            is sending the data.</span>
<span class="sd">        scatter_list (list[Tensor]): List of tensors to scatter. Required only</span>
<span class="sd">            in the process that is sending the data.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;src&#39;</span><span class="p">,</span> <span class="n">my_rank</span><span class="p">)</span>
    <span class="n">scatter_list</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;scatter_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">_group</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;got unexpected kwargs: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
    <span class="k">if</span> <span class="n">src</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scatter_list is a required argument in scatter source&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_scatter_send</span><span class="p">(</span><span class="n">scatter_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-empty can be given only to scatter source&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_scatter_recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span></div>


<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.barrier">[docs]</a><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Synchronizes all processes.</span>

<span class="sd">    This collective blocks processes until the whole group enters this function.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_barrier</span><span class="p">(</span><span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="new_group"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.new_group">[docs]</a><span class="k">def</span> <span class="nf">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a new distributed group.</span>

<span class="sd">    This function requires that all processes in the main group (i.e., all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group. Additionally, groups</span>
<span class="sd">    should be created in the same order in all processes.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        ranks (list[int]): List of ranks of group members.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A handle of distributed group that can be given to collective calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">ranks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">get_world_size</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_new_group</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_clear_group_cache</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clear the created distributed group&#39;s cached resource.</span>

<span class="sd">    Only NCCL backend is currently supported.</span>

<span class="sd">    Cached resource includes NCCL communicators and CUDA events.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_clear_group_cache</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_register_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.distributed.deprecated needs to be initialized first&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_register_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../_static/doctools.js"></script>
         <script type="text/javascript" src="../../../_static/language_data.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
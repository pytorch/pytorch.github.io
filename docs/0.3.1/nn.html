

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>torch.nn &mdash; PyTorch master documentation</title>
















    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.optim" href="optim.html" />
    <link rel="prev" title="torch.Storage" href="storage.html" />


  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">


  <div class="wy-grid-for-nav">


    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">



            <a href="index.html">




            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>

          </a>




              <div class="version">
                0.3.1 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>




<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">






              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id14"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id15"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id16"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id19"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id20"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id21"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id22"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id24"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id25"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id26"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id27"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id28"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id29"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id32"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id33"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id34"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id35"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id36"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id37">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id38">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#id40"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
</ul>



        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">


      <nav class="wy-nav-top" aria-label="top navigation">

          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>

      </nav>



      <div class="wy-nav-content">

        <div class="rst-content">

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">

      <li><a href="index.html">Docs</a> &raquo;</li>

      <li>torch.nn</li>


      <li class="wy-breadcrumbs-aside">


            <a href="_sources/nn.rst.txt" rel="nofollow"> View page source</a>


      </li>

  </ul>


  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <div class="section" id="module-torch.nn">
<span id="torch-nn"></span><h1>torch.nn<a class="headerlink" href="#module-torch.nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.nn.Parameter">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Parameter</code><a class="reference internal" href="_modules/torch/nn/parameter.html#Parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>A kind of Variable that is to be considered a module parameter.</p>
<p>Parameters are <a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> subclasses, that have a
very special property when used with <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a> s - when they&#8217;re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <a class="reference internal" href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-meth docutils literal"><span class="pre">parameters()</span></code></a> iterator.
Assigning a Variable doesn&#8217;t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<p>Another difference is that parameters can&#8217;t be volatile and that they
require gradient by default.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; parameter tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if the parameter requires gradient. See
<a class="reference internal" href="notes/autograd.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="containers">
<h2>Containers<a class="headerlink" href="#containers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module">
<h3><span class="hidden-section">Module</span><a class="headerlink" href="#module" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Module">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Module</code><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call .cuda(), etc.</p>
<dl class="method">
<dt id="torch.nn.Module.add_module">
<code class="descname">add_module</code><span class="sig-paren">(</span><em>name</em>, <em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.add_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; name of the child module. The child module can be
accessed from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) &#8211; child module to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.apply">
<code class="descname">apply</code><span class="sig-paren">(</span><em>fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">torch-nn-init</span>).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>fn</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a> -&gt; None) &#8211; function to be applied to each submodule</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear (2 -&gt; 2)</span>
<span class="go">Parameter containing:</span>
<span class="go"> 1  1</span>
<span class="go"> 1  1</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>
<span class="go">Linear (2 -&gt; 2)</span>
<span class="go">Parameter containing:</span>
<span class="go"> 1  1</span>
<span class="go"> 1  1</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>
<span class="go">Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.children">
<code class="descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> &#8211; a child module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cpu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cuda"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; if specified, all parameters will be
copied to that device</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.double"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to double datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on modules such as Dropout or BatchNorm.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.float"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to float datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overriden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.half"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to half datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em>, <em>strict=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal"><span class="pre">strict</span></code> is <code class="docutils literal"><span class="pre">True</span></code> then
the keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module&#8217;s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-func docutils literal"><span class="pre">state_dict()</span></code></a> function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) &#8211; A dict containing parameters and
persistent buffers.</li>
<li><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; Strictly enforce that the keys in <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal"><span class="pre">state_dict</span></code></a>
match the keys returned by this module&#8217;s <cite>:func:`state_dict()</cite>
function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.modules">
<code class="descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> &#8211; a module in the network</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal"><span class="pre">l</span></code> will be returned only once.</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="go">0 -&gt; Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear (2 -&gt; 2)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_children">
<code class="descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> &#8211; Tuple containing a name and child module</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_modules">
<code class="descname">named_modules</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> &#8211; Tuple of name and module</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal"><span class="pre">l</span></code> will be returned only once.</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="go">0 -&gt; (&#39;&#39;, Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear (2 -&gt; 2))</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_parameters">
<code class="descname">named_parameters</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Parameter)</em> &#8211; Tuple containing the name and parameter</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Parameter</em> &#8211; module parameter</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_backward_hook">
<code class="descname">register_backward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_backward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_buffer">
<code class="descname">register_buffer</code><span class="sig-paren">(</span><em>name</em>, <em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_buffer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm&#8217;s <code class="docutils literal"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; name of the buffer. The buffer can be accessed
from this module using the given name</li>
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; buffer to be registered.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_hook">
<code class="descname">register_forward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input or output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_pre_hook">
<code class="descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_parameter">
<code class="descname">register_parameter</code><span class="sig-paren">(</span><em>name</em>, <em>param</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; name of the parameter. The parameter can be accessed
from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>Parameter</em></a>) &#8211; parameter to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><em>destination=None</em>, <em>prefix=''</em>, <em>keep_vars=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<p>When keep_vars is <code class="docutils literal"><span class="pre">True</span></code>, it returns a Variable for each parameter
(rather than a Tensor).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a><em>, </em><em>optional</em>) &#8211; if not None, the return dictionary is stored into destination.
Default: None</li>
<li><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a><em>, </em><em>optional</em>) &#8211; Adds a prefix to the key (name) of every
parameter and buffer in the result dictionary. Default: &#8216;&#8217;</li>
<li><strong>keep_vars</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, returns a Variable for each
parameter. If <code class="docutils literal"><span class="pre">False</span></code>, returns a Tensor for each parameter.
Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a dictionary containing a whole state of the module</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on modules such as Dropout or BatchNorm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dst_type</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to dst_type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; the desired type</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential">
<h3><span class="hidden-section">Sequential</span><a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sequential">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, given is a small example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

<span class="c1"># Example of using Sequential with OrderedDict</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="modulelist">
<h3><span class="hidden-section">ModuleList</span><a class="headerlink" href="#modulelist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ModuleList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleList</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p>ModuleList can be indexed like a regular Python list, but modules it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>list</em><em>, </em><em>optional</em>) &#8211; a list of modules to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given module at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#8211; module to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends modules from a Python list at the end.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>list</em>) &#8211; list of modules to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterlist">
<h3><span class="hidden-section">ParameterList</span><a class="headerlink" href="#parameterlist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ParameterList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterList</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a list.</p>
<p>ParameterList can be indexed like a regular Python list, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>list</em><em>, </em><em>optional</em>) &#8211; a list of <code class="xref py py-class docutils literal"><span class="pre">Parameter`</span></code> to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>parameter</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given parameter at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) &#8211; parameter to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends parameters from a Python list at the end.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>list</em>) &#8211; list of parameters to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="convolution-layers">
<h2>Convolution Layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conv1d">
<h3><span class="hidden-section">Conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{in}, L)\)</span> and output <span class="math">\((N, C_{out}, L_{out})\)</span> can be
precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(L\)</span> is a length of signal sequence.</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<cite>in_channels</cite> and <cite>out_channels</cite> must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv
     layers side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently
     concatenated.
At groups=`in_channels`, each input channel is convolved with its
     own set of filters (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels = K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math">\((N, C_{in}, L_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} * K, ..., groups=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of
the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel
elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input
channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\_size - 1) - 1) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(out_channels, in_channels, kernel_size)</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape
(out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h3><span class="hidden-section">Conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{in}, H, W)\)</span> and output <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(H\)</span> is a height of input planes in pixels, and <span class="math">\(W\)</span> is
width in pixels.</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points for each dimension.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<cite>in_channels</cite> and <cite>out_channels</cite> must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv
     layers side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently
     concatenated.
At groups=`in_channels`, each input channel is convolved with its
     own set of filters (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels = K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} * K, ..., groups=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h3><span class="hidden-section">Conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points for each dimension.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs. <cite>in_channels</cite> and <cite>out_channels</cite>
must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
     side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently concatenated.
At groups=`in_channels`, each input channel is convolved with its own set of filters
     (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels = K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} * K, ..., groups=C_{in})\)</span></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to all three sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[2] - dilation[2] * (kernel\_size[2] - 1) - 1) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose1d">
<h3><span class="hidden-section">ConvTranspose1d</span><a class="headerlink" href="#convtranspose1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> controls the amount of implicit zero-paddings on</div>
<div class="line">both sides of the output for <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> number of points.</div>
<div class="line">number of points.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs. <cite>in_channels</cite> and <cite>out_channels</cite>
must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
     side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently concatenated.
At groups=`in_channels`, each input channel is convolved with its own set of filters
     (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where
<span class="math">\(L_{out} = (L_{in} - 1) * stride - 2 * padding + kernel\_size + output\_padding\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convtranspose2d">
<h3><span class="hidden-section">ConvTranspose2d</span><a class="headerlink" href="#convtranspose2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points for each dimension.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> controls the amount of implicit zero-paddings on</div>
<div class="line">both sides of the output for <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> number of points for</div>
<div class="line">each dimension.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs. <cite>in_channels</cite> and <cite>out_channels</cite>
must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
     side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently concatenated.
At groups=`in_channels`, each input channel is convolved with its own set of filters
     (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimensions</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose3d">
<h3><span class="hidden-section">ConvTranspose3d</span><a class="headerlink" href="#convtranspose3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both</div>
<div class="line-block">
<div class="line">sides for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points for each dimension.</div>
</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> controls the amount of implicit zero-paddings on</div>
<div class="line">both sides of the output for <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> number of points for</div>
<div class="line">each dimension.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs. <cite>in_channels</cite> and <cite>out_channels</cite>
must both be divisible by <cite>groups</cite>.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
     side by side, each seeing half the input channels,
     and producing half the output channels, and both subsequently concatenated.
At groups=`in_channels`, each input channel is convolved with its own set of filters
     (of size <cite>out_channels // in_channels</cite>).</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the depth, height and width dimensions</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to all three sides of the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0]\)</span>
<span class="math">\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\_size[2] + output\_padding[2]\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-layers">
<h2>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="maxpool1d">
<h3><span class="hidden-section">MaxPool1d</span><a class="headerlink" href="#maxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>
and output <span class="math">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, k)  = \max_{{m}=0}^{{kernel\_size}-1} input(N_i, C_j, stride * k + m)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\_size - 1) - 1) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool2d">
<h3><span class="hidden-section">MaxPool2d</span><a class="headerlink" href="#maxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, h, w)  = \max_{{m}=0}^{kH-1} \max_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * h + m, stride[1] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool3d">
<h3><span class="hidden-section">MaxPool3d</span><a class="headerlink" href="#maxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, d, h, w)  = \max_{{k}=0}^{kD-1} \max_{{m}=0}^{kH-1} \max_{{n}=0}^{kW-1}
                 input(N_i, C_j, stride[0] * k + d, stride[1] * h + m, stride[2] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on all three sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[2] - dilation[2] * (kernel\_size[2] - 1) - 1) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool1d">
<h3><span class="hidden-section">MaxUnpool1d</span><a class="headerlink" href="#maxunpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool1d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool1d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool1d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8</span>
<span class="go">[torch.FloatTensor of size 1x1x8]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example showcasing the use of output_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8   0</span>
<span class="go">[torch.FloatTensor of size 1x1x9]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8</span>
<span class="go">[torch.FloatTensor of size 1x1x8]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool2d">
<h3><span class="hidden-section">MaxUnpool2d</span><a class="headerlink" href="#maxunpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool2d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool2d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool2d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] -2 * padding[0] + kernel\_size[0]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[1] -2 * padding[1] + kernel\_size[1]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">]]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   0   0   0   0</span>
<span class="go">   0   6   0   8</span>
<span class="go">   0   0   0   0</span>
<span class="go">   0  14   0  16</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># specify a different output size than input size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   0   0   0   0   0</span>
<span class="go">   6   0   8   0   0</span>
<span class="go">   0   0   0  14   0</span>
<span class="go">  16   0   0   0   0</span>
<span class="go">   0   0   0   0   0</span>
<span class="go">[torch.FloatTensor of size 1x1x5x5]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool3d">
<h3><span class="hidden-section">MaxUnpool3d</span><a class="headerlink" href="#maxunpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a> is not fully invertible, since the non-maximal values are lost.
<a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool3d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool3d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs section below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool3d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0]\)</span>
<span class="math">\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\_size[2]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">15</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20, 16, 51, 33, 15])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool1d">
<h3><span class="hidden-section">AvgPool1d</span><a class="headerlink" href="#avgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>,
output <span class="math">\((N, C, L_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\(k\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, l)  = 1 / k * \sum_{{m}=0}^{k}
                       input(N_i, C_j, stride * l + m)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> can each be
an <code class="docutils literal"><span class="pre">int</span></code> or a one-element tuple.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - kernel\_size) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool with window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]])))</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  2  4  6</span>
<span class="go">[torch.FloatTensor of size 1x1x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool2d">
<h3><span class="hidden-section">AvgPool2d</span><a class="headerlink" href="#avgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, h, w)  = 1 / (kH * kW) * \sum_{{m}=0}^{kH-1} \sum_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * h + m, stride[1] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - kernel\_size[0]) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - kernel\_size[1]) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool3d">
<h3><span class="hidden-section">AvgPool3d</span><a class="headerlink" href="#avgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, d, h, w)  = 1 / (kD * kH * kW) * \sum_{{k}=0}^{kD-1} \sum_{{m}=0}^{kH-1} \sum_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * d + k, stride[1] * h + m, stride[2] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on all three sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on all three sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in} + 2 * padding[0] - kernel\_size[0]) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in} + 2 * padding[1] - kernel\_size[1]) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in} + 2 * padding[2] - kernel\_size[2]) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fractionalmaxpool2d">
<h3><span class="hidden-section">FractionalMaxPool2d</span><a class="headerlink" href="#fractionalmaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.FractionalMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">FractionalMaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>output_size=None</em>, <em>output_ratio=None</em>, <em>return_indices=False</em>, <em>_random_samples=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.FractionalMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p>
<p>Fractiona MaxPooling is described in detail in the paper <a class="reference external" href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham</p>
<p>The max-pooling operation is applied in kHxkW regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</li>
<li><strong>output_size</strong> &#8211; the target output size of the image of the form oH x oW.
Can be a tuple (oH, oW) or a single number oH for a square image oH x oH</li>
<li><strong>output_ratio</strong> &#8211; If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, and target output size 13x12</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window and target output size being half of input image size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lppool2d">
<h3><span class="hidden-section">LPPool2d</span><a class="headerlink" href="#lppool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LPPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool2d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#LPPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LPPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is: <span class="math">\(f(X) = pow(sum(pow(X, p)), 1/p)\)</span></p>
<blockquote>
<div><ul class="simple">
<li>At p = infinity, one gets Max Pooling</li>
<li>At p = 1, one gets Average Pooling</li>
</ul>
</div></blockquote>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window of power 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool1d">
<h3><span class="hidden-section">AdaptiveMaxPool1d</span><a class="headerlink" href="#adaptivemaxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool1d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size H</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool1d. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool2d">
<h3><span class="hidden-section">AdaptiveMaxPool2d</span><a class="headerlink" href="#adaptivemaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool2d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code class="docutils literal"><span class="pre">int</span></code>, or <code class="docutils literal"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool3d">
<h3><span class="hidden-section">AdaptiveMaxPool3d</span><a class="headerlink" href="#adaptivemaxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool3d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size of the image of the form D x H x W.
Can be a tuple (D, H, W) or a single D for a cube D x D x D.
D, H and W can be either a <code class="docutils literal"><span class="pre">int</span></code>, or <code class="docutils literal"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool3d. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool1d">
<h3><span class="hidden-section">AdaptiveAvgPool1d</span><a class="headerlink" href="#adaptiveavgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool1d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size H</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool2d">
<h3><span class="hidden-section">AdaptiveAvgPool2d</span><a class="headerlink" href="#adaptiveavgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool2d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H
H and W can be either a <code class="docutils literal"><span class="pre">int</span></code>, or <code class="docutils literal"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool3d">
<h3><span class="hidden-section">AdaptiveAvgPool3d</span><a class="headerlink" href="#adaptiveavgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool3d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size of the form D x H x W.
Can be a tuple (D, H, W) or a single number D for a cube D x D x D
D, H and W can be either a <code class="docutils literal"><span class="pre">int</span></code>, or <code class="docutils literal"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="padding-layers">
<h2>Padding Layers<a class="headerlink" href="#padding-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="reflectionpad2d">
<h3><span class="hidden-section">ReflectionPad2d</span><a class="headerlink" href="#reflectionpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReflectionPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReflectionPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; the size of the padding. If is int, uses the same
padding in all boundaries. If a 4-tuple, uses (paddingLeft, paddingRight, paddingTop, paddingBottom)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + paddingTop + paddingBottom\)</span>
<span class="math">\(W_{out} = W_{in} + paddingLeft + paddingRight\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad2d">
<h3><span class="hidden-section">ReplicationPad2d</span><a class="headerlink" href="#replicationpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; the size of the padding. If is int, uses the same
padding in all boundaries. If a 4-tuple, uses (paddingLeft, paddingRight, paddingTop, paddingBottom)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + paddingTop + paddingBottom\)</span>
<span class="math">\(W_{out} = W_{in} + paddingLeft + paddingRight\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad3d">
<h3><span class="hidden-section">ReplicationPad3d</span><a class="headerlink" href="#replicationpad3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad3d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; the size of the padding. If is int, uses the same
padding in all boundaries. If a 6-tuple, uses (paddingLeft, paddingRight,
paddingTop, paddingBottom, paddingFront, paddingBack)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = D_{in} + paddingFront + paddingBack\)</span>
<span class="math">\(H_{out} = H_{in} + paddingTop + paddingBottom\)</span>
<span class="math">\(W_{out} = W_{in} + paddingLeft + paddingRight\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="zeropad2d">
<h3><span class="hidden-section">ZeroPad2d</span><a class="headerlink" href="#zeropad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ZeroPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ZeroPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ZeroPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with zero.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; the size of the padding. If is int, uses the same
padding in all boundaries. If a 4-tuple, uses (paddingLeft, paddingRight, paddingTop, paddingBottom)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + paddingTop + paddingBottom\)</span>
<span class="math">\(W_{out} = W_{in} + paddingLeft + paddingRight\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad2d">
<h3><span class="hidden-section">ConstantPad2d</span><a class="headerlink" href="#constantpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad2d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For Nd-padding, use nn.functional.pad().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; the size of the padding. If is int, uses the same
padding in all boundaries. If a 4-tuple, uses (paddingLeft, paddingRight, paddingTop, paddingBottom)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + paddingTop + paddingBottom\)</span>
<span class="math">\(W_{out} = W_{in} + paddingLeft + paddingRight\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations">
<h2>Non-linear Activations<a class="headerlink" href="#non-linear-activations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="relu">
<h3><span class="hidden-section">ReLU</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise
<span class="math">\({ReLU}(x)= max(0, x)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu6">
<h3><span class="hidden-section">ReLU6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU6">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\({ReLU6}(x) = min(max(0,x), 6)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="elu">
<h3><span class="hidden-section">ELU</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>alpha</strong> &#8211; the alpha value for the ELU formulation. Default: 1.0</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="selu">
<h3><span class="hidden-section">SELU</span><a class="headerlink" href="#selu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SELU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#SELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = scale * (\max(0,x) + \min(0, alpha * (\exp(x) - 1)))\)</span>,
with <code class="docutils literal"><span class="pre">alpha=1.6732632423543772848170429916717</span></code> and
<code class="docutils literal"><span class="pre">scale=1.0507009873554804934193349852946</span></code>.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="prelu">
<h3><span class="hidden-section">PReLU</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>num_parameters=1</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math">\(PReLU(x) = max(0,x) + a * min(0,x)\)</span> Here &#8220;a&#8221; is a learnable
parameter. When called without arguments, nn.PReLU() uses a single
parameter &#8220;a&#8221; across all input channels. If called with nn.PReLU(nChannels),
a separate &#8220;a&#8221; is used for each input channel.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">weight decay should not be used when learning &#8220;a&#8221; for good performance.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_parameters</strong> &#8211; number of &#8220;a&#8221; to learn. Default: 1</li>
<li><strong>init</strong> &#8211; the initial value of &#8220;a&#8221;. Default: 0.25</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="leakyrelu">
<h3><span class="hidden-section">LeakyReLU</span><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LeakyReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = max(0, x) + {negative\_slope} * min(0, x)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>negative_slope</strong> &#8211; Controls the angle of the negative slope. Default: 1e-2</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="threshold">
<h3><span class="hidden-section">Threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Threshold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor</p>
<p>Threshold is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span>  <span class="n">x</span>        <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span>  <span class="n">threshold</span>
     <span class="n">value</span>    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">threshold</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>threshold</strong> &#8211; The value to threshold at</li>
<li><strong>value</strong> &#8211; The value to replace with</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h3><span class="hidden-section">Hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Hardtanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardtanh</code><span class="sig-paren">(</span><em>min_val=-1</em>, <em>max_val=1</em>, <em>inplace=False</em>, <em>min_value=None</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise</p>
<p>HardTanh is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span>  <span class="o">&gt;</span>  <span class="mi">1</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span>  <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span>  <span class="n">x</span><span class="p">,</span>  <span class="n">otherwise</span>
</pre></div>
</div>
<p>The range of the linear region <span class="math">\([-1, 1]\)</span> can be adjusted</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min_val</strong> &#8211; minimum value of the linear region range. Default: -1</li>
<li><strong>max_val</strong> &#8211; maximum value of the linear region range. Default: 1</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Keyword arguments <code class="xref py py-attr docutils literal"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">max_value</span></code>
have been deprecated in favor of <code class="xref py py-attr docutils literal"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">max_val</span></code></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h3><span class="hidden-section">Sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(f(x) = 1 / ( 1 + exp(-x))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanh">
<h3><span class="hidden-section">Tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanh</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsigmoid">
<h3><span class="hidden-section">LogSigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softplus">
<h3><span class="hidden-section">Softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softplus">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softplus</code><span class="sig-paren">(</span><em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(f(x) = 1/beta * log(1 + exp(beta * x_i))\)</span></p>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> &#8211; the beta value for the Softplus formulation. Default: 1</li>
<li><strong>threshold</strong> &#8211; values above this revert to a linear function. Default: 20</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h3><span class="hidden-section">Softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>SoftShrinkage operator is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="k">lambda</span> <span class="o">&gt;</span>  <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="k">lambda</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> &#8211; the lambda value for the Softshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softsign">
<h3><span class="hidden-section">Softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softsign">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softsign</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(f(x) = x / (1 + |x|)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h3><span class="hidden-section">Tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanhshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanhshrink</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(Tanhshrink(x) = x - Tanh(x)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmin">
<h3><span class="hidden-section">Softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmin">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmin</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <cite>(0, 1)</cite> and sum to 1</p>
<p><span class="math">\(f(x) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input, with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax">
<h3><span class="hidden-section">Softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>
<p>Softmax is defined as
<span class="math">\(f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
<tr class="field-even field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module doesn&#8217;t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use Logsoftmax instead (it&#8217;s faster and has better numerical properties).</p>
</div>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax2d">
<h3><span class="hidden-section">Softmax2d</span><a class="headerlink" href="#softmax2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax2d</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies SoftMax over features to each spatial location</p>
<p>When given an image of Channels x Height x Width, it will</p>
<p>apply Softmax to each location <span class="math">\((Channels, h_i, w_j)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you softmax over the 2nd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsoftmax">
<h3><span class="hidden-section">LogSoftmax</span><a class="headerlink" href="#logsoftmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSoftmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSoftmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.
The LogSoftmax formulation can be simplified as</p>
<p><span class="math">\(f_i(x) = log(exp(x_i) / sum_j exp(x_j) )\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-layers">
<h2>Normalization layers<a class="headerlink" href="#normalization-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batchnorm1d">
<h3><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 2d or 3d input that is seen as a
mini-batch.</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<p>Because the BatchNorm is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it&#8217;s common terminology to call this Temporal BatchNorm</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size
<cite>batch_size x num_features [x width]</cite></li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var
computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h3><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 4d input that is seen as a mini-batch
of 3d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<p>Because the BatchNorm is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it&#8217;s common terminology to call this Spatial BatchNorm</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of
size batch_size x num_features x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var
computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h3><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 5d input that is seen as a mini-batch
of 4d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<p>Because the BatchNorm is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it&#8217;s common terminology to call this Volumetric BatchNorm
or Spatio-temporal BatchNorm</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of
size batch_size x num_features x depth x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var
computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm1d">
<h3><span class="hidden-section">InstanceNorm1d</span><a class="headerlink" href="#instancenorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 3d input that is seen as a mini-batch.</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.use_running_stats(mode=True)</cite> method, and switch back to normal
behavior with <cite>.use_running_stats(mode=False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size <cite>batch_size x num_features x width</cite></li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm2d">
<h3><span class="hidden-section">InstanceNorm2d</span><a class="headerlink" href="#instancenorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.use_running_stats(mode=True)</cite> method, and switch back to normal
behavior with <cite>.use_running_stats(mode=False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm3d">
<h3><span class="hidden-section">InstanceNorm3d</span><a class="headerlink" href="#instancenorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.
Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.use_running_stats(mode=True)</cite> method, and switch back to normal
behavior with <cite>.use_running_stats(mode=False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x depth x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to <code class="docutils literal"><span class="pre">True</span></code>, gives the layer learnable
affine parameters. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="recurrent-layers">
<h2>Recurrent layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="rnn">
<h3><span class="hidden-section">RNN</span><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNN">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an
input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[h_t = \tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, and <span class="math">\(x_t\)</span> is
the hidden state of the previous layer at time <cite>t</cite> or <span class="math">\(input_t\)</span>
for the first layer. If nonlinearity=&#8217;relu&#8217;, then <cite>ReLU</cite> is used instead
of <cite>tanh</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>nonlinearity</strong> &#8211; The non-linearity to use [&#8216;tanh&#8217;|&#8217;relu&#8217;]. Default: &#8216;tanh&#8217;</li>
<li><strong>bias</strong> &#8211; If <code class="docutils literal"><span class="pre">False</span></code>, then the layer does not use bias weights b_ih and b_hh.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (seq_len, batch, input_size): tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor
containing the output features (h_k) from the last layer of the RNN,
for each k.  If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has
been given as the input, the output will also be a packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the hidden state for k=seq_len.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer,
of shape <cite>(hidden_size x input_size)</cite> for k=0. Otherwise, the shape is
<cite>(hidden_size x hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer,
of shape <cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstm">
<h3><span class="hidden-section">LSTM</span><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTM">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input
sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\
o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the cell
state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the hidden state of the previous layer at
time <cite>t</cite> or <span class="math">\(input_t\)</span> for the first layer, and <span class="math">\(i_t\)</span>,
<span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>, <span class="math">\(o_t\)</span> are the input, forget, cell,
and out gates, respectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>bias</strong> &#8211; If <code class="docutils literal"><span class="pre">False</span></code>, then the layer does not use bias weights b_ih and b_hh.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>input</strong> (seq_len, batch, input_size): tensor containing the features
of the input sequence.
The input can also be a packed variable length sequence.
See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a> for details.</p>
</li>
<li><p class="first"><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the initial hidden state for each element in the batch.</p>
</li>
<li><p class="first"><strong>c_0</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the initial cell state for each element in the batch.</p>
<p>If (h_0, c_0) is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: output, (h_n, c_n)</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor
containing the output features <cite>(h_t)</cite> from the last layer of the RNN,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the hidden state for t=seq_len</li>
<li><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the cell state for t=seq_len</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer
<cite>(W_ii|W_if|W_ig|W_io)</cite>, of shape <cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer
<cite>(W_hi|W_hf|W_hg|W_ho)</cite>, of shape <cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer
<cite>(b_ii|b_if|b_ig|b_io)</cite>, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer
<cite>(b_hi|b_hf|b_hg|b_ho)</cite>, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gru">
<h3><span class="hidden-section">GRU</span><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
z_t = \mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\
h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\
\end{array}\end{split}\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math">\(input_t\)</span> for the first
layer, and <span class="math">\(r_t\)</span>, <span class="math">\(z_t\)</span>, <span class="math">\(n_t\)</span> are the reset, input,
and new gates, respectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>bias</strong> &#8211; If <code class="docutils literal"><span class="pre">False</span></code>, then the layer does not use bias weights b_ih and b_hh.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (seq_len, batch, input_size): tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor
containing the output features h_t from the last layer of the RNN,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the hidden state for t=seq_len</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer
(W_ir|W_iz|W_in), of shape <cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer
(W_hr|W_hz|W_hn), of shape <cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer
(b_ir|b_iz|b_in), of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer
(b_hr|b_hz|b_hn), of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rnncell">
<h3><span class="hidden-section">RNNCell</span><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNNCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em>, <em>nonlinearity='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<div class="math">
\[h' = \tanh(w_{ih} * x + b_{ih}  +  w_{hh} * h + b_{hh})\]</div>
<p>If nonlinearity=&#8217;relu&#8217;, then ReLU is used in place of tanh.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If <code class="docutils literal"><span class="pre">False</span></code>, then the layer does not use bias weights b_ih and b_hh.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>nonlinearity</strong> &#8211; The non-linearity to use [&#8216;tanh&#8217;|&#8217;relu&#8217;]. Default: &#8216;tanh&#8217;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>hidden</strong> (batch, hidden_size): tensor containing the initial hidden
state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h&#8217;</dt>
<dd><ul class="first last simple">
<li><strong>h&#8217;</strong> (batch, hidden_size): tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape
<cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape
<cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hx</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstmcell">
<h3><span class="hidden-section">LSTMCell</span><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTMCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A long short-term memory (LSTM) cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i = \mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
f = \mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
g = \tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\
o = \mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
c' = f * c + i * g \\
h' = o * \tanh(c') \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>h_0</strong> (batch, hidden_size): tensor containing the initial hidden
state for each element in the batch.</li>
<li><strong>c_0</strong> (batch. hidden_size): tensor containing the initial cell state
for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h_1, c_1</dt>
<dd><ul class="first last simple">
<li><strong>h_1</strong> (batch, hidden_size): tensor containing the next hidden state
for each element in the batch</li>
<li><strong>c_1</strong> (batch, hidden_size): tensor containing the next cell state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape
<cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape
<cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="n">hx</span><span class="p">,</span> <span class="n">cx</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="grucell">
<h3><span class="hidden-section">GRUCell</span><a class="headerlink" href="#grucell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRUCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRUCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A gated recurrent unit (GRU) cell</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r = \mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
z = \mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\
n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
h' = (1 - z) * n + z * h
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>hidden</strong> (batch, hidden_size): tensor containing the initial hidden
state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h&#8217;</dt>
<dd><ul class="first last simple">
<li><strong>h&#8217;</strong>: (batch, hidden_size): tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape
<cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape
<cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hx</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="linear-layers">
<h2>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear">
<h3><span class="hidden-section">Linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Linear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = Ax + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_features</strong> &#8211; size of each input sample</li>
<li><strong>out_features</strong> &#8211; size of each output sample</li>
<li><strong>bias</strong> &#8211; If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</li>
<li>Output: <span class="math">\((N, *, out\_features)\)</span> where all but the last dimension
are the same shape as the input.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> &#8211; the learnable weights of the module of shape
(out_features x in_features)</li>
<li><strong>bias</strong> &#8211; the learnable bias of the module of shape (out_features)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h3><span class="hidden-section">Bilinear</span><a class="headerlink" href="#bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Bilinear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>in1_features</em>, <em>in2_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a bilinear transformation to the incoming data:
<span class="math">\(y = x_1 * A * x_2 + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in1_features</strong> &#8211; size of each first input sample</li>
<li><strong>in2_features</strong> &#8211; size of each second input sample</li>
<li><strong>out_features</strong> &#8211; size of each output sample</li>
<li><strong>bias</strong> &#8211; If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, in1\_features)\)</span>, <span class="math">\((N, in2\_features)\)</span></li>
<li>Output: <span class="math">\((N, out\_features)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> &#8211; the learnable weights of the module of shape
(out_features x in1_features x in2_features)</li>
<li><strong>bias</strong> &#8211; the learnable bias of the module of shape (out_features)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Bilinear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-layers">
<h2>Dropout layers<a class="headerlink" href="#dropout-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dropout">
<h3><span class="hidden-section">Dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <em>p</em> using samples from a bernoulli distribution.
The elements to zero are randomized on every forward call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature
detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <em>1/(1-p)</em> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> &#8211; probability of an element to be zeroed. Default: 0.5</li>
<li><strong>inplace</strong> &#8211; If set to <code class="docutils literal"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h3><span class="hidden-section">Dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout2d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero-out are randomized on every forward call.</p>
<p><em>Usually the input comes from Conv2d modules.</em></p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then iid dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If set to <code class="docutils literal"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h3><span class="hidden-section">Dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout3d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero are randomized on every forward call.</p>
<p><em>Usually the input comes from Conv3d modules.</em></p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then iid dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If set to <code class="docutils literal"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="alphadropout">
<h3><span class="hidden-section">AlphaDropout</span><a class="headerlink" href="#alphadropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AlphaDropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AlphaDropout</code><span class="sig-paren">(</span><em>p=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#AlphaDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AlphaDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Alpha Dropout over the input.</p>
<p>Alpha Dropout is a type of Dropout that maintains the self-normalizing
property.
For an input with zero mean and unit standard deviation, the output of
Alpha Dropout maintains the original mean and standard deviation of the
input.
Alpha Dropout goes hand-in-hand with SELU activation function, which ensures
that the outputs have zero mean and unit standard deviation.</p>
<p>During training, it randomly masks some of the elements of the input
tensor with probability <em>p</em> using samples from a bernoulli distribution.
The elements to masked are randomized on every forward call, and scaled
and shifted to maintain zero mean and unit standard deviation.</p>
<p>During evaluation the module simply computes an identity function.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; probability of an element to be dropped. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AlphaDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-layers">
<h2>Sparse layers<a class="headerlink" href="#sparse-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="embedding">
<h3><span class="hidden-section">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Embedding">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#Embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the size of each embedding vector</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; If given, pads the output with zeros whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; If given, will renormalize the embeddings to always have a norm lesser than this</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; The p of the p-norm to compute for the max_norm option</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; if given, this will scale gradients by the frequency of
the words in the mini-batch.</li>
<li><strong>sparse</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for
more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape (num_embeddings, embedding_dim)</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: LongTensor <cite>(N, W)</cite>, N = mini-batch, W = number of indices to extract per mini-batch</li>
<li>Output: <cite>(N, W, embedding_dim)</cite></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Keep in mind that only a limited number of optimizers support
sparse gradients: currently it&#8217;s <cite>optim.SGD</cite> (<cite>cuda</cite> and <cite>cpu</cite>),
<cite>optim.SparseAdam</cite> (<cite>cuda</cite> and <cite>cpu</cite>) and <cite>optim.Adagrad</cite> (<cite>cpu</cite>)</p>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go"> -1.0822  1.2522  0.2434</span>
<span class="go">  0.8393 -0.6062 -0.3348</span>
<span class="go">  0.6597  0.0350  0.0837</span>
<span class="go">  0.5521  0.9447  0.0498</span>

<span class="go">(1 ,.,.) =</span>
<span class="go">  0.6597  0.0350  0.0837</span>
<span class="go"> -0.1527  0.0877  0.4260</span>
<span class="go">  0.8393 -0.6062 -0.3348</span>
<span class="go"> -0.8738 -0.9054  0.4281</span>
<span class="go">[torch.FloatTensor of size 2x4x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  0.0000  0.0000  0.0000</span>
<span class="go">  0.3452  0.4937 -0.9361</span>
<span class="go">  0.0000  0.0000  0.0000</span>
<span class="go">  0.0706 -2.1962 -0.6276</span>
<span class="go">[torch.FloatTensor of size 1x4x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="embeddingbag">
<h3><span class="hidden-section">EmbeddingBag</span><a class="headerlink" href="#embeddingbag" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.EmbeddingBag">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">EmbeddingBag</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#EmbeddingBag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.EmbeddingBag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums or means of &#8216;bags&#8217; of embeddings, without instantiating the
intermediate embeddings.</p>
<dl class="docutils">
<dt>For bags of constant length,</dt>
<dd><ul class="first last simple">
<li>nn.EmbeddingBag with <cite>mode=sum</cite> is equivalent to nn.Embedding followed by <cite>torch.sum(dim=1)</cite></li>
<li>with <cite>mode=mean</cite> is equivalent to nn.Embedding followed by <cite>torch.mean(dim=1)</cite></li>
</ul>
</dd>
</dl>
<p>However, nn.EmbeddingBag is much more time and memory efficient than using a chain of these
operations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the size of each embedding vector</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; If given, will renormalize the embeddings to always have a norm lesser than this</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; The p of the p-norm to compute for the max_norm option</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; if given, this will scale gradients by the frequency of
the words in the dictionary.</li>
<li><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a><em>, </em><em>optional</em>) &#8211; &#8216;sum&#8217; | &#8216;mean&#8217;. Specifies the way to reduce the bag. Default: &#8216;mean&#8217;</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape (num_embeddings, embedding_dim)</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, offsets</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt><strong>input</strong> (N or BxN): LongTensor containing the indices of the embeddings</dt>
<dd>to extract. When <cite>input</cite> is 1D Tensor of shape <cite>N</cite>,
an <cite>offsets</cite> Tensor is given, that contains the
starting position of each new sequence in the
mini-batch.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>offsets</strong> (B or None): LongTensor containing the starting positions of</dt>
<dd>each sample in a mini-batch of variable length
sequences. If <cite>input</cite> is 2D (BxN), then offsets
does not need to be given, as the <cite>input</cite> is
treated as a mini-batch of fixed length sequences
of length <cite>N</cite> each.</dd>
</dl>
</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: LongTensor <cite>N</cite>, N = number of embeddings to extract</dt>
<dd><dl class="first last docutils">
<dt>(or) LongTensor <cite>BxN</cite>, B = number of sequences in mini-batch,</dt>
<dd>N = number of embeddings per sequence</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Offsets: LongTensor <cite>B</cite>, B = number of bags. The values are the</dt>
<dd>offsets in <cite>input</cite> for each bag, i.e. the cumsum of lengths.
Offsets is not given if Input is 2D <cite>BxN</cite> Tensor,
the input is considered to be of fixed-length sequences</dd>
</dl>
</li>
<li>Output: <cite>(B, embedding_dim)</cite></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>

<span class="go">Variable containing:</span>
<span class="go">-0.7296 -4.6926  0.3295</span>
<span class="go">-0.5186 -0.5631 -0.2792</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h2>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cosinesimilarity">
<h3><span class="hidden-section">CosineSimilarity</span><a class="headerlink" href="#cosinesimilarity" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineSimilarity">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineSimilarity</code><span class="sig-paren">(</span><em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#CosineSimilarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineSimilarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Dimension where cosine similarity is computed. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite></li>
<li>Input2: <span class="math">\((\ast_1, D, \ast_2)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math">\((\ast_1, \ast_2)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CosineSimilarity</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pairwisedistance">
<h3><span class="hidden-section">PairwiseDistance</span><a class="headerlink" href="#pairwisedistance" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PairwiseDistance">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>p=2</em>, <em>eps=1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#PairwiseDistance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors v1,v2:</p>
<div class="math">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<em>real</em>) &#8211; the norm degree. Default: 2</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid division by zero.
Default: 1e-6</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Input2: <span class="math">\((N, D)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pdist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PairwiseDistance</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="l1loss">
<h3><span class="hidden-section">L1Loss</span><a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.L1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean absolute value of the
element-wise difference between input <cite>x</cite> and target <cite>y</cite>:</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left| x_n - y_n \right|,\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then:</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the constructor argument
<cite>size_average=False</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for
each minibatch. Ignored when reduce is <code class="docutils literal"><span class="pre">False</span></code>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed
for each minibatch. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, the loss function returns
a loss per batch element instead and ignores size_average.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal"><span class="pre">False</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mseloss">
<h3><span class="hidden-section">MSELoss</span><a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MSELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MSELoss</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MSELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean squared error between
<cite>n</cite> elements in the input <cite>x</cite> and target <cite>y</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then:</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal variable
<cite>size_average</cite> to <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p>To get a batch of losses, a loss per batch element, set <cite>reduce</cite> to
<code class="docutils literal"><span class="pre">False</span></code>. These losses are not averaged and are not affected by
<cite>size_average</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for
each minibatch. Only applies when reduce is <code class="docutils literal"><span class="pre">True</span></code>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch, or summed, depending on
size_average. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, returns a loss per batch
element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="crossentropyloss">
<h3><span class="hidden-section">CrossEntropyLoss</span><a class="headerlink" href="#crossentropyloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CrossEntropyLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em>, <em>ignore_index=-100</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CrossEntropyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>LogSoftMax</cite> and <cite>NLLLoss</cite> in one single class.</p>
<p>It is useful when training a classification problem with <cite>C</cite> classes.
If provided, the optional argument <cite>weight</cite> should be a 1D <cite>Tensor</cite>
assigning weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>
<p>The <cite>input</cite> is expected to contain scores for each class.</p>
<p><cite>input</cite> has to be a 2D <cite>Tensor</cite> of size <cite>(minibatch, C)</cite>.</p>
<p>This criterion expects a class index (0 to C-1) as the
<cite>target</cite> for each value of a 1D tensor of size <cite>minibatch</cite></p>
<p>The loss can be described as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])))</span>
               <span class="o">=</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
</pre></div>
</div>
<p>or in the case of the <cite>weight</cite> argument being specified:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])))</span>
</pre></div>
</div>
<p>The losses are averaged across observations for each minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each class.
If given, has to be a Tensor of size &#8220;C&#8221;</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged over observations for each minibatch.
However, if the field size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are
instead summed for each minibatch. Ignored if reduce is <code class="docutils literal"><span class="pre">False</span></code>.</li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Specifies a target value that is ignored
and does not contribute to the input gradient. When size_average is
<code class="docutils literal"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed over
observations for each minibatch depending on size_average. When reduce
is <code class="docutils literal"><span class="pre">False</span></code>, returns a loss per batch element instead and ignores
size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite></li>
<li>Target: <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
<li>Output: scalar. If reduce is <code class="docutils literal"><span class="pre">False</span></code>, then <span class="math">\((N)\)</span> instead.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss">
<h3><span class="hidden-section">NLLLoss</span><a class="headerlink" href="#nllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.NLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em>, <em>ignore_index=-100</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#NLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>If provided, the optional argument <cite>weight</cite> should be a 1D Tensor assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.</p>
<p>The input given through a forward call is expected to contain
log-probabilities of each class: input has to be a 2D Tensor of size
<cite>(minibatch, C)</cite></p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The target that this loss expects is a class index
<cite>(0 to C-1, where C = number of classes)</cite></p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - x_{n,y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\},\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \sum_{n=1}^N w_{y_n} l_n \Big/ \sum_{n=1}^N w_{y_n} \cdot
    \mathbb{1}\{y_n \not= \text{ignore_index}\}, &amp; \text{if}\;
    \text{size_average} = \text{True},\\
    \sum_{n=1}^N w_{y_n} l_n,  &amp; \text{if}\;
    \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for
each minibatch. Ignored when reduce is <code class="docutils literal"><span class="pre">False</span></code>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Specifies a target value that is ignored
and does not contribute to the input gradient. When size_average
is <code class="docutils literal"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed
for each minibatch. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, the loss function returns
a loss per batch element instead and ignores size_average.
Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite>.</dt>
<dd>In the case of K-dimensional loss where <span class="math">\(K &gt;= 2\)</span>, then
<span class="math">\((N, C, *)\)</span> where <cite>*</cite> is <cite>K</cite> extra dimensions.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Target: <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite>.</dt>
<dd>In the case of K-dimensional loss, where <span class="math">\(K &gt;= 2\)</span>, then
<span class="math">\((N, C, *)\)</span> where <cite>*</cite> is <cite>K</cite> extra dimensions.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Output: scalar. If reduce is <code class="docutils literal"><span class="pre">False</span></code>, then <span class="math">\((N)\)</span> instead.</dt>
<dd>In the case of K-dimensional loss and reduce is <code class="docutils literal"><span class="pre">False</span></code>, then
<span class="math">\((N, C, *)\)</span>, the same size as the target.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poissonnllloss">
<h3><span class="hidden-section">PoissonNLLLoss</span><a class="headerlink" href="#poissonnllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PoissonNLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PoissonNLLLoss</code><span class="sig-paren">(</span><em>log_input=True</em>, <em>full=False</em>, <em>size_average=True</em>, <em>eps=1e-08</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#PoissonNLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PoissonNLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Negative log likelihood loss with Poisson distribution of target.</p>
<p>The loss can be described as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>target ~ Pois(input)
loss(input, target) = input - target * log(input) + log(target!)
</pre></div>
</div>
<p>The last term can be omitted or approximised with Stirling formula. The
approximation is used for target values more than 1. For targets less or
equal to 1 zeros are added to the loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_input</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if <code class="docutils literal"><span class="pre">True</span></code> the loss is computed as
<cite>exp(input) - target * input</cite>, if <code class="docutils literal"><span class="pre">False</span></code> the loss is
<cite>input - target * log(input+eps)</cite>.</li>
<li><strong>full</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; whether to compute full loss, i. e. to add the
Stirling approximation term
<cite>target * log(target) - target + 0.5 * log(2 * pi * target)</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged over
observations for each minibatch. However, if the field size_average
is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for each minibatch.</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid evaluation of log(0) when
log_input==``False``. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch, or summed, depending on
size_average. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, returns a loss per batch
element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss2d">
<h3><span class="hidden-section">NLLLoss2d</span><a class="headerlink" href="#nllloss2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.NLLLoss2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss2d</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em>, <em>ignore_index=-100</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#NLLLoss2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.NLLLoss2d" title="Permalink to this definition">¶</a></dt>
<dd><p>This is negative log likehood loss, but for image inputs. It computes
NLL loss per-pixel.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a 1D Tensor having as many elements,
as there are classes.</li>
<li><strong>size_average</strong> &#8211; By default, the losses are averaged over observations
for each minibatch. However, if the field size_average is set to
<code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for each minibatch.
Ignored when reduce is <code class="docutils literal"><span class="pre">False</span></code>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed
for each minibatch depending on size_average. When reduce is <code class="docutils literal"><span class="pre">False</span></code>,
the loss function returns a loss per batch element instead and
ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span> where <cite>C = number of classes</cite></li>
<li>Target: <span class="math">\((N, H, W)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
<li>Output: scalar. If reduce is <code class="docutils literal"><span class="pre">False</span></code>, then <span class="math">\((N, H, W)\)</span> instead.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C x height x width</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="kldivloss">
<h3><span class="hidden-section">KLDivLoss</span><a class="headerlink" href="#kldivloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.KLDivLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">KLDivLoss</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#KLDivLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.</p>
<p>As with <cite>NLLLoss</cite>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em>, however unlike <cite>ClassNLLLoss</cite>, <cite>input</cite> is not
restricted to a 2D Tensor, because the criterion is applied element-wise.</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = y_n \odot \left( \log y_n - x_n \right),\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then:</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>By default, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. However, if the field
<cite>size_average</cite> is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>(bool, optional</strong> (<em>size_average</em>) &#8211; By default, the losses are averaged
for each minibatch over observations <strong>as well as</strong> over
dimensions. However, if <code class="docutils literal"><span class="pre">False</span></code> the losses are instead summed.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch, or summed, depending on
size_average. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, returns a loss per batch
element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li><dl class="first docutils">
<dt>output: scalar. If <cite>reduce</cite> is <code class="docutils literal"><span class="pre">True</span></code>, then <span class="math">\((N, *)\)</span>,</dt>
<dd>same shape as the input</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="bceloss">
<h3><span class="hidden-section">BCELoss</span><a class="headerlink" href="#bceloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCELoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>y</cite> should be numbers
between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
&#8220;nbatch&#8221;.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for
each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bcewithlogitsloss">
<h3><span class="hidden-section">BCEWithLogitsLoss</span><a class="headerlink" href="#bcewithlogitsloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCEWithLogitsLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCEWithLogitsLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This loss combines a <cite>Sigmoid</cite> layer and the <cite>BCELoss</cite> in one single
class. This version is more numerically stable than using a plain <cite>Sigmoid</cite>
followed by a <cite>BCELoss</cite> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ t_n \cdot \log \sigma(x_n)
+ (1 - t_n) \cdot \log (1 - \sigma(x_n)) \right],\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal"><span class="pre">True</span></code>, then</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>t[i]</cite> should be numbers
between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
&#8220;nbatch&#8221;.</li>
<li><strong>size_average</strong> &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
size_average is set to <code class="docutils literal"><span class="pre">False</span></code>, the losses are instead summed for
each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="marginrankingloss">
<h3><span class="hidden-section">MarginRankingLoss</span><a class="headerlink" href="#marginrankingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MarginRankingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MarginRankingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MarginRankingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <cite>x1</cite>, <cite>x2</cite>, two 1D mini-batch <cite>Tensor`s,
and a label 1D mini-batch tensor `y</cite> with values (<cite>1</cite> or <cite>-1</cite>).</p>
<p>If <cite>y == 1</cite> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <cite>y == -1</cite>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">+</span> <span class="n">margin</span><span class="p">)</span>
</pre></div>
</div>
<p>if the internal variable <cite>size_average = True</cite>,
the loss function averages the loss over the batch samples;
if <cite>size_average = False</cite>, then the loss function sums over the batch
samples.
By default, <cite>size_average</cite> equals to <code class="docutils literal"><span class="pre">True</span></code>.</p>
</dd></dl>

</div>
<div class="section" id="hingeembeddingloss">
<h3><span class="hidden-section">HingeEmbeddingLoss</span><a class="headerlink" href="#hingeembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.HingeEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">HingeEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.HingeEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the loss given an input tensor <cite>x</cite> and a labels tensor <cite>y</cite>
containing values (<cite>1</cite> or <cite>-1</cite>).
This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as <cite>x</cite>, and is typically
used for learning nonlinear embeddings or semi-supervised learning:</p>
<p>The loss function for <span class="math">\(n\)</span>-th sample in the mini-batch is:</p>
<div class="math">
\[\begin{split}l_n = \begin{cases}
    x_n, &amp; \text{if}\; y_n = 1,\\
    \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,
\end{cases}\end{split}\]</div>
<p>and the total loss functions is</p>
<div class="math">
\[\begin{split}\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), &amp; \text{if}\; \text{size_average} = \text{True},\\
    \operatorname{sum}(L),  &amp; \text{if}\; \text{size_average} = \text{False}.
\end{cases}\end{split}\]</div>
<p>where <span class="math">\(L = \{l_1,\dots,l_N\}^\top\)</span>.</p>
<p><cite>x</cite> and <cite>y</cite> can be of arbitrary shapes with a total of <cite>n</cite> elements each.
The sum operation operates over all the elements.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal
variable <cite>size_average=False</cite>.</p>
<p>The <cite>margin</cite> has a default value of <cite>1</cite>, or can be set in the constructor.</p>
</dd></dl>

</div>
<div class="section" id="multilabelmarginloss">
<h3><span class="hidden-section">MultiLabelMarginLoss</span><a class="headerlink" href="#multilabelmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelMarginLoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>)
and output <cite>y</cite> (which is a 2D <cite>Tensor</cite> of target class indices).
For each sample in the mini-batch:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_ij</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>where <cite>i == 0</cite> to <cite>x.size(0)</cite>, <cite>j == 0</cite> to <cite>y.size(0)</cite>,
<cite>y[j] &gt;= 0</cite>, and <cite>i != y[j]</cite> for all <cite>i</cite> and <cite>j</cite>.</p>
<p><cite>y</cite> and <cite>x</cite> must have the same size.</p>
<p>The criterion only considers the first non-negative <cite>y[j]</cite> targets.</p>
<p>This allows for different samples to have variable amounts of target classes</p>
</dd></dl>

</div>
<div class="section" id="smoothl1loss">
<h3><span class="hidden-section">SmoothL1Loss</span><a class="headerlink" href="#smoothl1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SmoothL1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SmoothL1Loss</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SmoothL1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <cite>MSELoss</cite> and in some cases
prevents exploding gradients (e.g. see &#8220;Fast R-CNN&#8221; paper by Ross Girshick).
Also known as the Huber loss:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>                      <span class="p">{</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="k">if</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> \<span class="nb">sum</span> <span class="p">{</span>
                      <span class="p">{</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>   <span class="n">otherwise</span>
</pre></div>
</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each
the sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal variable
<cite>size_average</cite> to <code class="docutils literal"><span class="pre">False</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over all elements. However, if the field size_average is set to <code class="docutils literal"><span class="pre">False</span></code>,
the losses are instead summed. Ignored when reduce is <code class="docutils literal"><span class="pre">False</span></code>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed
over elements. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, the loss function returns
a loss per element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal"><span class="pre">False</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="softmarginloss">
<h3><span class="hidden-section">SoftMarginLoss</span><a class="headerlink" href="#softmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SoftMarginLoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a two-class classification
logistic loss between input <cite>x</cite> (a 2D mini-batch Tensor) and
target <cite>y</cite> (which is a tensor containing either <cite>1</cite> or <cite>-1</cite>).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_i</span> <span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
</pre></div>
</div>
<p>The normalization by the number of elements in the input can be disabled by
setting <cite>self.size_average</cite> to <code class="docutils literal"><span class="pre">False</span></code>.</p>
</dd></dl>

</div>
<div class="section" id="multilabelsoftmarginloss">
<h3><span class="hidden-section">MultiLabelSoftMarginLoss</span><a class="headerlink" href="#multilabelsoftmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelSoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelSoftMarginLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelSoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>) and
target <cite>y</cite> (a binary 2D <cite>Tensor</cite>). For each sample in the minibatch:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_i</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="p">)</span>
                  <span class="o">+</span> <span class="p">(</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
<p>where <cite>i == 0</cite> to <cite>x.nElement()-1</cite>, <cite>y[i]  in {0,1}</cite>.
<cite>y</cite> and <cite>x</cite> must have the same size.</p>
</dd></dl>

</div>
<div class="section" id="cosineembeddingloss">
<h3><span class="hidden-section">CosineEmbeddingLoss</span><a class="headerlink" href="#cosineembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given  an input tensors
x1, x2 and a <cite>Tensor</cite> label <cite>y</cite> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.</p>
<p><cite>margin</cite> should be a number from <cite>-1</cite> to <cite>1</cite>, <cite>0</cite> to <cite>0.5</cite> is suggested.
If <cite>margin</cite> is missing, the default value is <cite>0</cite>.</p>
<p>The loss function for each sample is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>             <span class="p">{</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cos</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span>              <span class="k">if</span> <span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span>
             <span class="p">{</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cos</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">margin</span><span class="p">),</span> <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>If the internal variable <cite>size_average</cite> is equal to <code class="docutils literal"><span class="pre">True</span></code>,
the loss function averages the loss over the batch samples;
if <cite>size_average</cite> is <code class="docutils literal"><span class="pre">False</span></code>, then the loss function sums over the
batch samples. By default, <cite>size_average = True</cite>.</p>
</dd></dl>

</div>
<div class="section" id="multimarginloss">
<h3><span class="hidden-section">MultiMarginLoss</span><a class="headerlink" href="#multimarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiMarginLoss</code><span class="sig-paren">(</span><em>p=1</em>, <em>margin=1</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class classification hinge
loss (margin-based loss) between input <cite>x</cite> (a 2D mini-batch <cite>Tensor</cite>) and
output <cite>y</cite> (which is a 1D tensor of target class indices,
<cite>0</cite> &lt;= <cite>y</cite> &lt;= <cite>x.size(1)</cite>):</p>
<p>For each mini-batch sample:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0)
             where `i == 0` to `x.size(0)` and `i != y`.
</pre></div>
</div>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <cite>weight</cite> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<blockquote>
<div>loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0)</div></blockquote>
<p>By default, the losses are averaged over observations for each minibatch.
However, if the field <cite>size_average</cite> is set to <code class="docutils literal"><span class="pre">False</span></code>,
the losses are instead summed.</p>
</dd></dl>

</div>
<div class="section" id="tripletmarginloss">
<h3><span class="hidden-section">TripletMarginLoss</span><a class="headerlink" href="#tripletmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.TripletMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">TripletMarginLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#TripletMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors x1, x2, x3 and a margin with a value greater than 0.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite>: anchor, positive examples and negative
example respectively. The shape of all input variables should be
<span class="math">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<div class="math">
\[L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\} \right)\]</div>
<p>where <span class="math">\(d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>anchor</strong> &#8211; anchor input tensor</li>
<li><strong>positive</strong> &#8211; positive input tensor</li>
<li><strong>negative</strong> &#8211; negative input tensor</li>
<li><strong>p</strong> &#8211; the norm degree. Default: 2</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input3</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="vision-layers">
<h2>Vision layers<a class="headerlink" href="#vision-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixelshuffle">
<h3><span class="hidden-section">PixelShuffle</span><a class="headerlink" href="#pixelshuffle" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PixelShuffle">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PixelShuffle</code><span class="sig-paren">(</span><em>upscale_factor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a Tensor of shape <span class="math">\((*, C * r^2, H, W]\)</span> to a
tensor of shape <span class="math">\((C, H * r, W * r)\)</span>.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math">\(1/r\)</span>.</p>
<p>Look at the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; factor to increase spatial resolution by</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C * {upscale\_factor}^2, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H * {upscale\_factor}, W * {upscale\_factor})\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsample">
<h3><span class="hidden-section">Upsample</span><a class="headerlink" href="#upsample" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Upsample">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Upsample</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#Upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form <cite>minibatch x channels x [depth] x [height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear
for 3D, 4D and 5D input Tensor, respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; a tuple of ints ([D_out], [H_out], W_out) output sizes</li>
<li><strong>scale_factor</strong> (<em>int / tuple of python:ints</em><em>, </em><em>optional</em>) &#8211; the multiplier for the image height / width / depth</li>
<li><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a><em>, </em><em>optional</em>) &#8211; the upsampling algorithm: nearest | linear | bilinear | trilinear. Default: nearest</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, W_{in})\)</span>, <span class="math">\((N, C, H_{in}, W_{in})\)</span> or <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, W_{out})\)</span>, <span class="math">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor(D_{in} * scale\_factor)\)</span> or <cite>size[-3]</cite>
<span class="math">\(H_{out} = floor(H_{in} * scale\_factor)\)</span> or <cite>size[-2]</cite>
<span class="math">\(W_{out} = floor(W_{in}  * scale\_factor)\)</span> or <cite>size[-1]</cite></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1.0000  1.3333  1.6667  2.0000</span>
<span class="go">  1.6667  2.0000  2.3333  2.6667</span>
<span class="go">  2.3333  2.6667  3.0000  3.3333</span>
<span class="go">  3.0000  3.3333  3.6667  4.0000</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  1  2  2</span>
<span class="go">  1  1  2  2</span>
<span class="go">  3  3  4  4</span>
<span class="go">  3  3  4  4</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingnearest2d">
<h3><span class="hidden-section">UpsamplingNearest2d</span><a class="headerlink" href="#upsamplingnearest2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingNearest2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingNearest2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code>
as it&#8217;s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image (h, w).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; a tuple of ints (H_out, W_out) output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the multiplier for the image height / width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor(H_{in} * scale\_factor)\)</span>
<span class="math">\(W_{out} = floor(W_{in}  * scale\_factor)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  1  2  2</span>
<span class="go">  1  1  2  2</span>
<span class="go">  3  3  4  4</span>
<span class="go">  3  3  4  4</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingbilinear2d">
<h3><span class="hidden-section">UpsamplingBilinear2d</span><a class="headerlink" href="#upsamplingbilinear2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingBilinear2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingBilinear2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code>
as it&#8217;s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image (h, w).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; a tuple of ints (H_out, W_out) output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the multiplier for the image height / width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor(H_{in} * scale\_factor)\)</span>
<span class="math">\(W_{out} = floor(W_{in}  * scale\_factor)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1.0000  1.3333  1.6667  2.0000</span>
<span class="go">  1.6667  2.0000  2.3333  2.6667</span>
<span class="go">  2.3333  2.6667  3.0000  3.3333</span>
<span class="go">  3.0000  3.3333  3.6667  4.0000</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-layers-multi-gpu-distributed">
<h2>DataParallel layers (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-layers-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataparallel">
<h3><span class="hidden-section">DataParallel</span><a class="headerlink" href="#dataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.DataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">DataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#DataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. In the forward pass, the module is replicated on each device,
and each replica handles a portion of the input. During the backwards
pass, gradients from each replica are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used. It should
also be an integer multiple of the number of GPUs so that each chunk is the
same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel EXCEPT Tensors. All variables will be scattered on dim
specified (default 0). Primitive types will be broadcasted, but all
other types will be a shallow copy and can be corrupted if written to in
the model&#8217;s forward pass.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backwrad hooks defined on <code class="xref py py-attr docutils literal"><span class="pre">module</span></code> and its submodules
won&#8217;t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code> method.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> &#8211; module to be parallelized</li>
<li><strong>device_ids</strong> &#8211; CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> &#8211; device location of output (default: device_ids[0])</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="distributeddataparallel">
<h3><span class="hidden-section">DistributedDataParallel</span><a class="headerlink" href="#distributeddataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/distributed.html#DistributedDataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine and each device, and
each such replica handles a portion of the input. During the backwards
pass, gradients from each node are averaged.</p>
<p>The batch size should be larger than the number of GPUs used locally. It
should also be an integer multiple of the number of GPUs so that each chunk
is the same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a class="reference internal" href="distributed.html#distributed-basics"><span class="std std-ref">Basics</span></a> and <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a>.
The same constraints on input as in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.DataParallel</span></code></a> apply.</p>
<p>Creation of this class requires the distributed package to be already
initialized in the process group mode
(see <a class="reference internal" href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code class="xref py py-func docutils literal"><span class="pre">torch.distributed.init_process_group()</span></code></a>).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module works only with the <code class="docutils literal"><span class="pre">gloo</span></code> backend.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different processes might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.
Same applies to buffers.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all buffers and gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module doesn&#8217;t work with <a class="reference internal" href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Parameters are never broadcast between processes. The module performs
an all-reduce step on gradients and assumes that they will be modified
by the optimizer in all processes in the same way. Buffers
(e.g. BatchNorm stats) are broadcast form the module in process of rank
0, to all other replicas in the system in every iteration.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backwrad hooks defined on <code class="xref py py-attr docutils literal"><span class="pre">module</span></code> and its submodules
won&#8217;t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code> method.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> &#8211; module to be parallelized</li>
<li><strong>device_ids</strong> &#8211; CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> &#8211; device location of output (default: device_ids[0])</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="clip-grad-norm">
<h3><span class="hidden-section">clip_grad_norm</span><a class="headerlink" href="#clip-grad-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_norm</code><span class="sig-paren">(</span><em>parameters</em>, <em>max_norm</em>, <em>norm_type=2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/clip_grad.html#clip_grad_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.clip_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>]</em><em></em>) &#8211; an iterable of Variables that will have
gradients normalized</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; max norm of the gradients</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; type of the used p-norm. Can be <code class="docutils literal"><span class="pre">'inf'</span></code> for
infinity norm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Total norm of the parameters (viewed as a single vector).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="weight-norm">
<h3><span class="hidden-section">weight_norm</span><a class="headerlink" href="#weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies weight normalization to a parameter in the given module.</p>
<div class="math">
\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}\]</div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <cite>name</cite> (e.g. &#8220;weight&#8221;) with two parameters: one specifying the magnitude
(e.g. &#8220;weight_g&#8221;) and one specifying the direction (e.g. &#8220;weight_v&#8221;).
Weight normalization is implemented via a hook that recomputes the weight
tensor from the magnitude and direction before every <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a>
call.</p>
<p>By default, with <cite>dim=0</cite>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<cite>dim=None</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#8211; containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em>, </em><em>optional</em>) &#8211; name of weight parameter</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; dimension over which to compute the norm</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original module with the weight norm hook</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="go">Linear (20 -&gt; 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-weight-norm">
<h3><span class="hidden-section">remove_weight_norm</span><a class="headerlink" href="#remove-weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.remove_weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#remove_weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the weight normalization reparameterization from a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#8211; containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em>, </em><em>optional</em>) &#8211; name of weight parameter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="packedsequence">
<h3><span class="hidden-section">PackedSequence</span><a class="headerlink" href="#packedsequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.PackedSequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">PackedSequence</code><span class="sig-paren">(</span><em>_cls</em>, <em>data</em>, <em>batch_sizes</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#PackedSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.PackedSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds the data and list of batch_sizes of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Instances of this class should never be created manually. They are meant
to be instantiated by functions like <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Variable containing packed sequence</li>
<li><strong>batch_sizes</strong> (<em>list</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; list of integers holding information about
the batch size at each sequence step</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-padded-sequence">
<h3><span class="hidden-section">pack_padded_sequence</span><a class="headerlink" href="#pack-padded-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_padded_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_padded_sequence</code><span class="sig-paren">(</span><em>input</em>, <em>lengths</em>, <em>batch_first=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pack_padded_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pack_padded_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a Variable containing padded sequences of variable length.</p>
<p>Input can be of size <code class="docutils literal"><span class="pre">TxBx*</span></code> where T is the length of the longest sequence
(equal to <code class="docutils literal"><span class="pre">lengths[0]</span></code>), B is the batch size, and * is any number of
dimensions (including 0). If <code class="docutils literal"><span class="pre">batch_first</span></code> is True <code class="docutils literal"><span class="pre">BxTx*</span></code> inputs are
expected.</p>
<p>The sequences should be sorted by length in a decreasing order, i.e.
<code class="docutils literal"><span class="pre">input[:,0]</span></code> should be the longest sequence, and <code class="docutils literal"><span class="pre">input[:,B-1]</span></code> the
shortest one.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function accept any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Variable can be retrieved from
a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">PackedSequence</span></code></a> object by accessing its <code class="docutils literal"><span class="pre">.data</span></code> attribute.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; padded batch of variable length sequences.</li>
<li><strong>lengths</strong> (<em>list</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; list of sequences lengths of each batch element.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, the input is expected in BxTx*
format.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">PackedSequence</span></code></a> object</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-packed-sequence">
<h3><span class="hidden-section">pad_packed_sequence</span><a class="headerlink" href="#pad-packed-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_packed_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_packed_sequence</code><span class="sig-paren">(</span><em>sequence</em>, <em>batch_first=False</em>, <em>padding_value=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pad_packed_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pad_packed_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>The returned Variable&#8217;s data will be of size TxBx*, where T is the length
of the longest sequence and B is the batch size. If <code class="docutils literal"><span class="pre">batch_first</span></code> is True,
the data will be transposed into BxTx* format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequence</strong> (<a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><em>PackedSequence</em></a>) &#8211; batch to pad</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if <code class="docutils literal"><span class="pre">True</span></code>, the output will be in BxTx*
format.</li>
<li><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; values for padded elements</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple of Variable containing the padded sequence, and a list of lengths
of each sequence in the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-functional">
<h1>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convolution-functions">
<h2>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id14">
<h3><span class="hidden-section">conv1d</span><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (out_channels x in_channels x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or
a one-element tuple (sW,). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a one-element tuple (padW,). Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a one-element tuple (dW,). Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id15">
<h3><span class="hidden-section">conv2d</span><a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters tensor (out_channels x in_channels/groups x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias tensor (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or a
tuple (sH, sW). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padH, padW). Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a tuple (dH, dW). Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by the
number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id16">
<h3><span class="hidden-section">conv3d</span><a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iT x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters tensor of shape (out_channels x in_channels x kT x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias tensor of shape (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or a
tuple (sT, sH, sW). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padT, padH, padW). Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a tuple (dT, dH, dW). Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h3><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv_transpose1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called &#8220;deconvolution&#8221;.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal"><span class="pre">ConvTranspose1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (in_channels x out_channels x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or a
tuple (sW,). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padW,). Default: 0</li>
<li><strong>output_padding</strong> &#8211; implicit zero-paddings of 0 &lt;= padding &lt; stride on both
sides of the output. Can be a single number or a tuple (out_padW,).
Default: 0</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a tuple (dW,). Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h3><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv_transpose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called &#8220;deconvolution&#8221;.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (in_channels x out_channels x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or a
tuple (sH, sW). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padH, padW). Default: 0</li>
<li><strong>output_padding</strong> &#8211; implicit zero-paddings of 0 &lt;= padding &lt; stride on both
sides of the output. Can be a single number or a tuple
(out_padH, out_padW). Default: 0</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a tuple (dH, dW). Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h3><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#conv_transpose3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called &#8220;deconvolution&#8221;</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iT x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (in_channels x out_channels x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels). Default: None</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or a
tuple (sT, sH, sW). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padT, padH, padW). Default: 0</li>
<li><strong>output_padding</strong> &#8211; implicit zero-paddings of 0 &lt;= padding &lt; stride on both
sides of the output. Can be a single number or a tuple
(out_padT, out_padH, out_padW). Default: 0</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Can be a single number or
a tuple (dT, dH, dW). Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h2>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="avg-pool1d">
<h3><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#avg_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iW)</li>
<li><strong>kernel_size</strong> &#8211; the size of the window. Can be a single number or a
tuple (kW,)</li>
<li><strong>stride</strong> &#8211; the stride of the window. Can be a single number or a tuple
(sW,). Default: <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padW,). Default: 0</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  2  4  6</span>
<span class="go">[torch.FloatTensor of size 1x1x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h3><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in kh x kw regions by step size
dh x dw steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iH x iW)</li>
<li><strong>kernel_size</strong> &#8211; size of the pooling region. Can be a single number or a
tuple (kH x kW)</li>
<li><strong>stride</strong> &#8211; stride of the pooling operation. Can be a single number or a
tuple (sH, sW). Default is equal to kernel size</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padH, padW). Default: 0</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in th
averaging calculation. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Default value for <code class="xref py py-attr docutils literal"><span class="pre">count_include_pad</span></code> was <code class="docutils literal"><span class="pre">True</span></code> in versions before 0.3, and will be changed back to <code class="docutils literal"><span class="pre">True</span></code> from 0.4.1 and forward.</p>
</div>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h3><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in kt x kh x kw regions by step
size dt x dh x dw steps. The number of output features is equal to the
number of input planes / dt.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-class docutils literal"><span class="pre">AvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iT x iH x iW)</li>
<li><strong>kernel_size</strong> &#8211; size of the pooling region. Can be a single number or a
tuple (kT x kH x kW)</li>
<li><strong>stride</strong> &#8211; stride of the pooling operation. Can be a single number or a
tuple (sT, sH, sW). Default is equal to kernel size</li>
<li><strong>padding</strong> &#8211; implicit zero paddings on both sides of the input. Can be a
single number or a tuple (padT, padH, padW), Default: 0</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in th
averaging calculation. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Default value for <code class="xref py py-attr docutils literal"><span class="pre">count_include_pad</span></code> was <code class="docutils literal"><span class="pre">True</span></code> in versions before 0.3, and will be changed back to <code class="docutils literal"><span class="pre">True</span></code> from 0.4.1 and forward.</p>
</div>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h3><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool2d">
<h3><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool3d">
<h3><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h3><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h3><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h3><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h3><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-class docutils literal"><span class="pre">LPPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h3><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size (single integer)</li>
<li><strong>return_indices</strong> &#8211; whether to return pooling indices. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h3><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size (single integer or
double-integer tuple)</li>
<li><strong>return_indices</strong> &#8211; whether to return pooling indices. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool3d">
<h3><span class="hidden-section">adaptive_max_pool3d</span><a class="headerlink" href="#adaptive-max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_max_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveMaxPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size (single integer or
triple-integer tuple)</li>
<li><strong>return_indices</strong> &#8211; whether to return pooling indices. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h3><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size (single integer)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h3><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size (single integer or
double-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool3d">
<h3><span class="hidden-section">adaptive_avg_pool3d</span><a class="headerlink" href="#adaptive-avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveAvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size (single integer or
triple-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h2>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id17">
<h3><span class="hidden-section">threshold</span><a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>See <a class="reference internal" href="#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-class docutils literal"><span class="pre">Threshold</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id18">
<h3><span class="hidden-section">relu</span><a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise. See
<a class="reference internal" href="#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal"><span class="pre">ReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id19">
<h3><span class="hidden-section">hardtanh</span><a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise. See <a class="reference internal" href="#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-class docutils literal"><span class="pre">Hardtanh</span></code></a> for more
details.</p>
</dd></dl>

</div>
<div class="section" id="id20">
<h3><span class="hidden-section">relu6</span><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#relu6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\({ReLU6}(x) = min(max(0,x), 6)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-class docutils literal"><span class="pre">ReLU6</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id21">
<h3><span class="hidden-section">elu</span><a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal"><span class="pre">ELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id22">
<h3><span class="hidden-section">selu</span><a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.selu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = scale * (\max(0,x) + \min(0, alpha * (\exp(x) - 1)))\)</span>,
with <code class="docutils literal"><span class="pre">alpha=1.6732632423543772848170429916717</span></code> and
<code class="docutils literal"><span class="pre">scale=1.0507009873554804934193349852946</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-class docutils literal"><span class="pre">SELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="leaky-relu">
<h3><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = max(0, x) + {negative\_slope} * min(0, x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal"><span class="pre">LeakyReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id23">
<h3><span class="hidden-section">prelu</span><a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#prelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math">\(PReLU(x) = max(0,x) + weight * min(0,x)\)</span> where weight is a
learnable parameter.</p>
<p>See <a class="reference internal" href="#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-class docutils literal"><span class="pre">PReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="rrelu">
<h3><span class="hidden-section">rrelu</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="glu">
<h3><span class="hidden-section">glu</span><a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.glu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">glu</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.glu" title="Permalink to this definition">¶</a></dt>
<dd><p>The gated linear unit. Computes:</p>
<div class="math">
\[H = A \times \sigma(B)\]</div>
<p>where <cite>input</cite> is split in half along <cite>dim</cite> to form <cite>A</cite> and <cite>B</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input variable</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension on which to split the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id24">
<h3><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-class docutils literal"><span class="pre">LogSigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="hardshrink">
<h3><span class="hidden-section">hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise</p>
<p>See <code class="xref py py-class docutils literal"><span class="pre">Hardshrink</span></code> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id25">
<h3><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(Tanhshrink(x) = x - Tanh(x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-class docutils literal"><span class="pre">Tanhshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id26">
<h3><span class="hidden-section">softsign</span><a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(f(x) = x / (1 + |x|)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-class docutils literal"><span class="pre">Softsign</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id27">
<h3><span class="hidden-section">softplus</span><a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id28">
<h3><span class="hidden-section">softmin</span><a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmin function.</p>
<p>Note that softmin(x) = softmax(-x). See softmax definition for mathematical formula.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-class docutils literal"><span class="pre">Softmin</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which softmin will be computed (so every slice
along dim will sum to 1).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id29">
<h3><span class="hidden-section">softmax</span><a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax function.</p>
<p>Softmax is defined as:</p>
<p><span class="math">\(softmax(x) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)</span></p>
<p>It is applied to all slices along dim, and will rescale them so that the elements
lie in the range <cite>(0, 1)</cite> and sum to 1.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-class docutils literal"><span class="pre">Softmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which softmax will be computed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function doesn&#8217;t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use log_softmax instead (it&#8217;s faster and has better numerical properties).</p>
</div>
</dd></dl>

</div>
<div class="section" id="id30">
<h3><span class="hidden-section">softshrink</span><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#softshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>See <a class="reference internal" href="#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-class docutils literal"><span class="pre">Softshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="log-softmax">
<h3><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax followed by a logarithm.</p>
<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.</p>
<p>See <a class="reference internal" href="#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-class docutils literal"><span class="pre">LogSoftmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; A dimension along which log_softmax will be computed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id31">
<h3><span class="hidden-section">tanh</span><a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-class docutils literal"><span class="pre">Tanh</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id32">
<h3><span class="hidden-section">sigmoid</span><a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(f(x) = 1 / ( 1 + exp(-x))\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal"><span class="pre">Sigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h2>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batch-norm">
<h3><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#batch_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="normalize">
<h3><span class="hidden-section">normalize</span><a class="headerlink" href="#normalize" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.normalize">
<code class="descclassname">torch.nn.functional.</code><code class="descname">normalize</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em>, <em>dim=1</em>, <em>eps=1e-12</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs <span class="math">\(L_p\)</span> normalization of inputs over specified dimension.</p>
<p>Does:</p>
<div class="math">
\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}\]</div>
<p>for each subtensor v over dimension dim of input. Each subtensor is
flattened into a vector, i.e. <span class="math">\(\lVert v \rVert_p\)</span> is not a matrix
norm.</p>
<p>With default arguments normalizes over the second dimension with Euclidean
norm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of any shape</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; the exponent value in the norm formulation. Default: 2</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; small value to avoid division by zero. Default: 1e-12</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h2>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id33">
<h3><span class="hidden-section">linear</span><a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span>.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</li>
<li>Weight: <span class="math">\((out\_features, in\_features)\)</span></li>
<li>Bias: <span class="math">\((out\_features)\)</span></li>
<li>Output: <span class="math">\((N, *, out\_features)\)</span></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h2>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id34">
<h3><span class="hidden-section">dropout</span><a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="alpha-dropout">
<h3><span class="hidden-section">alpha_dropout</span><a class="headerlink" href="#alpha-dropout" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.alpha_dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">alpha_dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#alpha_dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.alpha_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies alpha dropout to the input.</p>
<p>See <a class="reference internal" href="#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-class docutils literal"><span class="pre">AlphaDropout</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; the drop probability. Default: 0.5</li>
<li><strong>training</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; switch between training and evaluation mode. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id35">
<h3><span class="hidden-section">dropout2d</span><a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout2d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout2d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id36">
<h3><span class="hidden-section">dropout3d</span><a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout3d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout3d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="id37">
<h2>Distance functions<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pairwise-distance">
<h3><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2</em>, <em>eps=1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pairwise_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors v1,v2:</p>
<div class="math">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x1</strong> &#8211; first input tensor</li>
<li><strong>x2</strong> &#8211; second input tensor</li>
<li><strong>p</strong> &#8211; the norm degree. Default: 2</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid division by zero. Default: 1e-6</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="cosine-similarity">
<h3><span class="hidden-section">cosine_similarity</span><a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_similarity">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cosine_similarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x1</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; First input.</li>
<li><strong>x2</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Second input (of size matching x1).</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Dimension of vectors. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite>.</li>
<li>Output: <span class="math">\((\ast_1, \ast_2)\)</span> where 1 is at position <cite>dim</cite>.</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="id38">
<h2>Loss functions<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h2>
<div class="section" id="binary-cross-entropy">
<h3><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output.</p>
<p>See <a class="reference internal" href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal"><span class="pre">BCELoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable of arbitrary shape</li>
<li><strong>target</strong> &#8211; Variable of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight
if provided it&#8217;s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poisson-nll-loss">
<h3><span class="hidden-section">poisson_nll_loss</span><a class="headerlink" href="#poisson-nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.poisson_nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">poisson_nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>log_input=True</em>, <em>full=False</em>, <em>size_average=True</em>, <em>eps=1e-08</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#poisson_nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.poisson_nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Poisson negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-class docutils literal"><span class="pre">PoissonNLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; expectation of underlying Poisson distribution.</li>
<li><strong>target</strong> &#8211; random sample <span class="math">\(target \sim Pois(input)\)</span>.</li>
<li><strong>log_input</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code> the loss is computed as
<cite>exp(input) - target * input</cite>, if <code class="docutils literal"><span class="pre">False</span></code> then loss is
<cite>input - target * log(input+eps)</cite>. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>full</strong> &#8211; whether to compute full loss, i. e. to add the Stirling
approximation term. Default: <code class="docutils literal"><span class="pre">False</span></code>
<cite>target * log(target) - target + 0.5 * log(2 * pi * target)</cite>.</li>
<li><strong>size_average</strong> &#8211; By default, the losses are averaged over observations for
each minibatch. However, if the field sizeAverage is set to False,
the losses are instead summed for each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; Small value to avoid evaluation of log(0) when
log_input=False. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch, or summed, depending on
size_average. When reduce is <code class="docutils literal"><span class="pre">False</span></code>, returns a loss per batch
element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cosine-embedding-loss">
<h3><span class="hidden-section">cosine_embedding_loss</span><a class="headerlink" href="#cosine-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_embedding_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#cosine_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-class docutils literal"><span class="pre">CosineEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h3><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em>, <em>ignore_index=-100</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in a single
function.</p>
<p>See <a class="reference internal" href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal"><span class="pre">CrossEntropyLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite></li>
<li><strong>target</strong> &#8211; Variable <span class="math">\((N)\)</span> where each value is
<cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch. Ignored if reduce is False. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Specifies a target value that is ignored
and does not contribute to the input gradient. When size_average is
True, the loss is averaged over non-ignored targets. Default: -100</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged or summed over
observations for each minibatch depending on size_average. When reduce
is False, returns a loss per batch element instead and ignores
size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hinge-embedding-loss">
<h3><span class="hidden-section">hinge_embedding_loss</span><a class="headerlink" href="#hinge-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hinge_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hinge_embedding_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>margin=1.0</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#hinge_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hinge_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-class docutils literal"><span class="pre">HingeEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h3><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss.</p>
<p>See <a class="reference internal" href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable of arbitrary shape</li>
<li><strong>target</strong> &#8211; Variable of the same shape as input</li>
<li><strong>size_average</strong> &#8211; if <code class="docutils literal"><span class="pre">True</span></code> the output is divided by the number of elements
in input tensor. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch, or summed, depending on
size_average. When reduce is False, returns a loss per batch
element instead and ignores size_average. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="l1-loss">
<h3><span class="hidden-section">l1_loss</span><a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that takes the mean element-wise absolute value difference.</p>
<p>See <a class="reference internal" href="#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-class docutils literal"><span class="pre">L1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="mse-loss">
<h3><span class="hidden-section">mse_loss</span><a class="headerlink" href="#mse-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.mse_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em>, <em>reduce=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#mse_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the element-wise mean squared error.</p>
<p>See <a class="reference internal" href="#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-class docutils literal"><span class="pre">MSELoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="margin-ranking-loss">
<h3><span class="hidden-section">margin_ranking_loss</span><a class="headerlink" href="#margin-ranking-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.margin_ranking_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">margin_ranking_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#margin_ranking_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.margin_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-class docutils literal"><span class="pre">MarginRankingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-margin-loss">
<h3><span class="hidden-section">multilabel_margin_loss</span><a class="headerlink" href="#multilabel-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.multilabel_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-class docutils literal"><span class="pre">MultiLabelMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-soft-margin-loss">
<h3><span class="hidden-section">multilabel_soft_margin_loss</span><a class="headerlink" href="#multilabel-soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-class docutils literal"><span class="pre">MultiLabelSoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multi-margin-loss">
<h3><span class="hidden-section">multi_margin_loss</span><a class="headerlink" href="#multi-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multi_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multi_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>p=1</em>, <em>margin=1</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="reference internal" href="_modules/torch/nn/functional.html#multi_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multi_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-class docutils literal"><span class="pre">MultiMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="nll-loss">
<h3><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em>, <em>ignore_index=-100</em>, <em>reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal"><span class="pre">NLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K &gt; 1\)</span>
in the case of K-dimensional loss.</li>
<li><strong>target</strong> &#8211; <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite>,
or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K &gt;= 1\)</span> for
K-dimensional loss.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. If size_average
is False, the losses are summed for each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Specifies a target value that is ignored
and does not contribute to the input gradient. When size_average is
True, the loss is averaged over non-ignored targets. Default: -100</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy-with-logits">
<h3><span class="hidden-section">binary_cross_entropy_with_logits</span><a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy_with_logits">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy_with_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures Binary Cross Entropy between target and output
logits.</p>
<p>See <a class="reference internal" href="#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-class docutils literal"><span class="pre">BCEWithLogitsLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable of arbitrary shape</li>
<li><strong>target</strong> &#8211; Variable of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight
if provided it&#8217;s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch. Default: <code class="docutils literal"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h3><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.</p>
<p>See <a class="reference internal" href="#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-class docutils literal"><span class="pre">SmoothL1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="soft-margin-loss">
<h3><span class="hidden-section">soft_margin_loss</span><a class="headerlink" href="#soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span> &#x2192; Variable<a class="headerlink" href="#torch.nn.functional.soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-class docutils literal"><span class="pre">SoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="triplet-margin-loss">
<h3><span class="hidden-section">triplet_margin_loss</span><a class="headerlink" href="#triplet-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.triplet_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">triplet_margin_loss</code><span class="sig-paren">(</span><em>anchor</em>, <em>positive</em>, <em>negative</em>, <em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#triplet_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.triplet_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors x1, x2, x3 and a margin with a value greater than 0.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite>: anchor, positive examples and negative
example respectively. The shape of all input variables should be
<span class="math">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<div class="math">
\[L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\} \right)\]</div>
<p>where <span class="math">\(d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>anchor</strong> &#8211; anchor input tensor</li>
<li><strong>positive</strong> &#8211; positive input tensor</li>
<li><strong>negative</strong> &#8211; negative input tensor</li>
<li><strong>margin</strong> &#8211; the margin value. Default: 1</li>
<li><strong>p</strong> &#8211; the norm degree. Default: 2</li>
<li><strong>eps</strong> &#8211; small epsilon value to avoid numerical issues. Default: 1e-6</li>
<li><strong>swap</strong> &#8211; compute distance swap. Default: <code class="docutils literal"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input3</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h2>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixel-shuffle">
<h3><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><em>input</em>, <em>upscale_factor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pixel_shuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <code class="docutils literal"><span class="pre">[*,</span> <span class="pre">C*r^2,</span> <span class="pre">H,</span> <span class="pre">W]</span></code> to a
tensor of shape <code class="docutils literal"><span class="pre">[C,</span> <span class="pre">H*r,</span> <span class="pre">W*r]</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Input</li>
<li><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; factor to increase spatial resolution by</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h3><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<dl class="docutils">
<dt>Nd constant padding:  The number of dimensions to pad is</dt>
<dd>len(padding) // 2 and the dimensions that gets padded begins with the
last dimension and moves forward.  See below for examples.</dd>
<dt>1D, 2D and 3D &#8220;reflect&#8221;/&#8221;replicate&#8221; padding:</dt>
<dd>1D: 3D input with padding in form (pad_l, pad_r)
2D: 4D input tensor pad should be in form
(pad_l, pad_r, pad_t, pad_b ).
3D: 5D pad (pleft, pright, ptop, pbottom, pfront, pback). No &#8220;reflect&#8221;
implementation</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Nd tensor</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; m-elem tuple, where m // 2 &lt;= input dimensions and m % 2 == 0</li>
<li><strong>mode</strong> &#8211; &#8216;constant&#8217;, &#8216;reflect&#8217; or &#8216;replicate&#8217;. Default: &#8216;constant&#8217;</li>
<li><strong>value</strong> &#8211; fill value for &#8216;constant&#8217; padding. Default: 0</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># pad last dim by 1 on each side</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p1d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p2d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 8, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p3d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p3d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 9, 7, 3])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id40">
<h3><span class="hidden-section">upsample</span><a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code></p>
<p>The algorithm used for upsampling is determined by <code class="xref py py-attr docutils literal"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric upsampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [depth] x [height] x width</cite></p>
<p>The modes available for upsampling are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; multiplier for spatial size. Has to be an integer.</li>
<li><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; algorithm used for upsampling:
&#8216;nearest&#8217; | &#8216;linear&#8217; | &#8216;bilinear&#8217; | &#8216;trilinear&#8217;. Default: &#8216;nearest&#8217;</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="upsample-nearest">
<h3><span class="hidden-section">upsample_nearest</span><a class="headerlink" href="#upsample-nearest" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_nearest">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_nearest</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_nearest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_nearest" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using nearest neighbours&#8217; pixel values.</p>
<p><strong>Note:: This function is deprecated. Use nn.functional.upsample instead</strong></p>
<p>Currently spatial and volumetric upsampling are supported (i.e. expected
inputs are 4 or 5 dimensional).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; output spatia
size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; multiplier for spatial size. Has to be an integer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="upsample-bilinear">
<h3><span class="hidden-section">upsample_bilinear</span><a class="headerlink" href="#upsample-bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_bilinear</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Upscales the input, using bilinear upsampling.</p>
<p><strong>Note:: This function is deprecated. Use nn.functional.upsample instead</strong></p>
<p>Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo
volumetric (5 dimensional) inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; multiplier for spatial size</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="grid-sample">
<h3><span class="hidden-section">grid_sample</span><a class="headerlink" href="#grid-sample" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.grid_sample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">grid_sample</code><span class="sig-paren">(</span><em>input</em>, <em>grid</em>, <em>mode='bilinear'</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#grid_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.grid_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and a flow-field <code class="xref py py-attr docutils literal"><span class="pre">grid</span></code>, computes the
<cite>output</cite> using input pixel locations from the grid.</p>
<p>Uses bilinear interpolation to sample the input pixels.
Currently, only spatial (4 dimensional) inputs are supported.</p>
<p>For each output location, <code class="xref py py-attr docutils literal"><span class="pre">grid</span></code> has <cite>x</cite> and <cite>y</cite>
input pixel locations which are used to compute output.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">grid</span></code> has values in the range of <cite>[-1, 1]</cite>. This is because the
pixel locations are normalized by the input height and width.</p>
<dl class="docutils">
<dt>For example, values: x: -1, y: -1 is the left-top pixel of the input</dt>
<dd>values: x: 1, y: 1 is the right-bottom pixel of the input</dd>
</dl>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">grid</span></code> has values outside the range of <cite>[-1, 1]</cite>, those locations
are handled as defined by <cite>padding_mode</cite>. Options are <cite>zeros</cite> or <cite>border</cite>,
defining those locations to use 0 or image border values as contribution
to the bilinear interpolation.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is used in building Spatial Transformer Networks</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input batch of images (N x C x IH x IW)</li>
<li><strong>grid</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; flow-field of size (N x OH x OW x 2)</li>
<li><strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; padding mode for outside grid values
&#8216;zeros&#8217; | &#8216;border&#8217;. Default: &#8216;zeros&#8217;</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable">Variable</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="affine-grid">
<h3><span class="hidden-section">affine_grid</span><a class="headerlink" href="#affine-grid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.affine_grid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">affine_grid</code><span class="sig-paren">(</span><em>theta</em>, <em>size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#affine_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.affine_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a 2d flow field, given a batch of affine matrices <code class="xref py py-attr docutils literal"><span class="pre">theta</span></code>
Generally used in conjunction with <a class="reference internal" href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-func docutils literal"><span class="pre">grid_sample()</span></code></a> to
implement Spatial Transformer Networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; input batch of affine matrices (N x 2 x 3)</li>
<li><strong>size</strong> (<em>torch.Size</em>) &#8211; the target output image size (N x C x H x W)
Example: torch.Size((32, 3, 24, 24))</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor of size (N x H x W x 2)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="autograd.html#torch.autograd.Variable" title="torch.autograd.Variable">Variable</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-init">
<h1>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="torch.nn.init.calculate_gain">
<code class="descclassname">torch.nn.init.</code><code class="descname">calculate_gain</code><span class="sig-paren">(</span><em>nonlinearity</em>, <em>param=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#calculate_gain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.calculate_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the recommended gain value for the given nonlinearity function.
The values are as follows:</p>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="78%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">nonlinearity</th>
<th class="head">gain</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>linear</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>conv{1,2,3}d</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-even"><td>sigmoid</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>tanh</td>
<td><span class="math">\(5 / 3\)</span></td>
</tr>
<tr class="row-even"><td>relu</td>
<td><span class="math">\(\sqrt{2}\)</span></td>
</tr>
<tr class="row-odd"><td>leaky_relu</td>
<td><span class="math">\(\sqrt{2 / (1 + negative\_slope^2)}\)</span></td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nonlinearity</strong> &#8211; the nonlinear function (<cite>nn.functional</cite> name)</li>
<li><strong>param</strong> &#8211; optional parameter for the nonlinear function</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gain</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>b=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values drawn from the uniform
distribution <span class="math">\(U(a, b)\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>a</strong> &#8211; the lower bound of the uniform distribution</li>
<li><strong>b</strong> &#8211; the upper bound of the uniform distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>mean=0</em>, <em>std=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values drawn from the normal
distribution <span class="math">\(N(mean, std)\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>mean</strong> &#8211; the mean of the normal distribution</li>
<li><strong>std</strong> &#8211; the standard deviation of the normal distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.constant">
<code class="descclassname">torch.nn.init.</code><code class="descname">constant</code><span class="sig-paren">(</span><em>tensor</em>, <em>val</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with the value <cite>val</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>val</strong> &#8211; the value to fill the tensor with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.eye">
<code class="descclassname">torch.nn.init.</code><code class="descname">eye</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#eye"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.eye" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2-dimensional input Tensor or Variable with the identity
matrix. Preserves the identity of the inputs in Linear layers, where as
many inputs are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> &#8211; a 2-dimensional torch.Tensor or autograd.Variable</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.dirac">
<code class="descclassname">torch.nn.init.</code><code class="descname">dirac</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#dirac"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.dirac" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the {3, 4, 5}-dimensional input Tensor or Variable with the Dirac
delta function. Preserves the identity of the inputs in Convolutional
layers, where as many input channels are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> &#8211; a {3, 4, 5}-dimensional torch.Tensor or autograd.Variable</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">dirac</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method
described in &#8220;Understanding the difficulty of training deep feedforward
neural networks&#8221; - Glorot, X. &amp; Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled from
<span class="math">\(U(-a, a)\)</span> where
<span class="math">\(a = gain \times \sqrt{2 / (fan\_in + fan\_out)} \times \sqrt{3}\)</span>.
Also known as Glorot initialisation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>gain</strong> &#8211; an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method
described in &#8220;Understanding the difficulty of training deep feedforward
neural networks&#8221; - Glorot, X. &amp; Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled from
<span class="math">\(N(0, std)\)</span> where
<span class="math">\(std = gain \times \sqrt{2 / (fan\_in + fan\_out)}\)</span>.
Also known as Glorot initialisation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>gain</strong> &#8211; an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method
described in &#8220;Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification&#8221; - He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled from
<span class="math">\(U(-bound, bound)\)</span> where
<span class="math">\(bound = \sqrt{2 / ((1 + a^2) \times fan\_in)} \times \sqrt{3}\)</span>.
Also known as He initialisation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>a</strong> &#8211; the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> &#8211; either &#8216;fan_in&#8217; (default) or &#8216;fan_out&#8217;. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method
described in &#8220;Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification&#8221; - He, K. et al. (2015), using a
normal distribution. The resulting tensor will have values sampled from
<span class="math">\(N(0, std)\)</span> where
<span class="math">\(std = \sqrt{2 / ((1 + a^2) \times fan\_in)}\)</span>. Also known as He
initialisation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>a</strong> &#8211; the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> &#8211; either &#8216;fan_in&#8217; (default) or &#8216;fan_out&#8217;. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.orthogonal">
<code class="descclassname">torch.nn.init.</code><code class="descname">orthogonal</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#orthogonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.orthogonal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with a (semi) orthogonal matrix, as
described in &#8220;Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks&#8221; - Saxe, A. et al. (2013). The input tensor must have
at least 2 dimensions, and for tensors with more than 2 dimensions the
trailing dimensions are flattened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable, where n &gt;= 2</li>
<li><strong>gain</strong> &#8211; optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.sparse">
<code class="descclassname">torch.nn.init.</code><code class="descname">sparse</code><span class="sig-paren">(</span><em>tensor</em>, <em>sparsity</em>, <em>std=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#sparse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2D input Tensor or Variable as a sparse matrix, where the
non-zero elements will be drawn from the normal distribution
<span class="math">\(N(0, 0.01)\)</span>, as described in &#8220;Deep learning via
Hessian-free optimization&#8221; - Martens, J. (2010).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; an n-dimensional torch.Tensor or autograd.Variable</li>
<li><strong>sparsity</strong> &#8211; The fraction of elements in each column to be set to zero</li>
<li><strong>std</strong> &#8211; the standard deviation of the normal distribution used to generate</li>
<li><strong>non-zero values</strong> (<em>the</em>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>

          </div>
          <footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">

        <a href="optim.html" class="btn btn-neutral float-right" title="torch.optim" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>


        <a href="storage.html" class="btn btn-neutral" title="torch.Storage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>

    </div>


  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>





    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>





    <script type="text/javascript" src="_static/js/theme.js"></script>


  <script type="text/javascript">
      jQuery(function () {

          SphinxRtdTheme.Navigation.enableSticky();

      });
  </script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
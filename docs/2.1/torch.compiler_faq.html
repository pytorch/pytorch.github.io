


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Frequently Asked Questions &mdash; PyTorch 2.1 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/torch.compiler_faq.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="PyTorch 2.0 Troubleshooting" href="torch.compiler_troubleshooting.html" />
    <link rel="prev" title="Profiling to understand torch.compile performance" href="torch.compiler_profiling_torch_compile.html" />


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.1 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="torch.compiler.html">torch.compiler</a> &gt;</li>
        
      <li>Frequently Asked Questions</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/torch.compiler_faq.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">Â¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/msaroufim">Mark Saroufim</a></p>
<div class="section" id="does-torch-compile-support-training">
<h2>Does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support training?<a class="headerlink" href="#does-torch-compile-support-training" title="Permalink to this heading">Â¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> supports training, using AOTAutograd to capture backwards:</p>
<ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> graph and <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> is captured by
TorchDynamoâ€™s python <code class="docutils literal notranslate"><span class="pre">evalframe</span></code> frontend.</p></li>
<li><p>For each segment of <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> that torchdynamo captures, it uses
AOTAutograd to generate a backward graph segment.</p></li>
<li><p>Each pair of forward and backward graph are (optionally) min-cut
partitioned to save the minimal state between forward and backward.</p></li>
<li><p>The forward and backward pairs are wrapped in <code class="docutils literal notranslate"><span class="pre">autograd.function</span></code> modules.</p></li>
<li><p>Usercode calling<code class="docutils literal notranslate"><span class="pre">.backward()</span></code> still triggers eagerâ€™s autograd engine,
which runs each <em>compiled backward</em> graph as if it were one op, also running
any non-compiled eager opsâ€™ <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> functions.</p></li>
</ol>
</div>
<div class="section" id="do-you-support-distributed-code">
<h2>Do you support Distributed code?<a class="headerlink" href="#do-you-support-distributed-code" title="Permalink to this heading">Â¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> supports <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP).
Support for other distributed training libraries is being considered.</p>
<p>The main reason why Distributed code is challenging with dynamo is
because AOTAutograd unrolls both the forward and backward pass and
provides 2 graphs for backends to optimize. This is a problem for
distributed code because weâ€™d like to ideally overlap communication
operations with computations. Eager pytorch accomplishes this in
different ways for DDP/FSDP- using autograd hooks, module hooks, and
modifications/mutations of module states. In a naive application of
dynamo, hooks that should run directly after an operation during
backwards may be delayed until after the entire compiled region of
backwards ops, due to how AOTAutograd compiled functions interact with
dispatcher hooks.</p>
<p>The basic strategy for optimizing DDP with Dynamo is outlined in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/optimizations/distributed.py">distributed.py</a>
where the main idea will be to graph break on <a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html#internal-design">DDP bucket
boundaries</a>.</p>
<p>When each node in DDP needs to synchronize its weights with the other
nodes it organizes its gradients and parameters into buckets which
reduces communication times and allows a node to broadcast a fraction of
its gradients to other waiting nodes.</p>
<p>Graph breaks in distributed code mean you can expect dynamo and its
backends to optimize the compute overhead of a distributed program but
not its communication overhead. Graph-breaks may interfere with
compilation speedups, if the reduced graph-size robs the compiler of
fusion opportunities. However, there are diminishing returns with
increasing graph size since most of the current compute optimizations
are local fusions. So in practice this approach may be sufficient.</p>
</div>
<div class="section" id="do-i-still-need-to-export-whole-graphs">
<h2>Do I still need to export whole graphs?<a class="headerlink" href="#do-i-still-need-to-export-whole-graphs" title="Permalink to this heading">Â¶</a></h2>
<p>For the vast majority of models you probably donâ€™t and you can use
<code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> as is but there are a few situations where
full graphs are necessary and you can can ensure a full graph by simply
running <code class="docutils literal notranslate"><span class="pre">torch.compile(...,</span> <span class="pre">nopython=True)</span></code>. These situations include:</p>
<ul class="simple">
<li><p>Large scale training runs, such as $250K+ that require pipeline parallelism
and other advanced sharding strategies.</p></li>
<li><p>Inference optimizers like <a class="reference external" href="https://github.com/pytorch/TensorRT">TensorRT</a>
or <a class="reference external" href="https://github.com/facebookincubator/AITemplate">AITemplate</a> that
rely on fusing much more aggressively than training optimizers.</p></li>
<li><p>Mobile training or inference.</p></li>
</ul>
<p>Future work will include tracing communication operations into graphs,
coordinating these operations with compute optimizations, and optimizing
the communication operations.</p>
</div>
<div class="section" id="why-is-my-code-crashing">
<h2>Why is my code crashing?<a class="headerlink" href="#why-is-my-code-crashing" title="Permalink to this heading">Â¶</a></h2>
<p>If your code ran just fine without <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> and started to
crash with it is enabled, then the most important first step is figuring
out which part of the stack your failure occurred. To troubleshoot that,
follow the steps below and only try the next step if the previous one
succeeded.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.compile(...,</span> <span class="pre">backend=&quot;eager&quot;)</span></code> which only runs TorchDynamo
forward graph capture and then runs the captured graph with PyTorch.
If this fails then thereâ€™s an issue with TorchDynamo.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.compile(...,</span> <span class="pre">backend=&quot;aot_eager&quot;)</span></code>
which runs TorchDynamo to capture a forward graph, and then AOTAutograd
to trace the backward graph without any additional backend compiler
steps. PyTorch eager will then be used to run the forward and backward
graphs. If this fails then thereâ€™s an issue with AOTAutograd.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.compile(...,</span> <span class="pre">backend=&quot;inductor&quot;)</span></code> which runs TorchDynamo to capture a
forward graph, and then AOTAutograd to trace the backward graph with the
TorchInductor compiler. If this fails then thereâ€™s an issue with TorchInductor</p></li>
</ol>
</div>
<div class="section" id="why-is-compilation-slow">
<h2>Why is compilation slow?<a class="headerlink" href="#why-is-compilation-slow" title="Permalink to this heading">Â¶</a></h2>
<ul class="simple">
<li><p><strong>Dynamo Compilation</strong>â€“ TorchDynamo has a builtin stats function for
collecting and displaying the time spent in each compilation phase.
These stats can be accessed by calling <code class="docutils literal notranslate"><span class="pre">torch._dynamo.utils.compile_times()</span></code>
after executing <code class="docutils literal notranslate"><span class="pre">torch._dynamo</span></code>. By default, this returns a string
representation of the compile times spent in each TorchDynamo function by name.</p></li>
<li><p><strong>Inductor Compilation</strong>â€“ TorchInductor has a builtin stats and trace function
for displaying time spent in each compilation phase, output code, output
graph visualization and IR dump. <code class="docutils literal notranslate"><span class="pre">env</span> <span class="pre">TORCH_COMPILE_DEBUG=1</span> <span class="pre">python</span> <span class="pre">repro.py</span></code>.
This is a debugging tool designed to make it easier to debug/understand the
internals of TorchInductor with an output that will look something like
<a class="reference external" href="https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396">this</a>
Each file in that debug trace can be enabled/disabled via
<code class="docutils literal notranslate"><span class="pre">torch._inductor.config.trace.*</span></code>. The profile and the diagram are both
disabled by default since they are expensive to generate. See the
<a class="reference external" href="https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396">example debug directory
output</a>
for more examples.</p></li>
<li><p><strong>Excessive Recompilation</strong>
When TorchDynamo compiles a function (or part of one), it makes certain
assumptions about locals and globals in order to allow compiler
optimizations, and expresses these assumptions as guards that check
particular values at runtime. If any of these guards fail, Dynamo will
recompile that function (or part) up to
<code class="docutils literal notranslate"><span class="pre">torch._dynamo.config.cache_size_limit</span></code> times. If your program is
hitting the cache limit, you will first need to determine which guard is
failing and what part of your program is triggering it. The
<a class="reference external" href="#recompilation-profiler">recompilation profiler</a> automates the
process of setting TorchDynamoâ€™s cache limit to 1 and running your
program under an observation-only â€˜compilerâ€™ that records the causes of
any guard failures. You should be sure to run your program for at least
as long (as many iterations) as you were running when you ran into
trouble, and the profiler will accumulate statistics over this duration.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">CompileProfiler</span>

<span class="k">def</span> <span class="nf">my_model</span><span class="p">():</span>
    <span class="o">...</span>

<span class="k">with</span> <span class="n">CompileProfiler</span><span class="p">()</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">profiler_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">prof</span><span class="p">)</span>
    <span class="n">profiler_model</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">report</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="why-are-you-recompiling-in-production">
<h2>Why are you recompiling in production?<a class="headerlink" href="#why-are-you-recompiling-in-production" title="Permalink to this heading">Â¶</a></h2>
<p>In some cases, you may not want unexpected compiles after a program has
warmed up. For example, if you are serving production traffic in a
latency critical application. For this, TorchDynamo provides an
alternate mode where prior compiled graphs are used, but no new ones are
generated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">frozen_toy_example</span> <span class="o">=</span> <span class="n">dynamo</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">toy_example</span><span class="p">)</span>
<span class="n">frozen_toy_example</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="how-are-you-speeding-up-my-code">
<h2>How are you speeding up my code?<a class="headerlink" href="#how-are-you-speeding-up-my-code" title="Permalink to this heading">Â¶</a></h2>
<p>There are 3 major ways to accelerate PyTorch code:</p>
<ol class="arabic simple">
<li><p>Kernel fusion via vertical fusions which fuse sequential operations to avoid
excessive read/writes. For example, fuse 2 subsequent cosines means you
can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:
the simplest example being batching where a single matrix is multiplied
with a batch of examples but the more general scenario is a grouped GEMM
where a group of matrix multiplications are scheduled together</p></li>
<li><p>Out of order execution: A general optimization for compilers, by looking ahead
at the exact data dependencies within a graph we can decide on the most
opportune time to execute a node and which buffers can be reused</p></li>
<li><p>Automatic work placement: Similar of the out of order execution point,
but by matching nodes of a graph to resources like physical hardware or
memory we can design an appropriate schedule</p></li>
</ol>
<p>The above are general principles for accelerating PyTorch code but
different backends will each make different tradeoffs on what to
optimize. For example Inductor first takes care of fusing whatever it
can and only then generates <a class="reference external" href="https://openai.com/blog/triton/">Triton</a>
kernels. It can also</p>
<p>Triton in addition offers speedups because of automatic memory
coalescing, memory management and scheduling within each Streaming
Multiprocessor and has been designed to handle tiled computations.</p>
<p>However, regardless of the backend you use itâ€™s best to use a benchmark
and see approach so try out the PyTorch profiler, visually inspect the
generated kernels and try to see whatâ€™s going on for yourself.</p>
</div>
<div class="section" id="why-am-i-not-seeing-speedups">
<h2>Why am I not seeing speedups?<a class="headerlink" href="#why-am-i-not-seeing-speedups" title="Permalink to this heading">Â¶</a></h2>
<div class="section" id="graph-breaks">
<h3>Graph Breaks<a class="headerlink" href="#graph-breaks" title="Permalink to this heading">Â¶</a></h3>
<p>The main reason you wonâ€™t see the speedups youâ€™d like to by using dynamo
is excessive graph breaks. So whatâ€™s a graph break?</p>
<p>Given a program like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">some_fun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="o">...</span>

<span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">some_fun</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Torchdynamo will attempt to compile all of the torch/tensor operations
within <code class="docutils literal notranslate"><span class="pre">some_fun()</span></code> into a single FX graph, but it may fail to capture
everything into one graph.</p>
<p>Some graph break reasons are insurmountable to TorchDynamo like calling
into a C extension other than PyTorch is invisible to TorchDynamo, and
could do arbitrary things without TorchDynamo being able to introduce
necessary guards to ensure that the compiled program would be safe to reuse.</p>
<blockquote>
<div><p>To maximize performance, itâ€™s important to have as few graph breaks
as possible.</p>
</div></blockquote>
</div>
<div class="section" id="identifying-the-cause-of-a-graph-break">
<h3>Identifying the cause of a graph break<a class="headerlink" href="#identifying-the-cause-of-a-graph-break" title="Permalink to this heading">Â¶</a></h3>
<p>To identify all graph breaks in a program and the associated reasons for
the breaks, <code class="docutils literal notranslate"><span class="pre">torch._dynamo.explain</span></code> can be used. This tool runs
TorchDynamo on the supplied function and aggregates the graph breaks
that are encountered. Here is an example usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._dynamo</span> <span class="k">as</span> <span class="nn">dynamo</span>
<span class="k">def</span> <span class="nf">toy_example</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;woo&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">b</span>
<span class="n">explanation</span><span class="p">,</span> <span class="n">out_guards</span><span class="p">,</span> <span class="n">graphs</span><span class="p">,</span> <span class="n">ops_per_graph</span> <span class="o">=</span> <span class="n">dynamo</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="n">toy_example</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">explanation</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamo produced 3 graphs, with 2 graph break and 6 ops.</span>
<span class="sd"> Break reasons:</span>
<span class="sd">1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}</span>
<span class="sd">   File &quot;t2.py&quot;, line 16, in toy_example</span>
<span class="sd">    print(&quot;woo&quot;)</span>

<span class="sd">2. generic_jump</span>
<span class="sd">   File &quot;t2.py&quot;, line 17, in toy_example</span>
<span class="sd">    if b.sum() &lt; 0:</span>
<span class="sd"> &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>To throw an error on the first graph break encountered you can use
disable python fallback by using <code class="docutils literal notranslate"><span class="pre">nopython=True</span></code>, this should be
familiar if youâ€™ve worked with export based compilers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">toy_example</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
   <span class="o">...</span>

<span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">toy_example</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">backend</span><span class="o">=&lt;</span><span class="n">compiler</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="why-didnt-my-code-recompile-when-i-changed-it">
<h3>Why didnâ€™t my code recompile when I changed it?<a class="headerlink" href="#why-didnt-my-code-recompile-when-i-changed-it" title="Permalink to this heading">Â¶</a></h3>
<p>If you enabled dynamic shapes by setting
<code class="docutils literal notranslate"><span class="pre">env</span> <span class="pre">TORCHDYNAMO_DYNAMIC_SHAPES=1</span> <span class="pre">python</span> <span class="pre">model.py</span></code> then your code
wonâ€™t recompile on shape changes. Weâ€™ve added support for dynamic shapes
which avoids recompilations in the case when shapes vary by less than a
factor of 2. This is especially useful in scenarios like varying image
sizes in CV or variable sequence length in NLP. In inference scenarios
itâ€™s often not possible to know what a batch size will be beforehand
because you take what you can get from different client apps.</p>
<p>In general, TorchDynamo tries very hard not to recompile things
unnecessarily so if for example TorchDynamo finds 3 graphs and your
change only modified one graph then only that graph will recompile. So
another tip to avoid potentially slow compilation times is to warmup a
model by compiling it once after which subsequent compilations will be
much faster. Cold start compile times is still a metric we track
visibly.</p>
</div>
</div>
<div class="section" id="why-am-i-getting-incorrect-results">
<h2>Why am I getting incorrect results?<a class="headerlink" href="#why-am-i-getting-incorrect-results" title="Permalink to this heading">Â¶</a></h2>
<p>Accuracy issues can also be minified if you set the environment variable
<code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_REPRO_LEVEL=4</span></code>, it operates with a similar git bisect
model and a full repro might be something like
<code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_REPRO_AFTER=&quot;aot&quot;</span> <span class="pre">TORCHDYNAMO_REPRO_LEVEL=4</span></code> the reason
we need this is downstream compilers will codegen code whether itâ€™s
Triton code or the C++ backend, the numerics from those downstream
compilers can be different in subtle ways yet have dramatic impact on
your training stability. So the accuracy debugger is very useful for us
to detect bugs in our codegen or with a backend compiler.</p>
<p>If youâ€™d like to ensure that random number generation is the same across both torch
and triton then you can enable <code class="docutils literal notranslate"><span class="pre">torch._inductor.config.fallback_random</span> <span class="pre">=</span> <span class="pre">True</span></code></p>
</div>
<div class="section" id="why-am-i-getting-ooms">
<h2>Why am I getting OOMs?<a class="headerlink" href="#why-am-i-getting-ooms" title="Permalink to this heading">Â¶</a></h2>
<p>Dynamo is still an alpha product so thereâ€™s a few sources of OOMs and if
youâ€™re seeing an OOM try disabling the following configurations in this
order and then open an issue on GitHub so we can solve the root problem
1. If youâ€™re using dynamic shapes try disabling them, weâ€™ve disabled
them by default: <code class="docutils literal notranslate"><span class="pre">env</span> <span class="pre">TORCHDYNAMO_DYNAMIC_SHAPES=0</span> <span class="pre">python</span> <span class="pre">model.py</span></code> 2.
CUDA graphs with Triton are enabled by default in inductor but removing
them may alleviate some OOM issues: <code class="docutils literal notranslate"><span class="pre">torch._inductor.config.triton.cudagraphs</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
</div>
<div class="section" id="does-torch-func-work-with-torch-compile-for-grad-and-vmap-transforms">
<h2>Does <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> work with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> (for <cite>grad</cite> and <cite>vmap</cite> transforms)?<a class="headerlink" href="#does-torch-func-work-with-torch-compile-for-grad-and-vmap-transforms" title="Permalink to this heading">Â¶</a></h2>
<p>Applying a <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> transform to a function that uses <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>
does not work:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>This code will not work. There is an <a class="reference external" href="https://github.com/pytorch/pytorch/issues/100320">issue</a>
that you can track for this.</p>
<p>As a workaround, use <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> outside of the <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> function:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an experimental feature and can be used by setting <cite>torch._dynamo.config.capture_func_transforms=True</cite></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="calling-torch-func-transform-inside-of-a-function-handled-with-torch-compile">
<h3>Calling <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> transform inside of a function handled with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code><a class="headerlink" href="#calling-torch-func-transform-inside-of-a-function-handled-with-torch-compile" title="Permalink to this heading">Â¶</a></h3>
</div>
<div class="section" id="compiling-torch-func-grad-with-torch-compile">
<h3>Compiling <code class="docutils literal notranslate"><span class="pre">torch.func.grad</span></code> with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code><a class="headerlink" href="#compiling-torch-func-grad-with-torch-compile" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">wrapper_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">grad_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">wrapper_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-torch-vmap-with-torch-compile">
<h3>Compiling <code class="docutils literal notranslate"><span class="pre">torch.vmap</span></code> with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code><a class="headerlink" href="#compiling-torch-vmap-with-torch-compile" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">my_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading">Â¶</a></h3>
<p>There are currently a few cases which are not supported and lead to graph breaks
(that is, torch.compile falls back to eager-mode PyTorch on these). We are working
on improving the situation for the next release (PyTorch 2.2)</p>
<p>1. The inputs and outputs of the function being transformed over must be tensors.
We do not yet support things like tuple of Tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>

<span class="k">def</span> <span class="nf">my_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">fn</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Unsupported, falls back to eager-mode PyTorch</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_fn</span><span class="p">)((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Keyword arguments are not supported.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">my_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">fn</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Unsupported, falls back to eager-mode PyTorch</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>3. Functions with observable side effects. For example, it is OK to mutate a list created in the function,
but not OK to mutate a list created outside of the function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="n">some_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">some_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">my_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Unsupported, falls back to eager-mode PyTorch</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.vmap</span></code> over a function that calls one or more operators in the following list.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>â€˜strideâ€™, â€˜requires_gradâ€™, â€˜storage_offsetâ€™, â€˜layoutâ€™, â€˜dataâ€™, â€˜is_coalescedâ€™, â€˜is_complexâ€™,
â€˜is_conjâ€™, â€˜is_contiguousâ€™, â€˜is_cpuâ€™, â€˜is_cudaâ€™, â€˜is_distributedâ€™, â€˜is_floating_pointâ€™,
â€˜is_inferenceâ€™, â€˜is_ipuâ€™, â€˜is_leafâ€™, â€˜is_metaâ€™, â€˜is_mkldnnâ€™, â€˜is_mpsâ€™, â€˜is_negâ€™, â€˜is_nestedâ€™,
â€˜is_nonzeroâ€™, â€˜is_ortâ€™, â€˜is_pinnedâ€™, â€˜is_quantizedâ€™, â€˜is_same_sizeâ€™, â€˜is_set_toâ€™, â€˜is_sharedâ€™,
â€˜is_signedâ€™, â€˜is_sparseâ€™, â€˜is_sparse_csrâ€™, â€˜is_vulkanâ€™, â€˜is_xlaâ€™, â€˜is_xpuâ€™</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_func_transforms</span><span class="o">=</span><span class="kc">True</span>

<span class="k">def</span> <span class="nf">bad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">my_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">bad_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Unsupported, falls back to eager-mode PyTorch</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">my_fn</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-functions-besides-the-ones-which-are-supported-escape-hatch">
<h3>Compiling functions besides the ones which are supported (escape hatch)<a class="headerlink" href="#compiling-functions-besides-the-ones-which-are-supported-escape-hatch" title="Permalink to this heading">Â¶</a></h3>
<p>For other transforms, as a workaround, use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.allow_in_graph</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">allow_in_graph</span></code> is an escape hatch. If your code does not work with
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, which introspects Python bytecode, but you believe it
will work via a symbolic tracing approach (like <code class="docutils literal notranslate"><span class="pre">jax.jit</span></code>), then use
<code class="docutils literal notranslate"><span class="pre">allow_in_graph</span></code>.</p>
<p>By using <code class="docutils literal notranslate"><span class="pre">allow_in_graph</span></code> to annotate a function, you must make sure
your code meets the following requirements:</p>
<ul class="simple">
<li><p>All outputs in your function only depend on the inputs and
do not depend on any captured Tensors.</p></li>
<li><p>Your function is functional. That is, it does not mutate any state. This may
be relaxed; we actually support functions that appear to be functional from
the outside: they may have in-place PyTorch operations, but may not mutate
global state or inputs to the function.</p></li>
<li><p>Your function does not raise data-dependent errors.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">allow_in_graph</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>A common pitfall is using <code class="docutils literal notranslate"><span class="pre">allow_in_graph</span></code> to annotate a function that
invokes an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>. This is because the outputs now depend on the
parameters of the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>. To get this to work, use
<code class="docutils literal notranslate"><span class="pre">torch.func.functional_call</span></code> to extract the module state.</p>
</div>
</div>
<div class="section" id="does-numpy-work-with-torch-compile">
<h2>Does NumPy work with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?<a class="headerlink" href="#does-numpy-work-with-torch-compile" title="Permalink to this heading">Â¶</a></h2>
<p>Starting in 2.1, <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> understands native NumPy programs that
work on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch
to NumPy and back via <code class="docutils literal notranslate"><span class="pre">x.numpy()</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.from_numpy</span></code>, and related functions.</p>
<div class="section" id="which-numpy-features-does-torch-compile-support">
<span id="nonsupported-numpy-feats"></span><h3>Which NumPy features does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support?<a class="headerlink" href="#which-numpy-features-does-torch-compile-support" title="Permalink to this heading">Â¶</a></h3>
<p>NumPy within <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> follows NumPy 2.0 pre-release.</p>
<p>Generally, <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> is able to trace through most NumPy constructions,
and when it cannot, it falls back to eager and lets NumPy execute that piece of
code. Even then, there are a few features where <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> semantics
slightly deviate from those of NumPy:</p>
<ul class="simple">
<li><p>NumPy scalars: We model them as 0-D arrays. That is, <code class="docutils literal notranslate"><span class="pre">np.float32(3)</span></code> returns
a 0-D array under <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>. To avoid a graph break, it is best to use this 0-D
array. If this breaks your code, you can workaround this by casting the NumPy scalar
to the relevant Python scalar type <code class="docutils literal notranslate"><span class="pre">bool/int/float</span></code>.</p></li>
<li><p>Negative strides: <code class="docutils literal notranslate"><span class="pre">np.flip</span></code> and slicing with a negative step return a copy.</p></li>
<li><p>Type promotion: NumPyâ€™s type promotion will change in NumPy 2.0. The new rules
are described in <a class="reference external" href="https://numpy.org/neps/nep-0050-scalar-promotion.html)">NEP 50</a>.
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> implements NEP 50 rather than the current soon-to-be deprecated rules.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{tril,triu}_indices_from/{tril,triu}_indices</span></code> return arrays rather than a tuple of arrays.</p></li>
</ul>
<p>There are other features for which we do not support tracing and we gracefully
fallback to NumPy for their execution:</p>
<ul class="simple">
<li><p>Non-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays.</p></li>
<li><p>Long dtypes <code class="docutils literal notranslate"><span class="pre">np.float128/np.complex256</span></code> and some unsigned dtypes <code class="docutils literal notranslate"><span class="pre">np.uint16/np.uint32/np.uint64</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ndarray</span></code> subclasses.</p></li>
<li><p>Masked arrays.</p></li>
<li><p>Esoteric ufunc machinery like <code class="docutils literal notranslate"><span class="pre">axes=[(n,k),(k,m)-&gt;(n,m)]</span></code> and ufunc methods (e.g., <code class="docutils literal notranslate"><span class="pre">np.add.reduce</span></code>).</p></li>
<li><p>Sorting / ordering <code class="docutils literal notranslate"><span class="pre">complex64/complex128</span></code> arrays.</p></li>
<li><p>NumPy <code class="docutils literal notranslate"><span class="pre">np.poly1d</span></code> and <code class="docutils literal notranslate"><span class="pre">np.polynomial</span></code>.</p></li>
<li><p>Positional <code class="docutils literal notranslate"><span class="pre">out1,</span> <span class="pre">out2</span></code> args in functions with 2 or more returns (<code class="docutils literal notranslate"><span class="pre">out=tuple</span></code> does work).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__array_function__</span></code>, <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> and <code class="docutils literal notranslate"><span class="pre">__array_wrap__</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ndarray.ctypes</span></code> attribute.</p></li>
</ul>
</div>
<div class="section" id="can-i-execute-numpy-code-on-cuda-via-torch-compile">
<h3>Can I execute NumPy code on CUDA via <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?<a class="headerlink" href="#can-i-execute-numpy-code-on-cuda-via-torch-compile" title="Permalink to this heading">Â¶</a></h3>
<p>Yes you can! To do so, you may simply execute your code within a <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;)</span></code>
context. Consider the example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">numpy_fn</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">numpy_fn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">numpy_fn</span></code> will be executed in CUDA. For this to be
possible, <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> automatically moves <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code> from CPU
to CUDA, and then it moves the result <code class="docutils literal notranslate"><span class="pre">Z</span></code> from CUDA to CPU. If we are
executing this function several times in the same program run, we may want
to avoid all these rather expensive memory copies. To do so, we just need
to tweak our <code class="docutils literal notranslate"><span class="pre">numpy_fn</span></code> so that it accepts cuda Tensors and returns tensors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">numpy_fn</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">numpy_fn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>By doing this, we explicitly create the tensors in CUDA memory, and we keep
them there. In this case <code class="docutils literal notranslate"><span class="pre">X.numpy()</span></code> and <code class="docutils literal notranslate"><span class="pre">from_numpy()</span></code> are hints to the compiler
but no real data movement happens. Note that the original program would not run
on eager mode now. If you want to run it in eager mode, you would need to call
<code class="docutils literal notranslate"><span class="pre">.numpy(force=True)</span></code> doing <code class="docutils literal notranslate"><span class="pre">Z</span> <span class="pre">=</span> <span class="pre">Z.cuda()</span></code> before returning
<code class="docutils literal notranslate"><span class="pre">Z</span></code>. Of course, doing this would execute the program on eager mode NumPy, and
on CPU.</p>
</div>
<div class="section" id="how-do-i-debug-numpy-code-under-torch-compile">
<h3>How do I debug NumPy code under <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?<a class="headerlink" href="#how-do-i-debug-numpy-code-under-torch-compile" title="Permalink to this heading">Â¶</a></h3>
<p>Debugging JIT compiled code is challenging, given the complexity of modern
compilers and the daunting errors that they raise.
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_troubleshooting.html#diagnosing-runtime-errors">The tutorial on how to diagnose runtime errors within torch.compile</a>
contains a few tips and tricks on how to tackle this task.</p>
<p>If the above is not enough to pinpoint the origin of the issue, there are still
a few other NumPy-specific tools we can use. We can discern whether the bug
is entirely in the PyTorch code by disabling tracing through NumPy functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch._dynamo</span> <span class="kn">import</span> <span class="n">config</span>
<span class="n">config</span><span class="o">.</span><span class="n">trace_numpy</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>If the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>)
using PyTorch as a backend by importing <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch._numpy</span> <span class="pre">as</span> <span class="pre">np</span></code>.
This should just be used for <strong>debugging purposes</strong> and is in no way a
replacement for the PyTorch API, as it is <strong>much less performant</strong> and, as a
private API, <strong>may change without notice</strong>. At any rate, <code class="docutils literal notranslate"><span class="pre">torch._numpy</span></code> is a
Python implementation of NumPy in terms of PyTorch and it is used internally by <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> to
transform NumPy code into Pytorch code. It is rather easy to read and modify,
so if you find any bug in it feel free to submit a PR fixing it or simply open
an issue.</p>
<p>If the program does work when importing <code class="docutils literal notranslate"><span class="pre">torch._numpy</span> <span class="pre">as</span> <span class="pre">np</span></code>, chances are
that the bug is in TorchDynamo. If this is the case, please feel open an issue
with a <a class="reference external" href="https://pytorch.org/docs/2.1/torch.compiler_troubleshooting.html">minimal reproducer</a>.</p>
</div>
<div class="section" id="i-torch-compile-some-numpy-code-and-i-did-not-see-any-speed-up">
<h3>I <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> some NumPy code and I did not see any speed-up.<a class="headerlink" href="#i-torch-compile-some-numpy-code-and-i-did-not-see-any-speed-up" title="Permalink to this heading">Â¶</a></h3>
<p>The best place to start is the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_faq.html#why-am-i-not-seeing-speedups">tutorial with general advice for how to debug these sort of torch.compile issues</a>.</p>
<p>Some graph breaks may happen because of the use of unsupported features. See
<a class="reference internal" href="#nonsupported-numpy-feats"><span class="std std-ref">Which NumPy features does torch.compile support?</span></a>. More generally, it is useful to keep in mind
that some widely used NumPy features do not play well with compilers. For
example, in-place modifications make reasoning difficult within the compiler and
often yield worse performance than their out-of-place counterparts.As such, it is best to avoid
them. Same goes for the use of the <code class="docutils literal notranslate"><span class="pre">out=</span></code> parameter. Instead, prefer
out-of-place ops and let <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> optimize the memory use. Same goes
for data-dependent ops like masked indexing through boolean masks, or
data-dependent control flow like <code class="docutils literal notranslate"><span class="pre">if</span></code> or <code class="docutils literal notranslate"><span class="pre">while</span></code> constructions.</p>
</div>
</div>
<div class="section" id="which-api-to-use-for-fine-grain-tracing">
<h2>Which API to use for fine grain tracing?<a class="headerlink" href="#which-api-to-use-for-fine-grain-tracing" title="Permalink to this heading">Â¶</a></h2>
<p>In some cases, you might need to exclude small parts of your code from the
torch.compile compilations. This section provides some of the answers and
you can find more information in <a class="reference internal" href="torch.compiler_fine_grain_apis.html#torchdynamo-fine-grain-tracing"><span class="std std-ref">TorchDynamo APIs for fine-grained tracing</span></a>.</p>
<div class="section" id="how-do-i-graph-break-on-a-function">
<h3>How do I graph break on a function?<a class="headerlink" href="#how-do-i-graph-break-on-a-function" title="Permalink to this heading">Â¶</a></h3>
<p>Graph break on a function is not enough to sufficiently express what you  want
PyTorch to do. You need to be more specific about your use case. Some of the
most common use cases you might want to consider:</p>
<ul class="simple">
<li><p>If you want to disable compilation on this function frame and the recursively
invoked frames, use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code>.</p></li>
<li><p>If you want a particular operator, such as <code class="docutils literal notranslate"><span class="pre">fbgemm</span></code> to use the  eager mode,
use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disallow_in_graph</span></code>.</p></li>
</ul>
<p>Some of the uncommon use cases include:</p>
<ul class="simple">
<li><p>If you want to disable TorchDynamo on the function frame but enable it back
on the recursively invoked frames â€“ use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable(recursive=False)</span></code>.</p></li>
<li><p>If you want to prevent inlining of a function frame â€“ use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.graph_break</span></code>
at the beginning of the function you want to prevent inlining.</p></li>
</ul>
</div>
<div class="section" id="what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-disallow-in-graph">
<h3>Whatâ€™s the difference between <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code> and <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disallow_in_graph</span></code><a class="headerlink" href="#what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-disallow-in-graph" title="Permalink to this heading">Â¶</a></h3>
<p>Disallow-in-graph works at the level of operators, or more specifically,
the operators that you see in the TorchDynamo extracted graphs.</p>
<p>Disable works at the function frame level and decides if TorchDynamo
should look into the function frame or not.</p>
</div>
<div class="section" id="what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-skip">
<h3>Whatâ€™s the difference between <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code> and <code class="docutils literal notranslate"><span class="pre">torch._dynamo_skip</span></code><a class="headerlink" href="#what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-skip" title="Permalink to this heading">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch._dynamo_skip</span></code> is deprecated.</p>
</div>
<p>You most likely need <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code>. But in an unlikely scenario, you
might need even finer control. Suppose you want to disable the tracing on just
the <code class="docutils literal notranslate"><span class="pre">a_fn</span></code> function, but want to continue the tracing back in <code class="docutils literal notranslate"><span class="pre">aa_fn</span></code> and
<code class="docutils literal notranslate"><span class="pre">ab_fn</span></code>. The image below demonstrates this use case:</p>
<div class="figure align-default">
<img alt="diagram of torch.compile + disable(a_fn, recursive=False)" src="_images/call_stack_diagram.png" />
</div>
<p>In this case, you can use <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable(recursive=False)</span></code>.
In previous versions, this functionality was provided by <code class="docutils literal notranslate"><span class="pre">torch._dynamo.skip</span></code>.
This is now supported by the <code class="docutils literal notranslate"><span class="pre">recursive</span></code> flag inside <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code>.</p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torch.compiler_troubleshooting.html" class="btn btn-neutral float-right" title="PyTorch 2.0 Troubleshooting" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="torch.compiler_profiling_torch_compile.html" class="btn btn-neutral" title="Profiling to understand torch.compile performance" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Frequently Asked Questions</a><ul>
<li><a class="reference internal" href="#does-torch-compile-support-training">Does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support training?</a></li>
<li><a class="reference internal" href="#do-you-support-distributed-code">Do you support Distributed code?</a></li>
<li><a class="reference internal" href="#do-i-still-need-to-export-whole-graphs">Do I still need to export whole graphs?</a></li>
<li><a class="reference internal" href="#why-is-my-code-crashing">Why is my code crashing?</a></li>
<li><a class="reference internal" href="#why-is-compilation-slow">Why is compilation slow?</a></li>
<li><a class="reference internal" href="#why-are-you-recompiling-in-production">Why are you recompiling in production?</a></li>
<li><a class="reference internal" href="#how-are-you-speeding-up-my-code">How are you speeding up my code?</a></li>
<li><a class="reference internal" href="#why-am-i-not-seeing-speedups">Why am I not seeing speedups?</a><ul>
<li><a class="reference internal" href="#graph-breaks">Graph Breaks</a></li>
<li><a class="reference internal" href="#identifying-the-cause-of-a-graph-break">Identifying the cause of a graph break</a></li>
<li><a class="reference internal" href="#why-didnt-my-code-recompile-when-i-changed-it">Why didnâ€™t my code recompile when I changed it?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#why-am-i-getting-incorrect-results">Why am I getting incorrect results?</a></li>
<li><a class="reference internal" href="#why-am-i-getting-ooms">Why am I getting OOMs?</a></li>
<li><a class="reference internal" href="#does-torch-func-work-with-torch-compile-for-grad-and-vmap-transforms">Does <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> work with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> (for <cite>grad</cite> and <cite>vmap</cite> transforms)?</a><ul>
<li><a class="reference internal" href="#calling-torch-func-transform-inside-of-a-function-handled-with-torch-compile">Calling <code class="docutils literal notranslate"><span class="pre">torch.func</span></code> transform inside of a function handled with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li><a class="reference internal" href="#compiling-torch-func-grad-with-torch-compile">Compiling <code class="docutils literal notranslate"><span class="pre">torch.func.grad</span></code> with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li><a class="reference internal" href="#compiling-torch-vmap-with-torch-compile">Compiling <code class="docutils literal notranslate"><span class="pre">torch.vmap</span></code> with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li><a class="reference internal" href="#limitations">Limitations</a></li>
<li><a class="reference internal" href="#compiling-functions-besides-the-ones-which-are-supported-escape-hatch">Compiling functions besides the ones which are supported (escape hatch)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#does-numpy-work-with-torch-compile">Does NumPy work with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?</a><ul>
<li><a class="reference internal" href="#which-numpy-features-does-torch-compile-support">Which NumPy features does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> support?</a></li>
<li><a class="reference internal" href="#can-i-execute-numpy-code-on-cuda-via-torch-compile">Can I execute NumPy code on CUDA via <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?</a></li>
<li><a class="reference internal" href="#how-do-i-debug-numpy-code-under-torch-compile">How do I debug NumPy code under <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>?</a></li>
<li><a class="reference internal" href="#i-torch-compile-some-numpy-code-and-i-did-not-see-any-speed-up">I <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> some NumPy code and I did not see any speed-up.</a></li>
</ul>
</li>
<li><a class="reference internal" href="#which-api-to-use-for-fine-grain-tracing">Which API to use for fine grain tracing?</a><ul>
<li><a class="reference internal" href="#how-do-i-graph-break-on-a-function">How do I graph break on a function?</a></li>
<li><a class="reference internal" href="#what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-disallow-in-graph">Whatâ€™s the difference between <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code> and <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disallow_in_graph</span></code></a></li>
<li><a class="reference internal" href="#what-s-the-difference-between-torch-dynamo-disable-and-torch-dynamo-skip">Whatâ€™s the difference between <code class="docutils literal notranslate"><span class="pre">torch._dynamo.disable</span></code> and <code class="docutils literal notranslate"><span class="pre">torch._dynamo_skip</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
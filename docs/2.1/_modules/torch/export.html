


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.export &mdash; PyTorch 2.1 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/export.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.1 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch.export</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.export</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">typing</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">auto</span><span class="p">,</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">sympy</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx._pytree</span> <span class="k">as</span> <span class="nn">fx_pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">from</span> <span class="nn">torch.fx._compatibility</span> <span class="kn">import</span> <span class="n">compatibility</span>

<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">StrictMinMaxConstraint</span>

<span class="kn">from</span> <span class="nn">torch.fx.passes.infra.pass_base</span> <span class="kn">import</span> <span class="n">PassResult</span>
<span class="kn">from</span> <span class="nn">torch.fx.passes.infra.pass_manager</span> <span class="kn">import</span> <span class="n">PassManager</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;ArgumentKind&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ArgumentSpec&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Constraint&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ExportBackwardSignature&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ExportGraphSignature&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ExportedProgram&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ModuleCallEntry&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ModuleCallSignature&quot;</span><span class="p">,</span>
    <span class="s2">&quot;constrain_as_size&quot;</span><span class="p">,</span>
    <span class="s2">&quot;constrain_as_value&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dynamic_dim&quot;</span><span class="p">,</span>
    <span class="s2">&quot;export&quot;</span><span class="p">,</span>
    <span class="s2">&quot;load&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="n">PassType</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PassResult</span><span class="p">]]</span>


<div class="viewcode-block" id="ExportBackwardSignature"><a class="viewcode-back" href="../../export.html#torch.export.ExportBackwardSignature">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ExportBackwardSignature</span><span class="p">:</span>
    <span class="n">gradients_to_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">gradients_to_user_inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">loss_output</span><span class="p">:</span> <span class="nb">str</span></div>


<div class="viewcode-block" id="ExportGraphSignature"><a class="viewcode-back" href="../../export.html#torch.export.ExportGraphSignature">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ExportGraphSignature</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :class:`ExportGraphSignature` models the input/output signature of Export Graph,</span>
<span class="sd">    which is a fx.Graph with stronger invariants gurantees.</span>

<span class="sd">    Export Graph is functional and does not access &quot;states&quot; like parameters</span>
<span class="sd">    or buffers within the graph via ``getattr`` nodes. Instead, :func:`export`</span>
<span class="sd">    gurantees that parameters and buffers are lifted out of the graph as inputs.</span>
<span class="sd">    Similarly, any mutations to buffers are not included in the graph either,</span>
<span class="sd">    instead the updated values of mutated buffers are modeled as additional outputs</span>
<span class="sd">    of Export Graph.</span>

<span class="sd">    The ordering of all inputs and outputs are::</span>

<span class="sd">        Inputs = [*parameters_buffers, *flattened_user_inputs]</span>
<span class="sd">        Outputs = [*mutated_inputs, *flattened_user_outputs]</span>

<span class="sd">    e.g. If following module is exported::</span>

<span class="sd">        class CustomModule(nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super(CustomModule, self).__init__()</span>

<span class="sd">                # Define a parameter</span>
<span class="sd">                self.my_parameter = nn.Parameter(torch.tensor(2.0))</span>

<span class="sd">                # Define two buffers</span>
<span class="sd">                self.register_buffer(&#39;my_buffer1&#39;, torch.tensor(3.0))</span>
<span class="sd">                self.register_buffer(&#39;my_buffer2&#39;, torch.tensor(4.0))</span>

<span class="sd">            def forward(self, x1, x2):</span>
<span class="sd">                # Use the parameter, buffers, and both inputs in the forward method</span>
<span class="sd">                output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2</span>

<span class="sd">                # Mutate one of the buffers (e.g., increment it by 1)</span>
<span class="sd">                self.my_buffer2.add_(1.0) # In-place addition</span>

<span class="sd">                return output</span>

<span class="sd">    Resulting Graph would be::</span>

<span class="sd">        graph():</span>
<span class="sd">            %arg0_1 := placeholder[target=arg0_1]</span>
<span class="sd">            %arg1_1 := placeholder[target=arg1_1]</span>
<span class="sd">            %arg2_1 := placeholder[target=arg2_1]</span>
<span class="sd">            %arg3_1 := placeholder[target=arg3_1]</span>
<span class="sd">            %arg4_1 := placeholder[target=arg4_1]</span>
<span class="sd">            %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})</span>
<span class="sd">            %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})</span>
<span class="sd">            %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})</span>
<span class="sd">            %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})</span>
<span class="sd">            %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})</span>
<span class="sd">            return (add_tensor_2, add_tensor_1)</span>

<span class="sd">    Resulting ExportGraphSignature would be::</span>

<span class="sd">        ExportGraphSignature(</span>
<span class="sd">            # Indicates that there is one parameter named `my_parameter`</span>
<span class="sd">            parameters=[&#39;L__self___my_parameter&#39;],</span>

<span class="sd">            # Indicates that there are two buffers, `my_buffer1` and `my_buffer2`</span>
<span class="sd">            buffers=[&#39;L__self___my_buffer1&#39;, &#39;L__self___my_buffer2&#39;],</span>

<span class="sd">            # Indicates that the nodes `arg3_1` and `arg4_1` in produced graph map to</span>
<span class="sd">            # original user inputs, ie. x1 and x2</span>
<span class="sd">            user_inputs=[&#39;arg3_1&#39;, &#39;arg4_1&#39;],</span>

<span class="sd">            # Indicates that the node `add_tensor_1` maps to output of original program</span>
<span class="sd">            user_outputs=[&#39;add_tensor_1&#39;],</span>

<span class="sd">            # Indicates that there is one parameter (self.my_parameter) captured,</span>
<span class="sd">            # its name is now mangled to be `L__self___my_parameter`, which is now</span>
<span class="sd">            # represented by node `arg0_1` in the graph.</span>
<span class="sd">            inputs_to_parameters={&#39;arg0_1&#39;: &#39;L__self___my_parameter&#39;},</span>

<span class="sd">            # Indicates that there are two buffers (self.my_buffer1, self.my_buffer2) captured,</span>
<span class="sd">            # their name are now mangled to be `L__self___my_my_buffer1` and `L__self___my_buffer2`.</span>
<span class="sd">            # They are now represented by nodes `arg1_1` and `arg2_1` in the graph.</span>
<span class="sd">            inputs_to_buffers={&#39;arg1_1&#39;: &#39;L__self___my_buffer1&#39;, &#39;arg2_1&#39;: &#39;L__self___my_buffer2&#39;},</span>

<span class="sd">            # Indicates that one buffer named `L__self___my_buffer2` is mutated during execution,</span>
<span class="sd">            # its new value is output from the graph represented by the node named `add_tensor_2`</span>
<span class="sd">            buffers_to_mutate={&#39;add_tensor_2&#39;: &#39;L__self___my_buffer2&#39;},</span>

<span class="sd">            # Backward graph not captured</span>
<span class="sd">            backward_signature=None,</span>

<span class="sd">            # Work in progress feature, please ignore now.</span>
<span class="sd">            assertion_dep_token=None</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># A list of parameters uniquely identified by mangled fully qualified name</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

    <span class="c1"># A list of buffers uniquely identified by mangled fully qualified name</span>
    <span class="n">buffers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

    <span class="c1"># Graph node names of pytree-flattened inputs of original program</span>
    <span class="n">user_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

    <span class="c1"># Graph node names of pytree-flattened outputs of original program</span>
    <span class="n">user_outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

    <span class="c1"># A dictionary mapping graph input node names to parameters. If a graph input</span>
    <span class="c1"># name is found in this dictionary, it is guranteed to be a lifted parameter.</span>
    <span class="n">inputs_to_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>

    <span class="c1"># A dictionary mapping graph input node names to buffers. If a graph input</span>
    <span class="c1"># name is found in this dictionary, it is guranteed to be a lifted buffer.</span>
    <span class="n">inputs_to_buffers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>

    <span class="c1"># A dictionary mapping graph output node names to buffers that are mutated in the</span>
    <span class="c1"># original program. Buffers that are not mutated will not be found in this dictionary.</span>
    <span class="n">buffers_to_mutate</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>

    <span class="n">backward_signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ExportBackwardSignature</span><span class="p">]</span>

    <span class="c1"># Map from assertion dependency token index to assertion dep token output</span>
    <span class="c1"># name in output. The shape of output after aot_autograd will be like:</span>
    <span class="c1"># (updated_inputs, user_outputs, dep_token).</span>
    <span class="n">assertion_dep_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">assertion_dep_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertion_dep_token</span>
        <span class="k">if</span> <span class="n">assertion_dep_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">assertion_dep_token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">assertion_dep_token_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">assertion_dep_token</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">user_outputs</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffers_to_mutate</span><span class="p">)</span>
            <span class="o">==</span> <span class="n">assertion_dep_token_index</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ArgumentKind"><a class="viewcode-back" href="../../export.html#torch.export.ArgumentKind">[docs]</a><span class="k">class</span> <span class="nc">ArgumentKind</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">Tensor</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">SymInt</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">Constant</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<div class="viewcode-block" id="ArgumentSpec"><a class="viewcode-back" href="../../export.html#torch.export.ArgumentSpec">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ArgumentSpec</span><span class="p">:</span>
    <span class="n">kind</span><span class="p">:</span> <span class="n">ArgumentKind</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Any</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ArgumentKind</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ArgumentKind</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span></div>


<div class="viewcode-block" id="ModuleCallSignature"><a class="viewcode-back" href="../../export.html#torch.export.ModuleCallSignature">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ModuleCallSignature</span><span class="p">:</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ArgumentSpec</span><span class="p">]</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ArgumentSpec</span><span class="p">]</span>
    <span class="n">in_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span>
    <span class="n">out_spec</span><span class="p">:</span> <span class="n">pytree</span><span class="o">.</span><span class="n">TreeSpec</span></div>


<div class="viewcode-block" id="ModuleCallEntry"><a class="viewcode-back" href="../../export.html#torch.export.ModuleCallEntry">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ModuleCallEntry</span><span class="p">:</span>
    <span class="n">fqn</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">signature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleCallSignature</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="ExportedProgram"><a class="viewcode-back" href="../../export.html#torch.export.ExportedProgram">[docs]</a><span class="k">class</span> <span class="nc">ExportedProgram</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Package of a program from :func:`export`. It contains</span>
<span class="sd">    an :class:`torch.fx.Graph` that represents Tensor computation, a state_dict containing</span>
<span class="sd">    tensor values of all lifted parameters and buffers, and various metadata.</span>

<span class="sd">    You can call an ExportedProgram like the original callable traced by</span>
<span class="sd">    :func:`export` with the same calling convention.</span>

<span class="sd">    To perform transformations on the graph, use ``.module`` property to access</span>
<span class="sd">    an :class:`torch.fx.GraphModule`. You can then use</span>
<span class="sd">    `FX transformation &lt;https://pytorch.org/docs/stable/fx.html#writing-transformations&gt;`_</span>
<span class="sd">    to rewrite the graph. Afterwards, you can simply use :func:`export`</span>
<span class="sd">    again to construct a correct ExportedProgram.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">root</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">graph</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">,</span>
        <span class="n">graph_signature</span><span class="p">:</span> <span class="n">ExportGraphSignature</span><span class="p">,</span>
        <span class="n">call_spec</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]],</span>
        <span class="n">range_constraints</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">equality_constraints</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">module_call_graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ModuleCallEntry</span><span class="p">],</span>
        <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._export.exported_program</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">_create_graph_module_for_export</span><span class="p">,</span>
            <span class="n">CallSpec</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch._export.passes.add_runtime_assertions_for_constraints_pass</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">InputDim</span><span class="p">,</span>
            <span class="n">RangeConstraint</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Remove codegen related things from the graph. It should just be a flat graph.</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">_codegen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">CodeGen</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_module</span> <span class="o">=</span> <span class="n">_create_graph_module_for_export</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_graph_module</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">root</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_signature</span><span class="p">:</span> <span class="n">ExportGraphSignature</span> <span class="o">=</span> <span class="n">graph_signature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_spec</span><span class="p">:</span> <span class="n">CallSpec</span> <span class="o">=</span> <span class="n">call_spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_range_constraints</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">,</span> <span class="n">RangeConstraint</span><span class="p">]</span> <span class="o">=</span> <span class="n">range_constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_equality_constraints</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span>
            <span class="n">Tuple</span><span class="p">[</span><span class="n">InputDim</span><span class="p">,</span> <span class="n">InputDim</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">equality_constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_call_graph</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ModuleCallEntry</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_call_graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_example_inputs</span> <span class="o">=</span> <span class="n">example_inputs</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">graph_module</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_module</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">graph</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">graph_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_signature</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">call_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_spec</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">range_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_range_constraints</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">equality_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_equality_constraints</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">module_call_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_call_graph</span>

    <span class="nd">@property</span>
    <span class="nd">@compatibility</span><span class="p">(</span><span class="n">is_backward_compatible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">example_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_example_inputs</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">torch._export.error</span> <span class="k">as</span> <span class="nn">error</span>
        <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">combine_args_kwargs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">in_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">user_args</span> <span class="o">=</span> <span class="n">combine_args_kwargs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">fx_pytree</span><span class="o">.</span><span class="n">tree_flatten_spec</span><span class="p">(</span><span class="n">user_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">in_spec</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">received_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">user_args</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">error</span><span class="o">.</span><span class="n">InternalError</span><span class="p">(</span>
                    <span class="s2">&quot;Trying to flatten user inputs with exported input tree spec: </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">in_spec</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;but actually got inputs with tree spec of: </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">received_spec</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="n">ordered_params</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">parameters</span>
        <span class="p">)</span>
        <span class="n">ordered_buffers</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">buffers</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input_constraints</span><span class="p">(</span><span class="o">*</span><span class="n">ordered_params</span><span class="p">,</span> <span class="o">*</span><span class="n">ordered_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># NOTE: calling convention is first params, then buffers, then args as user supplied them.</span>
            <span class="c1"># See: torch/_functorch/aot_autograd.py#L1034</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="o">*</span><span class="n">ordered_params</span><span class="p">,</span> <span class="o">*</span><span class="n">ordered_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">enable_io_processing</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">out_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mutation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">buffers_to_mutate</span>
            <span class="n">num_mutated</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutation</span><span class="p">)</span>
            <span class="n">mutated_buffers</span> <span class="o">=</span> <span class="n">res</span><span class="p">[:</span><span class="n">num_mutated</span><span class="p">]</span>

            <span class="c1"># Exclude dependency token from final result.</span>
            <span class="n">assertion_dep_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">assertion_dep_token</span>
            <span class="k">if</span> <span class="n">assertion_dep_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">assertion_dep_token_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">assertion_dep_token</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[:</span><span class="n">assertion_dep_token_index</span><span class="p">]</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="n">num_mutated</span><span class="p">:]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">out_spec</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">received_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">error</span><span class="o">.</span><span class="n">InternalError</span><span class="p">(</span>
                    <span class="s2">&quot;Trying to flatten user outputs with exported output tree spec: </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="o">.</span><span class="n">out_spec</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;but actually got outputs with tree spec of: </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">received_spec</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="o">.</span><span class="n">buffers_to_mutate</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">buffer</span><span class="p">]</span> <span class="o">=</span> <span class="n">mutated_buffers</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
                    <span class="n">ix</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">graph_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">    &quot;</span>
        <span class="p">)</span>
        <span class="n">string</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;ExportedProgram:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">graph_module</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Graph Signature: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_signature</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Symbol to range: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">range_constraints</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">string</span>

<div class="viewcode-block" id="ExportedProgram.module"><a class="viewcode-back" href="../../export.html#torch.export.ExportedProgram.module">[docs]</a>    <span class="k">def</span> <span class="nf">module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a self contained GraphModule with all the parameters/buffers inlined.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">torch._export.exported_program</span> <span class="kn">import</span> <span class="n">unlift_exported_program_lifted_states</span>

        <span class="k">return</span> <span class="n">unlift_exported_program_lifted_states</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">passes</span><span class="p">:</span> <span class="n">PassType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ExportedProgram&quot;</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">torch._export.passes.add_runtime_assertions_for_constraints_pass</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">RangeConstraint</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pm</span> <span class="o">=</span> <span class="n">PassManager</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">passes</span><span class="p">))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">pm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="p">)</span>
        <span class="n">transformed_gm</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">graph_module</span> <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span>
        <span class="k">assert</span> <span class="n">transformed_gm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">def</span> <span class="nf">_get_updated_range_constraints</span><span class="p">(</span>
            <span class="n">gm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">sympy</span><span class="o">.</span><span class="n">Symbol</span><span class="p">,</span> <span class="n">RangeConstraint</span><span class="p">]:</span>
            <span class="k">def</span> <span class="nf">get_shape_env</span><span class="p">(</span><span class="n">gm</span><span class="p">):</span>
                <span class="n">vals</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span>
                    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">]</span>
                <span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">detect_fake_mode</span>

                <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">detect_fake_mode</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">fake_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">shape_env</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vals</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">shape_env</span>

            <span class="n">shape_env</span> <span class="o">=</span> <span class="n">get_shape_env</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">shape_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{}</span>
            <span class="n">range_constraints</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">RangeConstraint</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">lower</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">shape_env</span><span class="o">.</span><span class="n">var_to_range</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">range_constraints</span>

        <span class="k">def</span> <span class="nf">get_output_node_names</span><span class="p">(</span><span class="n">gm</span><span class="p">):</span>
            <span class="n">output_node</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">assert</span> <span class="n">output_node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span>

            <span class="k">return</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">output_node</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="k">def</span> <span class="nf">get_input_node_names</span><span class="p">(</span><span class="n">gm</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">_generate_new_graph_signature</span><span class="p">(</span><span class="n">old_ep</span><span class="p">,</span> <span class="n">new_gm</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Update graph_signature according to graph after transformation.</span>
<span class="sd">            Transformations can lead to node name changes, which are used in</span>
<span class="sd">            graph_signature to identify inputs and outputs. Therefore, after each</span>
<span class="sd">            transformation, we need to update the graph_signature according to</span>
<span class="sd">            new node names.</span>

<span class="sd">            WARNING: This implementation makes a few assumptions</span>
<span class="sd">                - The transformation doesn&#39;t change number of inputs/outputs</span>
<span class="sd">                - Each input/output still has the same meaning.</span>
<span class="sd">                    - For inputs, that means that the inputs in transformed</span>
<span class="sd">                        graph map to the same lifted parameter/buffer or user</span>
<span class="sd">                        input as the input of the same position in the graph</span>
<span class="sd">                        before transformation.</span>
<span class="sd">                    - Similarly for outputs, each output should correspond to the</span>
<span class="sd">                        same mutated buffer or user output as the output value of</span>
<span class="sd">                        the same position  in the graph before transformation.</span>

<span class="sd">            It is difficult to programatically validate these assumptions, but they</span>
<span class="sd">            should hold true most of the time as inputs/outputs of the graph rarely</span>
<span class="sd">            need to be changed.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">old_signature</span> <span class="o">=</span> <span class="n">old_ep</span><span class="o">.</span><span class="n">graph_signature</span>
            <span class="n">old_gm</span> <span class="o">=</span> <span class="n">old_ep</span><span class="o">.</span><span class="n">graph_module</span>

            <span class="n">old_graph_input_node_names</span> <span class="o">=</span> <span class="n">get_input_node_names</span><span class="p">(</span><span class="n">old_gm</span><span class="p">)</span>
            <span class="n">new_graph_input_node_names</span> <span class="o">=</span> <span class="n">get_input_node_names</span><span class="p">(</span><span class="n">new_gm</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_graph_input_node_names</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">new_graph_input_node_names</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                Number of input nodes changed from </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">old_graph_input_node_names</span><span class="p">)</span><span class="si">}</span>
<span class="s2">                to </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">new_graph_input_node_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> after transformation. This</span>
<span class="s2">                transformation is currently not supported.</span>
<span class="s2">                &quot;&quot;&quot;</span>

            <span class="n">old_graph_output_node_names</span> <span class="o">=</span> <span class="n">get_output_node_names</span><span class="p">(</span><span class="n">old_gm</span><span class="p">)</span>
            <span class="n">new_graph_output_node_names</span> <span class="o">=</span> <span class="n">get_output_node_names</span><span class="p">(</span><span class="n">new_gm</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_graph_output_node_names</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">new_graph_output_node_names</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                Number of output values changed from </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">old_graph_output_node_names</span><span class="p">)</span><span class="si">}</span>
<span class="s2">                to </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">new_graph_output_node_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> after transformation. This</span>
<span class="s2">                transformation is currently not supported.</span>
<span class="s2">                &quot;&quot;&quot;</span>

            <span class="n">node_names_mapping</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">old_graph_input_node_names</span> <span class="o">+</span> <span class="n">old_graph_output_node_names</span><span class="p">,</span>
                    <span class="n">new_graph_input_node_names</span> <span class="o">+</span> <span class="n">new_graph_output_node_names</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="n">new_signature</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">old_signature</span><span class="p">)</span>
            <span class="n">new_signature</span><span class="o">.</span><span class="n">user_inputs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">node_names_mapping</span><span class="p">[</span><span class="n">old_user_input</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">old_user_input</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">user_inputs</span>
            <span class="p">]</span>
            <span class="n">new_signature</span><span class="o">.</span><span class="n">user_outputs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">node_names_mapping</span><span class="p">[</span><span class="n">old_user_output</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">old_user_output</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">user_outputs</span>
            <span class="p">]</span>
            <span class="n">new_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">node_names_mapping</span><span class="p">[</span><span class="n">old_input_name</span><span class="p">]:</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span><span class="p">[</span>
                    <span class="n">old_input_name</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">old_input_name</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">inputs_to_parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">node_names_mapping</span><span class="p">[</span><span class="n">old_input_name</span><span class="p">]:</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span><span class="p">[</span>
                    <span class="n">old_input_name</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">old_input_name</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">inputs_to_buffers</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_signature</span><span class="o">.</span><span class="n">buffers_to_mutate</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">node_names_mapping</span><span class="p">[</span><span class="n">old_output_name</span><span class="p">]:</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">buffers_to_mutate</span><span class="p">[</span>
                    <span class="n">old_output_name</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">old_output_name</span> <span class="ow">in</span> <span class="n">old_signature</span><span class="o">.</span><span class="n">buffers_to_mutate</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">new_signature</span>

        <span class="n">new_graph_signature</span> <span class="o">=</span> <span class="n">_generate_new_graph_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformed_gm</span><span class="p">)</span>

        <span class="n">transformed_ep</span> <span class="o">=</span> <span class="n">ExportedProgram</span><span class="p">(</span>
            <span class="n">transformed_gm</span><span class="p">,</span>
            <span class="n">transformed_gm</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
            <span class="n">new_graph_signature</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call_spec</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">_get_updated_range_constraints</span><span class="p">(</span><span class="n">transformed_gm</span><span class="p">),</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">equality_constraints</span><span class="p">),</span>
            <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_call_graph</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">example_inputs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">transformed_ep</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>
        <span class="n">transformed_ep</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">meta</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">transformed_ep</span>

    <span class="k">def</span> <span class="nf">_check_input_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._export.passes.add_runtime_assertions_for_constraints_pass</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">_AddRuntimeAssertionsForConstraintsPass</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># TODO(zhxchen17) Don&#39;t generate a runtime graph on the fly.</span>
        <span class="n">_assertion_graph</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">({},</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="s2">&quot;placeholder&quot;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">new_p</span> <span class="o">=</span> <span class="n">_assertion_graph</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">new_p</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">meta</span>
        <span class="n">_assertion_graph</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">(())</span>
        <span class="n">_assertion_graph_res</span> <span class="o">=</span> <span class="n">_AddRuntimeAssertionsForConstraintsPass</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">range_constraints</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">equality_constraints</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">_assertion_graph</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">_assertion_graph_res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">_assertion_graph</span> <span class="o">=</span> <span class="n">_assertion_graph_res</span><span class="o">.</span><span class="n">graph_module</span>
        <span class="n">_assertion_graph</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO(zhxchen17) check for get_attr</span>
        <span class="c1"># TODO(zhxchen17) check for funcitonal ops</span>
        <span class="k">for</span> <span class="n">gm</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">gm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;call_function&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">node</span><span class="o">.</span><span class="n">target</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">higher_order</span><span class="o">.</span><span class="n">_export_tracepoint</span></div>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">_ConstraintTarget</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This represents input tensor dimensions.  Don&#39;t create this</span>
<span class="sd">    class directly; instead, use :func:`dynamic_dim`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">w_tensor</span><span class="p">:</span> <span class="n">Any</span>  <span class="c1"># weakref to torch.Tensor</span>
    <span class="c1"># TODO: We don&#39;t need t_id; we can get it off of w_tensor</span>
    <span class="n">t_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">class</span> <span class="nc">_ConstraintFactory</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Metaclass that ensures a private constructor for :class:`Constraint`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> has no public constructor. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Please use torch.export.dynamic_dim() to create one&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">w_tensor</span><span class="p">,</span> <span class="n">t_id</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">constraint_range</span><span class="p">,</span> <span class="n">shared</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">w_tensor</span><span class="p">,</span> <span class="n">t_id</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">constraint_range</span><span class="p">,</span> <span class="n">shared</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_constraint</span><span class="p">(</span><span class="n">w_tensor</span><span class="p">,</span> <span class="n">t_id</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">constraint_range</span><span class="p">,</span> <span class="n">shared</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Constraint</span><span class="o">.</span><span class="n">_create</span><span class="p">(</span><span class="n">w_tensor</span><span class="p">,</span> <span class="n">t_id</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">constraint_range</span><span class="p">,</span> <span class="n">shared</span><span class="p">)</span>


<div class="viewcode-block" id="Constraint"><a class="viewcode-back" href="../../export.html#torch.export.Constraint">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">Constraint</span><span class="p">(</span><span class="n">_ConstraintTarget</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">_ConstraintFactory</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Do not construct :class:`Constraint` directly, use :func:`dynamic_dim` instead.</span>

<span class="sd">    This represents constraints on input tensor dimensions, e.g., requiring</span>
<span class="sd">    them to be fully polymorphic or within some range.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE(avik): In the future, this could be Union[StrictMinMaxConstraint, &lt;other kinds&gt;]</span>
    <span class="n">constraint_range</span><span class="p">:</span> <span class="n">StrictMinMaxConstraint</span>
    <span class="c1"># Represent that `constraint_range` is shared with another _ConstraintTarget, which</span>
    <span class="c1"># typically arises because of a specified equality with another dynamic dimension.</span>
    <span class="n">shared</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ConstraintTarget</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_clone_with_range</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">sympy</span><span class="o">.</span><span class="n">oo</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch.utils._sympy.value_ranges</span> <span class="kn">import</span> <span class="n">ValueRanges</span>

        <span class="n">constraint_range</span> <span class="o">=</span> <span class="n">StrictMinMaxConstraint</span><span class="p">(</span>
            <span class="n">vr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">constraint_range</span><span class="o">.</span><span class="n">vr</span> <span class="o">&amp;</span> <span class="n">ValueRanges</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">),</span>
            <span class="n">warn_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_create_constraint</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">constraint_range</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clone_with_range</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clone_with_range</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clone_with_range</span><span class="p">(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clone_with_range</span><span class="p">(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NOTE(avik): We do not support compound expressions like a &lt;= x &lt;= b.</span>
        <span class="c1"># This is because Python implicitly desugars them into bool(a &lt;= x) and bool(x &lt;= b),</span>
        <span class="c1"># and moreover, enforces that any overload of __bool__ must return True or False.</span>
        <span class="c1"># FWIW, sympy also raises TypeError in this case.</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot determine truth value of Constraint. &quot;</span>
            <span class="s2">&quot;If you are trying to combine Constraint&#39;s with logical connectives, &quot;</span>
            <span class="s2">&quot;you can specify them separately instead.&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">serializable_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># We need a serialization compatible format of the constraint so that it</span>
        <span class="c1"># can be savedin the graph module w/o breaking the module serialization.</span>
        <span class="c1"># The saved constraints will be used directly for the post-exporting pass</span>
        <span class="c1"># that converts constraints to runtime assertion. The saved constraints</span>
        <span class="c1"># will not be saved in the serialized module.</span>
        <span class="c1"># TODO: A better way is needed. Currently we use &#39;t_id&#39; to map the constraint,</span>
        <span class="c1"># which is not reliable</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;t_id&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_id</span><span class="p">,</span>
            <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
            <span class="s2">&quot;min&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraint_range</span><span class="o">.</span><span class="n">vr</span><span class="o">.</span><span class="n">lower</span><span class="p">,</span>
            <span class="s2">&quot;max&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraint_range</span><span class="o">.</span><span class="n">vr</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span>
            <span class="s2">&quot;shared&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="kc">None</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="p">{</span>
                    <span class="s2">&quot;t_id&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">t_id</span><span class="p">,</span>
                    <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">),</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Constraint</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;A dynamic dim can be specified equal only to another dynamic dim. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Equality with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span>
            <span class="p">)</span>
        <span class="n">constraint_range</span> <span class="o">=</span> <span class="n">StrictMinMaxConstraint</span><span class="p">(</span>
            <span class="n">vr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">constraint_range</span><span class="o">.</span><span class="n">vr</span> <span class="o">&amp;</span> <span class="n">other</span><span class="o">.</span><span class="n">constraint_range</span><span class="o">.</span><span class="n">vr</span><span class="p">,</span>
            <span class="n">warn_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_create_constraint</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_tensor</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_id</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
            <span class="n">constraint_range</span><span class="p">,</span>
            <span class="n">shared</span><span class="o">=</span><span class="n">_ConstraintTarget</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">w_tensor</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">t_id</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dim</span><span class="p">),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="constrain_as_value"><a class="viewcode-back" href="../../export.html#torch.export.constrain_as_value">[docs]</a><span class="k">def</span> <span class="nf">constrain_as_value</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hint :func:`export` about the constraint of an intermediate scalar value so that subsequent</span>
<span class="sd">    branching behaviors that check on the range of aforementioned scalar value can be</span>
<span class="sd">    soundly traced.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        (Note that if the intermediate scalar value will be used like a size, including</span>
<span class="sd">        being passed as size arg to a tensor factory or view, call :func:`constrain_as_size`</span>
<span class="sd">        instead.)</span>

<span class="sd">    Args:</span>
<span class="sd">        symbol: Intermediate scalar value (int-only now) to apply range constraint on.</span>
<span class="sd">        min (Optional[int]): Minimum possible value of given symbol (inclusive)</span>
<span class="sd">        max (Optional[int]): Maximum possible value of given symbol (inclusive)</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    For example, following program can not be traced soundly::</span>

<span class="sd">        def fn(x):</span>
<span class="sd">            v = x.max().item()</span>
<span class="sd">            if v &gt; 1024:</span>
<span class="sd">                return x</span>
<span class="sd">            else:</span>
<span class="sd">                return x * 2</span>

<span class="sd">    ``v`` is a data-dependent value, which is assumed to have a range of (-inf, inf).</span>
<span class="sd">    :func:`export()` a hint about which branch to take would not be able to determine</span>
<span class="sd">    if the traced branching decision is correct or not. Thus :func:`export()`</span>
<span class="sd">    would give following error::</span>

<span class="sd">        torch._dynamo.exc.UserError: Consider annotating your code using</span>
<span class="sd">        torch.export.constrain_as_size() or torch.export().constrain_as_value() APIs.</span>
<span class="sd">        It appears that you&#39;re trying to get a value out of symbolic int/float whose value</span>
<span class="sd">        is data-dependent (and thus we do not know the true value.)  The expression we were</span>
<span class="sd">        trying to evaluate is f0 &gt; 1024 (unhinted: f0 &gt; 1024).</span>

<span class="sd">    Assuming the actual range of ``v`` can be between [10, 200], you can add a call to</span>
<span class="sd">    :func:`constrain_as_value` in the source code like this::</span>

<span class="sd">        def fn(x):</span>
<span class="sd">            v = x.max().item()</span>

<span class="sd">            # Give export() a hint</span>
<span class="sd">            torch.export.constrain_as_value(v, min=10, max=200)</span>

<span class="sd">            if v &gt; 1024:</span>
<span class="sd">                return x</span>
<span class="sd">            else:</span>
<span class="sd">                return x * 2</span>

<span class="sd">    With the additional hint, :func:`export` would be able to trace the program correctly by taking</span>
<span class="sd">    the ``else`` branch, resulting in following graph::</span>

<span class="sd">        graph():</span>
<span class="sd">            %arg0_1 := placeholder[target=arg0_1]</span>

<span class="sd">            # v = x.max().item()</span>
<span class="sd">            %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))</span>
<span class="sd">            %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))</span>

<span class="sd">            # Asserting 10 &lt;= v &lt;= 200</span>
<span class="sd">            %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 10))</span>
<span class="sd">            %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))</span>
<span class="sd">            %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](</span>
<span class="sd">                args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [10, 200].))</span>
<span class="sd">            %le := call_function[target=operator.le](args = (%_local_scalar_dense, 200))</span>
<span class="sd">            %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))</span>
<span class="sd">            %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](</span>
<span class="sd">                args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [10, 200].))</span>
<span class="sd">            %sym_constrain_range := call_function[target=torch.ops.aten.sym_constrain_range.default](</span>
<span class="sd">                args = (%_local_scalar_dense,), kwargs = {min: 10, max: 200})</span>

<span class="sd">            # Always taking `else` branch to multiply elements `x` by 2 due to hints above</span>
<span class="sd">            %mul := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 2), kwargs = {})</span>
<span class="sd">            return (mul,)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">torch._export.constraints</span> <span class="kn">import</span> <span class="n">constrain_as_value</span>

    <span class="k">return</span> <span class="n">constrain_as_value</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="constrain_as_size"><a class="viewcode-back" href="../../export.html#torch.export.constrain_as_size">[docs]</a><span class="k">def</span> <span class="nf">constrain_as_size</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hint :func:`export` about the constraint of an intermediate scalar value that</span>
<span class="sd">    represents shape of a tensor so that subsequent tensor constructors can be</span>
<span class="sd">    traced correctly because many operators need to make assumption about range</span>
<span class="sd">    of sizes.</span>

<span class="sd">    Args:</span>
<span class="sd">        symbol: Intermediate scalar value (int-only now) to apply range constraint on.</span>
<span class="sd">        min (Optional[int]): Minimum possible value of given symbol (inclusive)</span>
<span class="sd">        max (Optional[int]): Maximum possible value of given symbol (inclusive)</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    For example, following program can not be traced soundly wihout using</span>
<span class="sd">    :func:`constrain_as_size` to give :func:`export` a hint about shape ranges::</span>

<span class="sd">        def fn(x):</span>
<span class="sd">            d = x.max().item()</span>
<span class="sd">            return torch.ones(v)</span>

<span class="sd">    :func:`export` would give following error::</span>

<span class="sd">        torch._dynamo.exc.Unsupported: guard on data-dependent symbolic int/float</span>

<span class="sd">    Assuming the actual range of ``d`` can be between [3, 10], you can add a call to</span>
<span class="sd">    :func:`constrain_as_size` in the source code like this::</span>

<span class="sd">        def fn(x):</span>
<span class="sd">            d = x.max().item()</span>
<span class="sd">            torch.export.constrain_as_size(d, min=3, max=10)</span>
<span class="sd">            return torch.ones(d)</span>

<span class="sd">    With the additional hint, :func:`export` would be able to trace the program correctly by taking</span>
<span class="sd">    the ``else`` branch, resulting in following graph::</span>

<span class="sd">        graph():</span>
<span class="sd">            %arg0_1 := placeholder[target=arg0_1]</span>

<span class="sd">            # d = x.max().item()</span>
<span class="sd">            %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))</span>
<span class="sd">            %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))</span>

<span class="sd">            # Asserting 3 &lt;= d &lt;= 10</span>
<span class="sd">            %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 3))</span>
<span class="sd">            %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))</span>
<span class="sd">            %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](</span>
<span class="sd">                args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [3, 10].))</span>
<span class="sd">            %le := call_function[target=operator.le](args = (%_local_scalar_dense, 10))</span>
<span class="sd">            %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))</span>
<span class="sd">            %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](</span>
<span class="sd">                args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [3, 10].))</span>
<span class="sd">            %sym_constrain_range_for_size := call_function[target=torch.ops.aten.sym_constrain_range_for_size.default](</span>
<span class="sd">                args = (%_local_scalar_dense,), kwargs = {min: 3, max: 10})</span>

<span class="sd">            # Constructing new tensor with d</span>
<span class="sd">            %full := call_function[target=torch.ops.aten.full.default](</span>
<span class="sd">                args = ([%_local_scalar_dense], 1),</span>
<span class="sd">                kwargs = {dtype: torch.float32, layout: torch.strided, device: cpu, pin_memory: False})</span>

<span class="sd">            ......</span>


<span class="sd">    .. warning::</span>
<span class="sd">        if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1,</span>
<span class="sd">        these will SILENTLY report false and be bypassed</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="kn">from</span> <span class="nn">torch._export.constraints</span> <span class="kn">import</span> <span class="n">constrain_as_size</span>

    <span class="k">return</span> <span class="n">constrain_as_size</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">)</span></div>


<div class="viewcode-block" id="dynamic_dim"><a class="viewcode-back" href="../../export.html#torch.export.dynamic_dim">[docs]</a><span class="k">def</span> <span class="nf">dynamic_dim</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :func:`dynamic_dim` constructs a :class:`Constraint` object that describes the dynamism of</span>
<span class="sd">    a dimension ``index`` of tensor ``t``. :class:`Constraint` objects should be passed to</span>
<span class="sd">    ``constraints`` argument of :func:`export`.</span>

<span class="sd">    Args:</span>
<span class="sd">        t (torch.Tensor): Example input tensor that have dynamic dimension size(s)</span>
<span class="sd">        index (int): Index of dynamic dimension</span>

<span class="sd">    Returns:</span>
<span class="sd">        A :class:`Constraint` object that describes shape dynamism. It can be passed to :func:`export` so</span>
<span class="sd">        that :func:`export` does not assume static size of specified tensor, i.e. keeping it dynamic</span>
<span class="sd">        as a symbolic size rather than specializing according to size of example tracing input.</span>

<span class="sd">    Specifically :func:`dynamic_dim` can be used to express following types of dynamism.</span>

<span class="sd">    - Size of a dimension is dynamic and unbounded::</span>

<span class="sd">        t0 = torch.rand(2, 3)</span>
<span class="sd">        t1 = torch.rand(3, 4)</span>

<span class="sd">        # First dimension of t0 can be dynamic size rather than always being static size 2</span>
<span class="sd">        constraints = [dynamic_dim(t0, 0)]</span>
<span class="sd">        ep = export(fn, (t0, t1), constraints=constraints)</span>

<span class="sd">    - Size of a dimension is dynamic with a lower bound::</span>

<span class="sd">        t0 = torch.rand(10, 3)</span>
<span class="sd">        t1 = torch.rand(3, 4)</span>

<span class="sd">        # First dimension of t0 can be dynamic size with a lower bound of 5 (inclusive)</span>
<span class="sd">        # Second dimension of t1 can be dynamic size with a lower bound of 2 (exclusive)</span>
<span class="sd">        constraints = [</span>
<span class="sd">            dynamic_dim(t0, 0) &gt;= 5,</span>
<span class="sd">            dynamic_dim(t1, 1) &gt; 2,</span>
<span class="sd">        ]</span>
<span class="sd">        ep = export(fn, (t0, t1), constraints=constraints)</span>

<span class="sd">    - Size of a dimension is dynamic with an upper bound::</span>

<span class="sd">        t0 = torch.rand(10, 3)</span>
<span class="sd">        t1 = torch.rand(3, 4)</span>

<span class="sd">        # First dimension of t0 can be dynamic size with a upper bound of 16 (inclusive)</span>
<span class="sd">        # Second dimension of t1 can be dynamic size with a upper bound of 8 (exclusive)</span>
<span class="sd">        constraints = [</span>
<span class="sd">            dynamic_dim(t0, 0) &lt;= 16,</span>
<span class="sd">            dynamic_dim(t1, 1) &lt; 8,</span>
<span class="sd">        ]</span>
<span class="sd">        ep = export(fn, (t0, t1), constraints=constraints)</span>

<span class="sd">    - Size of a dimension is dynamic and it is always equal to size of another dynamic dimension::</span>

<span class="sd">        t0 = torch.rand(10, 3)</span>
<span class="sd">        t1 = torch.rand(3, 4)</span>

<span class="sd">        # Sizes of second dimension of t0 and first dimension are always equal</span>
<span class="sd">        constraints = [</span>
<span class="sd">            dynamic_dim(t0, 1) == dynamic_dim(t1, 0),</span>
<span class="sd">        ]</span>
<span class="sd">        ep = export(fn, (t0, t1), constraints=constraints)</span>

<span class="sd">    - Mix and match all types above as long as they do not express conflicting requirements</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">dynamic_dim</span>

    <span class="k">return</span> <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="export"><a class="viewcode-back" href="../../export.html#torch.export.export">[docs]</a><span class="k">def</span> <span class="nf">export</span><span class="p">(</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">constraints</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Constraint</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :func:`export` takes an arbitrary Python callable (an nn.Module, a function or</span>
<span class="sd">    a method) and produces a traced graph representing only the Tensor</span>
<span class="sd">    computation of the function in an Ahead-of-Time (AOT) fashion, which can</span>
<span class="sd">    subsequently be executed with different outputs or serialized.  The traced</span>
<span class="sd">    graph (1) produces a normalized operator set consisting only of functional</span>
<span class="sd">    `Core ATen Operator Set &lt;https://pytorch.org/docs/stable/ir.html&gt;`_</span>
<span class="sd">    and user specified custom operators, (2) has eliminated all Python control</span>
<span class="sd">    flow and data structures (except for certain</span>
<span class="sd">    conditions), and (3) has the set of shape constraints needed to show that</span>
<span class="sd">    this normalization and control flow elimination is sound for a future</span>
<span class="sd">    input.</span>

<span class="sd">    **Soundness Guarantee**</span>

<span class="sd">    While tracing, :func:`export()` takes note of shape-related assumptions</span>
<span class="sd">    made by the user program and the underlying PyTorch operator kernels.</span>
<span class="sd">    The output :class:`ExportedProgram` is considered valid only when these</span>
<span class="sd">    assumptions hold true.</span>

<span class="sd">    There are 2 types of assumptions made during tracing</span>

<span class="sd">    - Shapes (not values) of input tensors.</span>
<span class="sd">    - Ranges (lower and upper bound) of values extracted from intermediate tensors via ``.item()`` or direct indexing.</span>


<span class="sd">    All assumptions must be validated at graph capture time for :func:`export`</span>
<span class="sd">    to succeed. Specifically:</span>

<span class="sd">    - Assumptions on static shapes of input tensors are automatically validated without additional effort.</span>
<span class="sd">    - Assumptions on dynamic shape of input tensors require explicit `Input Constraint`</span>
<span class="sd">      constructed with :func:`dynamic_dim` APIs</span>
<span class="sd">    - Assumptions on range of intermediate values require explicit `Inline Constraint`,</span>
<span class="sd">      constructed use :func:`constrain_as_size` and :func:`constraint_as_value` APIs.</span>

<span class="sd">    If any assumption can not be validated, a fatal error will be raised. When that happens,</span>
<span class="sd">    the error message will include suggested code needed to construct necessary</span>
<span class="sd">    constraints to validate the assumptions, for example :func:`export` would suggest</span>
<span class="sd">    following code for input constraints::</span>

<span class="sd">        def specify_constraints(x):</span>
<span class="sd">            return [</span>
<span class="sd">                # x:</span>
<span class="sd">                dynamic_dim(x, 0) &lt;= 5,</span>
<span class="sd">            ]</span>

<span class="sd">    This example means the program requires the dim 0 of input ``x`` to be less</span>
<span class="sd">    than or equal to 5 to be valid. You can inspect the constraints needed and</span>
<span class="sd">    then copy this exact function into your code to generated needed</span>
<span class="sd">    constraints to be passed into ``constraints`` argument.</span>

<span class="sd">    Args:</span>
<span class="sd">        f: The callable to trace.</span>

<span class="sd">        args: Example positional inputs.</span>

<span class="sd">        kwargs: Optional example keyword inputs.</span>

<span class="sd">        constraints: An optional list of constraints on the dynamic arguments</span>
<span class="sd">         that specify their possible range of shapes. By default, shapes of</span>
<span class="sd">         input torch.Tensors are assumed to be static. If an input torch.Tensor</span>
<span class="sd">         is expected to have dynamic shapes, please use :func:`dynamic_dim`</span>
<span class="sd">         to define :class:`Constraint` objects that specify the dynamics and the possible</span>
<span class="sd">         range of shapes. See :func:`dynamic_dim` docstring for examples on</span>
<span class="sd">         how to use it.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An :class:`ExportedProgram` containing the traced callable.</span>

<span class="sd">    **Acceptable input/output types**</span>

<span class="sd">    Acceptable types of inputs (for ``args`` and ``kwargs``) and outputs include:</span>

<span class="sd">    - Primitive types, i.e. ``torch.Tensor``, ``int``, ``float``, ``bool`` and ``str``.</span>
<span class="sd">    - (Nested) Data structures comprising of ``dict``, ``list``, ``tuple``, ``namedtuple`` and</span>
<span class="sd">      ``OrderedDict`` containing all above types.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">export</span>

    <span class="k">return</span> <span class="n">export</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">constraints</span><span class="p">)</span></div>


<div class="viewcode-block" id="save"><a class="viewcode-back" href="../../export.html#torch.export.save">[docs]</a><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="n">ep</span><span class="p">:</span> <span class="n">ExportedProgram</span><span class="p">,</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">extra_files</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">opset_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Under active development, saved files may not be usable in newer versions</span>
<span class="sd">        of PyTorch.</span>

<span class="sd">    Saves an :class:`ExportedProgram` to a file-like object. It can then be</span>
<span class="sd">    loaded using the Python API :func:`torch.export.load &lt;torch.export.load&gt;`.</span>

<span class="sd">    Args:</span>
<span class="sd">        ep (ExportedProgram): The exported program to save.</span>

<span class="sd">        f (Union[str, pathlib.Path, io.BytesIO): A file-like object (has to</span>
<span class="sd">         implement write and flush) or a string containing a file name.</span>

<span class="sd">        extra_files (Optional[Dict[str, Any]]): Map from filename to contents</span>
<span class="sd">         which will be stored as part of f.</span>

<span class="sd">        opset_version (Optional[Dict[str, int]]): A map of opset names</span>
<span class="sd">         to the version of this opset</span>


<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        import io</span>

<span class="sd">        class MyModule(torch.nn.Module):</span>
<span class="sd">            def forward(self, x):</span>
<span class="sd">                return x + 10</span>

<span class="sd">        ep = torch.export.export(MyModule(), torch.randn(5))</span>

<span class="sd">        # Save to file</span>
<span class="sd">        torch.export.save(ep, &#39;exported_program.pt2&#39;)</span>

<span class="sd">        # Save to io.BytesIO buffer</span>
<span class="sd">        buffer = io.BytesIO()</span>
<span class="sd">        torch.export.save(ep, buffer)</span>

<span class="sd">        # Save with extra files</span>
<span class="sd">        extra_files = {&#39;foo.txt&#39;: b&#39;bar&#39;}</span>
<span class="sd">        torch.export.save(ep, &#39;exported_program.pt2&#39;, extra_files=extra_files)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">save</span>

    <span class="n">save</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">extra_files</span><span class="o">=</span><span class="n">extra_files</span><span class="p">,</span> <span class="n">opset_version</span><span class="o">=</span><span class="n">opset_version</span><span class="p">)</span></div>


<div class="viewcode-block" id="load"><a class="viewcode-back" href="../../export.html#torch.export.load">[docs]</a><span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">extra_files</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">expected_opset_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Under active development, saved files may not be usable in newer versions</span>
<span class="sd">        of PyTorch.</span>

<span class="sd">    Loads an :class:`ExportedProgram` previously saved with</span>
<span class="sd">    :func:`torch.export.save &lt;torch.export.save&gt;`.</span>

<span class="sd">    Args:</span>
<span class="sd">        ep (ExportedProgram): The exported program to save.</span>

<span class="sd">        f (Union[str, pathlib.Path, io.BytesIO): A file-like object (has to</span>
<span class="sd">         implement write and flush) or a string containing a file name.</span>

<span class="sd">        extra_files (Optional[Dict[str, Any]]): The extra filenames given in</span>
<span class="sd">         this map would be loaded and their content would be stored in the</span>
<span class="sd">         provided map.</span>

<span class="sd">        expected_opset_version (Optional[Dict[str, int]]): A map of opset names</span>
<span class="sd">         to expected opset versions</span>

<span class="sd">    Returns:</span>
<span class="sd">        An :class:`ExportedProgram` object</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        import io</span>

<span class="sd">        # Load ExportedProgram from file</span>
<span class="sd">        ep = torch.export.load(&#39;exported_program.pt2&#39;)</span>

<span class="sd">        # Load ExportedProgram from io.BytesIO object</span>
<span class="sd">        with open(&#39;exported_program.pt2&#39;, &#39;rb&#39;) as f:</span>
<span class="sd">            buffer = io.BytesIO(f.read())</span>
<span class="sd">        buffer.seek(0)</span>
<span class="sd">        ep = torch.export.load(buffer)</span>

<span class="sd">        # Load with extra files.</span>
<span class="sd">        extra_files = {&#39;foo.txt&#39;: &#39;&#39;}  # values will be replaced with data</span>
<span class="sd">        ep = torch.export.load(&#39;exported_program.pt2&#39;, extra_files=extra_files)</span>
<span class="sd">        print(extra_files[&#39;foo.txt&#39;])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">load</span>

    <span class="k">return</span> <span class="n">load</span><span class="p">(</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">extra_files</span><span class="o">=</span><span class="n">extra_files</span><span class="p">,</span> <span class="n">expected_opset_version</span><span class="o">=</span><span class="n">expected_opset_version</span>
    <span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/sphinx_highlight.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.utils.backend_registration &mdash; PyTorch 2.1 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/utils/backend_registration.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.1 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../utils.html">torch.utils</a> &gt;</li>
        
      <li>torch.utils.backend_registration</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.utils.backend_registration</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_rename_privateuse1_backend</span><span class="p">,</span> <span class="n">_get_privateuse1_backend_name</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;rename_privateuse1_backend&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_methods_for_privateuse1_backend&quot;</span><span class="p">]</span>

<span class="c1"># TODO: Should use `torch._C._get_privateuse1_backend_name()` to get</span>
<span class="c1"># renamed-backend name for `privateuse1`, but the func will cause an</span>
<span class="c1"># error with torch.jit.script, so we use the global variable named</span>
<span class="c1"># `_privateuse1_backend_name`.</span>
<span class="n">_privateuse1_backend_name</span> <span class="o">=</span> <span class="s2">&quot;privateuseone&quot;</span>

<div class="viewcode-block" id="rename_privateuse1_backend"><a class="viewcode-back" href="../../../generated/torch.utils.rename_privateuse1_backend.html#torch.utils.rename_privateuse1_backend">[docs]</a><span class="k">def</span> <span class="nf">rename_privateuse1_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    rename_privateuse1_backend(backend_name) -&gt; None</span>

<span class="sd">    This API should be use to rename the privateuse1 backend device to make</span>
<span class="sd">    it more convenient to use as a device name within PyTorch APIs.</span>

<span class="sd">    The steps are:</span>

<span class="sd">    (1) (In C++) implement kernels for various torch operations, and register them</span>
<span class="sd">        to the PrivateUse1 dispatch key.</span>
<span class="sd">    (2) (In python) call torch.utils.rename_privateuse1_backend(&quot;foo&quot;)</span>

<span class="sd">    You can now use &quot;foo&quot; as an ordinary device string in python.</span>

<span class="sd">    Note: this API can only be called once per process. Attempting to change</span>
<span class="sd">    the external backend after it&#39;s already been set will result in an error.</span>

<span class="sd">    Note(AMP): If you want to support AMP on your device, you can register a custom backend module.</span>
<span class="sd">    The backend must register a custom backend module with ``torch._register_device_module(&quot;foo&quot;, BackendModule)``.</span>
<span class="sd">    BackendModule needs to have the following API&#39;s:</span>

<span class="sd">    (1) ``get_amp_supported_dtype() -&gt; List[torch.dtype]``</span>
<span class="sd">        get the supported dtypes on your &quot;foo&quot; device in AMP, maybe the &quot;foo&quot; device supports one more dtype.</span>

<span class="sd">    (2) ``is_autocast_enabled() -&gt; bool``</span>
<span class="sd">        check the AMP is enabled or not on your &quot;foo&quot; device.</span>

<span class="sd">    (3) ``get_autocast_dtype() -&gt; torch.dtype``</span>
<span class="sd">        get the supported dtype on your &quot;foo&quot; device in AMP, which is set by ``set_autocast_dtype`` or the</span>
<span class="sd">        default dtype, and the default dtype is ``torch.float16``.</span>

<span class="sd">    (4) ``set_autocast_enabled(bool) -&gt; None``</span>
<span class="sd">        enable the AMP or not on your &quot;foo&quot; device.</span>

<span class="sd">    (5) ``set_autocast_dtype(dtype) -&gt; None``</span>
<span class="sd">        set the supported dtype on your &quot;foo&quot; device in AMP, and the dtype be contained in the dtypes got</span>
<span class="sd">        from ``get_amp_supported_dtype``.</span>

<span class="sd">    Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API&#39;s:</span>

<span class="sd">    (1) ``_is_in_bad_fork() -&gt; bool``</span>
<span class="sd">        Return ``True`` if now it is in bad_fork, else return ``False``.</span>

<span class="sd">    (2) ``manual_seed_all(seed int) -&gt; None``</span>
<span class="sd">        Sets the seed for generating random numbers for your devices.</span>

<span class="sd">    (3) ``device_count() -&gt; int``</span>
<span class="sd">        Returns the number of &quot;foo&quot;s available.</span>

<span class="sd">    (4) ``get_rng_state(device: Union[int, str, torch.device] = &#39;foo&#39;) -&gt; Tensor``</span>
<span class="sd">        Returns a list of ByteTensor representing the random number states of all devices.</span>

<span class="sd">    (5) ``set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = &#39;foo&#39;) -&gt; None``</span>
<span class="sd">        Sets the random number generator state of the specified &quot;foo&quot; device.</span>

<span class="sd">    And there are some common funcs:</span>

<span class="sd">    (1) ``is_available() -&gt; bool``</span>
<span class="sd">        Returns a bool indicating if &quot;foo&quot; is currently available.</span>

<span class="sd">    (2) ``current_device() -&gt; int``</span>
<span class="sd">        Returns the index of a currently selected device.</span>

<span class="sd">    For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend</span>
<span class="sd">    For an existing example, see https://github.com/bdhirsh/pytorch_open_registration_example</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;failing&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.utils.rename_privateuse1_backend(&quot;foo&quot;)</span>
<span class="sd">        # This will work, assuming that you&#39;ve implemented the right C++ kernels</span>
<span class="sd">        # to implement torch.ones.</span>
<span class="sd">        &gt;&gt;&gt; a = torch.ones(2, device=&quot;foo&quot;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
    <span class="n">_rename_privateuse1_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">)</span>
    <span class="k">global</span> <span class="n">_privateuse1_backend_name</span>
    <span class="n">_privateuse1_backend_name</span> <span class="o">=</span> <span class="n">backend_name</span></div>

<span class="k">def</span> <span class="nf">_check_register_once</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The custom device module of </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2"> has already been registered with </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_normalization_device</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_get_current_device_index</span><span class="p">():</span>
        <span class="n">_get_device_index</span> <span class="o">=</span> <span class="s2">&quot;current_device&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)</span> <span class="ow">and</span> \
                <span class="nb">hasattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">),</span> <span class="n">_get_device_index</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">),</span> <span class="n">_get_device_index</span><span class="p">)()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># The default device index is 0.</span>
            <span class="k">return</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_current_device_index</span><span class="p">()</span>
    <span class="c1"># if isinstance(device, str), this means that the parameter passed in is in the string format &quot;foo:0&quot;</span>
    <span class="c1"># convert str object to torch.device object, and then process it uniformly</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># variable devcie can only be torch.device type or int type</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="n">custom_backend_name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid device, must be </span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s2"> device&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_idx</span> <span class="o">=</span> <span class="n">_get_current_device_index</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_idx</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span>
    <span class="c1"># if isinstance(device, int), we can take the index number directly</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">device_idx</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">return</span> <span class="n">device_idx</span>


<span class="k">def</span> <span class="nf">_generate_tensor_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nd">@property</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="k">def</span> <span class="nf">wrap_tensor_backend</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">custom_backend_name</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wrap_tensor_backend</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap_tensor_to</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs Tensor device conversion. Call the to operator implementation.</span>

<span class="sd">        .. note::</span>
<span class="sd">            If the ``self`` Tensor already</span>
<span class="sd">            has the correct :class:`torch.device`, then ``self`` is returned.</span>
<span class="sd">            Otherwise, the returned tensor is a copy of ``self`` with the desired :class:`torch.device`.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (int, optional): if specified, all parameters will be copied to that device</span>
<span class="sd">            non_blocking (bool): If ``True`` and the source is in pinned memory,</span>
<span class="sd">                the copy will be asynchronous with respect to the host. Otherwise,</span>
<span class="sd">                the argument has no effect.</span>
<span class="sd">            **kwargs (dict): For compatibility, may contain the key ``memory_format`` argument.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device_idx</span> <span class="o">=</span> <span class="n">_normalization_device</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">device_idx</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">),</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">wrap_tensor_to</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_generate_module_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Generate Module attributes and methods depends on Tensor methods,</span>
    <span class="c1"># so we need to check whether Tensor methods is already registered.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Can not automatically generate </span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s2">() method for torch.nn.Module.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Because torch.Tensor doesn&#39;t has the method </span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s2">().&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;For this error, you can try setting for_tensor=True.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap_module_to</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                       <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the custom device.</span>

<span class="sd">        This also makes associated parameters and buffers different objects. So</span>
<span class="sd">        it should be called before constructing optimizer if the module will</span>
<span class="sd">        live on device while being optimized.</span>

<span class="sd">        .. note::</span>
<span class="sd">            This method modifies the module in-place.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (int, optional): if specified, all parameters will be copied to that device</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">wrap_module_to</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_generate_storage_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                                                      <span class="n">unsupported_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Attribute is registered in the _StorageBase class</span>
    <span class="c1"># and UntypedStorage obtains through inheritance.</span>
    <span class="nd">@property</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="k">def</span> <span class="nf">wrap_storage_backend</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_StorageBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the internal :class:`torch.UntypedStorage`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">custom_backend_name</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wrap_storage_backend</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap_storage_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a copy of this object in custom device memory.</span>

<span class="sd">        If this object is already in device memory and on the correct device, then</span>
<span class="sd">        no copy is performed and the original object is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (int): The destination device id. Defaults to the current device.</span>
<span class="sd">            non_blocking (bool): If ``True`` and the source is in pinned memory,</span>
<span class="sd">            the copy will be asynchronous with respect to the host. Otherwise,</span>
<span class="sd">            the argument has no effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># There should be a judgment related to storage device and a judgment related to storage type,</span>
        <span class="c1"># but it depends on the extended function, so this part is temporarily omitted in the automatic generation.</span>
        <span class="n">device_idx</span> <span class="o">=</span> <span class="n">_normalization_device</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">):</span>
            <span class="c1"># storage has already on expected device.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">==</span> <span class="n">device_idx</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span>
        <span class="c1"># For sparse storage, custom need to extend the implementation by themselves.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can not support a sparse storage move to </span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s2"> backend&quot;</span><span class="p">)</span>
        <span class="c1"># create untyped_storage and copy data</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="n">device_idx</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">untyped_storage</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">untyped_storage</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">wrap_storage_to</span><span class="p">)</span>

    <span class="c1"># Register the corresponding attribute for the TypedStorage class.</span>
    <span class="c1"># When the TypedStorage class is removed, the registration is also removed.</span>

    <span class="nd">@property</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="k">def</span> <span class="nf">wrap_typed_storage_backend</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">custom_backend_name</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;is_</span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wrap_typed_storage_backend</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap_typed_storage_to</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span>
                              <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">unsupported_dtype</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">unsupported_dtype</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cannot create </span><span class="si">{</span><span class="n">custom_backend_name</span><span class="si">}</span><span class="s2"> storage &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> dtype is not supported by this backend&quot;</span><span class="p">)</span>
        <span class="n">custom_backend_storage</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">custom_backend_storage</span><span class="p">)</span>

    <span class="n">_check_register_once</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">wrap_typed_storage_to</span><span class="p">)</span>


<div class="viewcode-block" id="generate_methods_for_privateuse1_backend"><a class="viewcode-back" href="../../../generated/torch.utils.generate_methods_for_privateuse1_backend.html#torch.utils.generate_methods_for_privateuse1_backend">[docs]</a><span class="k">def</span> <span class="nf">generate_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">for_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">for_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                             <span class="n">for_storage</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                                             <span class="n">unsupported_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    generate_methods_for_privateuse1_backend(for_tensor, for_module, for_storage, unsupported_dtype) -&gt; None</span>

<span class="sd">    Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.</span>
<span class="sd">    In the default scenario, storage-related methods will not be generated automatically.</span>

<span class="sd">    When you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key.</span>
<span class="sd">    And call the function torch.rename_privateuse1_backend(&quot;foo&quot;) to rename your backend name.</span>
<span class="sd">    At this point, you can easily register specific methods and attributes by calling this function.</span>
<span class="sd">    Just like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.</span>

<span class="sd">    Note: We recommend you use generic functions (check devices are equal or to(device=)).</span>
<span class="sd">    We provide these methods for convenience only and they will be &quot;monkey patched&quot; onto the objects</span>
<span class="sd">    and so will not be properly typed. For Storage methods generate, if you need to support sparse data storage,</span>
<span class="sd">    you need to extend the implementation yourself.</span>

<span class="sd">    Args:</span>
<span class="sd">        for_tensor (bool): whether register related methods for torch.Tensor class.</span>
<span class="sd">        for_module (bool): whether register related methods for torch.nn.Module class.</span>
<span class="sd">        for_storage (bool): whether register related methods for torch.Storage class.</span>
<span class="sd">        unsupported_dtype (List[torch.dtype]): takes effect only when the storage method needs to be generated,</span>
<span class="sd">            indicating that the storage does not support the torch.dtype type.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;failing&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.utils.rename_privateuse1_backend(&quot;foo&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.utils.generate_methods_for_privateuse1_backend()</span>
<span class="sd">        # Then automatically generate backend-related attributes and methods.</span>
<span class="sd">        &gt;&gt;&gt; a = torch.tensor(2).foo()</span>
<span class="sd">        &gt;&gt;&gt; a.is_foo</span>
<span class="sd">        &gt;&gt;&gt; hasattr(torch.nn.Module, &#39;foo&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="n">custom_backend_name</span> <span class="o">=</span> <span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">for_tensor</span><span class="p">:</span>
        <span class="n">_generate_tensor_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">for_module</span><span class="p">:</span>
        <span class="n">_generate_module_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">for_storage</span><span class="p">:</span>
        <span class="n">_generate_storage_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">custom_backend_name</span><span class="p">,</span> <span class="n">unsupported_dtype</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_get_custom_mod_func</span><span class="p">(</span><span class="n">func_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the func named `func_name` defined in custom device module. If not defined,</span>
<span class="sd">    return `None`. And the func is registered with `torch.utils.rename_privateuse1_backend(&#39;foo&#39;)`</span>
<span class="sd">    and `torch._register_device_module(&#39;foo&#39;, BackendModule)`.</span>
<span class="sd">    If the custom device module or the func is not defined, it will give warning or error message.</span>
<span class="sd">    Args:</span>
<span class="sd">        func_name (str): return the callable func named func_name defined in custom device module.</span>
<span class="sd">    Example::</span>
<span class="sd">        class DummyfooModule:</span>
<span class="sd">            @staticmethod</span>
<span class="sd">            def is_available():</span>
<span class="sd">                return True</span>
<span class="sd">            @staticmethod</span>
<span class="sd">            def func_name(*args, **kwargs):</span>
<span class="sd">                ....</span>
<span class="sd">        torch.utils.rename_privateuse1_backend(&quot;foo&quot;)</span>
<span class="sd">        torch._register_device_module(&quot;foo&quot;, DummyfooModule)</span>
<span class="sd">        foo_is_available_func = torch.utils.backend_registration._get_custom_mod_func(&quot;is_available&quot;)</span>
<span class="sd">        if foo_is_available_func:</span>
<span class="sd">            foo_is_available = foo_is_available_func()</span>
<span class="sd">        func_ = torch.utils.backend_registration._get_custom_mod_func(&quot;func_name&quot;)</span>
<span class="sd">        if func_:</span>
<span class="sd">            result = func_(*args, **kwargs)</span>
<span class="sd">    Attention: This function is not meant to be used directly by users, which is why</span>
<span class="sd">    it is marked as private. It is a convenience function for backend implementers to</span>
<span class="sd">    more easily call the hooks into their backend extensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;func_name must be `str`, but got `</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">func_name</span><span class="p">)</span><span class="si">}</span><span class="s2">`.&quot;</span>
    <span class="n">backend_name</span> <span class="o">=</span> <span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
    <span class="n">custom_device_mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
    <span class="n">function</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">custom_device_mod</span><span class="p">,</span> <span class="n">func_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
    <span class="k">if</span> <span class="n">custom_device_mod</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Try to call torch.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">.</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s1">. The backend must register a custom backend &#39;</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;module with `torch._register_device_module(&#39;</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s2">&#39;, BackendModule)`. And &quot;</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;BackendModule needs to have the following API&#39;s:</span><span class="se">\n</span><span class="s2"> `</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2">(*args, **kwargs)`. </span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">function</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
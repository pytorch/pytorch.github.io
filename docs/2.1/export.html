


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.export &mdash; PyTorch 2.1 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/export.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Writing Graph Transformations on ATen IR" href="torch.compiler_transformations.html" />
    <link rel="prev" title="torch.backends" href="backends.html" />


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.1 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.export</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/export.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torch-export">
<span id="id1"></span><h1>torch.export<a class="headerlink" href="#torch-export" title="Permalink to this heading">¶</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This feature is a prototype under active development and there WILL BE
BREAKING CHANGES in the future.</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a> takes an arbitrary Python callable (a
<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>, a function or a method) and produces a traced graph
representing only the Tensor computation of the function in an Ahead-of-Time
(AOT) fashion, which can subsequently be executed with different outputs or
serialized.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">exported_program</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">ExportedProgram</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">example_args</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_program</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportedProgram</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">GraphModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">arg1_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>
            <span class="c1"># code: a = torch.sin(x)</span>
            <span class="n">sin</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sin</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg0_1</span><span class="p">);</span>

            <span class="c1"># code: b = torch.cos(y)</span>
            <span class="n">cos</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">cos</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg1_1</span><span class="p">);</span>

            <span class="c1"># code: return a + b</span>
            <span class="n">add</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sin</span><span class="p">,</span> <span class="n">cos</span><span class="p">);</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">add</span><span class="p">,)</span>

    <span class="n">Graph</span> <span class="n">signature</span><span class="p">:</span> <span class="n">ExportGraphSignature</span><span class="p">(</span>
        <span class="n">parameters</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">buffers</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">user_inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;arg0_1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg1_1&#39;</span><span class="p">],</span>
        <span class="n">user_outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;add&#39;</span><span class="p">],</span>
        <span class="n">inputs_to_parameters</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">inputs_to_buffers</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">buffers_to_mutate</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">backward_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">assertion_dep_token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">Range</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">{}</span>
    <span class="n">Equality</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">[]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a clean intermediate representation (IR) with the
following invariants. More specifications about the IR can be found here (coming
soon!).</p>
<ul class="simple">
<li><p><strong>Soundness</strong>: It is guaranteed to be a sound representation of the original
program, and maintains the same calling conventions of the original program.</p></li>
<li><p><strong>Normalized</strong>: There are no Python semantics within the graph. Submodules
from the original programs are inlined to form one fully flattened
computational graph.</p></li>
<li><p><strong>Defined Operator Set</strong>: The graph produced contains only a small defined
<a class="reference internal" href="torch.compiler_ir.html#torch-compiler-ir"><span class="std std-ref">Core ATen IR</span></a> opset and registered custom
operators.</p></li>
<li><p><strong>Graph properties</strong>: The graph is purely functional, meaning it does not
contain operations with side effects such as mutations or aliasing. It does
not mutate any intermediate values, parameters, or buffers.</p></li>
<li><p><strong>Metadata</strong>: The graph contains metadata captured during tracing, such as a
stacktrace from user’s code.</p></li>
</ul>
<p>Under the hood, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> leverages the following latest technologies:</p>
<ul class="simple">
<li><p><strong>TorchDynamo (torch._dynamo)</strong> is an internal API that uses a CPython feature
called the Frame Evaluation API to safely trace PyTorch graphs. This
provides a massively improved graph capturing experience, with much fewer
rewrites needed in order to fully trace the PyTorch code.</p></li>
<li><p><strong>AOT Autograd</strong> provides a functionalized PyTorch graph and ensures the graph
is decomposed/lowered to the small defined Core ATen operator set.</p></li>
<li><p><strong>Torch FX (torch.fx)</strong> is the underlying representation of the graph,
allowing flexible Python-based transformations.</p></li>
</ul>
<div class="section" id="existing-frameworks">
<h3>Existing frameworks<a class="headerlink" href="#existing-frameworks" title="Permalink to this heading">¶</a></h3>
<p><a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> also utilizes the same PT2 stack as <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, but
is slightly different:</p>
<ul class="simple">
<li><p><strong>JIT vs. AOT</strong>: <a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> is a JIT compiler whereas
which is not intended to be used to produce compiled artifacts outside of
deployment.</p></li>
<li><p><strong>Partial vs. Full Graph Capture</strong>: When <a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> runs into an
untraceable part of a model, it will “graph break” and fall back to running
the program in the eager Python runtime. In comparison, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> aims
to get a full graph representation of a PyTorch model, so it will error out
when something untraceable is reached. Since <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a full
graph disjoint from any Python features or runtime, this graph can then be
saved, loaded, and run in different environments and languages.</p></li>
<li><p><strong>Usability tradeoff</strong>: Since <a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> is able to fallback to the
Python runtime whenever it reaches something untraceable, it is a lot more
flexible. <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will instead require users to provide more
information or rewrite their code to make it traceable.</p></li>
</ul>
<p>Compared to <a class="reference internal" href="fx.html#torch.fx.symbolic_trace" title="torch.fx.symbolic_trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fx.symbolic_trace()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> traces using
TorchDynamo which operates at the Python bytecode level, giving it the ability
to trace arbitrary Python constructs not limited by what Python operator
overloading supports. Additionally, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> keeps fine-grained track of
tensor metadata, so that conditionals on things like tensor shapes do not
fail tracing. In general, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is expected to work on more user
programs, and produce lower-level graphs (at the <code class="docutils literal notranslate"><span class="pre">torch.ops.aten</span></code> operator
level). Note that users can still use <a class="reference internal" href="fx.html#torch.fx.symbolic_trace" title="torch.fx.symbolic_trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fx.symbolic_trace()</span></code></a> as a
preprocessing step before <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p>Compared to <a class="reference internal" href="generated/torch.jit.script.html#torch.jit.script" title="torch.jit.script"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> does not capture Python
control flow or data structures, but it supports more Python language features
than TorchScript (as it is easier to have comprehensive coverage over Python
bytecodes). The resulting graphs are simpler and only have straight line control
flow (except for explicit control flow operators).</p>
<p>Compared to <a class="reference internal" href="generated/torch.jit.trace.html#torch.jit.trace" title="torch.jit.trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is sound: it is able to
trace code that performs integer computation on sizes and records all of the
side-conditions necessary to show that a particular trace is valid for other
inputs.</p>
</div>
</div>
<div class="section" id="exporting-a-pytorch-model">
<h2>Exporting a PyTorch Model<a class="headerlink" href="#exporting-a-pytorch-model" title="Permalink to this heading">¶</a></h2>
<div class="section" id="an-example">
<h3>An Example<a class="headerlink" href="#an-example" title="Permalink to this heading">¶</a></h3>
<p>The main entrypoint is through <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a>, which takes a
callable (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>, function, or method) and sample inputs, and
captures the computation graph into an <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.export.ExportedProgram</span></code></a>. An
example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>

<span class="c1"># Simple module for demonstration</span>
<span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">constant</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

<span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),)</span>
<span class="n">example_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;constant&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)}</span>

<span class="n">exported_program</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">ExportedProgram</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span>
    <span class="n">M</span><span class="p">(),</span> <span class="n">args</span><span class="o">=</span><span class="n">example_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">example_kwargs</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_program</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportedProgram</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">GraphModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">arg1_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span> <span class="n">arg2_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">arg3_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]):</span>

            <span class="c1"># code: a = self.conv(x)</span>
            <span class="n">convolution</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">convolution</span><span class="o">.</span><span class="n">default</span><span class="p">(</span>
                <span class="n">arg2_1</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">,</span> <span class="n">arg1_1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">1</span>
            <span class="p">);</span>

            <span class="c1"># code: a.add_(constant)</span>
            <span class="n">add</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">convolution</span><span class="p">,</span> <span class="n">arg3_1</span><span class="p">);</span>

            <span class="c1"># code: return self.maxpool(self.relu(a))</span>
            <span class="n">relu</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">add</span><span class="p">);</span>
            <span class="n">max_pool2d_with_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="o">.</span><span class="n">default</span><span class="p">(</span>
                <span class="n">relu</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
            <span class="p">);</span>
            <span class="n">getitem</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">85</span><span class="p">,</span> <span class="mi">85</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_pool2d_with_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">getitem</span><span class="p">,)</span>

    <span class="n">Graph</span> <span class="n">signature</span><span class="p">:</span> <span class="n">ExportGraphSignature</span><span class="p">(</span>
        <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;L__self___conv.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;L__self___conv.bias&#39;</span><span class="p">],</span>
        <span class="n">buffers</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">user_inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;arg2_1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3_1&#39;</span><span class="p">],</span>
        <span class="n">user_outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;getitem&#39;</span><span class="p">],</span>
        <span class="n">inputs_to_parameters</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;arg0_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___conv.weight&#39;</span><span class="p">,</span>
            <span class="s1">&#39;arg1_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___conv.bias&#39;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">inputs_to_buffers</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">buffers_to_mutate</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">backward_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">assertion_dep_token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">Range</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">{}</span>
    <span class="n">Equality</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">[]</span>
</pre></div>
</div>
<p>Inspecting the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, we can note the following:</p>
<ul class="simple">
<li><p>The <a class="reference internal" href="fx.html#torch.fx.Graph" title="torch.fx.Graph"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.fx.Graph</span></code></a> contains the computation graph of the original
program, along with records of the original code for easy debugging.</p></li>
<li><p>The graph contains only <code class="docutils literal notranslate"><span class="pre">torch.ops.aten</span></code> operators found in the
<a class="reference internal" href="torch.compiler_ir.html#torch-compiler-ir"><span class="std std-ref">Core ATen IR</span></a> opset and custom operators, and is
fully functional, without any inplace operators such as <code class="docutils literal notranslate"><span class="pre">torch.add_</span></code>.</p></li>
<li><p>The parameters (weight and bias to conv) are lifted as inputs to the graph,
resulting in no <code class="docutils literal notranslate"><span class="pre">get_attr</span></code> nodes in the graph, which previously existed in
the result of <a class="reference internal" href="fx.html#torch.fx.symbolic_trace" title="torch.fx.symbolic_trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fx.symbolic_trace()</span></code></a>.</p></li>
<li><p>The <a class="reference internal" href="#torch.export.ExportGraphSignature" title="torch.export.ExportGraphSignature"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.export.ExportGraphSignature</span></code></a> models the input and output
signature, along with specifying which inputs are parameters.</p></li>
<li><p>The resulting shape and dtype of tensors produced by each node in the graph is
noted. For example, the <code class="docutils literal notranslate"><span class="pre">convolution</span></code> node will result in a tensor of dtype
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> and shape (1, 16, 256, 256).</p></li>
</ul>
</div>
<div class="section" id="expressing-dynamism">
<h3>Expressing Dynamism<a class="headerlink" href="#expressing-dynamism" title="Permalink to this heading">¶</a></h3>
<p>By default <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will trace the program assuming all input shapes are
<strong>static</strong>, and specializing the exported program to those dimensions. However,
some dimensions, such as a batch dimension, can be dynamic and vary from run to
run. Such dimensions must be marked dynamic using the
<a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.dynamic_dim()</span></code></a> API, and passed into
<a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.export()</span></code></a> through the <code class="docutils literal notranslate"><span class="pre">constraints</span></code> argument. An example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span><span class="p">,</span> <span class="n">dynamic_dim</span>

<span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">branch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch2</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">out2</span><span class="p">)</span>

<span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># First dimension of each input is a dynamic batch size</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">example_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">example_args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span>
    <span class="c1"># The dynamic batch size between the inputs are equal</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">example_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">example_args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">exported_program</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">ExportedProgram</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span>
  <span class="n">M</span><span class="p">(),</span> <span class="n">args</span><span class="o">=</span><span class="n">example_args</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_program</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportedProgram</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">GraphModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">arg1_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span> <span class="n">arg2_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">arg3_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">64</span><span class="p">],</span> <span class="n">arg4_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span> <span class="n">arg5_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">arg6_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">128</span><span class="p">]):</span>

            <span class="c1"># code: out1 = self.branch1(x1)</span>
            <span class="n">permute</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">permute</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg0_1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]);</span>
            <span class="n">addmm</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg1_1</span><span class="p">,</span> <span class="n">arg5_1</span><span class="p">,</span> <span class="n">permute</span><span class="p">);</span>
            <span class="n">relu</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">addmm</span><span class="p">);</span>

            <span class="c1"># code: out2 = self.branch2(x2)</span>
            <span class="n">permute_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">permute</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg2_1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]);</span>
            <span class="n">addmm_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">addmm</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">arg3_1</span><span class="p">,</span> <span class="n">arg6_1</span><span class="p">,</span> <span class="n">permute_1</span><span class="p">);</span>
            <span class="n">relu_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">addmm_1</span><span class="p">);</span>  <span class="n">addmm_1</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># code: return (out1 + self.buffer, out2)</span>
            <span class="n">add</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="n">s0</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="n">arg4_1</span><span class="p">);</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">relu_1</span><span class="p">)</span>

    <span class="n">Graph</span> <span class="n">signature</span><span class="p">:</span> <span class="n">ExportGraphSignature</span><span class="p">(</span>
        <span class="n">parameters</span><span class="o">=</span><span class="p">[</span>
            <span class="s1">&#39;branch1.0.weight&#39;</span><span class="p">,</span>
            <span class="s1">&#39;branch1.0.bias&#39;</span><span class="p">,</span>
            <span class="s1">&#39;branch2.0.weight&#39;</span><span class="p">,</span>
            <span class="s1">&#39;branch2.0.bias&#39;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">buffers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;L__self___buffer&#39;</span><span class="p">],</span>
        <span class="n">user_inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;arg5_1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg6_1&#39;</span><span class="p">],</span>
        <span class="n">user_outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="s1">&#39;relu_1&#39;</span><span class="p">],</span>
        <span class="n">inputs_to_parameters</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;arg0_1&#39;</span><span class="p">:</span> <span class="s1">&#39;branch1.0.weight&#39;</span><span class="p">,</span>
            <span class="s1">&#39;arg1_1&#39;</span><span class="p">:</span> <span class="s1">&#39;branch1.0.bias&#39;</span><span class="p">,</span>
            <span class="s1">&#39;arg2_1&#39;</span><span class="p">:</span> <span class="s1">&#39;branch2.0.weight&#39;</span><span class="p">,</span>
            <span class="s1">&#39;arg3_1&#39;</span><span class="p">:</span> <span class="s1">&#39;branch2.0.bias&#39;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">inputs_to_buffers</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arg4_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___buffer&#39;</span><span class="p">},</span>
        <span class="n">buffers_to_mutate</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">backward_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">assertion_dep_token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">Range</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">{</span><span class="n">s0</span><span class="p">:</span> <span class="n">RangeConstraint</span><span class="p">(</span><span class="n">min_val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">9223372036854775806</span><span class="p">)}</span>
    <span class="n">Equality</span> <span class="n">constraints</span><span class="p">:</span> <span class="p">[(</span><span class="n">InputDim</span><span class="p">(</span><span class="n">input_name</span><span class="o">=</span><span class="s1">&#39;arg5_1&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">InputDim</span><span class="p">(</span><span class="n">input_name</span><span class="o">=</span><span class="s1">&#39;arg6_1&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))]</span>
</pre></div>
</div>
<p>Some additional things to note:</p>
<ul class="simple">
<li><p>Through the <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.dynamic_dim()</span></code></a> API, we specified the first
dimension of each input to be dynamic. Looking at the inputs <code class="docutils literal notranslate"><span class="pre">arg5_1</span></code> and
<code class="docutils literal notranslate"><span class="pre">arg6_1</span></code>, they have a symbolic shape of (s0, 64) and (s0, 128), instead of
the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs.
<code class="docutils literal notranslate"><span class="pre">s0</span></code> is a symbol representing that this dimension can be a range
of values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exported_program.range_constraints</span></code> describes the ranges of each symbol
appearing in the graph. In this case, we see that <code class="docutils literal notranslate"><span class="pre">s0</span></code> has the range
[2, inf]. For technical reasons that are difficult to explain here, they are
assumed to be not 0 or 1. This is not a bug, and does not necessarily mean
that the exported program will not work for dimensions 0 or 1. See
<a class="reference external" href="https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk">The 0/1 Specialization Problem</a>
for an in-depth discussion of this topic.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exported_program.equality_constraints</span></code> describes which dimensions are
required to be equal. Since we specified in the constraints that the first
dimension of each argument is equivalent,
(<code class="docutils literal notranslate"><span class="pre">dynamic_dim(example_args[0],</span> <span class="pre">0)</span> <span class="pre">==</span> <span class="pre">dynamic_dim(example_args[1],</span> <span class="pre">0)</span></code>),
we see in the equality constraints the tuple specifying that <code class="docutils literal notranslate"><span class="pre">arg5_1</span></code>
dimension 0 and <code class="docutils literal notranslate"><span class="pre">arg6_1</span></code> dimension 0 are equal.</p></li>
</ul>
</div>
<div class="section" id="serialization">
<h3>Serialization<a class="headerlink" href="#serialization" title="Permalink to this heading">¶</a></h3>
<p>To save the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, users can use the <a class="reference internal" href="#torch.export.save" title="torch.export.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.save()</span></code></a> and
<a class="reference internal" href="#torch.export.load" title="torch.export.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.load()</span></code></a> APIs. A convention is to save the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>
using a <code class="docutils literal notranslate"><span class="pre">.pt2</span></code> file extension.</p>
<p>An example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">io</span>

<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">10</span>

<span class="n">exported_program</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">MyModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">exported_program</span><span class="p">,</span> <span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">)</span>
<span class="n">saved_exported_program</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="specialization">
<h3>Specialization<a class="headerlink" href="#specialization" title="Permalink to this heading">¶</a></h3>
<div class="section" id="input-shapes">
<h4>Input shapes<a class="headerlink" href="#input-shapes" title="Permalink to this heading">¶</a></h4>
<p>As mentioned before, by default, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will trace the program
specializing on the input tensors’ shapes, unless a dimension is specified as
dynamic via the <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.dynamic_dim()</span></code></a> API. This means that if there
exists shape-dependent control flow, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will specialize on the
branch that is being taken with the given sample inputs. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>

<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>

<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">),)</span>
<span class="n">exported_program</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_program</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportedProgram</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">GraphModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
            <span class="n">add</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arg0_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">add</span><span class="p">,)</span>
</pre></div>
</div>
<p>The conditional of (<code class="docutils literal notranslate"><span class="pre">x.shape[0]</span> <span class="pre">&gt;</span> <span class="pre">5</span></code>) does not appear in the
<code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> because the example inputs have the static
shape of (10, 2). Since <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> specializes on the inputs’ static
shapes, the else branch (<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">-</span> <span class="pre">1</span></code>) will never be reached. To preserve the dynamic
branching behavior based on the shape of a tensor in the traced graph,
<a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.dynamic_dim()</span></code></a> will need to be used to specify the dimension
of the input tensor (<code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code>) to be dynamic, and the source code will
need to be <a class="reference internal" href="#data-shape-dependent-control-flow"><span class="std std-ref">rewritten</span></a>.</p>
</div>
<div class="section" id="non-tensor-inputs">
<h4>Non-tensor inputs<a class="headerlink" href="#non-tensor-inputs" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> also specializes the traced graph based on the values of inputs
that are not <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, such as <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">bool</span></code>, and <code class="docutils literal notranslate"><span class="pre">str</span></code>.
However, we will likely change this in the near future to not specialize on
inputs of primitive types.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>

<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">const</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">times</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">const</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">exported_program</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_program</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportedProgram</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">GraphModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg0_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">arg1_1</span><span class="p">,</span> <span class="n">arg2_1</span><span class="p">):</span>
            <span class="n">add</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arg0_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
            <span class="n">add_1</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
            <span class="n">add_2</span><span class="p">:</span> <span class="n">f32</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">add_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">add_2</span><span class="p">,)</span>
</pre></div>
</div>
<p>Because integers are specialized, the <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.Tensor</span></code> operations
are all computed with the inlined constant <code class="docutils literal notranslate"><span class="pre">1</span></code>, rather than <code class="docutils literal notranslate"><span class="pre">arg1_1</span></code>.
Additionally, the <code class="docutils literal notranslate"><span class="pre">times</span></code> iterator used in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is also “inlined”
in the graph through the 3 repeated <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.Tensor</span></code> calls, and the
input <code class="docutils literal notranslate"><span class="pre">arg2_1</span></code> is never used.</p>
</div>
</div>
</div>
<div class="section" id="limitations-of-torch-export">
<h2>Limitations of torch.export<a class="headerlink" href="#limitations-of-torch-export" title="Permalink to this heading">¶</a></h2>
<div class="section" id="graph-breaks">
<h3>Graph Breaks<a class="headerlink" href="#graph-breaks" title="Permalink to this heading">¶</a></h3>
<p>As <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is a one-shot process for capturing a computation graph from
a PyTorch program, it might ultimately run into untraceable parts of programs as
it is nearly impossible to support tracing all PyTorch and Python features. In
the case of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, an unsupported operation will cause a “graph
break” and the unsupported operation will be run with default Python evaluation.
In contrast, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will require users to provide additional
information or rewrite parts of their code to make it traceable. As the
tracing is based on TorchDynamo, which evaluates at the Python
bytecode level, there will be significantly fewer rewrites required compared to
previous tracing frameworks.</p>
<p>When a graph break is encountered, <a class="reference internal" href="generated/exportdb/index.html#torch-export-db"><span class="std std-ref">ExportDB</span></a> is a great
resource for learning about the kinds of programs that are supported and
unsupported, along with ways to rewrite programs to make them traceable.</p>
</div>
<div class="section" id="data-shape-dependent-control-flow">
<span id="id2"></span><h3>Data/Shape-Dependent Control Flow<a class="headerlink" href="#data-shape-dependent-control-flow" title="Permalink to this heading">¶</a></h3>
<p>Graph breaks can also be encountered on data-dependent control flow (<code class="docutils literal notranslate"><span class="pre">if</span>
<span class="pre">x.shape[0]</span> <span class="pre">&gt;</span> <span class="pre">2</span></code>) when shapes are not being specialized, as a tracing compiler cannot
possibly deal with without generating code for a combinatorially exploding
number of paths. In such cases, users will need to rewrite their code using
special control flow operators (coming soon!).</p>
</div>
<div class="section" id="data-dependent-accesses">
<h3>Data-Dependent Accesses<a class="headerlink" href="#data-dependent-accesses" title="Permalink to this heading">¶</a></h3>
<p>Data dependent behavior such as using the value inside of a tensor to construct
another tensor, or using the value of a tensor to slice into another tensor, is
also something the tracer cannot fully determine. Users will need to rewrite
their code using the inline constraint APIs
<a class="reference internal" href="#torch.export.constrain_as_size" title="torch.export.constrain_as_size"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.constrain_as_size()</span></code></a> and
<a class="reference internal" href="#torch.export.constrain_as_value" title="torch.export.constrain_as_value"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.constrain_as_value()</span></code></a>.</p>
</div>
<div class="section" id="missing-meta-kernels-for-operators">
<h3>Missing Meta Kernels for Operators<a class="headerlink" href="#missing-meta-kernels-for-operators" title="Permalink to this heading">¶</a></h3>
<p>When tracing, a META implementation (or “meta kernel”) is required for all
operators. This is used to reason about the input/output shapes for this
operator.</p>
<p>Note that the official API for registering custom meta kernels for custom ops is
currently undergoing development. While the final API is being refined, you can
refer to the documentation <a class="reference external" href="https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0">here</a>.</p>
<p>In the unfortunate case where your model uses an ATen operator that is does not
have a meta kernel implementation yet, please file an issue.</p>
</div>
</div>
<div class="section" id="read-more">
<h2>Read More<a class="headerlink" href="#read-more" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Additional Links for Export Users</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/exportdb/index.html">ExportDB</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Deep Dive for PyTorch Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler_deepdive.html">TorchDynamo Deep Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
</ul>
</div>
</div>
<div class="section" id="module-torch.export">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#module-torch.export" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.export.export">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#export"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.export" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> takes an arbitrary Python callable (an nn.Module, a function or
a method) and produces a traced graph representing only the Tensor
computation of the function in an Ahead-of-Time (AOT) fashion, which can
subsequently be executed with different outputs or serialized.  The traced
graph (1) produces a normalized operator set consisting only of functional
<a class="reference external" href="https://pytorch.org/docs/stable/ir.html">Core ATen Operator Set</a>
and user specified custom operators, (2) has eliminated all Python control
flow and data structures (except for certain
conditions), and (3) has the set of shape constraints needed to show that
this normalization and control flow elimination is sound for a future
input.</p>
<p><strong>Soundness Guarantee</strong></p>
<p>While tracing, <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> takes note of shape-related assumptions
made by the user program and the underlying PyTorch operator kernels.
The output <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a> is considered valid only when these
assumptions hold true.</p>
<p>There are 2 types of assumptions made during tracing</p>
<ul class="simple">
<li><p>Shapes (not values) of input tensors.</p></li>
<li><p>Ranges (lower and upper bound) of values extracted from intermediate tensors via <code class="docutils literal notranslate"><span class="pre">.item()</span></code> or direct indexing.</p></li>
</ul>
<p>All assumptions must be validated at graph capture time for <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>
to succeed. Specifically:</p>
<ul class="simple">
<li><p>Assumptions on static shapes of input tensors are automatically validated without additional effort.</p></li>
<li><p>Assumptions on dynamic shape of input tensors require explicit <cite>Input Constraint</cite>
constructed with <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a> APIs</p></li>
<li><p>Assumptions on range of intermediate values require explicit <cite>Inline Constraint</cite>,
constructed use <a class="reference internal" href="#torch.export.constrain_as_size" title="torch.export.constrain_as_size"><code class="xref py py-func docutils literal notranslate"><span class="pre">constrain_as_size()</span></code></a> and <code class="xref py py-func docutils literal notranslate"><span class="pre">constraint_as_value()</span></code> APIs.</p></li>
</ul>
<p>If any assumption can not be validated, a fatal error will be raised. When that happens,
the error message will include suggested code needed to construct necessary
constraints to validate the assumptions, for example <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> would suggest
following code for input constraints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">specify_constraints</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="c1"># x:</span>
        <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="p">]</span>
</pre></div>
</div>
<p>This example means the program requires the dim 0 of input <code class="docutils literal notranslate"><span class="pre">x</span></code> to be less
than or equal to 5 to be valid. You can inspect the constraints needed and
then copy this exact function into your code to generated needed
constraints to be passed into <code class="docutils literal notranslate"><span class="pre">constraints</span></code> argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)"><em>Callable</em></a>) – The callable to trace.</p></li>
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)"><em>Tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)"><em>Any</em></a><em>, </em><em>...</em><em>]</em>) – Example positional inputs.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)"><em>Optional</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)"><em>Dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)"><em>Any</em></a><em>]</em><em>]</em>) – Optional example keyword inputs.</p></li>
<li><p><strong>constraints</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)"><em>Optional</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.12)"><em>List</em></a><em>[</em><a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><em>Constraint</em></a><em>]</em><em>]</em>) – An optional list of constraints on the dynamic arguments
that specify their possible range of shapes. By default, shapes of
input torch.Tensors are assumed to be static. If an input torch.Tensor
is expected to have dynamic shapes, please use <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a>
to define <a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects that specify the dynamics and the possible
range of shapes. See <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a> docstring for examples on
how to use it.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a> containing the traced callable.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><em>ExportedProgram</em></a></p>
</dd>
</dl>
<p><strong>Acceptable input/output types</strong></p>
<p>Acceptable types of inputs (for <code class="docutils literal notranslate"><span class="pre">args</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>) and outputs include:</p>
<ul class="simple">
<li><p>Primitive types, i.e. <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">bool</span></code> and <code class="docutils literal notranslate"><span class="pre">str</span></code>.</p></li>
<li><p>(Nested) Data structures comprising of <code class="docutils literal notranslate"><span class="pre">dict</span></code>, <code class="docutils literal notranslate"><span class="pre">list</span></code>, <code class="docutils literal notranslate"><span class="pre">tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">namedtuple</span></code> and
<code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code> containing all above types.</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.export.dynamic_dim">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">dynamic_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#dynamic_dim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.dynamic_dim" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a> constructs a <a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> object that describes the dynamism of
a dimension <code class="docutils literal notranslate"><span class="pre">index</span></code> of tensor <code class="docutils literal notranslate"><span class="pre">t</span></code>. <a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects should be passed to
<code class="docutils literal notranslate"><span class="pre">constraints</span></code> argument of <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – Example input tensor that have dynamic dimension size(s)</p></li>
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – Index of dynamic dimension</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> object that describes shape dynamism. It can be passed to <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> so
that <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> does not assume static size of specified tensor, i.e. keeping it dynamic
as a symbolic size rather than specializing according to size of example tracing input.</p>
</dd>
</dl>
<p>Specifically <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a> can be used to express following types of dynamism.</p>
<ul>
<li><p>Size of a dimension is dynamic and unbounded:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># First dimension of t0 can be dynamic size rather than always being static size 2</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Size of a dimension is dynamic with a lower bound:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># First dimension of t0 can be dynamic size with a lower bound of 5 (inclusive)</span>
<span class="c1"># Second dimension of t1 can be dynamic size with a lower bound of 2 (exclusive)</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Size of a dimension is dynamic with an upper bound:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># First dimension of t0 can be dynamic size with a upper bound of 16 (inclusive)</span>
<span class="c1"># Second dimension of t1 can be dynamic size with a upper bound of 8 (exclusive)</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># Sizes of second dimension of t0 and first dimension are always equal</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">dynamic_dim</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Mix and match all types above as long as they do not express conflicting requirements</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.export.constrain_as_size">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">constrain_as_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#constrain_as_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.constrain_as_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Hint <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> about the constraint of an intermediate scalar value that
represents shape of a tensor so that subsequent tensor constructors can be
traced correctly because many operators need to make assumption about range
of sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbol</strong> – Intermediate scalar value (int-only now) to apply range constraint on.</p></li>
<li><p><strong>min</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em>) – Minimum possible value of given symbol (inclusive)</p></li>
<li><p><strong>max</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em>) – Maximum possible value of given symbol (inclusive)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>For example, following program can not be traced soundly wihout using
<a class="reference internal" href="#torch.export.constrain_as_size" title="torch.export.constrain_as_size"><code class="xref py py-func docutils literal notranslate"><span class="pre">constrain_as_size()</span></code></a> to give <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> a hint about shape ranges:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> would give following error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">exc</span><span class="o">.</span><span class="n">Unsupported</span><span class="p">:</span> <span class="n">guard</span> <span class="n">on</span> <span class="n">data</span><span class="o">-</span><span class="n">dependent</span> <span class="n">symbolic</span> <span class="nb">int</span><span class="o">/</span><span class="nb">float</span>
</pre></div>
</div>
<p>Assuming the actual range of <code class="docutils literal notranslate"><span class="pre">d</span></code> can be between [3, 10], you can add a call to
<a class="reference internal" href="#torch.export.constrain_as_size" title="torch.export.constrain_as_size"><code class="xref py py-func docutils literal notranslate"><span class="pre">constrain_as_size()</span></code></a> in the source code like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">constrain_as_size</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>With the additional hint, <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> would be able to trace the program correctly by taking
the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch, resulting in following graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">():</span>
    <span class="o">%</span><span class="n">arg0_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg0_1</span><span class="p">]</span>

    <span class="c1"># d = x.max().item()</span>
    <span class="o">%</span><span class="n">max_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg0_1</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_local_scalar_dense</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_local_scalar_dense</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">max_1</span><span class="p">,))</span>

    <span class="c1"># Asserting 3 &lt;= d &lt;= 10</span>
    <span class="o">%</span><span class="n">ge</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">ge</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="o">%</span><span class="n">scalar_tensor</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">ge</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_assert_async</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_assert_async</span><span class="o">.</span><span class="n">msg</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">scalar_tensor</span><span class="p">,</span> <span class="n">_local_scalar_dense</span> <span class="ow">is</span> <span class="n">outside</span> <span class="n">of</span> <span class="n">inline</span> <span class="n">constraint</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="p">))</span>
    <span class="o">%</span><span class="n">le</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">le</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="o">%</span><span class="n">scalar_tensor_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">le</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_assert_async_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_assert_async</span><span class="o">.</span><span class="n">msg</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">scalar_tensor_1</span><span class="p">,</span> <span class="n">_local_scalar_dense</span> <span class="ow">is</span> <span class="n">outside</span> <span class="n">of</span> <span class="n">inline</span> <span class="n">constraint</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="p">))</span>
    <span class="o">%</span><span class="n">sym_constrain_range_for_size</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_constrain_range_for_size</span><span class="o">.</span><span class="n">default</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="nb">min</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>

    <span class="c1"># Constructing new tensor with d</span>
    <span class="o">%</span><span class="n">full</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">full</span><span class="o">.</span><span class="n">default</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">([</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">layout</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">cpu</span><span class="p">,</span> <span class="n">pin_memory</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

    <span class="o">......</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1,
these will SILENTLY report false and be bypassed</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.export.constrain_as_value">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">constrain_as_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#constrain_as_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.constrain_as_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Hint <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> about the constraint of an intermediate scalar value so that subsequent
branching behaviors that check on the range of aforementioned scalar value can be
soundly traced.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>(Note that if the intermediate scalar value will be used like a size, including
being passed as size arg to a tensor factory or view, call <a class="reference internal" href="#torch.export.constrain_as_size" title="torch.export.constrain_as_size"><code class="xref py py-func docutils literal notranslate"><span class="pre">constrain_as_size()</span></code></a>
instead.)</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbol</strong> – Intermediate scalar value (int-only now) to apply range constraint on.</p></li>
<li><p><strong>min</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em>) – Minimum possible value of given symbol (inclusive)</p></li>
<li><p><strong>max</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em>) – Maximum possible value of given symbol (inclusive)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>For example, following program can not be traced soundly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">v</span></code> is a data-dependent value, which is assumed to have a range of (-inf, inf).
<a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> a hint about which branch to take would not be able to determine
if the traced branching decision is correct or not. Thus <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>
would give following error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">exc</span><span class="o">.</span><span class="n">UserError</span><span class="p">:</span> <span class="n">Consider</span> <span class="n">annotating</span> <span class="n">your</span> <span class="n">code</span> <span class="n">using</span>
<span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">constrain_as_size</span><span class="p">()</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="p">()</span><span class="o">.</span><span class="n">constrain_as_value</span><span class="p">()</span> <span class="n">APIs</span><span class="o">.</span>
<span class="n">It</span> <span class="n">appears</span> <span class="n">that</span> <span class="n">you</span><span class="s1">&#39;re trying to get a value out of symbolic int/float whose value</span>
<span class="ow">is</span> <span class="n">data</span><span class="o">-</span><span class="n">dependent</span> <span class="p">(</span><span class="ow">and</span> <span class="n">thus</span> <span class="n">we</span> <span class="n">do</span> <span class="ow">not</span> <span class="n">know</span> <span class="n">the</span> <span class="n">true</span> <span class="n">value</span><span class="o">.</span><span class="p">)</span>  <span class="n">The</span> <span class="n">expression</span> <span class="n">we</span> <span class="n">were</span>
<span class="n">trying</span> <span class="n">to</span> <span class="n">evaluate</span> <span class="ow">is</span> <span class="n">f0</span> <span class="o">&gt;</span> <span class="mi">1024</span> <span class="p">(</span><span class="n">unhinted</span><span class="p">:</span> <span class="n">f0</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>Assuming the actual range of <code class="docutils literal notranslate"><span class="pre">v</span></code> can be between [10, 200], you can add a call to
<a class="reference internal" href="#torch.export.constrain_as_value" title="torch.export.constrain_as_value"><code class="xref py py-func docutils literal notranslate"><span class="pre">constrain_as_value()</span></code></a> in the source code like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Give export() a hint</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">constrain_as_value</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
</pre></div>
</div>
<p>With the additional hint, <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> would be able to trace the program correctly by taking
the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch, resulting in following graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">():</span>
    <span class="o">%</span><span class="n">arg0_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg0_1</span><span class="p">]</span>

    <span class="c1"># v = x.max().item()</span>
    <span class="o">%</span><span class="n">max_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg0_1</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_local_scalar_dense</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_local_scalar_dense</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">max_1</span><span class="p">,))</span>

    <span class="c1"># Asserting 10 &lt;= v &lt;= 200</span>
    <span class="o">%</span><span class="n">ge</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">ge</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="o">%</span><span class="n">scalar_tensor</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">ge</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_assert_async</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_assert_async</span><span class="o">.</span><span class="n">msg</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">scalar_tensor</span><span class="p">,</span> <span class="n">_local_scalar_dense</span> <span class="ow">is</span> <span class="n">outside</span> <span class="n">of</span> <span class="n">inline</span> <span class="n">constraint</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span><span class="o">.</span><span class="p">))</span>
    <span class="o">%</span><span class="n">le</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">le</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
    <span class="o">%</span><span class="n">scalar_tensor_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="o">.</span><span class="n">default</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">le</span><span class="p">,))</span>
    <span class="o">%</span><span class="n">_assert_async_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">_assert_async</span><span class="o">.</span><span class="n">msg</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">scalar_tensor_1</span><span class="p">,</span> <span class="n">_local_scalar_dense</span> <span class="ow">is</span> <span class="n">outside</span> <span class="n">of</span> <span class="n">inline</span> <span class="n">constraint</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span><span class="o">.</span><span class="p">))</span>
    <span class="o">%</span><span class="n">sym_constrain_range</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">sym_constrain_range</span><span class="o">.</span><span class="n">default</span><span class="p">](</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">_local_scalar_dense</span><span class="p">,),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="nb">min</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="mi">200</span><span class="p">})</span>

    <span class="c1"># Always taking `else` branch to multiply elements `x` by 2 due to hints above</span>
    <span class="o">%</span><span class="n">mul</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg0_1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul</span><span class="p">,)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.export.save">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ep</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_files</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opset_version</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.save" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Under active development, saved files may not be usable in newer versions
of PyTorch.</p>
</div>
<p>Saves an <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a> to a file-like object. It can then be
loaded using the Python API <a class="reference internal" href="#torch.export.load" title="torch.export.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.load</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ep</strong> (<a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><em>ExportedProgram</em></a>) – The exported program to save.</p></li>
<li><p><strong>f</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)"><em>pathlib.Path</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/io.html#io.BytesIO" title="(in Python v3.12)"><em>io.BytesIO</em></a>) – A file-like object (has to
implement write and flush) or a string containing a file name.</p></li>
<li><p><strong>extra_files</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em>) – Map from filename to contents
which will be stored as part of f.</p></li>
<li><p><strong>opset_version</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em><em>]</em>) – A map of opset names
to the version of this opset</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">io</span>

<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">10</span>

<span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">MyModule</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># Save to file</span>
<span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">)</span>

<span class="c1"># Save to io.BytesIO buffer</span>
<span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span>

<span class="c1"># Save with extra files</span>
<span class="n">extra_files</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;foo.txt&#39;</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;bar&#39;</span><span class="p">}</span>
<span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">,</span> <span class="n">extra_files</span><span class="o">=</span><span class="n">extra_files</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.export.load">
<span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_files</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_opset_version</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.load" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Under active development, saved files may not be usable in newer versions
of PyTorch.</p>
</div>
<p>Loads an <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a> previously saved with
<a class="reference internal" href="#torch.export.save" title="torch.export.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export.save</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ep</strong> (<a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><em>ExportedProgram</em></a>) – The exported program to save.</p></li>
<li><p><strong>f</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)"><em>pathlib.Path</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/io.html#io.BytesIO" title="(in Python v3.12)"><em>io.BytesIO</em></a>) – A file-like object (has to
implement write and flush) or a string containing a file name.</p></li>
<li><p><strong>extra_files</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em>) – The extra filenames given in
this map would be loaded and their content would be stored in the
provided map.</p></li>
<li><p><strong>expected_opset_version</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>]</em><em>]</em>) – A map of opset names
to expected opset versions</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An <a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a> object</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.export.ExportedProgram" title="torch.export.ExportedProgram"><em>ExportedProgram</em></a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">io</span>

<span class="c1"># Load ExportedProgram from file</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">)</span>

<span class="c1"># Load ExportedProgram from io.BytesIO object</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="n">buffer</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>

<span class="c1"># Load with extra files.</span>
<span class="n">extra_files</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;foo.txt&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">}</span>  <span class="c1"># values will be replaced with data</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;exported_program.pt2&#39;</span><span class="p">,</span> <span class="n">extra_files</span><span class="o">=</span><span class="n">extra_files</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">extra_files</span><span class="p">[</span><span class="s1">&#39;foo.txt&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.Constraint">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">Constraint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#Constraint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.Constraint" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not construct <a class="reference internal" href="#torch.export.Constraint" title="torch.export.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> directly, use <a class="reference internal" href="#torch.export.dynamic_dim" title="torch.export.dynamic_dim"><code class="xref py py-func docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a> instead.</p>
</div>
<p>This represents constraints on input tensor dimensions, e.g., requiring
them to be fully polymorphic or within some range.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ExportedProgram">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ExportedProgram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">graph_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">call_spec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">range_constraints</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equality_constraints</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_call_graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ExportedProgram"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ExportedProgram" title="Permalink to this definition">¶</a></dt>
<dd><p>Package of a program from <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>. It contains
an <a class="reference internal" href="fx.html#torch.fx.Graph" title="torch.fx.Graph"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.fx.Graph</span></code></a> that represents Tensor computation, a state_dict containing
tensor values of all lifted parameters and buffers, and various metadata.</p>
<p>You can call an ExportedProgram like the original callable traced by
<a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> with the same calling convention.</p>
<p>To perform transformations on the graph, use <code class="docutils literal notranslate"><span class="pre">.module</span></code> property to access
an <a class="reference internal" href="fx.html#torch.fx.GraphModule" title="torch.fx.GraphModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.fx.GraphModule</span></code></a>. You can then use
<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#writing-transformations">FX transformation</a>
to rewrite the graph. Afterwards, you can simply use <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>
again to construct a correct ExportedProgram.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.export.ExportedProgram.module">
<span class="sig-name descname"><span class="pre">module</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ExportedProgram.module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ExportedProgram.module" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a self contained GraphModule with all the parameters/buffers inlined.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><p id="torch.nn.Module"/><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.modules.module.Module"><em>Module</em></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ExportBackwardSignature">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ExportBackwardSignature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients_to_parameters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradients_to_user_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ExportBackwardSignature"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ExportBackwardSignature" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ExportGraphSignature">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ExportGraphSignature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_to_parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_to_buffers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffers_to_mutate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_signature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assertion_dep_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ExportGraphSignature"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ExportGraphSignature" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#torch.export.ExportGraphSignature" title="torch.export.ExportGraphSignature"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExportGraphSignature</span></code></a> models the input/output signature of Export Graph,
which is a fx.Graph with stronger invariants gurantees.</p>
<p>Export Graph is functional and does not access “states” like parameters
or buffers within the graph via <code class="docutils literal notranslate"><span class="pre">getattr</span></code> nodes. Instead, <a class="reference internal" href="#torch.export.export" title="torch.export.export"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>
gurantees that parameters and buffers are lifted out of the graph as inputs.
Similarly, any mutations to buffers are not included in the graph either,
instead the updated values of mutated buffers are modeled as additional outputs
of Export Graph.</p>
<p>The ordering of all inputs and outputs are:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Inputs</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">parameters_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">flattened_user_inputs</span><span class="p">]</span>
<span class="n">Outputs</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">mutated_inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">flattened_user_outputs</span><span class="p">]</span>
</pre></div>
</div>
<p>e.g. If following module is exported:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CustomModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Define a parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">my_parameter</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>

        <span class="c1"># Define two buffers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;my_buffer1&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;my_buffer2&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.0</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="c1"># Use the parameter, buffers, and both inputs in the forward method</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">my_parameter</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">my_buffer1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">my_buffer2</span>

        <span class="c1"># Mutate one of the buffers (e.g., increment it by 1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">my_buffer2</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># In-place addition</span>

        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>Resulting Graph would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">():</span>
    <span class="o">%</span><span class="n">arg0_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg0_1</span><span class="p">]</span>
    <span class="o">%</span><span class="n">arg1_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg1_1</span><span class="p">]</span>
    <span class="o">%</span><span class="n">arg2_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg2_1</span><span class="p">]</span>
    <span class="o">%</span><span class="n">arg3_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg3_1</span><span class="p">]</span>
    <span class="o">%</span><span class="n">arg4_1</span> <span class="o">:=</span> <span class="n">placeholder</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">arg4_1</span><span class="p">]</span>
    <span class="o">%</span><span class="n">add_tensor</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg3_1</span><span class="p">,</span> <span class="o">%</span><span class="n">arg0_1</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">mul_tensor</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">add_tensor</span><span class="p">,</span> <span class="o">%</span><span class="n">arg1_1</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">mul_tensor_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">mul</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg4_1</span><span class="p">,</span> <span class="o">%</span><span class="n">arg2_1</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">add_tensor_1</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">mul_tensor</span><span class="p">,</span> <span class="o">%</span><span class="n">mul_tensor_1</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="o">%</span><span class="n">add_tensor_2</span> <span class="o">:=</span> <span class="n">call_function</span><span class="p">[</span><span class="n">target</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="p">](</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="o">%</span><span class="n">arg2_1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{})</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">add_tensor_2</span><span class="p">,</span> <span class="n">add_tensor_1</span><span class="p">)</span>
</pre></div>
</div>
<p>Resulting ExportGraphSignature would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExportGraphSignature</span><span class="p">(</span>
    <span class="c1"># Indicates that there is one parameter named `my_parameter`</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;L__self___my_parameter&#39;</span><span class="p">],</span>

    <span class="c1"># Indicates that there are two buffers, `my_buffer1` and `my_buffer2`</span>
    <span class="n">buffers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;L__self___my_buffer1&#39;</span><span class="p">,</span> <span class="s1">&#39;L__self___my_buffer2&#39;</span><span class="p">],</span>

    <span class="c1"># Indicates that the nodes `arg3_1` and `arg4_1` in produced graph map to</span>
    <span class="c1"># original user inputs, ie. x1 and x2</span>
    <span class="n">user_inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;arg3_1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg4_1&#39;</span><span class="p">],</span>

    <span class="c1"># Indicates that the node `add_tensor_1` maps to output of original program</span>
    <span class="n">user_outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;add_tensor_1&#39;</span><span class="p">],</span>

    <span class="c1"># Indicates that there is one parameter (self.my_parameter) captured,</span>
    <span class="c1"># its name is now mangled to be `L__self___my_parameter`, which is now</span>
    <span class="c1"># represented by node `arg0_1` in the graph.</span>
    <span class="n">inputs_to_parameters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arg0_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___my_parameter&#39;</span><span class="p">},</span>

    <span class="c1"># Indicates that there are two buffers (self.my_buffer1, self.my_buffer2) captured,</span>
    <span class="c1"># their name are now mangled to be `L__self___my_my_buffer1` and `L__self___my_buffer2`.</span>
    <span class="c1"># They are now represented by nodes `arg1_1` and `arg2_1` in the graph.</span>
    <span class="n">inputs_to_buffers</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arg1_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___my_buffer1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg2_1&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___my_buffer2&#39;</span><span class="p">},</span>

    <span class="c1"># Indicates that one buffer named `L__self___my_buffer2` is mutated during execution,</span>
    <span class="c1"># its new value is output from the graph represented by the node named `add_tensor_2`</span>
    <span class="n">buffers_to_mutate</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;add_tensor_2&#39;</span><span class="p">:</span> <span class="s1">&#39;L__self___my_buffer2&#39;</span><span class="p">},</span>

    <span class="c1"># Backward graph not captured</span>
    <span class="n">backward_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>

    <span class="c1"># Work in progress feature, please ignore now.</span>
    <span class="n">assertion_dep_token</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ArgumentKind">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ArgumentKind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ArgumentKind"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ArgumentKind" title="Permalink to this definition">¶</a></dt>
<dd><p>An enumeration.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ArgumentSpec">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ArgumentSpec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kind</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch.export.ArgumentKind" title="torch.export.ArgumentKind"><span class="pre">torch.export.ArgumentKind</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ArgumentSpec"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ArgumentSpec" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ModuleCallSignature">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ModuleCallSignature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.export.ArgumentSpec" title="torch.export.ArgumentSpec"><span class="pre">torch.export.ArgumentSpec</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.export.ArgumentSpec" title="torch.export.ArgumentSpec"><span class="pre">torch.export.ArgumentSpec</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_spec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils._pytree.TreeSpec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_spec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils._pytree.TreeSpec</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ModuleCallSignature"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ModuleCallSignature" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.export.ModuleCallEntry">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.export.</span></span><span class="sig-name descname"><span class="pre">ModuleCallEntry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fqn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">signature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torch.export.ModuleCallSignature" title="torch.export.ModuleCallSignature"><span class="pre">torch.export.ModuleCallSignature</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">NoneType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/export.html#ModuleCallEntry"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.export.ModuleCallEntry" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torch.compiler_transformations.html" class="btn btn-neutral float-right" title="Writing Graph Transformations on ATen IR" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="backends.html" class="btn btn-neutral" title="torch.backends" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.export</a><ul>
<li><a class="reference internal" href="#overview">Overview</a><ul>
<li><a class="reference internal" href="#existing-frameworks">Existing frameworks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#exporting-a-pytorch-model">Exporting a PyTorch Model</a><ul>
<li><a class="reference internal" href="#an-example">An Example</a></li>
<li><a class="reference internal" href="#expressing-dynamism">Expressing Dynamism</a></li>
<li><a class="reference internal" href="#serialization">Serialization</a></li>
<li><a class="reference internal" href="#specialization">Specialization</a><ul>
<li><a class="reference internal" href="#input-shapes">Input shapes</a></li>
<li><a class="reference internal" href="#non-tensor-inputs">Non-tensor inputs</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#limitations-of-torch-export">Limitations of torch.export</a><ul>
<li><a class="reference internal" href="#graph-breaks">Graph Breaks</a></li>
<li><a class="reference internal" href="#data-shape-dependent-control-flow">Data/Shape-Dependent Control Flow</a></li>
<li><a class="reference internal" href="#data-dependent-accesses">Data-Dependent Accesses</a></li>
<li><a class="reference internal" href="#missing-meta-kernels-for-operators">Missing Meta Kernels for Operators</a></li>
</ul>
</li>
<li><a class="reference internal" href="#read-more">Read More</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#module-torch.export">API Reference</a><ul>
<li><a class="reference internal" href="#torch.export.export"><code class="docutils literal notranslate"><span class="pre">export()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.dynamic_dim"><code class="docutils literal notranslate"><span class="pre">dynamic_dim()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.constrain_as_size"><code class="docutils literal notranslate"><span class="pre">constrain_as_size()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.constrain_as_value"><code class="docutils literal notranslate"><span class="pre">constrain_as_value()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.save"><code class="docutils literal notranslate"><span class="pre">save()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.load"><code class="docutils literal notranslate"><span class="pre">load()</span></code></a></li>
<li><a class="reference internal" href="#torch.export.Constraint"><code class="docutils literal notranslate"><span class="pre">Constraint</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ExportedProgram"><code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code></a><ul>
<li><a class="reference internal" href="#torch.export.ExportedProgram.module"><code class="docutils literal notranslate"><span class="pre">ExportedProgram.module()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.export.ExportBackwardSignature"><code class="docutils literal notranslate"><span class="pre">ExportBackwardSignature</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ExportGraphSignature"><code class="docutils literal notranslate"><span class="pre">ExportGraphSignature</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ArgumentKind"><code class="docutils literal notranslate"><span class="pre">ArgumentKind</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ArgumentSpec"><code class="docutils literal notranslate"><span class="pre">ArgumentSpec</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ModuleCallSignature"><code class="docutils literal notranslate"><span class="pre">ModuleCallSignature</span></code></a></li>
<li><a class="reference internal" href="#torch.export.ModuleCallEntry"><code class="docutils literal notranslate"><span class="pre">ModuleCallEntry</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
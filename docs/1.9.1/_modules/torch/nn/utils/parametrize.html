


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.utils.parametrize &mdash; PyTorch 1.9.1 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.9.1 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.utils.parametrize</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.utils.parametrize</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.container</span> <span class="kn">import</span> <span class="n">ModuleList</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">,</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>


<span class="n">_cache_enabled</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>


<div class="viewcode-block" id="cached"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">cached</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context manager that enables the caching system within parametrizations</span>
<span class="sd">    registered with :func:`register_parametrization`.</span>

<span class="sd">    The value of the parametrized objects is computed and cached the first time</span>
<span class="sd">    they are required when this context manager is active. The cached values are</span>
<span class="sd">    discarded when leaving the context manager.</span>

<span class="sd">    This is useful when using a parametrized parameter more than once in the forward pass.</span>
<span class="sd">    An example of this is when parametrizing the recurrent kernel of an RNN or when</span>
<span class="sd">    sharing weights.</span>

<span class="sd">    The simplest way to activate the cache is by wrapping the forward pass of the neural network</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import torch.nn.utils.parametrize as P</span>
<span class="sd">        ...</span>
<span class="sd">        with P.cached():</span>
<span class="sd">            output = model(inputs)</span>

<span class="sd">    in training and evaluation. One may also wrap the parts of the modules that use</span>
<span class="sd">    several times the parametrized tensors. For example, the loop of an RNN with a</span>
<span class="sd">    parametrized recurrent kernel:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        with P.cached():</span>
<span class="sd">            for x in xs:</span>
<span class="sd">                out_rnn = self.rnn_cell(x, out_rnn)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_cache</span>
    <span class="k">global</span> <span class="n">_cache_enabled</span>
    <span class="n">_cache_enabled</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_cache_enabled</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_cache_enabled</span><span class="p">:</span>
            <span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span></div>


<div class="viewcode-block" id="ParametrizationList"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList">[docs]</a><span class="k">class</span> <span class="nc">ParametrizationList</span><span class="p">(</span><span class="n">ModuleList</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A sequential container that holds and manages the ``original`` parameter or buffer of</span>
<span class="sd">    a parametrized :class:`torch.nn.Module`. It is the type of</span>
<span class="sd">    ``module.parametrizations[tensor_name]`` when ``module[tensor_name]`` has been parametrized</span>
<span class="sd">    with :func:`register_parametrization`.</span>

<span class="sd">    .. note ::</span>
<span class="sd">        This class is used internally by :func:`register_parametrization`. It is documented</span>
<span class="sd">        here for completeness. It should not be instantiated by the user.</span>

<span class="sd">    Args:</span>
<span class="sd">        modules (iterable): an iterable of modules representing the parametrizations</span>
<span class="sd">        original (Parameter or Tensor): parameter or buffer that is parametrized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original</span><span class="p">:</span> <span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">modules</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Module</span><span class="p">],</span> <span class="n">original</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>

<div class="viewcode-block" id="ParametrizationList.set_original_"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.set_original_">[docs]</a>    <span class="k">def</span> <span class="nf">set_original_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This method is called when assigning to a parametrized tensor.</span>

<span class="sd">        It calls the methods ``right_inverse`` (see :func:`register_parametrization`)</span>
<span class="sd">        of the parametrizations in the inverse order that they have been registered.</span>
<span class="sd">        Then, it assigns the result to ``self.original``.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (Tensor): Value to which initialize the module</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: if any of the parametrizations do not implement a ``right_inverse`` method</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/53103</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># type: ignore[call-overload]</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;right_inverse&quot;</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">right_inverse</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;The parametrization &#39;</span><span class="si">{}</span><span class="s2">&#39; does not implement a &#39;right_inverse&#39; method. &quot;</span>
                        <span class="s2">&quot;Assigning to a parametrized tensor is only possible when all the parametrizations &quot;</span>
                        <span class="s2">&quot;implement a &#39;right_inverse&#39; method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;The parametrization may not change the size of the parametrized tensor. &quot;</span>
                <span class="s2">&quot;Size of original tensor: </span><span class="si">{}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Size of parametrized tensor: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<span class="k">def</span> <span class="nf">_inject_new_class</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets up the parametrization mechanism used by parametrizations.</span>

<span class="sd">    This works by substituting the class of the module by a class</span>
<span class="sd">    that extends it to be able to inject a property</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module into which to inject the property</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span>

    <span class="k">def</span> <span class="nf">getstate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Serialization of parametrized modules is only &quot;</span>
            <span class="s2">&quot;supported through state_dict(). See:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;https://pytorch.org/tutorials/beginner/saving_loading_models.html&quot;</span>
            <span class="s2">&quot;#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training&quot;</span>
        <span class="p">)</span>

    <span class="n">param_cls</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span>
        <span class="s2">&quot;Parametrized</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span>
        <span class="p">(</span><span class="bp">cls</span><span class="p">,),</span>
        <span class="p">{</span>
            <span class="s2">&quot;__getstate__&quot;</span><span class="p">:</span> <span class="n">getstate</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="n">param_cls</span>


<span class="k">def</span> <span class="nf">_inject_property</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Injects a property into module[tensor_name].</span>

<span class="sd">    It assumes that the class in the module has already been modified from its</span>
<span class="sd">    original one using _inject_new_class and that the tensor under :attr:`tensor_name`</span>
<span class="sd">    has already been moved out</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module into which to inject the property</span>
<span class="sd">        tensor_name (str): name of the name of the property to create</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We check the precondition.</span>
    <span class="c1"># This should never fire if register_parametrization is correctly implemented</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_parametrized</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">_cache</span>

        <span class="n">parametrization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_cache_enabled</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">),</span> <span class="n">tensor_name</span><span class="p">)</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tensor</span> <span class="o">=</span> <span class="n">parametrization</span><span class="p">()</span>
                <span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
            <span class="k">return</span> <span class="n">tensor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If caching is not active, this function just evaluates the parametrization</span>
            <span class="k">return</span> <span class="n">parametrization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_original</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">set_original_</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">,</span> <span class="nb">property</span><span class="p">(</span><span class="n">get_parametrized</span><span class="p">,</span> <span class="n">set_original</span><span class="p">))</span>


<div class="viewcode-block" id="register_parametrization"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization">[docs]</a><span class="k">def</span> <span class="nf">register_parametrization</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">parametrization</span><span class="p">:</span> <span class="n">Module</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parametrization to a tensor in a module.</span>

<span class="sd">    Assume that ``tensor_name=&quot;weight&quot;`` for simplicity. When accessing ``module.weight``,</span>
<span class="sd">    the module will return the parametrized version ``parametrization(module.weight)``.</span>
<span class="sd">    If the original tensor requires a gradient, the backward pass will differentiate</span>
<span class="sd">    through the :attr:`parametrization`, and the optimizer will update the tensor accordingly.</span>

<span class="sd">    The first time that a module registers a parametrization, this function will add an attribute</span>
<span class="sd">    ``parametrizations`` to the module of type :class:`~ParametrizationList`.</span>

<span class="sd">    The list of parametrizations on a tensor will be accessible under</span>
<span class="sd">    ``module.parametrizations.weight``.</span>

<span class="sd">    The original tensor will be accessible under</span>
<span class="sd">    ``module.parametrizations.weight.original``.</span>

<span class="sd">    Parametrizations may be concatenated by registering several parametrizations</span>
<span class="sd">    on the same attribute.</span>

<span class="sd">    The training mode of the registered parametrizations are updated on registration</span>
<span class="sd">    if necessary to match the training mode of the host module</span>

<span class="sd">    Parametrized parameters and buffers have an inbuilt caching system that can be activated</span>
<span class="sd">    using the context manager :func:`cached`.</span>

<span class="sd">    A :attr:`parametrization` may optionally implement a method with signature</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def right_inverse(self, X: Tensor) -&gt; Tensor</span>

<span class="sd">    If :attr:`parametrization` implements this method, it will be possible to assign</span>
<span class="sd">    to the parametrized tensor. This may be used to initialize the tensor, as shown in the example.</span>

<span class="sd">    In most situations, ``right_inverse`` will be a function such that</span>
<span class="sd">    ``forward(right_inverse(X)) == X`` (see</span>
<span class="sd">    `right inverse &lt;https://en.wikipedia.org/wiki/Inverse_function#Right_inverses&gt;`_).</span>
<span class="sd">    Sometimes, when the parametrization is not surjective, it may be reasonable</span>
<span class="sd">    to relax this, as shown in the example below.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module on which to register the parametrization</span>
<span class="sd">        tensor_name (str): name of the parameter or buffer on which to register</span>
<span class="sd">            the parametrization</span>
<span class="sd">        parametrization (nn.Module): the parametrization to register</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: module</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the module does not have a parameter or a buffer named :attr:`tensor_name`</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.nn.utils.parametrize as P</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; class Symmetric(torch.nn.Module):</span>
<span class="sd">        &gt;&gt;&gt;     def forward(self, X):</span>
<span class="sd">        &gt;&gt;&gt;         return X.triu() + X.triu(1).T  # Return a symmetric matrix</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def right_inverse(self, A):</span>
<span class="sd">        &gt;&gt;&gt;         return A.triu()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; m = torch.nn.Linear(5, 5)</span>
<span class="sd">        &gt;&gt;&gt; P.register_parametrization(m, &quot;weight&quot;, Symmetric())</span>
<span class="sd">        &gt;&gt;&gt; print(torch.allclose(m.weight, m.weight.T))  # m.weight is now symmetric</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; A = torch.rand(5, 5)</span>
<span class="sd">        &gt;&gt;&gt; A = A + A.T   # A is now symmetric</span>
<span class="sd">        &gt;&gt;&gt; m.weight = A  # Initialize the weight to be the symmetric matrix A</span>
<span class="sd">        &gt;&gt;&gt; print(torch.allclose(m.weight, A))</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parametrization</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
        <span class="c1"># Just add the new parametrization to the parametrization list</span>
        <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parametrization</span><span class="p">)</span>  <span class="c1"># type: ignore[index, union-attr]</span>
    <span class="k">elif</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span> <span class="ow">or</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="c1"># Set the parametrization mechanism</span>
        <span class="c1"># Fetch the original buffer or parameter</span>
        <span class="n">original</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># Delete the previous parameter or buffer</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># If this is the first parametrization registered on the module,</span>
        <span class="c1"># we prepare the module to inject the property</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="c1"># Change the class</span>
            <span class="n">_inject_new_class</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="c1"># Inject the a ``ModuleDict`` into the instance under module.parametrizations</span>
            <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span> <span class="o">=</span> <span class="n">ModuleDict</span><span class="p">()</span>
        <span class="c1"># Add a property into the class</span>
        <span class="n">_inject_property</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># Add a ParametrizationList</span>
        <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">ParametrizationList</span><span class="p">(</span>  <span class="c1"># type: ignore[assignment, index, operator]</span>
            <span class="p">[</span><span class="n">parametrization</span><span class="p">],</span> <span class="n">original</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Module &#39;</span><span class="si">{}</span><span class="s2">&#39; does not have a parameter, a buffer, or a &quot;</span>
            <span class="s2">&quot;parametrized element with name &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span></div>


<div class="viewcode-block" id="is_parametrized"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized">[docs]</a><span class="k">def</span> <span class="nf">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns ``True`` if module has an active parametrization.</span>

<span class="sd">    If the argument :attr:`tensor_name` is specified, returns ``True`` if</span>
<span class="sd">    ``module[tensor_name]`` is parametrized.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module to query</span>
<span class="sd">        name (str, optional): attribute in the module to query</span>
<span class="sd">            Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parametrizations</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;parametrizations&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">parametrizations</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">tensor_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Check that there is at least one parametrized buffer or Parameter</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">parametrizations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">parametrizations</span></div>


<div class="viewcode-block" id="remove_parametrizations"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations">[docs]</a><span class="k">def</span> <span class="nf">remove_parametrizations</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">leave_parametrized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Removes the parametrizations on a tensor in a module.</span>

<span class="sd">    - If ``leave_parametrized=True``, ``module[tensor_name]`` will be set to</span>
<span class="sd">      its current output. In this case, the parametrization shall not change the ``dtype``</span>
<span class="sd">      of the tensor.</span>
<span class="sd">    - If ``leave_parametrized=False``, ``module[tensor_name]`` will be set to</span>
<span class="sd">      the unparametrised tensor in ``module.parametrizations[tensor_name].original``.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module from which remove the parametrization</span>
<span class="sd">        tensor_name (str): name of the parametrization to be removed</span>
<span class="sd">        leave_parametrized (bool, optional): leave the attribute :attr:`tensor_name` parametrized.</span>
<span class="sd">            Default: ``True``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: module</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if ``module[tensor_name]`` is not parametrized</span>
<span class="sd">        ValueError: if ``leave_parametrized=True`` and the parametrization changes the size or dtype</span>
<span class="sd">            of the tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Module </span><span class="si">{}</span><span class="s2"> does not have a parametrization on </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Fetch the original tensor</span>
    <span class="n">original</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">original</span>  <span class="c1"># type: ignore[index, union-attr]</span>
    <span class="k">if</span> <span class="n">leave_parametrized</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># If they have the same dtype, we reuse the original tensor.</span>
        <span class="c1"># We do this so that the parameter does not to change the id()</span>
        <span class="c1"># This way the user does not need to update the optimizer</span>
        <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">original</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">original</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The parametrization changes the dtype of the tensor from </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2">. &quot;</span>
                <span class="s2">&quot;It is not supported to leave the tensor parametrized (`leave_parametrized=True`) &quot;</span>
                <span class="s2">&quot;in this case.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="c1"># Delete the property that manages the parametrization</span>
    <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
    <span class="c1"># Delete the ParametrizationList</span>
    <span class="k">del</span> <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>  <span class="c1"># type: ignore[operator, union-attr]</span>

    <span class="c1"># Restore the parameter / buffer into the main class</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>

    <span class="c1"># Roll back the parametrized class if no other buffer or parameter</span>
    <span class="c1"># is currently parametrized in this class</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;parametrizations&quot;</span><span class="p">)</span>
        <span class="c1"># Restore class</span>
        <span class="n">orig_cls</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="n">orig_cls</span>
    <span class="k">return</span> <span class="n">module</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>